[
    {
        "doc_id": "B1x8anVFPr",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The authors should have performed multiple runs of their experiments and shown the standard deviations across runs to make their results more convincing.",
                "The authors claim that using Pre-LN rather than Post-LN transformers leads to faster convergence, but the models converge to the same result. This work is incremental as Wang et al. have already shown the benefits of Pre-LN rather than Post-LN transformers for machine translation.",
                "The paper is very long for an incremental improvement.",
                "It would be good to see repeats for Figure 2 and the standard deviations for Figure 3.",
                "Without reading the appendix it is not clear where the assumption that W^Q and W^K are zero is used. Making some connection with how this assumption relates to the lemmas would be useful.",
                "In Lemma two, the authors are comparing the magnitudes for the input in the Pre-LN and the output in the Post-LN transformer according to how x_{l,i}^post and x_{l, i}^pre are defined in Table 1.",
                "In Figure 3(b) the gradients are clearly decreasing with the number of layers, are there any comments on this? In the limit this could cause vanishing gradients?",
                "Figure 2 and 4 appear to contradict. Is the difference a result of using RAdam? If so, this should be made very clear. If not, why are the results contradictory?"
            ],
            [
                "The novelty of the paper is limited as Pre-LN placement has been used in prior work.",
                "There are concerns about the relation between the analytic investigation of the gradient norms and the empirical results that are presented.",
                "The theoretical results do not clearly demonstrate that there is a problem with Post-LN and that this problem is fixed by switching to Pre-LN.",
                "The paper does not clearly explain the relationship between the theoretical results and the empirical study.",
                "The paper does not adequately explain the scaling of the gradient changes throughout the layers, which is examined in appendix section F.",
                "The connection between the expression that scales in the upper bound and the actual gradient norm is tenuous and there are multiple places where the argument could break down.",
                "The paper does not verify the sqrt(2/3) scaling on the data from the empirical study that is shown.",
                "The paper does not explain where the expectation near the bottom of page 20, which is approximately 2/3, comes from."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper mentions the sensitivity of the validation accuracy to random seeds. This could potentially be seen as a shortcoming if not properly addressed. It would be beneficial if the authors could provide more details on how they handled this issue in their experiments. For example, they could conduct multiple runs with different random seeds and report the average and standard deviation of the results.",
                "The paper mentions a gap between the BLEU score and the validation loss when the model is not well-trained. This could potentially be seen as a limitation, depending on the context. It would be helpful if the authors could provide more explanation on this issue. For example, they could discuss why this gap occurs and how it affects the interpretation of the results."
            ],
            "novelty_impact": [
                "The paper provides a novel perspective on the Transformer architecture, focusing on the role of layer normalization and the learning rate warm-up stage. The authors' proposal to place layer normalization inside the residual blocks is innovative and could have significant implications for the training of Transformer models.",
                "The authors provide a clear motivation for their work, highlighting the challenges associated with training Transformer models and the importance of the learning rate warm-up stage. However, the goals of the paper could be more clearly defined.",
                "The key findings of the paper are well-presented and supported by empirical evidence. However, more detailed visualizations or quantitative analyses of the gradients at initialization and during training for both the original and modified Transformer models would strengthen the paper.",
                "The paper builds on existing literature on Transformer models and contributes to it by exploring the role of layer normalization and proposing a modified architecture. However, the authors could do a better job of situating their work within the broader literature and discussing how it expands on previous work.",
                "The authors' claim that their modified Transformer model does not require a learning rate warm-up stage is significant and well-justified by their experiments. However, additional experiments comparing the performance of the modified model with and without a warm-up stage under a variety of conditions would further strengthen this claim.",
                "The authors' failure to consider other optimizers besides Adam and SGD is a limitation of the study. Future work should explore how the modified model performs with other popular optimizers.",
                "The authors do not discuss any potential trade-offs or downsides of their approach. Future work should acknowledge and discuss these trade-offs to provide a more balanced view of the work.",
                "The authors' plan to further investigate strategies of positioning the layer normalization is promising. However, they should provide some preliminary insights or hypotheses to guide this research in future work.",
                "The inclusion of mathematical proofs of several lemmas related to the Transformer model is a strength of the paper. However, the authors should discuss in more detail how these lemmas specifically relate to their modified Transformer model and its performance.",
                "The authors' findings appear to be applicable to Transformer-based models in general, which is a significant contribution. However, future work should assess the performance of the modified model on a wider range of tasks and datasets to further assess its generalizability."
            ],
            "clarity_reproducibility": [
                "The paper could provide more detailed explanations of the Post-LN and Pre-LN Transformer architectures, specifically how the layer normalization is located inside the residual blocks in the Pre-LN Transformer. This would help readers better understand the differences between the two architectures and why the Pre-LN Transformer is easier and faster to train.",
                "The paper could clarify how exactly the learning rate warm-up stage was removed in the Pre-LN Transformer and what the alternative approach was. This would help readers understand the training process of the Pre-LN Transformer and why it does not require a learning rate warm-up stage.",
                "The paper could provide more information about the parameter initialization setting and theoretical findings related to the initialization of the Transformer. This would help readers understand the importance of the learning rate warm-up stage in training the Post-LN Transformer and how it is closely related to the position of the layer normalization.",
                "The paper could provide more details about the experiments conducted to study the gradients at initialization for the Post-LN/Pre-LN Transformer in real scenarios. This would help readers understand the reasons that the Post-LN Transformer needs a careful learning rate scheduling in the beginning."
            ],
            "all": [
                "The paper mentions the sensitivity of the validation accuracy to random seeds. This could potentially be seen as a shortcoming if not properly addressed. It would be beneficial if the authors could provide more details on how they handled this issue in their experiments. For example, they could conduct multiple runs with different random seeds and report the average and standard deviation of the results.",
                "The paper mentions a gap between the BLEU score and the validation loss when the model is not well-trained. This could potentially be seen as a limitation, depending on the context. It would be helpful if the authors could provide more explanation on this issue. For example, they could discuss why this gap occurs and how it affects the interpretation of the results.",
                "The paper provides a novel perspective on the Transformer architecture, focusing on the role of layer normalization and the learning rate warm-up stage. The authors' proposal to place layer normalization inside the residual blocks is innovative and could have significant implications for the training of Transformer models.",
                "The authors provide a clear motivation for their work, highlighting the challenges associated with training Transformer models and the importance of the learning rate warm-up stage. However, the goals of the paper could be more clearly defined.",
                "The key findings of the paper are well-presented and supported by empirical evidence. However, more detailed visualizations or quantitative analyses of the gradients at initialization and during training for both the original and modified Transformer models would strengthen the paper.",
                "The paper builds on existing literature on Transformer models and contributes to it by exploring the role of layer normalization and proposing a modified architecture. However, the authors could do a better job of situating their work within the broader literature and discussing how it expands on previous work.",
                "The authors' claim that their modified Transformer model does not require a learning rate warm-up stage is significant and well-justified by their experiments. However, additional experiments comparing the performance of the modified model with and without a warm-up stage under a variety of conditions would further strengthen this claim.",
                "The authors' failure to consider other optimizers besides Adam and SGD is a limitation of the study. Future work should explore how the modified model performs with other popular optimizers.",
                "The authors do not discuss any potential trade-offs or downsides of their approach. Future work should acknowledge and discuss these trade-offs to provide a more balanced view of the work.",
                "The authors' plan to further investigate strategies of positioning the layer normalization is promising. However, they should provide some preliminary insights or hypotheses to guide this research in future work.",
                "The inclusion of mathematical proofs of several lemmas related to the Transformer model is a strength of the paper. However, the authors should discuss in more detail how these lemmas specifically relate to their modified Transformer model and its performance.",
                "The authors' findings appear to be applicable to Transformer-based models in general, which is a significant contribution. However, future work should assess the performance of the modified model on a wider range of tasks and datasets to further assess its generalizability.",
                "The paper could provide more detailed explanations of the Post-LN and Pre-LN Transformer architectures, specifically how the layer normalization is located inside the residual blocks in the Pre-LN Transformer. This would help readers better understand the differences between the two architectures and why the Pre-LN Transformer is easier and faster to train.",
                "The paper could clarify how exactly the learning rate warm-up stage was removed in the Pre-LN Transformer and what the alternative approach was. This would help readers understand the training process of the Pre-LN Transformer and why it does not require a learning rate warm-up stage.",
                "The paper could provide more information about the parameter initialization setting and theoretical findings related to the initialization of the Transformer. This would help readers understand the importance of the learning rate warm-up stage in training the Post-LN Transformer and how it is closely related to the position of the layer normalization.",
                "The paper could provide more details about the experiments conducted to study the gradients at initialization for the Post-LN/Pre-LN Transformer in real scenarios. This would help readers understand the reasons that the Post-LN Transformer needs a careful learning rate scheduling in the beginning."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The authors should have performed multiple runs of their experiments and shown the standard deviations across runs to make their results more convincing.",
                    "predicted_comment": "The paper mentions the sensitivity of the validation accuracy to random seeds. This could potentially be seen as a shortcoming if not properly addressed. It would be beneficial if the authors could provide more details on how they handled this issue in their experiments. For example, they could conduct multiple runs with different random seeds and report the average and standard deviation of the results."
                }
            ],
            "novelty_impact": [],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The authors should have performed multiple runs of their experiments and shown the standard deviations across runs to make their results more convincing.",
                    "predicted_comment": "The paper mentions the sensitivity of the validation accuracy to random seeds. This could potentially be seen as a shortcoming if not properly addressed. It would be beneficial if the authors could provide more details on how they handled this issue in their experiments. For example, they could conduct multiple runs with different random seeds and report the average and standard deviation of the results."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 16,
                "n_generated": 2,
                "n_total_aligns": 1,
                "recall": 0.0625,
                "precision": 0.25,
                "pseudo_jaccard": 0.05555555555555555,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 16,
                "n_generated": 10,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "clarity_reproducibility": {
                "n_reference": 16,
                "n_generated": 4,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 16,
                "n_generated": 16,
                "n_total_aligns": 1,
                "recall": 0.0625,
                "precision": 0.03125,
                "pseudo_jaccard": 0.021739130434782608,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.125,
                    "precision": 0.5,
                    "pseudo_jaccard": 0.1111111111111111,
                    "n_real": 8
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                }
            ],
            "all": [
                {
                    "recall": 0.125,
                    "precision": 0.0625,
                    "pseudo_jaccard": 0.043478260869565216,
                    "n_real": 8
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                }
            ]
        }
    },
    {
        "doc_id": "o2UwRc8fbXI",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper does not clearly state the main claims. The proposed method does not demonstrate a speed advantage or a clear performance boost.",
                "The reason for coupling the proposed method with CapsGCN is unclear, especially if the RL module is intended to tune the depth and width of convolutional layers.",
                "The paper does not clearly differentiate the proposed method from Neural Architecture Search (NAS).",
                "The motivations and experimental results presented in the paper are weak."
            ],
            [
                "The use of Reinforcement Learning (RL) during the training process to search for and modify the neural structures may create stability issues. The paper should provide some theoretical justifications or 'proofs' of convergence, or at the minimum, provide some empirical analysis on the stability of the training process.",
                "The evaluation of the proposed Adaptive Graph Capsule Convolutional Networks (AdaGCCN) could be more thorough. The paper should include ablation studies to show how effective the RL is in adapting the network structure, and discuss the runtime complexity, among other things."
            ],
            [
                "The motivations for the paper are not well discussed.",
                "The experimental methodology used in the paper is not well detailed, making it difficult to reproduce the reported results.",
                "The authors' claims are not always supported by the experimental evidence.",
                "The authors do not provide information about how the models considered during the experiments were validated. The policy used to validate the hyperparameters of a model highly impacts the model's performance.",
                "The authors do not report the standard deviation of the accuracies in table 5. This is crucial to correctly assess the model performance.",
                "The results in Table 5 suggest that using the AM to optimize the D and W does not justify the increase of complexity due to the use of this external model.",
                "The results reported in Table 5 are outperformed by results published in the literature for Enzymes datasets and in NCI.",
                "The performance of the model in terms of memory consumption is not completely discussed. Using AM seems to significantly impact the memory consumption and/or the training time of the model.",
                "The authors state that adjusting the model structure through an RL-based searching process improves the explainability of the model. This point should be further discussed with empirical results."
            ],
            [
                "The capsule graph neural network section (2.2) is somewhat shallow and could benefit from a more detailed introduction to the topics discussed to make the paper more self-contained.",
                "The paper lacks ablation experiments on the epoch sliding window size and choice of search space for D and W. It's also unclear how the method would perform without the capsule components, i.e. is the assistant module strategy practically useful for other architectures.",
                "There are concerns about reporting test accuracy throughout training in figure 3, as models could be overfit to the test set, rendering the results potentially incomparable to previous works.",
                "No standard deviations are reported in the results in table 2, and generally only marginal improvements are observed despite the increase in runtime complexity of the proposed method.",
                "The paper lacks meaningful runtime comparisons between the proposed method and the various previous works without the assistant module. For example, a comparison to a parallel cross validation version of CapsGNN in figure 4 is suggested.",
                "In the discussion, the authors claim the proposed approach improves the explainability of the model, but it's not clear how that is the case. The authors should either elaborate on this or exclude it from the paper.",
                "The purpose of using dynamic routing in capsule networks is to adaptively modify the network connectivity based on the input. Therefore, it might be possible to reduce the search space by only considering depth D and not width W, since dynamic routing already takes care of 'pruning' W. The authors should address this point."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper has done a commendable job in conducting a comprehensive set of experiments to validate the proposed AdaGCCN model. The comparison with several baseline models and the use of multiple datasets provide a robust evaluation of the model's performance.",
                "However, the paper acknowledges some limitations, such as the computation cost brought by the assistant module and the manual setting of the \u2206D list and the \u2206W list in the current AM, which could restrict the extraction ability of AdaGCCN. It would be beneficial if the authors could address these limitations in more detail and suggest potential solutions or future work to overcome these issues.",
                "The paper could also benefit from a more detailed discussion on the choice of the \u2206D list and the \u2206W list in the current AM. Providing more insights into how these lists were determined and how they impact the model's performance could strengthen the paper.",
                "While the paper discusses the use of parallel processing to alleviate computation overhead, it would be helpful if the authors could provide more details on the scheduling strategy used to assign an appropriate number of workers. Specifically, how does the strategy determine the optimal number of workers, and how does it impact the overall efficiency of the model?",
                "Lastly, the paper claims that AdaGCCN achieves state-of-the-art results on almost all datasets. It would be beneficial if the authors could provide a more detailed comparison with the state-of-the-art models, discussing the specific areas where AdaGCCN outperforms these models and where it falls short. This would provide a more balanced view of the model's performance and its potential areas for improvement."
            ],
            "novelty_impact": [
                "The choice of CapsGNN as the base model could be better justified with a comparative analysis against other GNN models. This would help validate the choice and strengthen the argument for the novelty and significance of AdaGCCN.",
                "The assumption that the RL process in the assistant module will always find the optimal solution could be a potential pitfall. Discussing the limitations of the RL process and how they plan to address them would improve the paper.",
                "The observations about the impacts of the model's depth and width on its performance are interesting, but more empirical evidence or theoretical analysis would strengthen these claims.",
                "The assumption about the availability of computational resources for the proposed parallel computation strategy could limit the applicability of the solution. Discussing alternative strategies for systems that may not support parallel computation would improve the paper.",
                "A detailed cost-benefit analysis would be beneficial to justify the assumption that the benefits of AdaGCCN outweigh its computational costs. This would provide a more comprehensive understanding of the trade-offs involved and strengthen the paper's significance.",
                "The proposed optimizations sound promising, but without empirical evidence or theoretical analysis, their feasibility or effectiveness remains uncertain. Providing more support for these proposed optimizations would improve the paper.",
                "The paper could benefit from a more thorough justification of its assumptions and proposed solutions, as well as a more comprehensive analysis of the trade-offs involved. This would improve the clarity and significance of the paper."
            ],
            "clarity_reproducibility": [
                "The paper should provide specific theoretical or empirical evidence to support the claim that scalar-valued neurons in GCNs can lead to latent information loss.",
                "The paper should provide a detailed formula for calculating the Q-value in the RL mechanism of AdaGCCN. This is a crucial part of the RL mechanism and its omission could make it difficult for readers to fully understand the model.",
                "The paper should provide detailed information about the ten typical benchmark datasets used in the experiments, such as the number of samples, the number of features, or the distribution of classes. This information is crucial for understanding the experimental setup and for reproducing the experiments.",
                "The paper should provide specific applications and results for AdaGCCN in bioinformatics and social fields. This information is important for understanding the practical implications and effectiveness of AdaGCCN.",
                "The paper should provide information about the reproducibility of the experiments, such as the availability of the code, data, or other resources used in the experiments.",
                "The paper should provide a direct comparison of the computational cost of AdaGCCN with other models like GCNs or CapsGNN. This information is important for understanding the relative efficiency of AdaGCCN.",
                "The paper should explicitly list the limitations of AdaGCCN. This information is important for understanding the current state of AdaGCCN and for guiding future research."
            ],
            "all": [
                "The paper has done a commendable job in conducting a comprehensive set of experiments to validate the proposed AdaGCCN model. The comparison with several baseline models and the use of multiple datasets provide a robust evaluation of the model's performance.",
                "However, the paper acknowledges some limitations, such as the computation cost brought by the assistant module and the manual setting of the \u2206D list and the \u2206W list in the current AM, which could restrict the extraction ability of AdaGCCN. It would be beneficial if the authors could address these limitations in more detail and suggest potential solutions or future work to overcome these issues.",
                "The paper could also benefit from a more detailed discussion on the choice of the \u2206D list and the \u2206W list in the current AM. Providing more insights into how these lists were determined and how they impact the model's performance could strengthen the paper.",
                "While the paper discusses the use of parallel processing to alleviate computation overhead, it would be helpful if the authors could provide more details on the scheduling strategy used to assign an appropriate number of workers. Specifically, how does the strategy determine the optimal number of workers, and how does it impact the overall efficiency of the model?",
                "Lastly, the paper claims that AdaGCCN achieves state-of-the-art results on almost all datasets. It would be beneficial if the authors could provide a more detailed comparison with the state-of-the-art models, discussing the specific areas where AdaGCCN outperforms these models and where it falls short. This would provide a more balanced view of the model's performance and its potential areas for improvement.",
                "The choice of CapsGNN as the base model could be better justified with a comparative analysis against other GNN models. This would help validate the choice and strengthen the argument for the novelty and significance of AdaGCCN.",
                "The assumption that the RL process in the assistant module will always find the optimal solution could be a potential pitfall. Discussing the limitations of the RL process and how they plan to address them would improve the paper.",
                "The observations about the impacts of the model's depth and width on its performance are interesting, but more empirical evidence or theoretical analysis would strengthen these claims.",
                "The assumption about the availability of computational resources for the proposed parallel computation strategy could limit the applicability of the solution. Discussing alternative strategies for systems that may not support parallel computation would improve the paper.",
                "A detailed cost-benefit analysis would be beneficial to justify the assumption that the benefits of AdaGCCN outweigh its computational costs. This would provide a more comprehensive understanding of the trade-offs involved and strengthen the paper's significance.",
                "The proposed optimizations sound promising, but without empirical evidence or theoretical analysis, their feasibility or effectiveness remains uncertain. Providing more support for these proposed optimizations would improve the paper.",
                "The paper could benefit from a more thorough justification of its assumptions and proposed solutions, as well as a more comprehensive analysis of the trade-offs involved. This would improve the clarity and significance of the paper.",
                "The paper should provide specific theoretical or empirical evidence to support the claim that scalar-valued neurons in GCNs can lead to latent information loss.",
                "The paper should provide a detailed formula for calculating the Q-value in the RL mechanism of AdaGCCN. This is a crucial part of the RL mechanism and its omission could make it difficult for readers to fully understand the model.",
                "The paper should provide detailed information about the ten typical benchmark datasets used in the experiments, such as the number of samples, the number of features, or the distribution of classes. This information is crucial for understanding the experimental setup and for reproducing the experiments.",
                "The paper should provide specific applications and results for AdaGCCN in bioinformatics and social fields. This information is important for understanding the practical implications and effectiveness of AdaGCCN.",
                "The paper should provide information about the reproducibility of the experiments, such as the availability of the code, data, or other resources used in the experiments.",
                "The paper should provide a direct comparison of the computational cost of AdaGCCN with other models like GCNs or CapsGNN. This information is important for understanding the relative efficiency of AdaGCCN.",
                "The paper should explicitly list the limitations of AdaGCCN. This information is important for understanding the current state of AdaGCCN and for guiding future research."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [
                {
                    "reference_id": 2,
                    "predicted_id": 10,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The authors' claims are not always supported by the experimental evidence.",
                    "predicted_comment": "The proposed optimizations sound promising, but without empirical evidence or theoretical analysis, their feasibility or effectiveness remains uncertain. Providing more support for these proposed optimizations would improve the paper."
                },
                {
                    "reference_id": 5,
                    "predicted_id": 9,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The results in Table 5 suggest that using the AM to optimize the D and W does not justify the increase of complexity due to the use of this external model.",
                    "predicted_comment": "A detailed cost-benefit analysis would be beneficial to justify the assumption that the benefits of AdaGCCN outweigh its computational costs. This would provide a more comprehensive understanding of the trade-offs involved and strengthen the paper's significance."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 11,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The motivations for the paper are not well discussed.",
                    "predicted_comment": "The paper could benefit from a more thorough justification of its assumptions and proposed solutions, as well as a more comprehensive analysis of the trade-offs involved. This would improve the clarity and significance of the paper."
                },
                {
                    "reference_id": 3,
                    "predicted_id": 9,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "No standard deviations are reported in the results in table 2, and generally only marginal improvements are observed despite the increase in runtime complexity of the proposed method.",
                    "predicted_comment": "A detailed cost-benefit analysis would be beneficial to justify the assumption that the benefits of AdaGCCN outweigh its computational costs. This would provide a more comprehensive understanding of the trade-offs involved and strengthen the paper's significance."
                }
            ],
            "clarity_reproducibility": [
                {
                    "reference_id": 1,
                    "predicted_id": 16,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The experimental methodology used in the paper is not well detailed, making it difficult to reproduce the reported results.",
                    "predicted_comment": "The paper should provide information about the reproducibility of the experiments, such as the availability of the code, data, or other resources used in the experiments."
                },
                {
                    "reference_id": 7,
                    "predicted_id": 17,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The performance of the model in terms of memory consumption is not completely discussed. Using AM seems to significantly impact the memory consumption and/or the training time of the model.",
                    "predicted_comment": "The paper should provide a direct comparison of the computational cost of AdaGCCN with other models like GCNs or CapsGNN. This information is important for understanding the relative efficiency of AdaGCCN."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 14,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The experimental methodology used in the paper is not well detailed, making it difficult to reproduce the reported results.",
                    "predicted_comment": "The paper should provide detailed information about the ten typical benchmark datasets used in the experiments, such as the number of samples, the number of features, or the distribution of classes. This information is crucial for understanding the experimental setup and for reproducing the experiments."
                }
            ],
            "all": [
                {
                    "reference_id": 2,
                    "predicted_id": 10,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The authors' claims are not always supported by the experimental evidence.",
                    "predicted_comment": "The proposed optimizations sound promising, but without empirical evidence or theoretical analysis, their feasibility or effectiveness remains uncertain. Providing more support for these proposed optimizations would improve the paper."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 16,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The experimental methodology used in the paper is not well detailed, making it difficult to reproduce the reported results.",
                    "predicted_comment": "The paper should provide information about the reproducibility of the experiments, such as the availability of the code, data, or other resources used in the experiments."
                },
                {
                    "reference_id": 5,
                    "predicted_id": 9,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The results in Table 5 suggest that using the AM to optimize the D and W does not justify the increase of complexity due to the use of this external model.",
                    "predicted_comment": "A detailed cost-benefit analysis would be beneficial to justify the assumption that the benefits of AdaGCCN outweigh its computational costs. This would provide a more comprehensive understanding of the trade-offs involved and strengthen the paper's significance."
                },
                {
                    "reference_id": 7,
                    "predicted_id": 17,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The performance of the model in terms of memory consumption is not completely discussed. Using AM seems to significantly impact the memory consumption and/or the training time of the model.",
                    "predicted_comment": "The paper should provide a direct comparison of the computational cost of AdaGCCN with other models like GCNs or CapsGNN. This information is important for understanding the relative efficiency of AdaGCCN."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 11,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The motivations for the paper are not well discussed.",
                    "predicted_comment": "The paper could benefit from a more thorough justification of its assumptions and proposed solutions, as well as a more comprehensive analysis of the trade-offs involved. This would improve the clarity and significance of the paper."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 14,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The experimental methodology used in the paper is not well detailed, making it difficult to reproduce the reported results.",
                    "predicted_comment": "The paper should provide detailed information about the ten typical benchmark datasets used in the experiments, such as the number of samples, the number of features, or the distribution of classes. This information is crucial for understanding the experimental setup and for reproducing the experiments."
                },
                {
                    "reference_id": 3,
                    "predicted_id": 9,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "No standard deviations are reported in the results in table 2, and generally only marginal improvements are observed despite the increase in runtime complexity of the proposed method.",
                    "predicted_comment": "A detailed cost-benefit analysis would be beneficial to justify the assumption that the benefits of AdaGCCN outweigh its computational costs. This would provide a more comprehensive understanding of the trade-offs involved and strengthen the paper's significance."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 22,
                "n_generated": 5,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 22,
                "n_generated": 7,
                "n_total_aligns": 4,
                "recall": 0.11904761904761904,
                "precision": 0.14285714285714285,
                "pseudo_jaccard": 0.07692307692307693,
                "specificity_more": 0.75,
                "specificity_less": 0.0,
                "specificity_same": 0.25
            },
            "clarity_reproducibility": {
                "n_reference": 22,
                "n_generated": 7,
                "n_total_aligns": 3,
                "recall": 0.05555555555555555,
                "precision": 0.10714285714285714,
                "pseudo_jaccard": 0.046296296296296294,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "all": {
                "n_reference": 22,
                "n_generated": 19,
                "n_total_aligns": 7,
                "recall": 0.1746031746031746,
                "precision": 0.09210526315789473,
                "pseudo_jaccard": 0.07111111111111111,
                "specificity_more": 0.8571428571428571,
                "specificity_less": 0.0,
                "specificity_same": 0.14285714285714285
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 9
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.42857142857142855,
                    "pseudo_jaccard": 0.23076923076923078,
                    "n_real": 9
                },
                {
                    "recall": 0.14285714285714285,
                    "precision": 0.14285714285714285,
                    "pseudo_jaccard": 0.07692307692307693,
                    "n_real": 7
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.2222222222222222,
                    "precision": 0.42857142857142855,
                    "pseudo_jaccard": 0.18518518518518517,
                    "n_real": 9
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.5555555555555556,
                    "precision": 0.3157894736842105,
                    "pseudo_jaccard": 0.24444444444444444,
                    "n_real": 9
                },
                {
                    "recall": 0.14285714285714285,
                    "precision": 0.05263157894736842,
                    "pseudo_jaccard": 0.04,
                    "n_real": 7
                }
            ]
        }
    },
    {
        "doc_id": "tJCwZBHm-jW",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The novelty of the paper is limited as the idea of transforming a 2D image pre-train backbone into a 3D version has already been used in other 2D-3D domains. The paper needs to place extra emphasis on discussing why it works and conducting ablation studies.",
                "The discussion section provides some experiments, but the results seem quite random (figure 4). The explanation for why 'shape representation' are better transferred is not convincing.",
                "The paper does not discuss if there is a difference on which axis to inflate (x, y, or z) when designing inflation. This is different to video since 3 axis is symmetric.",
                "The paper does not compare the parameter size in section 4.1. This raises concerns about the fairness of the comparison with the baseline, as the performance gain could be solely due to the increase in parameters.",
                "In section 4.2, there is a possibility that the ResNet structure is not a good structure to train on point cloud, making it quite bad on scratch training. The training method for pointnet++ is also questionable."
            ],
            [
                "The paper's visualizations do not adequately explain why the transfer works or what information can be transferred.",
                "The conclusion on page 8 that 'shape representations are better transferred from image to point-cloud' cannot be inferred from overall dataset performances. The term 'shape representations' is also not clearly defined."
            ],
            [
                "The paper lacks comparisons between results from datasets of different sizes in Table 1. It is unclear whether a larger dataset would lead to better performance.",
                "The paper does not include results of state-of-the-art task specific methods for comparison. It would be beneficial to compare with different self-supervised learning methods, such as Foldingnet and Pointcontrast.",
                "The paper does not provide a theoretical analysis to explain why inflating 2D filters to 3D is reasonable, given the significant domain gap between 2D and 3D datasets."
            ],
            [
                "The idea of inflating model weight from 2D to 3D is not new, as shown in the work of Carreira and Zisserman.",
                "The effectiveness of pretrained weight would be more convincing if the author could do further ablation study to initialize new model with expanding subsets of layers and exploring the effect on model performance. This would indicate which part of the pretrained model is really useful in this transferred learning setting."
            ],
            [
                "The paper is missing detailed information, making it difficult to follow the process. For example, in Table 1, the performances of pointnet++ and the performances are listed, but it's unclear how the inflation in point net++ is performed.",
                "The authors only perform experiments on classification and segmentation tasks. The performance on detection tasks should also be evaluated.",
                "The paper does not clearly address why the proposed method of inflating 2D CNN of image for 3D pointcloud works."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper does not provide a clear explanation of how the pretrained-image models are adapted for point-cloud recognition. The process of \"inflating\" a 2D pretrained ConvNet and minimal finetuning is mentioned, but the specifics of this process are not detailed. It would be beneficial for the authors to provide a more detailed explanation of this process, including any specific techniques or parameters used.",
                "The paper mentions that the pretrained-image models can achieve very competitive performance on 3D point-cloud classification, even beating a wide range of point-cloud models that adopt a variety of tricks. However, it does not provide a clear comparison or benchmarking against these other models. The authors should provide a more detailed comparison, including specific models used for comparison, the metrics used for comparison, and the results of these comparisons.",
                "The paper does not provide a clear explanation of how the performance improvements were measured. It mentions top-1 accuracy, mIoU, and other metrics, but does not provide a clear explanation of how these metrics were calculated or what they represent. The authors should provide a more detailed explanation of these metrics, including how they were calculated and why they were chosen.",
                "The paper does not provide a clear explanation of how the data for the experiments was collected or prepared. It mentions several datasets, but does not provide details about how these datasets were used in the experiments. The authors should provide more details about the data collection and preparation process, including any specific techniques or parameters used.",
                "While the paper does a good job of conducting a comprehensive set of experiments, it would be beneficial to conduct ablation studies to understand the contribution of different components of their method. This would help to understand why their method works and how it could be improved.",
                "The experiments should test the models on a variety of datasets or tasks to demonstrate the generalizability of the method. This would provide a stronger evidence of the method's effectiveness in different scenarios."
            ],
            "novelty_impact": [
                "The choice of ResNet as the base architecture should be justified. It would be beneficial to discuss why ResNet was chosen and whether other architectures might also be suitable for this task.",
                "The use of the same pretrained weights for both 2D images and 3D point-clouds needs a stronger theoretical foundation. While the experimental results are promising, a theoretical reasoning or prior evidence to support this approach would strengthen the paper.",
                "The comparison of the proposed method with other models should include the latest state-of-the-art models. This would help demonstrate the competitiveness of the proposed method.",
                "The experiments conducted to understand why image pretraining can be utilized for point-cloud understanding should be explained in more detail. Providing more information about the design of these experiments and their specific findings would be beneficial.",
                "Comparative results with other models on the ModelNet 3D Wharehouse dataset should be provided. This would help demonstrate the competitiveness of the proposed method.",
                "A discussion about the trade-off between the improvements in data efficiency and training speed and the additional complexity of the method should be included. This would provide a more balanced view of the proposed method."
            ],
            "clarity_reproducibility": [
                "The paper should specify the value of K in the process of inflating 2D convolutional filters to 3D. This detail is crucial for reproducing the method and understanding its implications.",
                "The paper should provide specific steps and hyperparameters used in the finetuning process for the three settings: FIP-IO, FIP-IO+BN, and FIP-ALL. This information is necessary for others to reproduce the results and understand the differences between these settings.",
                "The paper should provide specific details about the size of the few-shot learning task. This detail is important for understanding the exact experimental setup and the context of the reported results.",
                "The paper should provide specific methods and parameters used for the preprocessing steps for each dataset. This information is necessary for others to reproduce the results and understand the impact of these steps on the results.",
                "The paper should provide specific details on any setup for parallel processing or distributed training. This information is necessary for others to reproduce the results and understand the computational requirements of the method.",
                "The paper should provide specific sources for the pretrained models. This information is necessary for others to reproduce the results and understand the starting point of the method.",
                "The paper should provide details on what the SSG version entails for the PointNet++ pretraining on ImageNet1K. This detail is important for understanding the exact method used and its implications.",
                "The paper should provide specific steps or modifications made to incorporate the pretrained weights from Dosovitskiy et al. (2020) into the ViT models. This information is necessary for others to reproduce the results and understand the modifications made to the original models."
            ],
            "all": [
                "The paper does not provide a clear explanation of how the pretrained-image models are adapted for point-cloud recognition. The process of \"inflating\" a 2D pretrained ConvNet and minimal finetuning is mentioned, but the specifics of this process are not detailed. It would be beneficial for the authors to provide a more detailed explanation of this process, including any specific techniques or parameters used.",
                "The paper mentions that the pretrained-image models can achieve very competitive performance on 3D point-cloud classification, even beating a wide range of point-cloud models that adopt a variety of tricks. However, it does not provide a clear comparison or benchmarking against these other models. The authors should provide a more detailed comparison, including specific models used for comparison, the metrics used for comparison, and the results of these comparisons.",
                "The paper does not provide a clear explanation of how the performance improvements were measured. It mentions top-1 accuracy, mIoU, and other metrics, but does not provide a clear explanation of how these metrics were calculated or what they represent. The authors should provide a more detailed explanation of these metrics, including how they were calculated and why they were chosen.",
                "The paper does not provide a clear explanation of how the data for the experiments was collected or prepared. It mentions several datasets, but does not provide details about how these datasets were used in the experiments. The authors should provide more details about the data collection and preparation process, including any specific techniques or parameters used.",
                "While the paper does a good job of conducting a comprehensive set of experiments, it would be beneficial to conduct ablation studies to understand the contribution of different components of their method. This would help to understand why their method works and how it could be improved.",
                "The experiments should test the models on a variety of datasets or tasks to demonstrate the generalizability of the method. This would provide a stronger evidence of the method's effectiveness in different scenarios.",
                "The choice of ResNet as the base architecture should be justified. It would be beneficial to discuss why ResNet was chosen and whether other architectures might also be suitable for this task.",
                "The use of the same pretrained weights for both 2D images and 3D point-clouds needs a stronger theoretical foundation. While the experimental results are promising, a theoretical reasoning or prior evidence to support this approach would strengthen the paper.",
                "The comparison of the proposed method with other models should include the latest state-of-the-art models. This would help demonstrate the competitiveness of the proposed method.",
                "The experiments conducted to understand why image pretraining can be utilized for point-cloud understanding should be explained in more detail. Providing more information about the design of these experiments and their specific findings would be beneficial.",
                "Comparative results with other models on the ModelNet 3D Wharehouse dataset should be provided. This would help demonstrate the competitiveness of the proposed method.",
                "A discussion about the trade-off between the improvements in data efficiency and training speed and the additional complexity of the method should be included. This would provide a more balanced view of the proposed method.",
                "The paper should specify the value of K in the process of inflating 2D convolutional filters to 3D. This detail is crucial for reproducing the method and understanding its implications.",
                "The paper should provide specific steps and hyperparameters used in the finetuning process for the three settings: FIP-IO, FIP-IO+BN, and FIP-ALL. This information is necessary for others to reproduce the results and understand the differences between these settings.",
                "The paper should provide specific details about the size of the few-shot learning task. This detail is important for understanding the exact experimental setup and the context of the reported results.",
                "The paper should provide specific methods and parameters used for the preprocessing steps for each dataset. This information is necessary for others to reproduce the results and understand the impact of these steps on the results.",
                "The paper should provide specific details on any setup for parallel processing or distributed training. This information is necessary for others to reproduce the results and understand the computational requirements of the method.",
                "The paper should provide specific sources for the pretrained models. This information is necessary for others to reproduce the results and understand the starting point of the method.",
                "The paper should provide details on what the SSG version entails for the PointNet++ pretraining on ImageNet1K. This detail is important for understanding the exact method used and its implications.",
                "The paper should provide specific steps or modifications made to incorporate the pretrained weights from Dosovitskiy et al. (2020) into the ViT models. This information is necessary for others to reproduce the results and understand the modifications made to the original models."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 0,
                    "predicted_id": 4,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The novelty of the paper is limited as the idea of transforming a 2D image pre-train backbone into a 3D version has already been used in other 2D-3D domains. The paper needs to place extra emphasis on discussing why it works and conducting ablation studies.",
                    "predicted_comment": "While the paper does a good job of conducting a comprehensive set of experiments, it would be beneficial to conduct ablation studies to understand the contribution of different components of their method. This would help to understand why their method works and how it could be improved."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper is missing detailed information, making it difficult to follow the process. For example, in Table 1, the performances of pointnet++ and the performances are listed, but it's unclear how the inflation in point net++ is performed.",
                    "predicted_comment": "The paper does not provide a clear explanation of how the pretrained-image models are adapted for point-cloud recognition. The process of \"inflating\" a 2D pretrained ConvNet and minimal finetuning is mentioned, but the specifics of this process are not detailed. It would be beneficial for the authors to provide a more detailed explanation of this process, including any specific techniques or parameters used."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper does not clearly address why the proposed method of inflating 2D CNN of image for 3D pointcloud works.",
                    "predicted_comment": "The paper does not provide a clear explanation of how the pretrained-image models are adapted for point-cloud recognition. The process of \"inflating\" a 2D pretrained ConvNet and minimal finetuning is mentioned, but the specifics of this process are not detailed. It would be beneficial for the authors to provide a more detailed explanation of this process, including any specific techniques or parameters used."
                }
            ],
            "novelty_impact": [
                {
                    "reference_id": 1,
                    "predicted_id": 9,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The discussion section provides some experiments, but the results seem quite random (figure 4). The explanation for why 'shape representation' are better transferred is not convincing.",
                    "predicted_comment": "The experiments conducted to understand why image pretraining can be utilized for point-cloud understanding should be explained in more detail. Providing more information about the design of these experiments and their specific findings would be beneficial."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 7,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper does not clearly address why the proposed method of inflating 2D CNN of image for 3D pointcloud works.",
                    "predicted_comment": "The use of the same pretrained weights for both 2D images and 3D point-clouds needs a stronger theoretical foundation. While the experimental results are promising, a theoretical reasoning or prior evidence to support this approach would strengthen the paper."
                }
            ],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 0,
                    "predicted_id": 4,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The novelty of the paper is limited as the idea of transforming a 2D image pre-train backbone into a 3D version has already been used in other 2D-3D domains. The paper needs to place extra emphasis on discussing why it works and conducting ablation studies.",
                    "predicted_comment": "While the paper does a good job of conducting a comprehensive set of experiments, it would be beneficial to conduct ablation studies to understand the contribution of different components of their method. This would help to understand why their method works and how it could be improved."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 9,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The discussion section provides some experiments, but the results seem quite random (figure 4). The explanation for why 'shape representation' are better transferred is not convincing.",
                    "predicted_comment": "The experiments conducted to understand why image pretraining can be utilized for point-cloud understanding should be explained in more detail. Providing more information about the design of these experiments and their specific findings would be beneficial."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 7,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper does not clearly address why the proposed method of inflating 2D CNN of image for 3D pointcloud works.",
                    "predicted_comment": "The use of the same pretrained weights for both 2D images and 3D point-clouds needs a stronger theoretical foundation. While the experimental results are promising, a theoretical reasoning or prior evidence to support this approach would strengthen the paper."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper is missing detailed information, making it difficult to follow the process. For example, in Table 1, the performances of pointnet++ and the performances are listed, but it's unclear how the inflation in point net++ is performed.",
                    "predicted_comment": "The paper does not provide a clear explanation of how the pretrained-image models are adapted for point-cloud recognition. The process of \"inflating\" a 2D pretrained ConvNet and minimal finetuning is mentioned, but the specifics of this process are not detailed. It would be beneficial for the authors to provide a more detailed explanation of this process, including any specific techniques or parameters used."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper does not clearly address why the proposed method of inflating 2D CNN of image for 3D pointcloud works.",
                    "predicted_comment": "The paper does not provide a clear explanation of how the pretrained-image models are adapted for point-cloud recognition. The process of \"inflating\" a 2D pretrained ConvNet and minimal finetuning is mentioned, but the specifics of this process are not detailed. It would be beneficial for the authors to provide a more detailed explanation of this process, including any specific techniques or parameters used."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 15,
                "n_generated": 6,
                "n_total_aligns": 3,
                "recall": 0.17333333333333334,
                "precision": 0.06666666666666667,
                "pseudo_jaccard": 0.06000000000000001,
                "specificity_more": 0.6666666666666666,
                "specificity_less": 0.0,
                "specificity_same": 0.3333333333333333
            },
            "novelty_impact": {
                "n_reference": 15,
                "n_generated": 6,
                "n_total_aligns": 2,
                "recall": 0.10666666666666666,
                "precision": 0.06666666666666667,
                "pseudo_jaccard": 0.045,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 15,
                "n_generated": 8,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 15,
                "n_generated": 20,
                "n_total_aligns": 5,
                "recall": 0.21333333333333332,
                "precision": 0.04,
                "pseudo_jaccard": 0.03643892339544513,
                "specificity_more": 0.8,
                "specificity_less": 0.0,
                "specificity_same": 0.2
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.2,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.1,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.6666666666666666,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.2,
                    "n_real": 3
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.2,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.1,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.125,
                    "n_real": 3
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "all": [
                {
                    "recall": 0.4,
                    "precision": 0.1,
                    "pseudo_jaccard": 0.08695652173913043,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.6666666666666666,
                    "precision": 0.1,
                    "pseudo_jaccard": 0.09523809523809523,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "H1enKkrFDB",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The role of the partitioning index in the proposed algorithm is unclear, as it does not appear to be used in the experiments. The authors should clarify its importance and usage.",
                "The empirical evaluation of the normalization technique is insufficient to demonstrate its practical usefulness. The authors should focus more on showing how their regularization can improve generalization in practice.",
                "The authors should provide experiments on at least one additional dataset, such as CIFAR10 or ImageNet, to strengthen their empirical results.",
                "The reported accuracies for ResNet and DenseNet on the CIFAR100 dataset are significantly lower than what is reported in the literature. This issue needs to be resolved.",
                "The current results are based on training with a fixed number of epochs. Instead, the authors should train with a stopping criterion based on the cross-entropy loss on the training set and use the same stopping criterion for all models. They should also add plots showing training and test errors based on the number of epochs."
            ],
            [
                "The paper should characterize the runtime difference between SRN and SN. It is presently unclear how computationally intensive the method is. What is the difference in time per training iteration? The authors should also indicate their hardware setup and overall training time.",
                "Table 1 is confusing as it lacks the test error. If there is a difference in testing error, then this table is misleading. If the test errors are approximately the same, then why should I care if the training error is higher? This would just be a way to decrease the stated \u201cgeneralization gap,\u201d which is not necessarily indicative of a better model.",
                "The paper does not explain what \u201cStable-50\u201d, \u201cSRN-50\u201d, and \u201cSRN-50%\u201d are. This should be explicitly stated so that the reader knows which model corresponds to which settings."
            ],
            [
                "The paper could mention some related older introductory approaches, such as representing linear layers as 'bottlenecks' to enforce low rank explicitly, or solving in the manifold of reduced-rank matrices directly.",
                "The authors target the same stable rank (srank) r=c*min(m,n) for all layers, even though only the sum of sranks is important. It would be interesting to see if there is any observable difference for CNNs with only a few linear layers by lightly deviating from this.",
                "The paper could explore whether the first linear layer typically contributes the majority to the sum of sranks.",
                "The paper could discuss how 'stable rank' concepts might be extended to the convolutional layers. As a starting point, spectral values of the block-circulant matrices corresponding to convolutions have been described in previous work."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "While the paper provides a theoretical analysis of the stable rank and Lipschitz constant, it could benefit from a more detailed discussion on the implications of these quantities on the performance of neural networks. The authors could provide more insights into how controlling these quantities can improve the generalization behaviour of neural networks.",
                "The paper could provide more details on the training of models with different learning rates and weight decay. While the paper presents results on these experiments, a more detailed discussion on the choice of learning rates and weight decay parameters could be beneficial.",
                "The paper could also provide more details on the local Lipschitz upper-bound for neural networks. While the paper explains how the Jacobian is used in the case of neural networks, a more detailed discussion on the implications of the local Lipschitz upper-bound on the performance of neural networks could be beneficial.",
                "While the paper presents empirical Lipschitz plots for the purpose of analysis, it could provide a more detailed discussion on the interpretation of these plots and their implications on the performance of neural networks.",
                "The paper could also provide more details on the use of Inception and Frechet Inception Distance (FID) scores for the evaluation of the generated samples in the context of GANs. A more detailed discussion on the choice of these metrics and their implications on the evaluation of GANs could be beneficial.",
                "While the paper introduces SRN-GAN and shows that it provides improved Neural divergence score and Inception and FID scores in most experiments, it could provide a more detailed discussion on the implications of these results on the performance of GANs."
            ],
            "novelty_impact": [
                "The paper introduces a novel method, Stable Rank Normalization (SRN), which is a significant contribution to the field. However, it would be beneficial to provide a more detailed comparison of SRN with other normalization methods to highlight its uniqueness and advantages.",
                "The motivations and goals of the paper are clear, and the key findings are well presented. However, the paper could benefit from a more detailed explanation of the concept of stable rank and why minimizing it is beneficial.",
                "The paper provides extensive analyses across a wide variety of neural networks, which is commendable. However, it would be helpful to provide more specific details on the improvements observed in each type of network.",
                "The impact of SRN when applied to the discriminator of Generative Adversarial Networks (GANs) is well discussed. However, a more detailed explanation of why this is significant would be beneficial.",
                "The introduction of the Empirical Lipschitz Constant (eLhist) metric is a significant contribution. However, a more detailed explanation of what it measures and why it's important in the context of this study would be helpful.",
                "The mathematical proofs and derivations included in the paper are comprehensive. However, a brief overview of these concepts and their relevance to the study would be beneficial for readers who are not familiar with these concepts.",
                "The paper discusses the effect of rank on the empirical Lipschitz constants, which is interesting. However, a more detailed explanation of this relationship would be beneficial.",
                "The paper provides details on various network architectures used in the experiments, which is commendable. However, a more detailed explanation of why these specific architectures were chosen would be beneficial.",
                "The additional experiments on generalization and GAN objective functions are well conducted. However, more detailed information on their results would be beneficial.",
                "The comparison between SRN-GAN and other GANs such as SN-GAN, WGAN-GP, and Ortho-GAN is well conducted. However, a more detailed discussion of the specific advantages of SRN-GAN over these other GANs would be beneficial.",
                "The paper could benefit from visual aids such as tables, charts, or graphs to enhance the clarity of the results and findings.",
                "The paper could provide more clarity on the implications of the findings on the practical application of neural networks.",
                "While the mathematical equations and proofs are comprehensive, they could be made more accessible to readers who may not have a strong mathematical background."
            ],
            "clarity_reproducibility": [
                "The paper could provide more explicit details on how the partitioning index 'k' is chosen in the Stable Rank Normalization (SRN) method. While the paper mentions that the given matrix is partitioned into two parts based on a design choice of partitioning index k, it does not provide a clear guideline on how to choose 'k'. This could potentially affect the reproducibility of the method.",
                "The paper could provide more details on the application of SRN in neural networks (NNs) and its use to normalize both the stable rank and the spectral norm of each linear layer of a NN simultaneously. While the paper proposes that SRN can be used to simultaneously control the Lipschitz constant and the stable rank of a linear operator in a NN, the specific process of applying SRN to normalize both the stable rank and the spectral norm of each linear layer of a NN simultaneously is not detailed.",
                "The paper could provide more details on how the Empirical Lipschitz Constant (eLhist) is calculated and used in the analysis. While the paper introduces this metric, it does not provide a clear explanation of how it is calculated and used.",
                "The paper could provide more details on how the effects of Stable Rank on eLhist and Inception Score were measured and analyzed. While the paper discusses these effects, it does not provide a clear explanation of how these effects were measured and analyzed.",
                "The paper could provide more details on the experimental setup for SRN-GAN, including the hyperparameters used, the training and testing splits, and any data augmentation techniques used. This information is crucial for the reproducibility of the experiments.",
                "The paper could provide more details on the proofs for the optimal stable rank normalization theorem. While the paper provides these proofs, it does not clearly state whether they are based on existing mathematical theorems or are novel contributions of this paper.",
                "The paper could provide more details on the calculation of the local Lipschitz constant using the Jacobian in the case of neural networks. While the paper mentions this calculation, it does not provide a clear explanation of how it is done.",
                "The paper could provide more details on the experimental details of various models including WideResNet-28-10, ResNet-110, DenseNet-100, VGG19, and AlexNet, including their architecture, the hyperparameters used, and any modifications made to the original models. This information is crucial for the reproducibility of the experiments.",
                "The paper could provide more details on the section on empirical Lipschitz plots and the use of conditional GANs. While the paper mentions these topics, it does not provide a clear explanation of them.",
                "The paper could provide more details on the additional experiments on generalization and GAN objective functions, including the experimental setups and results of these experiments. This information is crucial for the reproducibility of the experiments."
            ],
            "all": [
                "While the paper provides a theoretical analysis of the stable rank and Lipschitz constant, it could benefit from a more detailed discussion on the implications of these quantities on the performance of neural networks. The authors could provide more insights into how controlling these quantities can improve the generalization behaviour of neural networks.",
                "The paper could provide more details on the training of models with different learning rates and weight decay. While the paper presents results on these experiments, a more detailed discussion on the choice of learning rates and weight decay parameters could be beneficial.",
                "The paper could also provide more details on the local Lipschitz upper-bound for neural networks. While the paper explains how the Jacobian is used in the case of neural networks, a more detailed discussion on the implications of the local Lipschitz upper-bound on the performance of neural networks could be beneficial.",
                "While the paper presents empirical Lipschitz plots for the purpose of analysis, it could provide a more detailed discussion on the interpretation of these plots and their implications on the performance of neural networks.",
                "The paper could also provide more details on the use of Inception and Frechet Inception Distance (FID) scores for the evaluation of the generated samples in the context of GANs. A more detailed discussion on the choice of these metrics and their implications on the evaluation of GANs could be beneficial.",
                "While the paper introduces SRN-GAN and shows that it provides improved Neural divergence score and Inception and FID scores in most experiments, it could provide a more detailed discussion on the implications of these results on the performance of GANs.",
                "The paper introduces a novel method, Stable Rank Normalization (SRN), which is a significant contribution to the field. However, it would be beneficial to provide a more detailed comparison of SRN with other normalization methods to highlight its uniqueness and advantages.",
                "The motivations and goals of the paper are clear, and the key findings are well presented. However, the paper could benefit from a more detailed explanation of the concept of stable rank and why minimizing it is beneficial.",
                "The paper provides extensive analyses across a wide variety of neural networks, which is commendable. However, it would be helpful to provide more specific details on the improvements observed in each type of network.",
                "The impact of SRN when applied to the discriminator of Generative Adversarial Networks (GANs) is well discussed. However, a more detailed explanation of why this is significant would be beneficial.",
                "The introduction of the Empirical Lipschitz Constant (eLhist) metric is a significant contribution. However, a more detailed explanation of what it measures and why it's important in the context of this study would be helpful.",
                "The mathematical proofs and derivations included in the paper are comprehensive. However, a brief overview of these concepts and their relevance to the study would be beneficial for readers who are not familiar with these concepts.",
                "The paper discusses the effect of rank on the empirical Lipschitz constants, which is interesting. However, a more detailed explanation of this relationship would be beneficial.",
                "The paper provides details on various network architectures used in the experiments, which is commendable. However, a more detailed explanation of why these specific architectures were chosen would be beneficial.",
                "The additional experiments on generalization and GAN objective functions are well conducted. However, more detailed information on their results would be beneficial.",
                "The comparison between SRN-GAN and other GANs such as SN-GAN, WGAN-GP, and Ortho-GAN is well conducted. However, a more detailed discussion of the specific advantages of SRN-GAN over these other GANs would be beneficial.",
                "The paper could benefit from visual aids such as tables, charts, or graphs to enhance the clarity of the results and findings.",
                "The paper could provide more clarity on the implications of the findings on the practical application of neural networks.",
                "While the mathematical equations and proofs are comprehensive, they could be made more accessible to readers who may not have a strong mathematical background.",
                "The paper could provide more explicit details on how the partitioning index 'k' is chosen in the Stable Rank Normalization (SRN) method. While the paper mentions that the given matrix is partitioned into two parts based on a design choice of partitioning index k, it does not provide a clear guideline on how to choose 'k'. This could potentially affect the reproducibility of the method.",
                "The paper could provide more details on the application of SRN in neural networks (NNs) and its use to normalize both the stable rank and the spectral norm of each linear layer of a NN simultaneously. While the paper proposes that SRN can be used to simultaneously control the Lipschitz constant and the stable rank of a linear operator in a NN, the specific process of applying SRN to normalize both the stable rank and the spectral norm of each linear layer of a NN simultaneously is not detailed.",
                "The paper could provide more details on how the Empirical Lipschitz Constant (eLhist) is calculated and used in the analysis. While the paper introduces this metric, it does not provide a clear explanation of how it is calculated and used.",
                "The paper could provide more details on how the effects of Stable Rank on eLhist and Inception Score were measured and analyzed. While the paper discusses these effects, it does not provide a clear explanation of how these effects were measured and analyzed.",
                "The paper could provide more details on the experimental setup for SRN-GAN, including the hyperparameters used, the training and testing splits, and any data augmentation techniques used. This information is crucial for the reproducibility of the experiments.",
                "The paper could provide more details on the proofs for the optimal stable rank normalization theorem. While the paper provides these proofs, it does not clearly state whether they are based on existing mathematical theorems or are novel contributions of this paper.",
                "The paper could provide more details on the calculation of the local Lipschitz constant using the Jacobian in the case of neural networks. While the paper mentions this calculation, it does not provide a clear explanation of how it is done.",
                "The paper could provide more details on the experimental details of various models including WideResNet-28-10, ResNet-110, DenseNet-100, VGG19, and AlexNet, including their architecture, the hyperparameters used, and any modifications made to the original models. This information is crucial for the reproducibility of the experiments.",
                "The paper could provide more details on the section on empirical Lipschitz plots and the use of conditional GANs. While the paper mentions these topics, it does not provide a clear explanation of them.",
                "The paper could provide more details on the additional experiments on generalization and GAN objective functions, including the experimental setups and results of these experiments. This information is crucial for the reproducibility of the experiments."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 1,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The empirical evaluation of the normalization technique is insufficient to demonstrate its practical usefulness. The authors should focus more on showing how their regularization can improve generalization in practice.",
                    "predicted_comment": "While the paper provides a theoretical analysis of the stable rank and Lipschitz constant, it could benefit from a more detailed discussion on the implications of these quantities on the performance of neural networks. The authors could provide more insights into how controlling these quantities can improve the generalization behaviour of neural networks."
                }
            ],
            "novelty_impact": [],
            "clarity_reproducibility": [
                {
                    "reference_id": 0,
                    "predicted_id": 19,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The role of the partitioning index in the proposed algorithm is unclear, as it does not appear to be used in the experiments. The authors should clarify its importance and usage.",
                    "predicted_comment": "The paper could provide more explicit details on how the partitioning index 'k' is chosen in the Stable Rank Normalization (SRN) method. While the paper mentions that the given matrix is partitioned into two parts based on a design choice of partitioning index k, it does not provide a clear guideline on how to choose 'k'. This could potentially affect the reproducibility of the method."
                }
            ],
            "all": [
                {
                    "reference_id": 0,
                    "predicted_id": 19,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The role of the partitioning index in the proposed algorithm is unclear, as it does not appear to be used in the experiments. The authors should clarify its importance and usage.",
                    "predicted_comment": "The paper could provide more explicit details on how the partitioning index 'k' is chosen in the Stable Rank Normalization (SRN) method. While the paper mentions that the given matrix is partitioned into two parts based on a design choice of partitioning index k, it does not provide a clear guideline on how to choose 'k'. This could potentially affect the reproducibility of the method."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The empirical evaluation of the normalization technique is insufficient to demonstrate its practical usefulness. The authors should focus more on showing how their regularization can improve generalization in practice.",
                    "predicted_comment": "While the paper provides a theoretical analysis of the stable rank and Lipschitz constant, it could benefit from a more detailed discussion on the implications of these quantities on the performance of neural networks. The authors could provide more insights into how controlling these quantities can improve the generalization behaviour of neural networks."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 12,
                "n_generated": 6,
                "n_total_aligns": 1,
                "recall": 0.06666666666666667,
                "precision": 0.05555555555555555,
                "pseudo_jaccard": 0.03333333333333333,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 12,
                "n_generated": 13,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "clarity_reproducibility": {
                "n_reference": 12,
                "n_generated": 10,
                "n_total_aligns": 1,
                "recall": 0.06666666666666667,
                "precision": 0.03333333333333333,
                "pseudo_jaccard": 0.023809523809523808,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "all": {
                "n_reference": 12,
                "n_generated": 29,
                "n_total_aligns": 2,
                "recall": 0.13333333333333333,
                "precision": 0.022988505747126436,
                "pseudo_jaccard": 0.020833333333333332,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.2,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.1,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.2,
                    "precision": 0.1,
                    "pseudo_jaccard": 0.07142857142857142,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                }
            ],
            "all": [
                {
                    "recall": 0.4,
                    "precision": 0.06896551724137931,
                    "pseudo_jaccard": 0.0625,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                }
            ]
        }
    },
    {
        "doc_id": "Qm7R_SdqTpT",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper should provide insights on why switching actions at some fixed number of frames beats using the Gaussian variance for FVD.",
                "The paper should discuss the importance of the heuristic of changing states when using GP, and how the performance would be affected if a different threshold is used."
            ],
            [
                "The experimental section could be improved. More details about how the comparison to some baselines was made would be appreciated.",
                "The results for the VRNN model in Figure 4 and 5 do not follow the results in the literature, where it outperforms SVG and SAVP. It's unclear whether this is due to an architectural change, suboptimal hyperparameters, or a different reimplementation.",
                "The VRNN model is missing from some other comparisons such as Table 1.",
                "For SAVP, the results for Figure 4 seem much worse than those reported in the original paper."
            ],
            [
                "The application of Gaussian Processes (GP) in this work is straightforward and naive. Dynamical modeling with GP has been widely investigated since the introduction of Gaussian Process Dynamical Model in NIPs 2005.",
                "The modules of LSTM Frame Generation and GP Frame Generation in Equation (4) are not clearly explained or depicted in Figure 3. It is unclear if these modules correspond to 'D' in Stage 3.",
                "The use of GP to generate images is questionable as GP is more suitable for working in the latent space.",
                "The datasets used in the study are not representative due to their simplicity and experimental nature.",
                "The paper does not demonstrate the utility of the proposed method for high-level research topics such as large-scale action recognition, video captioning, etc."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "Diversity Metrics: The authors have used traditional metrics like SSIM and PSNR, but it's unclear if they have used any diversity metrics to evaluate the diversity of the generated sequences. As one of the main goals of the paper is to generate diverse future frames, it would be beneficial to include diversity metrics in the evaluation.",
                "Ablation Studies: The authors have conducted ablation studies by trying different variants of recurrent modules for their temporal dynamics encoder networks. However, it's unclear if they have conducted ablation studies for other components of their model, such as the Gaussian Process (GP) and the GP variance. Conducting ablation studies for these components could provide more insight into their contribution to the model's performance.",
                "Temporal Dynamics Analysis: It's unclear if the authors have conducted any analysis of the temporal dynamics of their model. Given that the model leverages the changing GP distribution over time to estimate when an on-going action sequence completes, this could be an important aspect to analyze.",
                "Robustness Tests: It's unclear if the authors have conducted any robustness tests to evaluate the model's performance under different conditions. Conducting robustness tests could provide more insight into the model's generalizability and reliability.",
                "Qualitative Analysis: While the authors have evaluated their model quantitatively, it's unclear if they have provided any qualitative results. Providing visual examples of the generated video sequences could help to demonstrate both the diversity and the quality of the generated sequences."
            ],
            "novelty_impact": [
                "The paper should provide a justification for the assumption of future states being normally distributed or discuss the implications if this assumption does not hold. This is a significant assumption that could affect the performance of the Diverse Video Generator (DVG).",
                "The robustness of the method to noise in the data should be discussed in the paper. While Gaussian Processes have inherent noise handling capabilities, it would be beneficial to understand how noise in the data specifically affects the DVG.",
                "The paper should discuss the limitations of the DVG and how it handles outliers or anomalies in the data. These are important aspects that should be addressed for a balanced view of the method.",
                "More information about the training and validation process of the action classifiers used in the evaluation strategy should be provided. This would help understand if there could be a potential bias in the evaluation.",
                "The potential approximation errors introduced by the use of pseudo inputs in the Sparse Variational Gaussian Process (SVGP) and how they might affect the performance of the DVG should be discussed in the paper.",
                "More detailed information about the hyperparameters used in the GP layer implementation and optimization process should be provided. Understanding how sensitive the performance of the DVG is to these hyperparameters would be beneficial."
            ],
            "clarity_reproducibility": [],
            "all": [
                "Diversity Metrics: The authors have used traditional metrics like SSIM and PSNR, but it's unclear if they have used any diversity metrics to evaluate the diversity of the generated sequences. As one of the main goals of the paper is to generate diverse future frames, it would be beneficial to include diversity metrics in the evaluation.",
                "Ablation Studies: The authors have conducted ablation studies by trying different variants of recurrent modules for their temporal dynamics encoder networks. However, it's unclear if they have conducted ablation studies for other components of their model, such as the Gaussian Process (GP) and the GP variance. Conducting ablation studies for these components could provide more insight into their contribution to the model's performance.",
                "Temporal Dynamics Analysis: It's unclear if the authors have conducted any analysis of the temporal dynamics of their model. Given that the model leverages the changing GP distribution over time to estimate when an on-going action sequence completes, this could be an important aspect to analyze.",
                "Robustness Tests: It's unclear if the authors have conducted any robustness tests to evaluate the model's performance under different conditions. Conducting robustness tests could provide more insight into the model's generalizability and reliability.",
                "Qualitative Analysis: While the authors have evaluated their model quantitatively, it's unclear if they have provided any qualitative results. Providing visual examples of the generated video sequences could help to demonstrate both the diversity and the quality of the generated sequences.",
                "The paper should provide a justification for the assumption of future states being normally distributed or discuss the implications if this assumption does not hold. This is a significant assumption that could affect the performance of the Diverse Video Generator (DVG).",
                "The robustness of the method to noise in the data should be discussed in the paper. While Gaussian Processes have inherent noise handling capabilities, it would be beneficial to understand how noise in the data specifically affects the DVG.",
                "The paper should discuss the limitations of the DVG and how it handles outliers or anomalies in the data. These are important aspects that should be addressed for a balanced view of the method.",
                "More information about the training and validation process of the action classifiers used in the evaluation strategy should be provided. This would help understand if there could be a potential bias in the evaluation.",
                "The potential approximation errors introduced by the use of pseudo inputs in the Sparse Variational Gaussian Process (SVGP) and how they might affect the performance of the DVG should be discussed in the paper.",
                "More detailed information about the hyperparameters used in the GP layer implementation and optimization process should be provided. Understanding how sensitive the performance of the DVG is to these hyperparameters would be beneficial."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [],
            "clarity_reproducibility": [],
            "all": []
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 11,
                "n_generated": 5,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 11,
                "n_generated": 6,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "clarity_reproducibility": {
                "n_reference": 11,
                "n_generated": 0,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 11,
                "n_generated": 11,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                }
            ]
        }
    },
    {
        "doc_id": "K5j7D81ABvt",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper does not clarify whether 'S_F = S_sTEX' refers to exact string equality or after white space normalization. The authors should specify the exact normalizations and provide the success rate before and after them.",
                "The paper does not discuss the potential impact of using larger GPT models.",
                "The paper does not explore the potential usefulness of unsupervised learning methods, which have recently seen significant improvements.",
                "The term 'fully disambiguated' is used in definition 4.1 without considering the types of variables. In systems with subtypes, the meaning and provability of a statement can change depending on the quantification over different types of numbers."
            ],
            [
                "The current benchmark lacks sufficient training and evaluation data. The lack of training corpora may be addressed by pretraining and building synthetic data.",
                "A larger and high-quality evaluation set is needed to validate any actual progress on this problem. The current evaluation set is too small and covers limited math topics.",
                "The evaluation protocol is unclear. The best evaluation protocol should be checking if S_F belongs to STEX(S_STEX), which is not used in this work. The authors should consider implementing this protocol.",
                "An ablation study on the corresponding contributions of pretraining and synthetic data is needed for the proposed approach."
            ],
            [
                "The paper does not compare the proposed method against other systems listed in section 4 that also aim to automate the formalization process.",
                "The choice of phrasing the task as a translation problem is questionable, as it seems to set up the NMT baselines to perform poorly due to the identity transformation required for much of the document.",
                "The evaluation methodology is confusing, particularly the use of automated procedures to generate some of the data. This raises questions about why these automated methods are not sufficient for the end task.",
                "The results section does not present the results of the baselines, making it difficult to assess their performance relative to the proposed method.",
                "The paper does not provide a clear understanding of the problem setting, what is being learned, and what is failing. This makes it difficult to assess the importance and difficulty of the targeted problem.",
                "The experimental section is short and lacks important comparisons to other work in this area.",
                "The model is used in a very off-the-shelf way and does not treat the problem setting any differently than a standard translation task, which raises questions about the novelty of the modeling contribution.",
                "The paper uses a lot of domain-specific terminology that may be difficult for a general ML audience to understand. For example, the terms 'formal' and 'informal' are used in a way that is not immediately clear.",
                "The paper needs major restructuring and refocusing, more experiments, and more analysis to be considered for acceptance."
            ],
            [
                "The problem definition was not clear. A concrete example demonstrating what exactly an algorithm for the problem gets as input and what is its output is missing.",
                "The description of the dataset was unclear. While there is a formal description, the lack of examples leaves the description at a very abstract level, making it difficult to understand what exactly should be expected in the dataset.",
                "The authors do not show awareness of work in semantic parsing and in style transfer. These works are very important both for the algorithmic approach and for understanding the challenges of evaluation. For example, there is lots of semantic parsing research on transferring text into SQL queries or on solving textual mathematical problems. The connection to relevant ideas and approaches should be part of the introduction of the task."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "Ablation Study: The paper would benefit from an ablation study to understand the contribution of different components of the proposed method. This would help readers understand which parts of the method are driving its performance and could potentially lead to further improvements.",
                "Performance Metrics: The paper should explicitly mention the performance metrics used to evaluate the proposed method. This would provide a clear measure of the method's success and allow for comparisons with other methods.",
                "Generalization Ability: The paper should discuss the generalization ability of the proposed method. This would give insight into how well the method is expected to perform on unseen data.",
                "Scalability: The paper should discuss the scalability of the proposed method. This would provide information on the method's applicability to larger datasets and its potential for use in practical applications."
            ],
            "novelty_impact": [
                "The approach of treating the task of disambiguating symbolic expressions as a neural machine translation task is novel and interesting. However, it would be beneficial if the authors could provide more evidence or references to support this approach.",
                "The authors should provide more details on how they ensured the representativeness of the pre-training data from arxiv.org. This is crucial for the validity of the results obtained from the model.",
                "The authors' choice of tools (sT E X package, the SMGloM library, and the MMT system) seems justified. However, a comparison with other potential tools that could be used for this task would strengthen the paper.",
                "The authors should discuss potential limitations or challenges of semantically annotating informal documents. This would provide a more balanced view of their approach and its applicability in real-world scenarios.",
                "The authors' future plans to combine the proposed symbolic disambiguation approach with an autoformalization framework is intriguing. However, some preliminary results or evidence to suggest that this combination would indeed yield better results would strengthen the paper.",
                "The authors' suggestion of integrating formal methods in a L A T E X development environment is interesting. However, a discussion on potential challenges or limitations of this integration would be beneficial.",
                "The authors' claim of using NMT models for disambiguating plain L A T E X math expressions via sT E X annotations needs to be supported with evidence or preliminary results.",
                "The authors should provide a clear definition of what they mean by \"specificity\" in the context of this task. This would help readers understand the unique aspects of this task compared to other NMT problems.",
                "The authors should justify why they chose the specific evaluation techniques for their model. This would help readers understand how these techniques contribute to the overall evaluation of the model.",
                "The authors should discuss how their proposed model addresses the issue of the baseline models failing to yield even syntactically valid L A T E X before overfitting. This would help readers understand the improvements made by the proposed model."
            ],
            "clarity_reproducibility": [],
            "all": [
                "Ablation Study: The paper would benefit from an ablation study to understand the contribution of different components of the proposed method. This would help readers understand which parts of the method are driving its performance and could potentially lead to further improvements.",
                "Performance Metrics: The paper should explicitly mention the performance metrics used to evaluate the proposed method. This would provide a clear measure of the method's success and allow for comparisons with other methods.",
                "Generalization Ability: The paper should discuss the generalization ability of the proposed method. This would give insight into how well the method is expected to perform on unseen data.",
                "Scalability: The paper should discuss the scalability of the proposed method. This would provide information on the method's applicability to larger datasets and its potential for use in practical applications.",
                "The approach of treating the task of disambiguating symbolic expressions as a neural machine translation task is novel and interesting. However, it would be beneficial if the authors could provide more evidence or references to support this approach.",
                "The authors should provide more details on how they ensured the representativeness of the pre-training data from arxiv.org. This is crucial for the validity of the results obtained from the model.",
                "The authors' choice of tools (sT E X package, the SMGloM library, and the MMT system) seems justified. However, a comparison with other potential tools that could be used for this task would strengthen the paper.",
                "The authors should discuss potential limitations or challenges of semantically annotating informal documents. This would provide a more balanced view of their approach and its applicability in real-world scenarios.",
                "The authors' future plans to combine the proposed symbolic disambiguation approach with an autoformalization framework is intriguing. However, some preliminary results or evidence to suggest that this combination would indeed yield better results would strengthen the paper.",
                "The authors' suggestion of integrating formal methods in a L A T E X development environment is interesting. However, a discussion on potential challenges or limitations of this integration would be beneficial.",
                "The authors' claim of using NMT models for disambiguating plain L A T E X math expressions via sT E X annotations needs to be supported with evidence or preliminary results.",
                "The authors should provide a clear definition of what they mean by \"specificity\" in the context of this task. This would help readers understand the unique aspects of this task compared to other NMT problems.",
                "The authors should justify why they chose the specific evaluation techniques for their model. This would help readers understand how these techniques contribute to the overall evaluation of the model.",
                "The authors should discuss how their proposed model addresses the issue of the baseline models failing to yield even syntactically valid L A T E X before overfitting. This would help readers understand the improvements made by the proposed model."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [
                {
                    "reference_id": 1,
                    "predicted_id": 4,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The choice of phrasing the task as a translation problem is questionable, as it seems to set up the NMT baselines to perform poorly due to the identity transformation required for much of the document.",
                    "predicted_comment": "The approach of treating the task of disambiguating symbolic expressions as a neural machine translation task is novel and interesting. However, it would be beneficial if the authors could provide more evidence or references to support this approach."
                }
            ],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 1,
                    "predicted_id": 4,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The choice of phrasing the task as a translation problem is questionable, as it seems to set up the NMT baselines to perform poorly due to the identity transformation required for much of the document.",
                    "predicted_comment": "The approach of treating the task of disambiguating symbolic expressions as a neural machine translation task is novel and interesting. However, it would be beneficial if the authors could provide more evidence or references to support this approach."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 20,
                "n_generated": 4,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 20,
                "n_generated": 10,
                "n_total_aligns": 1,
                "recall": 0.027777777777777776,
                "precision": 0.025,
                "pseudo_jaccard": 0.013888888888888888,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 20,
                "n_generated": 0,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 20,
                "n_generated": 14,
                "n_total_aligns": 1,
                "recall": 0.027777777777777776,
                "precision": 0.017857142857142856,
                "pseudo_jaccard": 0.011363636363636364,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 9
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.1111111111111111,
                    "precision": 0.1,
                    "pseudo_jaccard": 0.05555555555555555,
                    "n_real": 9
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 9
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.1111111111111111,
                    "precision": 0.07142857142857142,
                    "pseudo_jaccard": 0.045454545454545456,
                    "n_real": 9
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "rkxZCJrtwS",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper should include an argument explaining that accurately estimating Q using Q_phi does not necessarily mean that the gradients of Q_phi will be good approximations of the true gradients of Q. The reviewer suggests referring to Fig 1 of arxiv.org/pdf/1705.07107.pdf for an informative example.",
                "The choice of Euclidean norm in line 10 needs justification. The reviewer suggests considering the importance of the direction of the gradient over the norm and proposes the use of cosine similarity instead.",
                "The paper argues that Deep Reinforcement Learning (DRL) is superior to Model Predictive Control (MPC) due to better exploration. The reviewer suggests exploring the possibility of using simulator gradients to further improve exploration in DRL."
            ],
            [
                "The paper's contribution is empirical in nature, and a promising future direction would be to add derivative information to and evaluate similar variants of newer actor-critic methods such as TD3 and SAC."
            ],
            [
                "The form of gradient given in equation (2) might contain an error. In DDPG, the action is given by a deterministic policy, thus we have $a_t = \\pi(s_t)$ for all $t\\geq 0$. For critic estimation, it seems you are basing on the Bellman equation $ Q(s,a) = r(s,a) + Q(s', \\pi(s'))$, where $s'$ is the next state following $(s,a)$. Then, it seems that Equation (2) is obtained by taking gradient with respect to $(s,a)$. However, I cannot understand what $\\nabla_{\\pi} Q$ stands for. If it is $\\nabla_a Q(s_{i+1}, a_{i+1}) \\cdot \\nabla_s \\pi(s_{i+1}) $, then that makes sense.",
                "Based on the experiments, the proposed method does not always outperform MPC or DDPG, even in a small-scale control problem Mountaincar. Moreover, it seems that the performance is similar to that of the DDPG.",
                "The model-based gradient in equation (2) is defined by only unroll one-step forward by going from $s_i, a_i$ to $s_{i+1}$. It would be interesting to see how the number of unroll steps affect the algorithm, which is a gradient version of TD($\\lambda$)."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "Ablation Study: The paper would benefit from an ablation study to understand the contribution of each component of the proposed algorithm. This would help identify which parts of the algorithm are contributing to its performance and which parts are not.",
                "Sensitivity Analysis: A sensitivity analysis is recommended to understand the robustness of the algorithm to changes in its parameters. This would provide insights into how sensitive the algorithm's performance is to its parameter settings, which is important for practical applications.",
                "Statistical Analysis: A statistical analysis is necessary to ensure the validity of the results. This would help determine if the observed differences in performance between the algorithms are due to chance or are statistically significant.",
                "Computational Cost Analysis: A computational cost analysis is suggested to assess the practicality of the algorithm. This would provide insights into whether the algorithm is feasible to use in real-world applications, where computational resources may be limited."
            ],
            "novelty_impact": [
                "The paper presents a novel hybrid control algorithm that combines the strengths of model-based control methods and deep reinforcement learning (DRL). This is a significant contribution to the field, as it addresses the limitations of both methods and potentially offers a more efficient and robust solution for robot control tasks.",
                "The motivations and goals of the work are clearly explained. The authors have identified the limitations of existing methods and proposed a hybrid algorithm to overcome these limitations. However, the paper could benefit from a more detailed discussion on how the proposed method builds and expands on existing work in a meaningful way.",
                "The key findings of the paper are well-presented and supported by the results of the experiments. However, the paper could provide more empirical evidence to support some of its claims, such as the superiority of the hybrid algorithm over MPC in terms of applicability to a wider range of reward structures and robustness.",
                "The paper should explicitly discuss how the algorithm handles uncertainties or errors in the model. While the use of DRL in the hybrid algorithm may provide some robustness to model inaccuracies, this needs to be clearly stated and supported with evidence.",
                "The paper should provide detailed information on the limitations of the algorithm and under what conditions it might not perform as well. The performance of the algorithm in a 3D environment or on more complex tasks should also be discussed.",
                "The paper should discuss how the algorithm handles real-world scenarios where collisions might not be perfectly modeled by the two simulation methods discussed.",
                "The paper should discuss how the algorithm performs under computational constraints. The use of true gradients from a differentiable physical simulator to increase the convergence rate may help to mitigate the computational cost of DRL, but this needs to be clearly stated and supported with evidence.",
                "The paper should discuss how the algorithm handles situations where the true gradients might not be readily available or accurately calculated. The use of a differentiable physical simulator in the algorithm may help to ensure the availability and accuracy of the gradients, but this needs to be clearly stated and supported with evidence."
            ],
            "clarity_reproducibility": [
                "The authors should provide more details about the parameter space noise used for exploration. Even though it is mentioned that it is applied to the actor network for exploration as proposed in (Plappert et al., 2017), the authors should explain how it was implemented in this specific study.",
                "The authors should provide more information about the architecture of the critic network. The input and output of the network are explained, but the structure of the network itself is not detailed. This information is crucial for understanding the learning process and for reproducing the algorithm.",
                "The authors should provide more details about the baselines used for comparison. Even though DDPG, MPC with iLQR, and gradient descent are mentioned, the specific implementation details are not provided. This information is necessary for understanding the comparative analysis.",
                "The authors should provide more details about the differentiable rigid body simulator used for the 2D control tasks. How was it implemented and how does it contribute to the performance of the algorithm? This information is necessary for understanding the simulation environment and for reproducing the tasks.",
                "The authors should provide more details about the regularization term used in the critic gradient fitting step. How was it calculated and how does it contribute to the learning process? This information is crucial for understanding the modification made to the DDPG method.",
                "The authors should provide more details about the seven 2D robot control tasks. Specifically, how were the tasks implemented in the differentiable rigid body simulator? This information is necessary for understanding the tasks and for reproducing the results.",
                "The authors should provide more details about the computation of the gradients of the system dynamics in the differentiable physical simulator. This information is crucial for understanding the learning process and for reproducing the algorithm."
            ],
            "all": [
                "Ablation Study: The paper would benefit from an ablation study to understand the contribution of each component of the proposed algorithm. This would help identify which parts of the algorithm are contributing to its performance and which parts are not.",
                "Sensitivity Analysis: A sensitivity analysis is recommended to understand the robustness of the algorithm to changes in its parameters. This would provide insights into how sensitive the algorithm's performance is to its parameter settings, which is important for practical applications.",
                "Statistical Analysis: A statistical analysis is necessary to ensure the validity of the results. This would help determine if the observed differences in performance between the algorithms are due to chance or are statistically significant.",
                "Computational Cost Analysis: A computational cost analysis is suggested to assess the practicality of the algorithm. This would provide insights into whether the algorithm is feasible to use in real-world applications, where computational resources may be limited.",
                "The paper presents a novel hybrid control algorithm that combines the strengths of model-based control methods and deep reinforcement learning (DRL). This is a significant contribution to the field, as it addresses the limitations of both methods and potentially offers a more efficient and robust solution for robot control tasks.",
                "The motivations and goals of the work are clearly explained. The authors have identified the limitations of existing methods and proposed a hybrid algorithm to overcome these limitations. However, the paper could benefit from a more detailed discussion on how the proposed method builds and expands on existing work in a meaningful way.",
                "The key findings of the paper are well-presented and supported by the results of the experiments. However, the paper could provide more empirical evidence to support some of its claims, such as the superiority of the hybrid algorithm over MPC in terms of applicability to a wider range of reward structures and robustness.",
                "The paper should explicitly discuss how the algorithm handles uncertainties or errors in the model. While the use of DRL in the hybrid algorithm may provide some robustness to model inaccuracies, this needs to be clearly stated and supported with evidence.",
                "The paper should provide detailed information on the limitations of the algorithm and under what conditions it might not perform as well. The performance of the algorithm in a 3D environment or on more complex tasks should also be discussed.",
                "The paper should discuss how the algorithm handles real-world scenarios where collisions might not be perfectly modeled by the two simulation methods discussed.",
                "The paper should discuss how the algorithm performs under computational constraints. The use of true gradients from a differentiable physical simulator to increase the convergence rate may help to mitigate the computational cost of DRL, but this needs to be clearly stated and supported with evidence.",
                "The paper should discuss how the algorithm handles situations where the true gradients might not be readily available or accurately calculated. The use of a differentiable physical simulator in the algorithm may help to ensure the availability and accuracy of the gradients, but this needs to be clearly stated and supported with evidence.",
                "The authors should provide more details about the parameter space noise used for exploration. Even though it is mentioned that it is applied to the actor network for exploration as proposed in (Plappert et al., 2017), the authors should explain how it was implemented in this specific study.",
                "The authors should provide more information about the architecture of the critic network. The input and output of the network are explained, but the structure of the network itself is not detailed. This information is crucial for understanding the learning process and for reproducing the algorithm.",
                "The authors should provide more details about the baselines used for comparison. Even though DDPG, MPC with iLQR, and gradient descent are mentioned, the specific implementation details are not provided. This information is necessary for understanding the comparative analysis.",
                "The authors should provide more details about the differentiable rigid body simulator used for the 2D control tasks. How was it implemented and how does it contribute to the performance of the algorithm? This information is necessary for understanding the simulation environment and for reproducing the tasks.",
                "The authors should provide more details about the regularization term used in the critic gradient fitting step. How was it calculated and how does it contribute to the learning process? This information is crucial for understanding the modification made to the DDPG method.",
                "The authors should provide more details about the seven 2D robot control tasks. Specifically, how were the tasks implemented in the differentiable rigid body simulator? This information is necessary for understanding the tasks and for reproducing the results.",
                "The authors should provide more details about the computation of the gradients of the system dynamics in the differentiable physical simulator. This information is crucial for understanding the learning process and for reproducing the algorithm."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [],
            "clarity_reproducibility": [],
            "all": []
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 7,
                "n_generated": 4,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 7,
                "n_generated": 8,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "clarity_reproducibility": {
                "n_reference": 7,
                "n_generated": 7,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 7,
                "n_generated": 19,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "xP37gkVKa_0",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper's proposed method, LBS, only supports one agent in a two-player game.",
                "The authors mention learning a centralized reward function using VDN, while other agent\u2019s q-value functions are unavailable. It's unclear how this is achieved.",
                "The method for using the total reward expectation to avoid having to unroll episodes until the end of the game is not clear."
            ],
            [
                "The paper needs to address how overestimation is handled when using argmax of $Q(a^i|\\tau^i)$ in n-step rollout, especially considering the large variance in rollout. The use of any variance reduction technique should be clarified.",
                "The experiments are only evaluated on Hanabi, raising concerns about the generalizability of the method. The paper should include at least one experiment evaluated on a different game, such as Leduc, to demonstrate its applicability to different imperfect information games.",
                "The paper needs to provide clear definitions and explanations for terms and concepts used, such as 'BP' on page 1, 'blueprint policies' in section 2.1, and the belief definition in section 3.",
                "Equations 2, 3, and 4 are confusing and need further explanation, as they are crucial to the paper."
            ],
            [
                "The motivation of why Learned Belief Search (LBS) can be challenging and useful for DEC-POMDP or the multi-agent game domains is not convincingly explained.",
                "The experiment settings, problem description, and discussions need more elaboration.",
                "The results in Table 1 and 3 are not discussed in the main text, making it hard to understand the reported numbers. The authors should elaborate on these in their response.",
                "A brief description of the Hanabi game should be included in the Appendix.",
                "Comparisons of different design/hyperparameter choices should be included to help judge the benefit of using learning for belief representation."
            ],
            [
                "The paper's proposed method, LBS, is highly dependent on the well-trained Blue Policy (BP). The importance of BP's contribution to the final performance is unclear.",
                "It is not clear how LBS would perform when based on a broken or flawed BP policy.",
                "The paper does not provide clarity on how LBS would perform when using an unconverging learned belief model or value model.",
                "The choice of the number of factorized private features in the belief model is not clear.",
                "The efficacy of LBS, especially in more complex environments like Contract Bridge, is not convincingly demonstrated despite the provided ablation studies."
            ],
            [
                "More detail is required to explain why multi-agent search is not theoretically sound. If the trained model is only accurate for single agent search, is it not possible to train a model that is compatible with multi-agent search under different settings for max range?",
                "The authors focus on 2-player Hanabi and claim it to be straightforward to extend to any number of players. However, SPARTA itself was computationally limited to single-agent search for these 3-player and up variants. Applying your approach in these experiments would make a stronger case for elevation of the state-of-the-art than the 6-card variant.",
                "The experimental setup requires significantly more details on the hardware used for training, testing and validating.",
                "Have you examined how this would look for more general partially observable settings? In particular, work on POSGs with public observations [Hor\u00e1k, K.; and Bosansk\u00fd, B. 2019. Solving Partially Observable Stochastic Games with Public Observations. In AAAI Conf. on Artificial Intelligence, 2029\u20132036. AAAI Press.]",
                "Are there any other DEC-POMDP settings that you considered? It may be insightful to compare the effects of different feature spaces."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper could benefit from testing the LBS method in a variety of environments. While the focus on 2-player Hanabi is understandable, testing the method in other settings would provide a more robust demonstration of its generalizability.",
                "The authors note a gap between learned belief and exact belief, especially in the later stage of the game. It would be beneficial to explore more powerful models such as transformers to improve the belief learning, as suggested by the authors.",
                "The use of a modified version of Hanabi where each player holds 6 cards instead of 5 is not an official variant of Hanabi. It would be more convincing to see the performance of LBS in the standard form of the game.",
                "A more detailed comparison between LBS and other search methods, including the blueprint policy, would strengthen the paper. This could include statistical tests to determine the significance of the differences in performance.",
                "An ablation study to understand the contribution of each component of LBS would be a valuable addition to the paper.",
                "The authors suggest retraining the belief model on data generated from LBS as future work. It would be interesting to see some preliminary results of this experiment in the paper."
            ],
            "novelty_impact": [
                "The paper presents a novel search algorithm for POMDPs, Learned Belief Search (LBS), which is a significant contribution to the field. However, the paper could benefit from providing more details on how the LBS method determines inconsistency. This is a crucial part of the LBS method and it would be beneficial to have more information on this process.",
                "The paper reports a 35x reduction in compute requirements in the context of the game of Hanabi. It would be helpful to know if this reduction is consistent across different game scenarios or if it varies. If it varies, what are the factors that could affect this reduction? The authors should provide more information on this.",
                "The paper does not provide evidence of the performance of LBS in other POMDPs. This raises questions about the generalizability of the LBS method. The authors should provide evidence of the performance of LBS in other POMDPs.",
                "The paper does not explicitly state that the current LBS method does not involve any form of search during training. However, the suggestion of integrating the search process into the training process in future work implies that this might be the case. The authors should clarify this.",
                "The paper does not provide a direct comparison of LBS with other existing methods beyond the exact search method. It would be beneficial to have a comparison with other existing methods to understand the relative performance of LBS. The authors should provide a comparison with other existing methods.",
                "The potential limitations of the LBS method mentioned include the accuracy of the learned belief model, the computational cost of training this model, and the generalizability of the method to other POMDPs. The authors should discuss these limitations in the paper."
            ],
            "clarity_reproducibility": [
                "The training process of the belief model in Learned Belief Search (LBS) could be described in more detail. Specifically, information about how the training data is split into training and validation sets, and how the model's performance is evaluated during training would be helpful.",
                "More details about the public-private model architecture could be provided. Specifically, information about the public and private features, including what kind of information they contain and how they are represented, would be beneficial.",
                "The blueprint training section could provide more details about the reinforcement learning algorithm used, the reward function, and the definition of the state and action spaces.",
                "The belief learning process could provide more details about the supervised loss used for training. Specifically, information about the type of loss function and how it is computed would be beneficial.",
                "More details about the exact search methods that LBS is compared to could be provided. This would help readers understand how they work and what their strengths and weaknesses are.",
                "The experimental setup used to evaluate LBS could be described in more detail. Specifically, information about the hardware used and the hyperparameters for the training process would be helpful."
            ],
            "all": [
                "The paper could benefit from testing the LBS method in a variety of environments. While the focus on 2-player Hanabi is understandable, testing the method in other settings would provide a more robust demonstration of its generalizability.",
                "The authors note a gap between learned belief and exact belief, especially in the later stage of the game. It would be beneficial to explore more powerful models such as transformers to improve the belief learning, as suggested by the authors.",
                "The use of a modified version of Hanabi where each player holds 6 cards instead of 5 is not an official variant of Hanabi. It would be more convincing to see the performance of LBS in the standard form of the game.",
                "A more detailed comparison between LBS and other search methods, including the blueprint policy, would strengthen the paper. This could include statistical tests to determine the significance of the differences in performance.",
                "An ablation study to understand the contribution of each component of LBS would be a valuable addition to the paper.",
                "The authors suggest retraining the belief model on data generated from LBS as future work. It would be interesting to see some preliminary results of this experiment in the paper.",
                "The paper presents a novel search algorithm for POMDPs, Learned Belief Search (LBS), which is a significant contribution to the field. However, the paper could benefit from providing more details on how the LBS method determines inconsistency. This is a crucial part of the LBS method and it would be beneficial to have more information on this process.",
                "The paper reports a 35x reduction in compute requirements in the context of the game of Hanabi. It would be helpful to know if this reduction is consistent across different game scenarios or if it varies. If it varies, what are the factors that could affect this reduction? The authors should provide more information on this.",
                "The paper does not provide evidence of the performance of LBS in other POMDPs. This raises questions about the generalizability of the LBS method. The authors should provide evidence of the performance of LBS in other POMDPs.",
                "The paper does not explicitly state that the current LBS method does not involve any form of search during training. However, the suggestion of integrating the search process into the training process in future work implies that this might be the case. The authors should clarify this.",
                "The paper does not provide a direct comparison of LBS with other existing methods beyond the exact search method. It would be beneficial to have a comparison with other existing methods to understand the relative performance of LBS. The authors should provide a comparison with other existing methods.",
                "The potential limitations of the LBS method mentioned include the accuracy of the learned belief model, the computational cost of training this model, and the generalizability of the method to other POMDPs. The authors should discuss these limitations in the paper.",
                "The training process of the belief model in Learned Belief Search (LBS) could be described in more detail. Specifically, information about how the training data is split into training and validation sets, and how the model's performance is evaluated during training would be helpful.",
                "More details about the public-private model architecture could be provided. Specifically, information about the public and private features, including what kind of information they contain and how they are represented, would be beneficial.",
                "The blueprint training section could provide more details about the reinforcement learning algorithm used, the reward function, and the definition of the state and action spaces.",
                "The belief learning process could provide more details about the supervised loss used for training. Specifically, information about the type of loss function and how it is computed would be beneficial.",
                "More details about the exact search methods that LBS is compared to could be provided. This would help readers understand how they work and what their strengths and weaknesses are.",
                "The experimental setup used to evaluate LBS could be described in more detail. Specifically, information about the hardware used and the hyperparameters for the training process would be helpful."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 4,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The efficacy of LBS, especially in more complex environments like Contract Bridge, is not convincingly demonstrated despite the provided ablation studies.",
                    "predicted_comment": "The paper could benefit from testing the LBS method in a variety of environments. While the focus on 2-player Hanabi is understandable, testing the method in other settings would provide a more robust demonstration of its generalizability."
                }
            ],
            "novelty_impact": [
                {
                    "reference_id": 3,
                    "predicted_id": 8,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "Have you examined how this would look for more general partially observable settings? In particular, work on POSGs with public observations [Hor\u00e1k, K.; and Bosansk\u00fd, B. 2019. Solving Partially Observable Stochastic Games with Public Observations. In AAAI Conf. on Artificial Intelligence, 2029\u20132036. AAAI Press.]",
                    "predicted_comment": "The paper does not provide evidence of the performance of LBS in other POMDPs. This raises questions about the generalizability of the LBS method. The authors should provide evidence of the performance of LBS in other POMDPs."
                }
            ],
            "clarity_reproducibility": [
                {
                    "reference_id": 1,
                    "predicted_id": 17,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The experiment settings, problem description, and discussions need more elaboration.",
                    "predicted_comment": "The experimental setup used to evaluate LBS could be described in more detail. Specifically, information about the hardware used and the hyperparameters for the training process would be helpful."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 17,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The experimental setup requires significantly more details on the hardware used for training, testing and validating.",
                    "predicted_comment": "The experimental setup used to evaluate LBS could be described in more detail. Specifically, information about the hardware used and the hyperparameters for the training process would be helpful."
                }
            ],
            "all": [
                {
                    "reference_id": 1,
                    "predicted_id": 17,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The experiment settings, problem description, and discussions need more elaboration.",
                    "predicted_comment": "The experimental setup used to evaluate LBS could be described in more detail. Specifically, information about the hardware used and the hyperparameters for the training process would be helpful."
                },
                {
                    "reference_id": 4,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The efficacy of LBS, especially in more complex environments like Contract Bridge, is not convincingly demonstrated despite the provided ablation studies.",
                    "predicted_comment": "The paper could benefit from testing the LBS method in a variety of environments. While the focus on 2-player Hanabi is understandable, testing the method in other settings would provide a more robust demonstration of its generalizability."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 17,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The experimental setup requires significantly more details on the hardware used for training, testing and validating.",
                    "predicted_comment": "The experimental setup used to evaluate LBS could be described in more detail. Specifically, information about the hardware used and the hyperparameters for the training process would be helpful."
                },
                {
                    "reference_id": 3,
                    "predicted_id": 8,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "Have you examined how this would look for more general partially observable settings? In particular, work on POSGs with public observations [Hor\u00e1k, K.; and Bosansk\u00fd, B. 2019. Solving Partially Observable Stochastic Games with Public Observations. In AAAI Conf. on Artificial Intelligence, 2029\u20132036. AAAI Press.]",
                    "predicted_comment": "The paper does not provide evidence of the performance of LBS in other POMDPs. This raises questions about the generalizability of the LBS method. The authors should provide evidence of the performance of LBS in other POMDPs."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 22,
                "n_generated": 6,
                "n_total_aligns": 1,
                "recall": 0.04,
                "precision": 0.03333333333333333,
                "pseudo_jaccard": 0.02,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 22,
                "n_generated": 6,
                "n_total_aligns": 1,
                "recall": 0.04,
                "precision": 0.03333333333333333,
                "pseudo_jaccard": 0.02,
                "specificity_more": 0.0,
                "specificity_less": 0.0,
                "specificity_same": 1.0
            },
            "clarity_reproducibility": {
                "n_reference": 22,
                "n_generated": 6,
                "n_total_aligns": 2,
                "recall": 0.08,
                "precision": 0.06666666666666667,
                "pseudo_jaccard": 0.04,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "all": {
                "n_reference": 22,
                "n_generated": 18,
                "n_total_aligns": 4,
                "recall": 0.16,
                "precision": 0.04444444444444444,
                "pseudo_jaccard": 0.03722943722943723,
                "specificity_more": 0.75,
                "specificity_less": 0.0,
                "specificity_same": 0.25
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.2,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.1,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.2,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.1,
                    "n_real": 5
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.2,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.1,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.2,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.1,
                    "n_real": 5
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.2,
                    "precision": 0.05555555555555555,
                    "pseudo_jaccard": 0.045454545454545456,
                    "n_real": 5
                },
                {
                    "recall": 0.2,
                    "precision": 0.05555555555555555,
                    "pseudo_jaccard": 0.045454545454545456,
                    "n_real": 5
                },
                {
                    "recall": 0.4,
                    "precision": 0.1111111111111111,
                    "pseudo_jaccard": 0.09523809523809523,
                    "n_real": 5
                }
            ]
        }
    },
    {
        "doc_id": "-qB7ZgRNRq",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The motivation for the new task proposed in the paper is not clear.",
                "The paper does not compare the proposed model, DDNet, to any other baselines that incorporate the audio information in a different way.",
                "The paper incorrectly claims that the method from Serdyuk et al. (2018) was 'concurrent', despite it being published in 2018.",
                "It is unclear why the Spoken-CoQA dataset includes text transcripts as well as the audio. It would make more sense for the text transcripts to be part of the model solving the dataset."
            ],
            [
                "The work is limited in its scope and is not yet ready to be published at ICLR due to lack of comparisons made to prior approaches, making it hard to assess the merits of the proposed approach.",
                "The authors do not mention any plans of releasing the new dataset described in this work, which is a concern.",
                "It would be interesting to show how F1 scores vary on the test instances as a function of the number of turns in the conversation, and how test F1 scores vary as a function of ASR accuracy of the spoken documents/questions.",
                "The error rate of the ASR system on the spoken documents and spoken questions should be provided to give the reader an idea of the accuracy of the transcriptions fed as input to the student model.",
                "In Table 4, the F1 scores using SDNet are higher for S-CoQA compared to CoQA which is unexpected. The authors should provide an explanation for this.",
                "The reliance of the proposed system on clean text is a significant issue that needs to be addressed."
            ],
            [
                "The paper's contributions and novelty are unclear, and it's uncertain if they warrant acceptance.",
                "The paper's methods, including the cross attention mechanism for speech and text embedding fusion and knowledge distillation for combatting ASR errors, are existing, well-studied methods.",
                "The Spoken-CoQA dataset, which is a significant contribution of the paper, is not made available.",
                "The paper should clarify whether the knowledge distillation and cross attention mechanisms can be combined for better performance.",
                "The paper should address whether the textual input is more useful than the audio input in the Spoken-CoQA dataset, especially considering the audio input is larger and more difficult to process."
            ],
            [
                "The biggest weakness of this paper is in its data construction. When the ASR output is noisy, the spans refer to ghost token positions based on the clean text. This makes the data difficult to learn from and may result in models predicting the wrong span.",
                "The paper falls short of delivering a usable dataset for speech-based conversational QA. The authors should consider returning the audio segments as answers instead of using text-based spans. This might require additional annotation but could make the dataset more meaningful and usable.",
                "The use of a knowledge distillation (KD) layer to address the issue of noisy ASR text is questionable. The KD layer shows the model the gold clean transcription (original passage), which could defeat the purpose of speech-based conversational QA as motivated in this paper."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "Lack of Noise Robustness Evaluation: The paper could benefit from an experiment evaluating the robustness of the DDNet to noisy ASR transcriptions. This is a crucial aspect to evaluate given that the DDNet is designed to handle noisy ASR transcriptions. The authors could consider adding an experiment where they artificially add noise to the ASR transcriptions and evaluate the performance of the DDNet.",
                "Lack of Knowledge Distillation Evaluation: The authors discuss the use of knowledge distillation for training models on noisy ASR transcriptions, but it is not clear if they conducted an experiment to evaluate the effectiveness of this approach. The authors could consider adding an experiment where they compare the performance of models trained with and without knowledge distillation.",
                "Lack of Dataset Evaluation: The authors introduce a new Spoken-CoQA dataset, but it is not clear if they conducted an experiment to evaluate the quality of this dataset. The authors could consider adding an experiment where they evaluate the quality of the Spoken-CoQA dataset, for example, by comparing the performance of models trained on the Spoken-CoQA dataset and other similar datasets.",
                "Lack of Detailed Ablation Study: While the authors provide a thorough analysis of different components of their method, it is not clear if they conducted a detailed ablation study to understand the contribution of each component to the overall performance of the DDNet. The authors could consider adding an ablation study where they remove each component of the DDNet one by one and evaluate the performance of the resulting models.",
                "Lack of Detailed Analysis of the Fusion Mechanism: The authors introduce a novel fusion mechanism Con Fusion, but it is not clear if they conducted a detailed analysis to understand the contribution of this mechanism to the overall performance of the DDNet. The authors could consider adding an experiment where they compare the performance of the DDNet with and without the Con Fusion mechanism."
            ],
            "novelty_impact": [
                "The paper should provide more information about the diversity of the Spoken-CoQA dataset in terms of accents, languages, and topics. This is crucial for the generalizability of the model.",
                "The paper should provide more details about how the DDNet handles noise in the audio data, such as how it distinguishes between noise and speech and how it handles different types of noise.",
                "The paper should provide a detailed comparison between the \"novel unified data distillation approach\" and the \"multi-modality fusion mechanism\" and existing methods. This would help readers understand the novelty and advantages of these methods.",
                "The paper should provide a specific definition or measurement for \"misalignment\" between automatic speech recognition hypotheses and reference transcriptions. This is crucial for evaluating the effectiveness of the DDNet.",
                "The paper should provide more details about the baselines, such as their strengths and weaknesses, and why they were chosen as baselines. This would help readers understand the context of the paper's claims of superiority.",
                "The paper should discuss why EM and F1 scores are the most appropriate metrics for this task. If there are other potentially relevant metrics, the paper should discuss why they were not used.",
                "The paper should provide more details about how the new task, Spoken Conversational Question Answering (SCQA), differs from existing tasks in the field. This would help readers understand the novelty and importance of this task.",
                "The paper should provide a detailed justification for using the concept of knowledge distillation in the context of its goals and methods. This would help readers understand why this concept is relevant and useful for the paper's goals and methods."
            ],
            "clarity_reproducibility": [
                "The paper should provide specific steps for the DDNet's data distillation process. This is crucial for reproducibility and for other researchers to evaluate its effectiveness.",
                "The paper should provide specific details about the types of attention layers used in the baseline models and the method used by the Output Layer to compute the probability distribution. These details are essential for understanding the CMRC module's operation and for reproducing it.",
                "The paper should provide a specific learning algorithm or method used for the student model to learn from the teacher model in the Knowledge Distillation method. This information is crucial for understanding the method and for reproducing it.",
                "The paper should provide specific details about the training process and the hyperparameters used for the baseline models in the experimental setup. These details are essential for reproducing the experiments and for evaluating the models' performance.",
                "The paper should provide more details about the Con Fusion mechanism. Without these details, it's difficult for other researchers to understand its benefits or reproduce it."
            ],
            "all": [
                "Lack of Noise Robustness Evaluation: The paper could benefit from an experiment evaluating the robustness of the DDNet to noisy ASR transcriptions. This is a crucial aspect to evaluate given that the DDNet is designed to handle noisy ASR transcriptions. The authors could consider adding an experiment where they artificially add noise to the ASR transcriptions and evaluate the performance of the DDNet.",
                "Lack of Knowledge Distillation Evaluation: The authors discuss the use of knowledge distillation for training models on noisy ASR transcriptions, but it is not clear if they conducted an experiment to evaluate the effectiveness of this approach. The authors could consider adding an experiment where they compare the performance of models trained with and without knowledge distillation.",
                "Lack of Dataset Evaluation: The authors introduce a new Spoken-CoQA dataset, but it is not clear if they conducted an experiment to evaluate the quality of this dataset. The authors could consider adding an experiment where they evaluate the quality of the Spoken-CoQA dataset, for example, by comparing the performance of models trained on the Spoken-CoQA dataset and other similar datasets.",
                "Lack of Detailed Ablation Study: While the authors provide a thorough analysis of different components of their method, it is not clear if they conducted a detailed ablation study to understand the contribution of each component to the overall performance of the DDNet. The authors could consider adding an ablation study where they remove each component of the DDNet one by one and evaluate the performance of the resulting models.",
                "Lack of Detailed Analysis of the Fusion Mechanism: The authors introduce a novel fusion mechanism Con Fusion, but it is not clear if they conducted a detailed analysis to understand the contribution of this mechanism to the overall performance of the DDNet. The authors could consider adding an experiment where they compare the performance of the DDNet with and without the Con Fusion mechanism.",
                "The paper should provide more information about the diversity of the Spoken-CoQA dataset in terms of accents, languages, and topics. This is crucial for the generalizability of the model.",
                "The paper should provide more details about how the DDNet handles noise in the audio data, such as how it distinguishes between noise and speech and how it handles different types of noise.",
                "The paper should provide a detailed comparison between the \"novel unified data distillation approach\" and the \"multi-modality fusion mechanism\" and existing methods. This would help readers understand the novelty and advantages of these methods.",
                "The paper should provide a specific definition or measurement for \"misalignment\" between automatic speech recognition hypotheses and reference transcriptions. This is crucial for evaluating the effectiveness of the DDNet.",
                "The paper should provide more details about the baselines, such as their strengths and weaknesses, and why they were chosen as baselines. This would help readers understand the context of the paper's claims of superiority.",
                "The paper should discuss why EM and F1 scores are the most appropriate metrics for this task. If there are other potentially relevant metrics, the paper should discuss why they were not used.",
                "The paper should provide more details about how the new task, Spoken Conversational Question Answering (SCQA), differs from existing tasks in the field. This would help readers understand the novelty and importance of this task.",
                "The paper should provide a detailed justification for using the concept of knowledge distillation in the context of its goals and methods. This would help readers understand why this concept is relevant and useful for the paper's goals and methods.",
                "The paper should provide specific steps for the DDNet's data distillation process. This is crucial for reproducibility and for other researchers to evaluate its effectiveness.",
                "The paper should provide specific details about the types of attention layers used in the baseline models and the method used by the Output Layer to compute the probability distribution. These details are essential for understanding the CMRC module's operation and for reproducing it.",
                "The paper should provide a specific learning algorithm or method used for the student model to learn from the teacher model in the Knowledge Distillation method. This information is crucial for understanding the method and for reproducing it.",
                "The paper should provide specific details about the training process and the hyperparameters used for the baseline models in the experimental setup. These details are essential for reproducing the experiments and for evaluating the models' performance.",
                "The paper should provide more details about the Con Fusion mechanism. Without these details, it's difficult for other researchers to understand its benefits or reproduce it."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 5,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The reliance of the proposed system on clean text is a significant issue that needs to be addressed.",
                    "predicted_comment": "Lack of Noise Robustness Evaluation: The paper could benefit from an experiment evaluating the robustness of the DDNet to noisy ASR transcriptions. This is a crucial aspect to evaluate given that the DDNet is designed to handle noisy ASR transcriptions. The authors could consider adding an experiment where they artificially add noise to the ASR transcriptions and evaluate the performance of the DDNet."
                }
            ],
            "novelty_impact": [
                {
                    "reference_id": 0,
                    "predicted_id": 7,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The work is limited in its scope and is not yet ready to be published at ICLR due to lack of comparisons made to prior approaches, making it hard to assess the merits of the proposed approach.",
                    "predicted_comment": "The paper should provide a detailed comparison between the \"novel unified data distillation approach\" and the \"multi-modality fusion mechanism\" and existing methods. This would help readers understand the novelty and advantages of these methods."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 7,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper's contributions and novelty are unclear, and it's uncertain if they warrant acceptance.",
                    "predicted_comment": "The paper should provide a detailed comparison between the \"novel unified data distillation approach\" and the \"multi-modality fusion mechanism\" and existing methods. This would help readers understand the novelty and advantages of these methods."
                }
            ],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 0,
                    "predicted_id": 7,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The work is limited in its scope and is not yet ready to be published at ICLR due to lack of comparisons made to prior approaches, making it hard to assess the merits of the proposed approach.",
                    "predicted_comment": "The paper should provide a detailed comparison between the \"novel unified data distillation approach\" and the \"multi-modality fusion mechanism\" and existing methods. This would help readers understand the novelty and advantages of these methods."
                },
                {
                    "reference_id": 5,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The reliance of the proposed system on clean text is a significant issue that needs to be addressed.",
                    "predicted_comment": "Lack of Noise Robustness Evaluation: The paper could benefit from an experiment evaluating the robustness of the DDNet to noisy ASR transcriptions. This is a crucial aspect to evaluate given that the DDNet is designed to handle noisy ASR transcriptions. The authors could consider adding an experiment where they artificially add noise to the ASR transcriptions and evaluate the performance of the DDNet."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 7,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper's contributions and novelty are unclear, and it's uncertain if they warrant acceptance.",
                    "predicted_comment": "The paper should provide a detailed comparison between the \"novel unified data distillation approach\" and the \"multi-modality fusion mechanism\" and existing methods. This would help readers understand the novelty and advantages of these methods."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 18,
                "n_generated": 5,
                "n_total_aligns": 1,
                "recall": 0.041666666666666664,
                "precision": 0.05,
                "pseudo_jaccard": 0.025,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 18,
                "n_generated": 8,
                "n_total_aligns": 2,
                "recall": 0.09166666666666667,
                "precision": 0.0625,
                "pseudo_jaccard": 0.04006410256410256,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 18,
                "n_generated": 5,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 18,
                "n_generated": 18,
                "n_total_aligns": 3,
                "recall": 0.13333333333333333,
                "precision": 0.041666666666666664,
                "pseudo_jaccard": 0.03409090909090909,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.16666666666666666,
                    "precision": 0.2,
                    "pseudo_jaccard": 0.1,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.16666666666666666,
                    "precision": 0.125,
                    "pseudo_jaccard": 0.07692307692307693,
                    "n_real": 6
                },
                {
                    "recall": 0.2,
                    "precision": 0.125,
                    "pseudo_jaccard": 0.08333333333333333,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.1111111111111111,
                    "pseudo_jaccard": 0.09090909090909091,
                    "n_real": 6
                },
                {
                    "recall": 0.2,
                    "precision": 0.05555555555555555,
                    "pseudo_jaccard": 0.045454545454545456,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "0NQdxInFWT_",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The method is a small extension to the DPS method where the network that selects which rows to samples is conditioned on the existing measurements.",
                "The paper did not compare to any other active sampling strategies. The authors made no effort to replicate existing methods.",
                "The Markov chain example in section 4.1 was hard to follow and more distracting than informative.",
                "The results with active sampling were only marginally better than results with a fixed (learned) sampling strategy.",
                "The method is applicable only to true subsampling problems, not general sensing. That is, one isn't designing the rows of a measurement matrix on the fly but rather selecting which row from an existing matrix (identity in most of the examples) that one would like to sample from.",
                "Because of the nonlinearity in the forward model, equation (9) is not actually proximal gradient descent. There's a sign(F^HD\\circFX) term missing from the (sub) gradient.",
                "The expression A^H(Ax-Y\\circ sign(A(x))) is a subgradient of 1/2|| Y - |A(X)|||^2 but A^H(|Ax|-Y) is not. I would avoid calling (9) projected gradient descent as the 'gradient' isn't really a gradient."
            ],
            [
                "The proposed approach appears to be incremental: ADPS adds a simple sequential update structure (of a context vector) to DPS, which can be described by only two equations (6 and 7). The simplicity of the changes proposed (over DPS) is not a limitation, but it could be accompanied by an in-depth theoretical analysis, a convincing qualitative discussion or extensive experiments demonstrating the practical relevance of the proposed approach.",
                "The paper contributes by extending DPS into an active/sequential learning framework achieving significant performance gains over DPS, mainly on toy problems. However, there is room for more extensive experimental validation, perhaps beyond MRI."
            ],
            [
                "The paper lacks comparison to other active sub-sampling schemes, such as those proposed by Zhang et al., 2019 and Jin et al., 2019. It would be beneficial to see if the proposed method produces better results than these existing methods.",
                "The toy example provided is overly constructed and difficult to understand, which does not improve the quality of the paper.",
                "It is unclear what happens when the MNIST sampling ratio in Figure 3a is further increased. It would be helpful to know if A-DPS consistently outperforms DPS in low sampling ratio regimes."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "While the paper has conducted tests on a toy example, the MNIST database, and the NYU fastMRI database, it would be beneficial to test the A-DPS network on a wider variety of tasks and datasets. This would provide a more comprehensive evaluation of the network's performance and its ability to generalize to different types of data and tasks.",
                "The ablation study on the LSTM provides valuable insights into its role in the A-DPS network. However, it would be beneficial to conduct further ablation studies on other components of the network to fully understand their individual contributions to the network's performance. In particular, conducting the ablation study on all the tasks (toy example, MNIST classification, MRI reconstruction) would help to understand how LSTM contributes to the performance of A-DPS across different tasks.",
                "The discussion on the computational complexity of A-DPS and the comparison of DPS and A-DPS in terms of training time per epoch provide useful insights into the computational efficiency of the network. However, a more detailed analysis of the computational complexity, including the inference time and memory usage, would provide a more comprehensive understanding of the network's computational efficiency. Additionally, it would be useful to compare the computational complexity of A-DPS not only to DPS but also to other state-of-the-art methods in the field."
            ],
            "novelty_impact": [
                "The paper presents a novel method, A-DPS, which is an extension of the DPS method. The motivation behind this work is clear and relevant, aiming to improve the efficiency of data acquisition in various fields by reducing the number of sample acquisitions needed. However, the paper does not provide evidence or theoretical discussion on how A-DPS could be adapted to fields other than MRI. This limits the generalizability of the method and should be addressed in future work.",
                "The authors do not discuss the sensitivity of their method to changes in the data distribution or strategies for dealing with such changes. This could potentially affect the performance of the model in practice. Future work should address this point to strengthen the robustness of the method.",
                "The potential suboptimality of the current conditioning of the sampling scheme is not discussed, which could affect the reliability and generalizability of the results. The authors should address this point in future work.",
                "The method is not tested on more complex and realistic datasets, which limits the understanding of how the method performs in practice. Future work should include testing on more complex and realistic datasets to validate the effectiveness of the method in real-world scenarios.",
                "The authors do not provide more detail on the trade-off between computational complexity and adaptation rate. This makes it difficult to assess the practicality of the method. Future work should provide more detail on this trade-off to help readers understand the practical implications of using the A-DPS method.",
                "The paper does not provide specific statistics to quantify the improvement of A-DPS over other methods. This makes it difficult to assess the practical significance of the claimed improvements. Future work should include specific statistics to quantify the improvement of A-DPS over other methods.",
                "The authors do not discuss potential biases in the NYU fastMRI database and how it might affect the generalizability of their results. This could potentially bias the results. Future work should address this point to strengthen the reliability and generalizability of the results.",
                "The authors do not discuss specific assumptions about the LSTM model used in the A-DPS method. This could limit the applicability of the method to datasets where these assumptions do not hold. Future work should address this point to strengthen the robustness of the method."
            ],
            "clarity_reproducibility": [
                "The paper should provide more specific details about the Active Deep Probabilistic Subsampling (A-DPS) method, including how it actively picks the next sample based on the information acquired so far. More information about the context vector and the output of the task model is needed.",
                "The paper should provide more specific details about the \"challenging toy problem\", including how the task model determines which elements are informative and how it observes one color channel out of every three.",
                "The paper should provide more specific details about the training of the A-DPS network, including the size of the training and validation datasets.",
                "The paper should provide more specific details about the experiments with the MNIST database and MRI data, including the sampling rate for the MNIST database and how the original image was estimated from the partial measurements in the MRI data.",
                "The paper should provide more specific details about the computational complexity of A-DPS, including what is meant by \"unrolling of iterations\" and how the sampling ratio \"\u03c1\" is determined.",
                "The paper should provide more specific details about the ablation study, including why replacing the LSTM with a fully connected layer resulted in unstable training.",
                "The paper should provide information about the reproducibility of the experiments, including the specific equipment, software, or versions used.",
                "The paper should discuss any limitations or potential drawbacks of the A-DPS method."
            ],
            "all": [
                "While the paper has conducted tests on a toy example, the MNIST database, and the NYU fastMRI database, it would be beneficial to test the A-DPS network on a wider variety of tasks and datasets. This would provide a more comprehensive evaluation of the network's performance and its ability to generalize to different types of data and tasks.",
                "The ablation study on the LSTM provides valuable insights into its role in the A-DPS network. However, it would be beneficial to conduct further ablation studies on other components of the network to fully understand their individual contributions to the network's performance. In particular, conducting the ablation study on all the tasks (toy example, MNIST classification, MRI reconstruction) would help to understand how LSTM contributes to the performance of A-DPS across different tasks.",
                "The discussion on the computational complexity of A-DPS and the comparison of DPS and A-DPS in terms of training time per epoch provide useful insights into the computational efficiency of the network. However, a more detailed analysis of the computational complexity, including the inference time and memory usage, would provide a more comprehensive understanding of the network's computational efficiency. Additionally, it would be useful to compare the computational complexity of A-DPS not only to DPS but also to other state-of-the-art methods in the field.",
                "The paper presents a novel method, A-DPS, which is an extension of the DPS method. The motivation behind this work is clear and relevant, aiming to improve the efficiency of data acquisition in various fields by reducing the number of sample acquisitions needed. However, the paper does not provide evidence or theoretical discussion on how A-DPS could be adapted to fields other than MRI. This limits the generalizability of the method and should be addressed in future work.",
                "The authors do not discuss the sensitivity of their method to changes in the data distribution or strategies for dealing with such changes. This could potentially affect the performance of the model in practice. Future work should address this point to strengthen the robustness of the method.",
                "The potential suboptimality of the current conditioning of the sampling scheme is not discussed, which could affect the reliability and generalizability of the results. The authors should address this point in future work.",
                "The method is not tested on more complex and realistic datasets, which limits the understanding of how the method performs in practice. Future work should include testing on more complex and realistic datasets to validate the effectiveness of the method in real-world scenarios.",
                "The authors do not provide more detail on the trade-off between computational complexity and adaptation rate. This makes it difficult to assess the practicality of the method. Future work should provide more detail on this trade-off to help readers understand the practical implications of using the A-DPS method.",
                "The paper does not provide specific statistics to quantify the improvement of A-DPS over other methods. This makes it difficult to assess the practical significance of the claimed improvements. Future work should include specific statistics to quantify the improvement of A-DPS over other methods.",
                "The authors do not discuss potential biases in the NYU fastMRI database and how it might affect the generalizability of their results. This could potentially bias the results. Future work should address this point to strengthen the reliability and generalizability of the results.",
                "The authors do not discuss specific assumptions about the LSTM model used in the A-DPS method. This could limit the applicability of the method to datasets where these assumptions do not hold. Future work should address this point to strengthen the robustness of the method.",
                "The paper should provide more specific details about the Active Deep Probabilistic Subsampling (A-DPS) method, including how it actively picks the next sample based on the information acquired so far. More information about the context vector and the output of the task model is needed.",
                "The paper should provide more specific details about the \"challenging toy problem\", including how the task model determines which elements are informative and how it observes one color channel out of every three.",
                "The paper should provide more specific details about the training of the A-DPS network, including the size of the training and validation datasets.",
                "The paper should provide more specific details about the experiments with the MNIST database and MRI data, including the sampling rate for the MNIST database and how the original image was estimated from the partial measurements in the MRI data.",
                "The paper should provide more specific details about the computational complexity of A-DPS, including what is meant by \"unrolling of iterations\" and how the sampling ratio \"\u03c1\" is determined.",
                "The paper should provide more specific details about the ablation study, including why replacing the LSTM with a fully connected layer resulted in unstable training.",
                "The paper should provide information about the reproducibility of the experiments, including the specific equipment, software, or versions used.",
                "The paper should discuss any limitations or potential drawbacks of the A-DPS method."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 1,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper contributes by extending DPS into an active/sequential learning framework achieving significant performance gains over DPS, mainly on toy problems. However, there is room for more extensive experimental validation, perhaps beyond MRI.",
                    "predicted_comment": "While the paper has conducted tests on a toy example, the MNIST database, and the NYU fastMRI database, it would be beneficial to test the A-DPS network on a wider variety of tasks and datasets. This would provide a more comprehensive evaluation of the network's performance and its ability to generalize to different types of data and tasks."
                }
            ],
            "novelty_impact": [
                {
                    "reference_id": 0,
                    "predicted_id": 3,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The proposed approach appears to be incremental: ADPS adds a simple sequential update structure (of a context vector) to DPS, which can be described by only two equations (6 and 7). The simplicity of the changes proposed (over DPS) is not a limitation, but it could be accompanied by an in-depth theoretical analysis, a convincing qualitative discussion or extensive experiments demonstrating the practical relevance of the proposed approach.",
                    "predicted_comment": "The paper presents a novel method, A-DPS, which is an extension of the DPS method. The motivation behind this work is clear and relevant, aiming to improve the efficiency of data acquisition in various fields by reducing the number of sample acquisitions needed. However, the paper does not provide evidence or theoretical discussion on how A-DPS could be adapted to fields other than MRI. This limits the generalizability of the method and should be addressed in future work."
                }
            ],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 1,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper contributes by extending DPS into an active/sequential learning framework achieving significant performance gains over DPS, mainly on toy problems. However, there is room for more extensive experimental validation, perhaps beyond MRI.",
                    "predicted_comment": "While the paper has conducted tests on a toy example, the MNIST database, and the NYU fastMRI database, it would be beneficial to test the A-DPS network on a wider variety of tasks and datasets. This would provide a more comprehensive evaluation of the network's performance and its ability to generalize to different types of data and tasks."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 3,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The proposed approach appears to be incremental: ADPS adds a simple sequential update structure (of a context vector) to DPS, which can be described by only two equations (6 and 7). The simplicity of the changes proposed (over DPS) is not a limitation, but it could be accompanied by an in-depth theoretical analysis, a convincing qualitative discussion or extensive experiments demonstrating the practical relevance of the proposed approach.",
                    "predicted_comment": "The paper presents a novel method, A-DPS, which is an extension of the DPS method. The motivation behind this work is clear and relevant, aiming to improve the efficiency of data acquisition in various fields by reducing the number of sample acquisitions needed. However, the paper does not provide evidence or theoretical discussion on how A-DPS could be adapted to fields other than MRI. This limits the generalizability of the method and should be addressed in future work."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 12,
                "n_generated": 3,
                "n_total_aligns": 1,
                "recall": 0.16666666666666666,
                "precision": 0.1111111111111111,
                "pseudo_jaccard": 0.08333333333333333,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 12,
                "n_generated": 8,
                "n_total_aligns": 1,
                "recall": 0.16666666666666666,
                "precision": 0.041666666666666664,
                "pseudo_jaccard": 0.037037037037037035,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 12,
                "n_generated": 8,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 12,
                "n_generated": 19,
                "n_total_aligns": 2,
                "recall": 0.3333333333333333,
                "precision": 0.03508771929824561,
                "pseudo_jaccard": 0.03508771929824561,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.5,
                    "precision": 0.3333333333333333,
                    "pseudo_jaccard": 0.25,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.5,
                    "precision": 0.125,
                    "pseudo_jaccard": 0.1111111111111111,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 1.0,
                    "precision": 0.10526315789473684,
                    "pseudo_jaccard": 0.10526315789473684,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "-spj8FZD4y2",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper lacks a complete theoretical analysis, such as the upper bound/lower bound on the regret of the algorithm.",
                "The algorithm should be explicitly written out with its pseudocode.",
                "The paper states that the communication is one-way from the decision-maker towards the controller, but it is strange not to consider the realization history observed by the decision-maker as communication. The goal of this work and how it differs from existing papers is unclear.",
                "The problem setup is unclear: it is not specified whether the state is i.i.d. sampled from the distribution $P_S$ or not.",
                "The relation between the constraint quantities $(\\rho,R)$ and the discussions in Section 3 is unclear.",
                "The constraints seem to be placed in the posterior probabilities involved in the TS algorithms. It is not clear what would happen if we consider UCB-class algorithms.",
                "The definitions of $D_\\alpha$ and reverse KL divergence $D_{KL}$ are missing. Usually, the original KL divergence is denoted by $D_{KL}$.",
                "In Section 3.3, it states ''If $S$ and $A$ are independent...'', which is not a common assumption in contextual bandits."
            ],
            [
                "There are inconsistencies and imprecisions in the writing, particularly in Section 2, which defines quantities, regrets, and gains that are not analyzed afterwards. Clarification is needed on the definitions and usage of G, \\rho, and R.",
                "The paper does not provide regret bounds. While it may be out of the scope of the paper, some commentary on this would be beneficial.",
                "The algorithm needs to be clearly outlined. While detailed explanations are provided, a pseudo-code would greatly improve readability and understanding.",
                "The paper could be better connected with existing literature on contextual bandit, such as contextual bandits with context uncertainties or Partially Observable MDPs."
            ],
            [
                "The results in the paper are largely adapted from known information-theoretic results, which limits the overall novelty of the work.",
                "The assumption that each agent's state is observed by the decision-maker, but not the agent itself, is unusual. The authors should clarify why they make this assumption.",
                "The practical coding scheme is about sending the state to the corresponding agent, which may not be necessary in practice.",
                "The controller 'decoder' function should have the joint states of all agents as an input to enhance the decoder design."
            ],
            [
                "The paper lacks theoretical performance guarantees for the clustering coding scheme. A characterization of how regret depends on the number of states, the rate constraint, number of actions, etc. would provide a more in depth understanding of the performances of the clustering scheme in various scenarios.",
                "The paper does not discuss how the proposed clustering schemes\u2019 performance decays as the total number of states grows. More states would induce larger clusters and hence larger distortion in the policy defined over the compressed states (compared to the target policy). This is particularly relevant as the paper uses personalized ad placements as a motivating example, where the content owner\u2019s valuation for a user view (i.e. impression) or conversion serves as the 'context' (or state), and may take a wide range of values."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper lacks a clear comparison between the performance of the policy obtained through the asymptotic information theoretic formulation and the one obtained through the clustering scheme. This comparison is crucial to validate the effectiveness of the proposed methods and to understand their relative strengths and weaknesses. It is suggested that the authors conduct an experiment where the performance of the policy obtained through the asymptotic information theoretic formulation is directly compared with the performance of the policy obtained through the clustering scheme. The metrics for comparison could be the regret and the rate of data transmission.",
                "The paper lacks a clear analysis of the relation between the asymptotic rate bound and the learning phase of agents. Understanding this relation is crucial to understand how to save communication resources when training a multi-agent system. It is suggested that the authors conduct an experiment where the relation between the asymptotic rate bound and the learning phase of agents is analyzed. The metrics for analysis could be the regret and the rate of data transmission.",
                "In both cases, the experiments should be designed in such a way that the results can be compared with theoretical predictions. This will help to validate the accuracy of the theoretical analysis and the effectiveness of the proposed methods."
            ],
            "novelty_impact": [
                "The paper provides a clear definition of the Contextual Multi-Armed Bandit (CMAB) problem and introduces a novel Rate-Constrained CMAB (RC-CMAB) problem. However, it would be beneficial to provide a more detailed comparison of the RC-CMAB problem with the standard CMAB problem, particularly highlighting the specific constraints on the communication rate.",
                "The paper proposes a practical coding scheme based on state reduction and uses the Lloyd algorithm to group states into clusters. However, the paper does not explicitly compare this scheme with other existing coding schemes. Including such a comparison in future work would help readers assess the relative performance and advantages of the proposed scheme.",
                "The paper does not discuss the reliability of the communication channel or the impact of errors or losses in communication. Considering these factors in future work could increase the applicability of the results in real-world scenarios where communication errors are common.",
                "The paper makes several strong assumptions, such as a limited communication link between the decision-maker and the controller, a uniform state distribution, and the best action response not being a one-to-one mapping with the state. Discussing potential extensions or modifications of the proposed methods for scenarios where these assumptions do not hold would be beneficial. This would help readers understand how the proposed methods perform under different conditions and increase the applicability of the results.",
                "The paper provides a clear discussion of the use of the KL-divergence in a rate-distortion optimization problem and the application of the rate-distortion function in three different problems. However, it would be beneficial to provide more details on how the KL-divergence is defined and used in this context, particularly for readers who are not familiar with this measure.",
                "The paper applies the clustered policy schemes to solve the RC-CMAB problem and compares the performance of different agents. However, it would be beneficial to provide more details on the criteria for this comparison and whether any benchmarks or standard measures are used.",
                "The paper provides a clear discussion of the relation between the asymptotic rate bound and the learning phase of agents. However, it would be beneficial to provide more empirical evidence or theoretical proof to support this claim.",
                "The paper concludes by comparing the performances of the policy obtained through the asymptotic information theoretic formulation and the one obtained through the clustering scheme. However, it would be beneficial to provide more quantitative results or statistical tests to support this comparison.",
                "Overall, the paper presents a novel approach to the CMAB problem and provides a clear discussion of the proposed methods. However, considering the above suggestions could further improve the paper and increase its impact."
            ],
            "clarity_reproducibility": [],
            "all": [
                "The paper lacks a clear comparison between the performance of the policy obtained through the asymptotic information theoretic formulation and the one obtained through the clustering scheme. This comparison is crucial to validate the effectiveness of the proposed methods and to understand their relative strengths and weaknesses. It is suggested that the authors conduct an experiment where the performance of the policy obtained through the asymptotic information theoretic formulation is directly compared with the performance of the policy obtained through the clustering scheme. The metrics for comparison could be the regret and the rate of data transmission.",
                "The paper lacks a clear analysis of the relation between the asymptotic rate bound and the learning phase of agents. Understanding this relation is crucial to understand how to save communication resources when training a multi-agent system. It is suggested that the authors conduct an experiment where the relation between the asymptotic rate bound and the learning phase of agents is analyzed. The metrics for analysis could be the regret and the rate of data transmission.",
                "In both cases, the experiments should be designed in such a way that the results can be compared with theoretical predictions. This will help to validate the accuracy of the theoretical analysis and the effectiveness of the proposed methods.",
                "The paper provides a clear definition of the Contextual Multi-Armed Bandit (CMAB) problem and introduces a novel Rate-Constrained CMAB (RC-CMAB) problem. However, it would be beneficial to provide a more detailed comparison of the RC-CMAB problem with the standard CMAB problem, particularly highlighting the specific constraints on the communication rate.",
                "The paper proposes a practical coding scheme based on state reduction and uses the Lloyd algorithm to group states into clusters. However, the paper does not explicitly compare this scheme with other existing coding schemes. Including such a comparison in future work would help readers assess the relative performance and advantages of the proposed scheme.",
                "The paper does not discuss the reliability of the communication channel or the impact of errors or losses in communication. Considering these factors in future work could increase the applicability of the results in real-world scenarios where communication errors are common.",
                "The paper makes several strong assumptions, such as a limited communication link between the decision-maker and the controller, a uniform state distribution, and the best action response not being a one-to-one mapping with the state. Discussing potential extensions or modifications of the proposed methods for scenarios where these assumptions do not hold would be beneficial. This would help readers understand how the proposed methods perform under different conditions and increase the applicability of the results.",
                "The paper provides a clear discussion of the use of the KL-divergence in a rate-distortion optimization problem and the application of the rate-distortion function in three different problems. However, it would be beneficial to provide more details on how the KL-divergence is defined and used in this context, particularly for readers who are not familiar with this measure.",
                "The paper applies the clustered policy schemes to solve the RC-CMAB problem and compares the performance of different agents. However, it would be beneficial to provide more details on the criteria for this comparison and whether any benchmarks or standard measures are used.",
                "The paper provides a clear discussion of the relation between the asymptotic rate bound and the learning phase of agents. However, it would be beneficial to provide more empirical evidence or theoretical proof to support this claim.",
                "The paper concludes by comparing the performances of the policy obtained through the asymptotic information theoretic formulation and the one obtained through the clustering scheme. However, it would be beneficial to provide more quantitative results or statistical tests to support this comparison.",
                "Overall, the paper presents a novel approach to the CMAB problem and provides a clear discussion of the proposed methods. However, considering the above suggestions could further improve the paper and increase its impact."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [],
            "clarity_reproducibility": [],
            "all": []
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 18,
                "n_generated": 3,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 18,
                "n_generated": 9,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "clarity_reproducibility": {
                "n_reference": 18,
                "n_generated": 0,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 18,
                "n_generated": 12,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                }
            ]
        }
    },
    {
        "doc_id": "7udZAsEzd60",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The implication of Theorem 4.2 is not fully understood. The size(rho, kappa) critically depends on r, the rank of the weight matrix W. But if we decompose W=UV, where U is d*r and V=r*k, it seems like the VC dimension of HANN is still higher than the number of effective parameters, dr+rk. In this case, what benefit do HANN have over ReLU and sigmoid networks?",
                "The role of the binary activation in HANN is unclear. The critical assumption of Theorem 4.2 is that the weight matrix is low-rank. In this case, can't the layer with low-rank weight be reformulated as two layers with a small bottleneck hidden layer of dimensionality r, which has much smaller number of parameters than the equivalent network? If this is the case, the result seems to be only relevant to the low-rank assumption, not the HANN itself."
            ],
            [
                "The title and introduction of the paper suggest that a more general class of Quantized Neural Networks (QNNs) have small VC dimension, but the model studied is essentially a two-layer NN. This raises doubts about whether any non-toy quantized models will have small VC dimension. An experiment similar to Zhang et al 2017 could easily verify this.",
                "The paper's focus on studying the VC dimension of Neural Networks (NNs) is questionable. The authors should clarify what we are aiming to learn from this, as in practice VC dimensions often provide non-meaningful bounds.",
                "The title and introduction of the paper may overstate the significance of the results, and the motivation for the study may not be strong enough."
            ],
            [
                "The bound on the VC dimension for hyperplane augmented neural networks (HANNs) seems quite large: O(k^r). This seems to be a major limitation in the application of the main result of the paper.",
                "The paper could significantly benefit from a discussion on the large bound on the VC dimension, either explaining more cases where the results are useful or clarifying this limitation.",
                "Using the numbers given in the experiments, with a dataset of size 77904 and k=100, and r=3 seems to result in vacuous bounds on generalization again, which questions the effectiveness of the main result."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "Lack of Generalization Testing: The paper could benefit from including experiments testing the generalization of the neural networks on unseen data. This is a key aspect of machine learning and would strengthen the paper's claims about the performance of HANNs.",
                "Insufficient Testing of VC Dimension and Expressiveness: The paper could provide more empirical evidence to support the claims about the VC dimension and expressiveness of the HANNs. While the theoretical proofs are strong, empirical tests would provide additional validation.",
                "No Empirical Validation of Theoretical Claims: While the paper provides mathematical proofs for many of its claims, empirical tests to validate these claims would strengthen the paper. The authors could consider including such tests in future work.",
                "Lack of Comparison with State-of-the-Art Models: The paper could provide more comparisons of the performance of the HANNs with state-of-the-art models. This would help to benchmark the new method against existing ones and demonstrate its effectiveness.",
                "Insufficient Testing of Training Heuristics and Hyperparameters: The paper could include more experiments to test the effectiveness of different training heuristics and the impact of the hyperparameter 'k'. These are important aspects of the model's performance and should be thoroughly tested."
            ],
            "novelty_impact": [
                "The paper presents a novel approach to understanding the generalization error of overparametrized neural networks using hyperplane arrangement neural networks (HANNs). This is a significant contribution to the field, as it provides a new perspective on a complex problem.",
                "The motivations and goals of the paper are clearly explained. The authors aim to address the limitations of Vapnik-Chervonenkis (VC) theory in explaining the small generalization error of overparametrized neural networks, and they propose HANNs as a solution.",
                "The key findings of the paper are well-presented. The authors demonstrate that HANNs can have a VC dimension significantly smaller than the number of weights, while still being highly expressive. They also show that empirical risk minimization over HANNs in the overparametrized regime achieves the minimax rate for classification with Lipschitz posterior class probability.",
                "The paper fits well into the related literature in the field. The authors build on existing work on VC theory and overparametrized neural networks, and they expand on this work in a meaningful way by introducing HANNs.",
                "However, there are several areas where the paper could be improved:\n\n   - The authors should provide a clear definition or measure of expressiveness for HANNs to strengthen their argument.\n   - The authors should provide some reasoning or hypothesis to support their findings that SwishSign outperforms SteSign.\n   - The authors should include a thorough statistical analysis to support their claim that HANN models achieved comparable accuracy to other neural networks with fewer weights on 121 UCI datasets.\n   - The authors should discuss any potential limitations or biases associated with the use of the VC theorem to ensure a balanced and comprehensive analysis.\n   - The authors should provide more information about the specific features or characteristics of the HANN15 model to help readers understand its relevance and significance in the study.\n   - The authors should include a section discussing the limitations of their study to provide a balanced view of their work."
            ],
            "clarity_reproducibility": [
                "The paper could provide a more detailed explanation of the Vapnik-Chervonenkis (VC) theory and its limitations in explaining the small generalization error of overparametrized neural networks. This would help readers who are not familiar with VC theory to better understand the context and significance of the paper's findings.",
                "The proof of the theorem involving the construction of a convex polyhedron in a (d + 1)k-dimensional space could be explained in a more intuitive way. The current explanation is quite technical and may be difficult for readers to follow.",
                "The paper could provide more details about the heuristics for training networks with threshold activation, specifically the straight-through-estimator (SteSign) and the SwishSign. This would help readers understand why SwishSign was chosen for the experiments.",
                "The paper could provide more information about the probability function R(fn) and its relationship with other variables. This would help readers understand the mathematical part of the paper.",
                "The paper could provide more details about the data preprocessing using the StandardScaler function from sklearn and how the dropout rate is tuned in the hyperparameter grid of size 3. This would improve the reproducibility of the experiments."
            ],
            "all": [
                "Lack of Generalization Testing: The paper could benefit from including experiments testing the generalization of the neural networks on unseen data. This is a key aspect of machine learning and would strengthen the paper's claims about the performance of HANNs.",
                "Insufficient Testing of VC Dimension and Expressiveness: The paper could provide more empirical evidence to support the claims about the VC dimension and expressiveness of the HANNs. While the theoretical proofs are strong, empirical tests would provide additional validation.",
                "No Empirical Validation of Theoretical Claims: While the paper provides mathematical proofs for many of its claims, empirical tests to validate these claims would strengthen the paper. The authors could consider including such tests in future work.",
                "Lack of Comparison with State-of-the-Art Models: The paper could provide more comparisons of the performance of the HANNs with state-of-the-art models. This would help to benchmark the new method against existing ones and demonstrate its effectiveness.",
                "Insufficient Testing of Training Heuristics and Hyperparameters: The paper could include more experiments to test the effectiveness of different training heuristics and the impact of the hyperparameter 'k'. These are important aspects of the model's performance and should be thoroughly tested.",
                "The paper presents a novel approach to understanding the generalization error of overparametrized neural networks using hyperplane arrangement neural networks (HANNs). This is a significant contribution to the field, as it provides a new perspective on a complex problem.",
                "The motivations and goals of the paper are clearly explained. The authors aim to address the limitations of Vapnik-Chervonenkis (VC) theory in explaining the small generalization error of overparametrized neural networks, and they propose HANNs as a solution.",
                "The key findings of the paper are well-presented. The authors demonstrate that HANNs can have a VC dimension significantly smaller than the number of weights, while still being highly expressive. They also show that empirical risk minimization over HANNs in the overparametrized regime achieves the minimax rate for classification with Lipschitz posterior class probability.",
                "The paper fits well into the related literature in the field. The authors build on existing work on VC theory and overparametrized neural networks, and they expand on this work in a meaningful way by introducing HANNs.",
                "However, there are several areas where the paper could be improved:\n\n   - The authors should provide a clear definition or measure of expressiveness for HANNs to strengthen their argument.\n   - The authors should provide some reasoning or hypothesis to support their findings that SwishSign outperforms SteSign.\n   - The authors should include a thorough statistical analysis to support their claim that HANN models achieved comparable accuracy to other neural networks with fewer weights on 121 UCI datasets.\n   - The authors should discuss any potential limitations or biases associated with the use of the VC theorem to ensure a balanced and comprehensive analysis.\n   - The authors should provide more information about the specific features or characteristics of the HANN15 model to help readers understand its relevance and significance in the study.\n   - The authors should include a section discussing the limitations of their study to provide a balanced view of their work.",
                "The paper could provide a more detailed explanation of the Vapnik-Chervonenkis (VC) theory and its limitations in explaining the small generalization error of overparametrized neural networks. This would help readers who are not familiar with VC theory to better understand the context and significance of the paper's findings.",
                "The proof of the theorem involving the construction of a convex polyhedron in a (d + 1)k-dimensional space could be explained in a more intuitive way. The current explanation is quite technical and may be difficult for readers to follow.",
                "The paper could provide more details about the heuristics for training networks with threshold activation, specifically the straight-through-estimator (SteSign) and the SwishSign. This would help readers understand why SwishSign was chosen for the experiments.",
                "The paper could provide more information about the probability function R(fn) and its relationship with other variables. This would help readers understand the mathematical part of the paper.",
                "The paper could provide more details about the data preprocessing using the StandardScaler function from sklearn and how the dropout rate is tuned in the hyperparameter grid of size 3. This would improve the reproducibility of the experiments."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [],
            "clarity_reproducibility": [
                {
                    "reference_id": 1,
                    "predicted_id": 10,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper could significantly benefit from a discussion on the large bound on the VC dimension, either explaining more cases where the results are useful or clarifying this limitation.",
                    "predicted_comment": "The paper could provide a more detailed explanation of the Vapnik-Chervonenkis (VC) theory and its limitations in explaining the small generalization error of overparametrized neural networks. This would help readers who are not familiar with VC theory to better understand the context and significance of the paper's findings."
                }
            ],
            "all": [
                {
                    "reference_id": 1,
                    "predicted_id": 10,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper could significantly benefit from a discussion on the large bound on the VC dimension, either explaining more cases where the results are useful or clarifying this limitation.",
                    "predicted_comment": "The paper could provide a more detailed explanation of the Vapnik-Chervonenkis (VC) theory and its limitations in explaining the small generalization error of overparametrized neural networks. This would help readers who are not familiar with VC theory to better understand the context and significance of the paper's findings."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 8,
                "n_generated": 5,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 8,
                "n_generated": 5,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "clarity_reproducibility": {
                "n_reference": 8,
                "n_generated": 5,
                "n_total_aligns": 1,
                "recall": 0.1111111111111111,
                "precision": 0.06666666666666667,
                "pseudo_jaccard": 0.047619047619047616,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "all": {
                "n_reference": 8,
                "n_generated": 15,
                "n_total_aligns": 1,
                "recall": 0.1111111111111111,
                "precision": 0.022222222222222223,
                "pseudo_jaccard": 0.0196078431372549,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.2,
                    "pseudo_jaccard": 0.14285714285714285,
                    "n_real": 3
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.06666666666666667,
                    "pseudo_jaccard": 0.058823529411764705,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "QmKblFEgQJ",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The empirical performance gains provided by the DIGRAC method are difficult to understand. In many of the ARI graphs on synthetic data, the performance of various methods seems quite close. In the real data, methods overlap in their error bars.",
                "More explanation is needed about when DIGRAC's model for clustering is appropriate for a dataset and when it is not, and how adaptive the model can be in terms of performance despite those considerations."
            ],
            [
                "The practicality of the problem and the method studied in this paper is questionable. The authors mention that 'real data sets with ground-truth flow imbalances are not available to date', so they use normalized imbalance scores to evaluate clustering performance. This approach seems to go against the typical order of how machine learning models are developed.",
                "The experiments in Section 4 do not demonstrate the practical importance of clustering with respect to flow imbalances. As this is an empirical paper, it is necessary to provide solid empirical evidence that showcases the need for clustering with respect to flow imbalances.",
                "The authors use 80% of all nodes for training in their empirical evaluations. This percentage seems too high and it is suggested to consider using a smaller percentage, such as 5%, of all nodes for training."
            ],
            [
                "The theoretical properties of the loss function defined in Eq. (2) and (3) are not clear. The paper should provide a theoretical characterization for this loss and discuss cases where it could lead to degenerate solutions.",
                "The constraints of the optimization problem need to be clearly presented.",
                "The paper should examine how the loss behaves in terms of convergence.",
                "The novelty of the DIMPA algorithm is unclear. The paper should clarify what are the novel parts of this model with respect to previous works.",
                "The baseline models used for comparison could be enhanced. The paper should consider adding models that capture information flow, including algorithms that rely on stochastic blockmodels and a model that is based on directed modularity.",
                "The paper lacks scalability experiments with the running time of the algorithm and its comparison to baseline models."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "Ablation Studies: The paper would benefit from ablation studies to evaluate the individual contributions of the novel components of the method, such as the probabilistic imbalance loss and the directed mixed path aggregation scheme (DIMPA). This could involve running experiments where one component is removed or modified at a time, and the impact on the results is observed. This would help to understand the importance of each component and how they contribute to the overall performance.",
                "Incorporation of Label Information: The paper mentions that the method can naturally incorporate node features and does not require label supervision. However, it would be beneficial to test the method's performance when additional label information is available. This could involve conducting experiments where a small amount of labeled data is used in addition to the unlabeled data, and the performance of the semi-supervised learning approach is compared with the unsupervised learning approach.",
                "Potential Improvements: The paper discusses potential improvements to the model, such as semi-supervised learning, automatic detection of the number of clusters, and adaptations for large scale applications. However, it would be beneficial to conduct experiments to evaluate these improvements. This would provide more concrete evidence of the method's potential and areas for future work."
            ],
            "novelty_impact": [
                "The imbalance score is a novel contribution of this paper. However, since it's a new measure, it would be beneficial if the authors could compare it with other existing measures in the field. This would help to validate its effectiveness and provide a more comprehensive evaluation of DIGRAC's performance.",
                "The paper does not provide a direct comparison of computational efficiency between DIGRAC and existing methods. This is an important aspect to consider, especially when discussing the potential for large scale industrial applications. The authors should provide this information to give a complete picture of DIGRAC's performance.",
                "It would be interesting to know how DIGRAC handles \"ambient nodes\". Are they simply ignored or does the algorithm have a specific way of dealing with them?",
                "The authors should provide more details on how DIGRAC can detect alternative patterns in the data, such as meta-graph structures, and how it can include exogenous information. These are important features that could set DIGRAC apart from existing methods, so they should be explained more thoroughly.",
                "The authors should discuss any potential limitations or assumptions associated with the self-supervised loss function and the directed mixed path aggregation scheme. This would help to identify any potential shortcomings and areas for future improvement.",
                "The lack of information on existing methods that incorporate semi-supervised settings and automatic detection of the number of clusters is a missed opportunity. The authors should provide a comparison with these methods to show how DIGRAC could potentially improve upon them."
            ],
            "clarity_reproducibility": [
                "The paper should provide more details about the novel probabilistic imbalance loss, specifically how the CI vol_sum is calculated in the equation mentioned.",
                "The paper should provide more details about the Directed Mixed Path Aggregation (DIMPA) scheme, specifically how the weighted average of information from neighbors is calculated and what factors determine the weight assigned to each neighbor.",
                "The paper should provide more details on how the Adjusted Rand Index (ARI) and accuracy for node classification and link direction prediction are calculated.",
                "The paper should provide more guidance on how to choose the value of K when using the largest K eigenvectors of a Hermitian matrix to construct an input feature matrix.",
                "The paper should provide more guidance on how to select seed nodes for the supervised loss function.",
                "The paper should provide more details on how DIGRAC could potentially be used in a semi-supervised setting.",
                "The paper should provide more details on the hyperparameters, equipment, and material specifications used in the experiments to make it easier for other researchers to reproduce the experiments."
            ],
            "all": [
                "Ablation Studies: The paper would benefit from ablation studies to evaluate the individual contributions of the novel components of the method, such as the probabilistic imbalance loss and the directed mixed path aggregation scheme (DIMPA). This could involve running experiments where one component is removed or modified at a time, and the impact on the results is observed. This would help to understand the importance of each component and how they contribute to the overall performance.",
                "Incorporation of Label Information: The paper mentions that the method can naturally incorporate node features and does not require label supervision. However, it would be beneficial to test the method's performance when additional label information is available. This could involve conducting experiments where a small amount of labeled data is used in addition to the unlabeled data, and the performance of the semi-supervised learning approach is compared with the unsupervised learning approach.",
                "Potential Improvements: The paper discusses potential improvements to the model, such as semi-supervised learning, automatic detection of the number of clusters, and adaptations for large scale applications. However, it would be beneficial to conduct experiments to evaluate these improvements. This would provide more concrete evidence of the method's potential and areas for future work.",
                "The imbalance score is a novel contribution of this paper. However, since it's a new measure, it would be beneficial if the authors could compare it with other existing measures in the field. This would help to validate its effectiveness and provide a more comprehensive evaluation of DIGRAC's performance.",
                "The paper does not provide a direct comparison of computational efficiency between DIGRAC and existing methods. This is an important aspect to consider, especially when discussing the potential for large scale industrial applications. The authors should provide this information to give a complete picture of DIGRAC's performance.",
                "It would be interesting to know how DIGRAC handles \"ambient nodes\". Are they simply ignored or does the algorithm have a specific way of dealing with them?",
                "The authors should provide more details on how DIGRAC can detect alternative patterns in the data, such as meta-graph structures, and how it can include exogenous information. These are important features that could set DIGRAC apart from existing methods, so they should be explained more thoroughly.",
                "The authors should discuss any potential limitations or assumptions associated with the self-supervised loss function and the directed mixed path aggregation scheme. This would help to identify any potential shortcomings and areas for future improvement.",
                "The lack of information on existing methods that incorporate semi-supervised settings and automatic detection of the number of clusters is a missed opportunity. The authors should provide a comparison with these methods to show how DIGRAC could potentially improve upon them.",
                "The paper should provide more details about the novel probabilistic imbalance loss, specifically how the CI vol_sum is calculated in the equation mentioned.",
                "The paper should provide more details about the Directed Mixed Path Aggregation (DIMPA) scheme, specifically how the weighted average of information from neighbors is calculated and what factors determine the weight assigned to each neighbor.",
                "The paper should provide more details on how the Adjusted Rand Index (ARI) and accuracy for node classification and link direction prediction are calculated.",
                "The paper should provide more guidance on how to choose the value of K when using the largest K eigenvectors of a Hermitian matrix to construct an input feature matrix.",
                "The paper should provide more guidance on how to select seed nodes for the supervised loss function.",
                "The paper should provide more details on how DIGRAC could potentially be used in a semi-supervised setting.",
                "The paper should provide more details on the hyperparameters, equipment, and material specifications used in the experiments to make it easier for other researchers to reproduce the experiments."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [
                {
                    "reference_id": 5,
                    "predicted_id": 4,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "The paper lacks scalability experiments with the running time of the algorithm and its comparison to baseline models.",
                    "predicted_comment": "The paper does not provide a direct comparison of computational efficiency between DIGRAC and existing methods. This is an important aspect to consider, especially when discussing the potential for large scale industrial applications. The authors should provide this information to give a complete picture of DIGRAC's performance."
                }
            ],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 5,
                    "predicted_id": 4,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "The paper lacks scalability experiments with the running time of the algorithm and its comparison to baseline models.",
                    "predicted_comment": "The paper does not provide a direct comparison of computational efficiency between DIGRAC and existing methods. This is an important aspect to consider, especially when discussing the potential for large scale industrial applications. The authors should provide this information to give a complete picture of DIGRAC's performance."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 11,
                "n_generated": 3,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 11,
                "n_generated": 6,
                "n_total_aligns": 1,
                "recall": 0.05555555555555555,
                "precision": 0.05555555555555555,
                "pseudo_jaccard": 0.030303030303030304,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 11,
                "n_generated": 7,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 11,
                "n_generated": 16,
                "n_total_aligns": 1,
                "recall": 0.05555555555555555,
                "precision": 0.020833333333333332,
                "pseudo_jaccard": 0.015873015873015872,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.16666666666666666,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.09090909090909091,
                    "n_real": 6
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.16666666666666666,
                    "precision": 0.0625,
                    "pseudo_jaccard": 0.047619047619047616,
                    "n_real": 6
                }
            ]
        }
    },
    {
        "doc_id": "b-ny3x071E5",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The name 'matching function' makes the presentation of the method a little bit harder to digest. Since the function is a pseudometric (i.e., the larger it is, the larger the distance from the target), it should really be called with a name that reminds the reader of this nature (e.g., 'mismatch function').",
                "The theoretical results only deal with targets of specific forms and, especially, with $L=1$ only. Ideally, theoretical result with a dependency on $L$ would shed some light on the benefits and limitations of longer bootstrapping horizons.",
                "The connection between the way the bootstrapping target is formed in the method and traditional temporal difference learning needs to be elaborated on. In particular, the grounding role of that subtracted gradient 'nudging the trajectory in a descent direction' is the same as the one of the reward in temporal difference learning; but, while the reward is at the beginning of the trajectory, the grounding is here at the end of the optimization subtrajectory. Is there any mathematical connection beyond the general shared motivation?",
                "When the underlying function is highly nonlinear, there is the risk that the bootstrapping mechanism can lead the optimization process in worse areas of the landscape. For instance, if the function in Figure 1 had a bump/plateaux where $\tilde w$ is, the bootstrapping mechanism would cause more troubles than standard meta-gradients. Why is this not happening in practice?"
            ],
            [
                "The algorithm description in section 3 is very abstract. The paper would benefit from a running example and a dedicated section and pseudo-code describing the algorithm and how it can be instantiated in different experimental settings.",
                "The theoretical analysis is limited to noiseless 1-step target updates.",
                "The experimental evaluation in the multi-task meta-learning setting is limited to only compare with MAML on computer vision applications.",
                "Some engineering / handcrafting is still required by the machine learning practitioner to select what 'target' the meta-learner is going to optimize, as well as the proper 'metric' for the meta-learner to optimize for. Could the authors comment a bit about what heuristics they used when making these decisions? and whether the automation for this process is possible or not?",
                "What would it take to extend the analysis beyond 1-step noiseless target updates?",
                "How does the performance of BMG compare to alternative meta-learning algorithms like R2D2, Meta-OPT-net and prototypical networks? Have the authors experimented with other meta-learning benchmarks beyond image classification?"
            ],
            [
                "Given that you say BMG is compatible with any update function (so long as it is differentiable in the meta-parameters), it would be nice to have some experiments on learned sequence model update rules (e.g. RNN). All current experiments use update rules with a fixed functional form.",
                "I am not putting much weight on section 4 ('Performance Guarantees') given the gap between its assumptions and results vs. what is actually implemented, and the restriction to local optimization."
            ],
            [
                "The paper could benefit from more detailed discussions in certain areas to improve clarity and understanding.",
                "The methodologies of the experiments are difficult to follow, making it hard to link the results back to intuition or interpretation. More detailed descriptions or an algorithm box in the main text could help address this.",
                "The implementation and implications of the experiment on multi-task few-shot learning (Sec 6) are unclear. The authors should clarify the intuition of why a 'hot' expert transforms more information than a 'cold' expert, and why BMG is able to use this to improve performance.",
                "The training procedure for BMG in the multi-task few-shot learning experiment should be formally described in the main text.",
                "The analysis presented in Sec 4 ('Performance Guarantees') is not especially insightful. The conclusion drawn from this section is evident from the experiments themselves."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "Ensure Reproducibility: Please provide more details about the experimental setup, the parameters used, and the versions of the software or libraries used to ensure that others can reproduce your results.",
                "Verify Statistical Significance: Please provide more information about the statistical tests used to verify the significance of your results. This will help readers understand whether the observed differences in performance between the proposed method and the baselines are not due to random chance.",
                "Conduct Multiple Runs: Please provide more information about the number of runs for each experiment and report the average performance over these runs, along with measures of variability such as the standard deviation or confidence intervals.",
                "Ensure Fair Comparison: Please provide more information about the resources given to each method during the comparison and how each method was optimized to ensure a fair comparison.",
                "Conduct a Comprehensive Ablation Study: Please provide more information about the ablation study, covering all major components of the proposed method. This will help readers understand the contribution of each component to the overall performance."
            ],
            "novelty_impact": [
                "The paper presents a novel approach to meta-learning with the introduction of a bootstrapping method. However, the theoretical basis for this method could be better explained. While the paper mentions that it is based on the concept of meta-learning and draws from temporal difference algorithms in reinforcement learning, it would be beneficial to provide a more detailed explanation of how these concepts are applied in the proposed method.",
                "The authors claim that the BMG algorithm improves performance and efficiency in multi-task meta-learning. However, the definitions of \"performance\" and \"efficiency\" could be more clearly defined. It would be helpful to provide a more detailed explanation of how these metrics are measured and why they are important in the context of this research.",
                "The paper discusses the performance guarantees of MG and BMG in the context of online optimization and multi-task settings. However, the exact nature of these guarantees and how they are derived could be better explained. Providing more detail about these guarantees would strengthen the paper's arguments and make it more accessible to readers who are not familiar with these concepts.",
                "The authors introduce the concept of a target bootstrap (TB) and discuss its role in performance improvement. However, the exact form of the TB and how it contributes to performance improvement could be better explained. Providing more detail about this concept would help readers understand the proposed method and its benefits.",
                "The authors claim that BMG outperforms MG in a reinforcement learning setup and in the Atari Arcade Learning Environment. However, the specific metrics used to evaluate performance in these settings could be more clearly defined. Providing more detail about these metrics would help readers understand the results and their significance.",
                "The authors apply BMG in the context of Atari games and Multi-task Few-Shot Learning. However, it is unclear how the authors ensure that the results are not specific to these particular applications, but are generalizable to other domains. Providing more detail about the generalizability of the results would strengthen the paper's arguments and make it more relevant to a wider audience.",
                "The authors conduct a series of experiments to test the effectiveness of their proposed methods in mitigating short-horizon bias and learning an exploration schedule without explicit domain knowledge. However, the definitions of \"short-horizon bias\" and \"exploration schedule\" could be more clearly defined. Providing more detail about these concepts and how they are measured would help readers understand the experiments and their results.",
                "The authors perform ablation studies to understand the role of meta-regularization and the importance of taking a final step under a different update rule. However, the paper could provide more detail about these ablation studies and their findings. Providing more detail about these studies would help readers understand the proposed method and its benefits.",
                "The authors show that changing the target update rule from SGD to RMSProp, which corrects for curvature, yields a substantial gain. However, the quantification of this gain and the significance of correcting for curvature could be better explained. Providing more detail about these concepts would help readers understand the proposed method and its benefits.",
                "In the Few-Shot MiniImagenet section, the authors compare BMG and MG in terms of data-efficiency and computational efficiency. However, the definitions of these efficiencies and how they are measured could be more clearly defined. Providing more detail about these concepts would help readers understand the experiments and their results.",
                "The authors analyze the effect of BMG on ill-conditioning and meta-gradient variance. However, the definitions of \"ill-conditioning\" and \"meta-gradient variance\" and how they are measured could be more clearly defined. Providing more detail about these concepts would help readers understand the experiments and their results.",
                "The authors claim that BMG is 50% slower than MAML in terms of throughput. However, the definition of \"throughput\" and how it is measured could be more clearly defined. Providing more detail about this concept would help readers understand the experiments and their results."
            ],
            "clarity_reproducibility": [
                "The paper could benefit from providing more detail about the specific steps of the Bootstrapped Meta-Gradient (BMG) algorithm. This would make it easier for others to understand and implement the algorithm.",
                "The authors should provide more detail about how the metrics (KL-divergence and squared Euclidean distance) are used in the context of the BMG algorithm.",
                "The paper discusses performance guarantees for the MG and BMG updates, but does not provide specific details about these guarantees. The authors should provide more detail about these performance guarantees.",
                "The paper presents several mathematical lemmas, theorems, and corollaries, but does not provide these in the sections of the paper that the agents have. The authors should provide these mathematical lemmas, theorems, and corollaries, along with their proofs.",
                "The paper does not provide a detailed explanation of how a typical reinforcement learning problem is modeled as an MDP. The authors should provide more detail about this.",
                "The paper does not provide specific details about the experiments conducted in the Atari Arcade Learning Environment (ALE). The authors should provide more detail about these experiments, including the specific settings and how the results were measured.",
                "The paper does not provide specific details about the application of BMG in the context of Atari games and Multi-Task Few-Shot Learning. The authors should provide more detail about these applications.",
                "The paper does not provide specific details about the experiments conducted to study how the BMG objective behaves in the multi-task setting. The authors should provide more detail about these experiments, including the specific settings and how the results were measured.",
                "The paper does not provide specific details about the experiment setup using the MiniImagenet benchmark. The authors should provide more detail about this setup, including the specific settings and how the results were measured.",
                "The paper does not provide specific details about the experiments discussed in the conclusion in the two-colors domain. The authors should provide more detail about these experiments, including the specific settings and how the results were measured.",
                "The paper does not provide definitions or explanations for all the terms and concepts used in the paper. The authors should provide clear definitions and explanations for all the terms and concepts used in the paper.",
                "Some sections of the paper do not provide enough detail about the methods and experimental settings that they could be reproduced by others. The authors should provide more detail about the methods and experimental settings in these sections."
            ],
            "all": [
                "Ensure Reproducibility: Please provide more details about the experimental setup, the parameters used, and the versions of the software or libraries used to ensure that others can reproduce your results.",
                "Verify Statistical Significance: Please provide more information about the statistical tests used to verify the significance of your results. This will help readers understand whether the observed differences in performance between the proposed method and the baselines are not due to random chance.",
                "Conduct Multiple Runs: Please provide more information about the number of runs for each experiment and report the average performance over these runs, along with measures of variability such as the standard deviation or confidence intervals.",
                "Ensure Fair Comparison: Please provide more information about the resources given to each method during the comparison and how each method was optimized to ensure a fair comparison.",
                "Conduct a Comprehensive Ablation Study: Please provide more information about the ablation study, covering all major components of the proposed method. This will help readers understand the contribution of each component to the overall performance.",
                "The paper presents a novel approach to meta-learning with the introduction of a bootstrapping method. However, the theoretical basis for this method could be better explained. While the paper mentions that it is based on the concept of meta-learning and draws from temporal difference algorithms in reinforcement learning, it would be beneficial to provide a more detailed explanation of how these concepts are applied in the proposed method.",
                "The authors claim that the BMG algorithm improves performance and efficiency in multi-task meta-learning. However, the definitions of \"performance\" and \"efficiency\" could be more clearly defined. It would be helpful to provide a more detailed explanation of how these metrics are measured and why they are important in the context of this research.",
                "The paper discusses the performance guarantees of MG and BMG in the context of online optimization and multi-task settings. However, the exact nature of these guarantees and how they are derived could be better explained. Providing more detail about these guarantees would strengthen the paper's arguments and make it more accessible to readers who are not familiar with these concepts.",
                "The authors introduce the concept of a target bootstrap (TB) and discuss its role in performance improvement. However, the exact form of the TB and how it contributes to performance improvement could be better explained. Providing more detail about this concept would help readers understand the proposed method and its benefits.",
                "The authors claim that BMG outperforms MG in a reinforcement learning setup and in the Atari Arcade Learning Environment. However, the specific metrics used to evaluate performance in these settings could be more clearly defined. Providing more detail about these metrics would help readers understand the results and their significance.",
                "The authors apply BMG in the context of Atari games and Multi-task Few-Shot Learning. However, it is unclear how the authors ensure that the results are not specific to these particular applications, but are generalizable to other domains. Providing more detail about the generalizability of the results would strengthen the paper's arguments and make it more relevant to a wider audience.",
                "The authors conduct a series of experiments to test the effectiveness of their proposed methods in mitigating short-horizon bias and learning an exploration schedule without explicit domain knowledge. However, the definitions of \"short-horizon bias\" and \"exploration schedule\" could be more clearly defined. Providing more detail about these concepts and how they are measured would help readers understand the experiments and their results.",
                "The authors perform ablation studies to understand the role of meta-regularization and the importance of taking a final step under a different update rule. However, the paper could provide more detail about these ablation studies and their findings. Providing more detail about these studies would help readers understand the proposed method and its benefits.",
                "The authors show that changing the target update rule from SGD to RMSProp, which corrects for curvature, yields a substantial gain. However, the quantification of this gain and the significance of correcting for curvature could be better explained. Providing more detail about these concepts would help readers understand the proposed method and its benefits.",
                "In the Few-Shot MiniImagenet section, the authors compare BMG and MG in terms of data-efficiency and computational efficiency. However, the definitions of these efficiencies and how they are measured could be more clearly defined. Providing more detail about these concepts would help readers understand the experiments and their results.",
                "The authors analyze the effect of BMG on ill-conditioning and meta-gradient variance. However, the definitions of \"ill-conditioning\" and \"meta-gradient variance\" and how they are measured could be more clearly defined. Providing more detail about these concepts would help readers understand the experiments and their results.",
                "The authors claim that BMG is 50% slower than MAML in terms of throughput. However, the definition of \"throughput\" and how it is measured could be more clearly defined. Providing more detail about this concept would help readers understand the experiments and their results.",
                "The paper could benefit from providing more detail about the specific steps of the Bootstrapped Meta-Gradient (BMG) algorithm. This would make it easier for others to understand and implement the algorithm.",
                "The authors should provide more detail about how the metrics (KL-divergence and squared Euclidean distance) are used in the context of the BMG algorithm.",
                "The paper discusses performance guarantees for the MG and BMG updates, but does not provide specific details about these guarantees. The authors should provide more detail about these performance guarantees.",
                "The paper presents several mathematical lemmas, theorems, and corollaries, but does not provide these in the sections of the paper that the agents have. The authors should provide these mathematical lemmas, theorems, and corollaries, along with their proofs.",
                "The paper does not provide a detailed explanation of how a typical reinforcement learning problem is modeled as an MDP. The authors should provide more detail about this.",
                "The paper does not provide specific details about the experiments conducted in the Atari Arcade Learning Environment (ALE). The authors should provide more detail about these experiments, including the specific settings and how the results were measured.",
                "The paper does not provide specific details about the application of BMG in the context of Atari games and Multi-Task Few-Shot Learning. The authors should provide more detail about these applications.",
                "The paper does not provide specific details about the experiments conducted to study how the BMG objective behaves in the multi-task setting. The authors should provide more detail about these experiments, including the specific settings and how the results were measured.",
                "The paper does not provide specific details about the experiment setup using the MiniImagenet benchmark. The authors should provide more detail about this setup, including the specific settings and how the results were measured.",
                "The paper does not provide specific details about the experiments discussed in the conclusion in the two-colors domain. The authors should provide more detail about these experiments, including the specific settings and how the results were measured.",
                "The paper does not provide definitions or explanations for all the terms and concepts used in the paper. The authors should provide clear definitions and explanations for all the terms and concepts used in the paper.",
                "Some sections of the paper do not provide enough detail about the methods and experimental settings that they could be reproduced by others. The authors should provide more detail about the methods and experimental settings in these sections."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper could benefit from more detailed discussions in certain areas to improve clarity and understanding.",
                    "predicted_comment": "Ensure Reproducibility: Please provide more details about the experimental setup, the parameters used, and the versions of the software or libraries used to ensure that others can reproduce your results."
                }
            ],
            "novelty_impact": [],
            "clarity_reproducibility": [
                {
                    "reference_id": 0,
                    "predicted_id": 28,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper could benefit from more detailed discussions in certain areas to improve clarity and understanding.",
                    "predicted_comment": "Some sections of the paper do not provide enough detail about the methods and experimental settings that they could be reproduced by others. The authors should provide more detail about the methods and experimental settings in these sections."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 21,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper could benefit from more detailed discussions in certain areas to improve clarity and understanding.",
                    "predicted_comment": "The paper does not provide a detailed explanation of how a typical reinforcement learning problem is modeled as an MDP. The authors should provide more detail about this."
                }
            ],
            "all": [
                {
                    "reference_id": 0,
                    "predicted_id": 28,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper could benefit from more detailed discussions in certain areas to improve clarity and understanding.",
                    "predicted_comment": "Some sections of the paper do not provide enough detail about the methods and experimental settings that they could be reproduced by others. The authors should provide more detail about the methods and experimental settings in these sections."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 21,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper could benefit from more detailed discussions in certain areas to improve clarity and understanding.",
                    "predicted_comment": "The paper does not provide a detailed explanation of how a typical reinforcement learning problem is modeled as an MDP. The authors should provide more detail about this."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper could benefit from more detailed discussions in certain areas to improve clarity and understanding.",
                    "predicted_comment": "Ensure Reproducibility: Please provide more details about the experimental setup, the parameters used, and the versions of the software or libraries used to ensure that others can reproduce your results."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 17,
                "n_generated": 5,
                "n_total_aligns": 1,
                "recall": 0.05,
                "precision": 0.05,
                "pseudo_jaccard": 0.027777777777777776,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 17,
                "n_generated": 12,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "clarity_reproducibility": {
                "n_reference": 17,
                "n_generated": 12,
                "n_total_aligns": 2,
                "recall": 0.05,
                "precision": 0.041666666666666664,
                "pseudo_jaccard": 0.024193548387096774,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "all": {
                "n_reference": 17,
                "n_generated": 29,
                "n_total_aligns": 3,
                "recall": 0.05,
                "precision": 0.02586206896551724,
                "pseudo_jaccard": 0.015625,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.2,
                    "precision": 0.2,
                    "pseudo_jaccard": 0.1111111111111111,
                    "n_real": 5
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.2,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.0967741935483871,
                    "n_real": 5
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.2,
                    "precision": 0.10344827586206896,
                    "pseudo_jaccard": 0.0625,
                    "n_real": 5
                }
            ]
        }
    },
    {
        "doc_id": "uB12zutkXJR",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper does not convincingly demonstrate model novelty or significant improvement to the state-of-the-art on the provided baseline.",
                "The model architecture seems very closely based on the HOPPITY baseline, with the primary change being to the multi-head graph encoder. This does not introduce a fundamentally new idea.",
                "Quantitative evaluations show only marginal improvement at best in comparison to the large pre-trained models.",
                "The evaluation omits a comparison to the closely related Yao 2021 model that is cited throughout the paper.",
                "The pre-training regime, which is presented as a contribution of this paper, does not seem to produce markedly stronger results. It might be valid to apply the same pre-training regime to some of the other models for a fair comparison.",
                "The qualitative evaluation does not yield clear conclusions. It would be beneficial to provide evidence that 'GRAPHIX demonstrates strong inductive biases in learning complex bug-fix patterns' or some conclusion on the kinds of bugs that GRAPHIX tends to fix.",
                "The pre-training regime is not particularly novel. The authors suggest deleting elements of subtrees from existing code and reconstructing them, which is not sufficiently novel."
            ],
            [
                "The paper does not clearly explain the relevance of the presented work despite not beating the state of the art. A discussion on this topic is missing.",
                "The anecdotal examples are not compared to the results other approaches give, making it unclear what to make of them.",
                "The claim that the model is more than 10x smaller than 'current large scale sequence models' is misleading as only one of the evaluated baselines, 'BART', is that much larger. The presented approach is even outperformed by CodeT5-small, which is less than 2x as large.",
                "The paper lacks a discussion on the accuracy of detecting bug fixes by filtering for 'fix', 'bug', 'error', and 'exception'.",
                "The reasoning behind having exactly 600 nodes is not explained."
            ],
            [
                "The pre-training task, while claimed to be novel, is not particularly innovative. It involves 'masking sub-trees' on AST, which essentially asks the model to do 'missing code prediction', covering only a small subset of general program repair samples. The pre-training task could be more innovative if more thought was put into generating the pre-training dataset, such as how to generate data for pre-training other edits like copy, remove, and update.",
                "Pre-training is costly and does not provide a significant performance gain (<2%). This issue is more pronounced with medium-sized data. The reviewer suggests two questions for consideration: 1) Why is pre-training only done on 'sub-trees between 2 and 6 descendants'? Could the size be increased to cover longer edit sequences during pre-training? Is it possible to add such an experiment? 2) Could the lack of performance gain be due to the pre-training task only asking the model to predict a sequence of addition operations? Could more diverse tasks be added to allow the model to predict a variety of edits?",
                "The multi-head graph encoder does not seem particularly novel.",
                "The paper does not mention related works such as 'TFix: Learning to Fix Coding Errors with a Text-to-Text Transformer' and 'Learning semantic program embeddings with graph interval neural network'.",
                "The last paragraph in Section 4.1 mentions a single-head 'base' model, which is not mentioned in any experiment in the main paper. The reviewer suggests moving this description into the Appendix or including the 'base' model and its performance in Table 1/2.",
                "In Listing 2 Top, the code is not valid if the highlighted green part is removed. The reviewer asks for clarification on the before and after of that code snippet.",
                "The paper claims 'these observations suggest that the code abstraction may not be necessary'. The reviewer finds this argument weak and disagrees with the initial premise that 'code abstraction is necessary'."
            ],
            [
                "The paper lacks a comprehensive comparison with prior work, making the contributions over previous work hard to assess.",
                "The efficacy of the proposed multihead Graph Neural Network (GNN) scheme is not adequately ablated. The paper needs to include ablations with models of the same size.",
                "There are concerns about the scalability of the model given the need to encode the entire tree at each intermediate step.",
                "The paper needs to provide equal comparisons of Hoppity and one or more deeper non-multihead Graphix models in the 32M parameter domain.",
                "The contribution of pretraining seems quite slim; performance only increases a little on the Small dataset and actually decreases on the Medium data. This needs more analysis.",
                "The paper needs to analyze and discuss the potential scalability problems of reinstantiating and encoding the program graph on every edit, especially on longer chains of edits.",
                "The paper needs to clarify whether the ensemble-of-graphs model is meant to be a contribution in general or just for source code.",
                "The paper needs to provide a more detailed analysis of the ASDL-guided decoder, especially in comparison with a small instantiation of Hoppity."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "Comparison with Other Models: The paper claims that GRAPHIX is competitive with other state-of-the-art pre-trained Transformer models despite using fewer parameters. However, a detailed comparison or analysis supporting this claim is missing. It would be beneficial to include a thorough comparison of GRAPHIX with these models, detailing the performance on the same benchmark and using the same metrics. This would provide a clearer picture of how GRAPHIX stands in comparison to these models and validate the claim of competitive performance with fewer parameters.",
                "Impact of Pre-training Strategy: The paper introduces a novel pre-training strategy, deleted sub-tree reconstruction, which is claimed to enrich the model with implicit knowledge of program structures from unlabeled source code. However, a detailed analysis of the impact of this strategy on the performance of GRAPHIX is missing. It would be beneficial to include an analysis or study showing how this pre-training strategy contributes to the performance of GRAPHIX. This could be done by comparing the performance of GRAPHIX with and without the pre-training strategy on the same benchmark and using the same metrics."
            ],
            "novelty_impact": [
                "The paper should provide specific details on the computational resources required for their multi-head graph encoder. This information is important for understanding the practicality of the model.",
                "The deleted sub-tree reconstruction strategy is specific to ASTs with an underlying syntax language and not applicable to arbitrary graph structures. This could potentially limit the generalizability of the model to other programming languages or code structures. The authors should discuss this limitation and provide evidence to support their claim that this strategy could be extended to other programming languages.",
                "The use of code change data could potentially introduce biases into the model, as code changes could be influenced by individual programmer's styles or preferences. The authors should discuss this potential issue and how it might affect the model's performance.",
                "The authors suggest incorporating more program dependencies into the graph representation as future work, but do not discuss the potential increase in complexity and interpretability issues this could cause. The authors should discuss this trade-off in more detail.",
                "The paper does not provide a clear definition of what it considers to be \"meaningful\" and \"interpretable\". This lack of clarity could affect the reproducibility of the study and the ability of other researchers to build upon this work. The authors should provide a clear definition of these terms.",
                "The paper does not discuss potential ethical implications of automated program repair, such as the impact on employment in the software development industry. It would be beneficial for the authors to at least acknowledge these potential implications.",
                "The paper should provide more details on the unsuccessful cases of bug fixes suggested by the model. Discussing common patterns or reasons for these failures could provide valuable insights for improving the model.",
                "The paper should provide more details on the limitations of the multi-head graph encoder and the potential issues with this approach.",
                "The paper should provide more details on the limitations and potential issues with the implementation and training process of GRAPHIX.",
                "The paper should provide more details on the limitations and potential issues with the high precision regime for GRAPHIX.",
                "The paper should provide more details on the limitations and potential issues with the use of the Patches in The Wild Java benchmark for evaluation.",
                "The paper should provide more details on the limitations and potential issues with the proposed future directions.",
                "The paper should provide more details on the limitations and potential issues with the ablation studies comparing the performance of GRAPHIX with other models.",
                "The paper should provide more details on the limitations and potential issues with the use of code change data to address the lack of large human-labeled datasets."
            ],
            "clarity_reproducibility": [
                "The paper should provide more specific details about how the parameters for the multi-head graph encoder were chosen. This includes any optimization or experimentation process involved. This information is crucial for understanding the rationale behind the chosen parameters and for ensuring the reproducibility of the work.",
                "The paper should provide more details about the Abstract Syntax Description Language (ASDL) and how it guides the decoder in maintaining the syntax of the code during the editing process. This information is crucial for understanding the functioning of the decoder and the role of the ASDL in it.",
                "The paper should provide more information on how the size of the sub-tree to be deleted is randomly selected in the deleted sub-tree reconstruction pre-training strategy. This includes any specific method or distribution used. This information is crucial for understanding the pre-training strategy and for ensuring the reproducibility of the work.",
                "The paper should provide more details about the beam search of size 5 used during inference in the Patches in The Wild Java benchmark. This includes how the size of the beam search was chosen. This information is crucial for understanding the inference process and for ensuring the reproducibility of the work.",
                "The paper should provide more information on why the authors think GRAPHIX is insensitive to the naming of variables, types, and APIs. This could include any additional experiments or analyses conducted to support this claim. This information is crucial for understanding the robustness of GRAPHIX.",
                "The paper should provide more details about how the baselines used for comparison were modified for the comparison. This includes how HOPPITY was modified to handle Java source code and to add a sub-tree. This information is crucial for understanding the comparison and for ensuring the reproducibility of the work.",
                "The paper should provide more details about the hyperparameters used in the implementation and experimental setup. This includes any learning rates, batch sizes, number of epochs, etc. This information is crucial for understanding the implementation and experimental setup and for ensuring the reproducibility of the work."
            ],
            "all": [
                "Comparison with Other Models: The paper claims that GRAPHIX is competitive with other state-of-the-art pre-trained Transformer models despite using fewer parameters. However, a detailed comparison or analysis supporting this claim is missing. It would be beneficial to include a thorough comparison of GRAPHIX with these models, detailing the performance on the same benchmark and using the same metrics. This would provide a clearer picture of how GRAPHIX stands in comparison to these models and validate the claim of competitive performance with fewer parameters.",
                "Impact of Pre-training Strategy: The paper introduces a novel pre-training strategy, deleted sub-tree reconstruction, which is claimed to enrich the model with implicit knowledge of program structures from unlabeled source code. However, a detailed analysis of the impact of this strategy on the performance of GRAPHIX is missing. It would be beneficial to include an analysis or study showing how this pre-training strategy contributes to the performance of GRAPHIX. This could be done by comparing the performance of GRAPHIX with and without the pre-training strategy on the same benchmark and using the same metrics.",
                "The paper should provide specific details on the computational resources required for their multi-head graph encoder. This information is important for understanding the practicality of the model.",
                "The deleted sub-tree reconstruction strategy is specific to ASTs with an underlying syntax language and not applicable to arbitrary graph structures. This could potentially limit the generalizability of the model to other programming languages or code structures. The authors should discuss this limitation and provide evidence to support their claim that this strategy could be extended to other programming languages.",
                "The use of code change data could potentially introduce biases into the model, as code changes could be influenced by individual programmer's styles or preferences. The authors should discuss this potential issue and how it might affect the model's performance.",
                "The authors suggest incorporating more program dependencies into the graph representation as future work, but do not discuss the potential increase in complexity and interpretability issues this could cause. The authors should discuss this trade-off in more detail.",
                "The paper does not provide a clear definition of what it considers to be \"meaningful\" and \"interpretable\". This lack of clarity could affect the reproducibility of the study and the ability of other researchers to build upon this work. The authors should provide a clear definition of these terms.",
                "The paper does not discuss potential ethical implications of automated program repair, such as the impact on employment in the software development industry. It would be beneficial for the authors to at least acknowledge these potential implications.",
                "The paper should provide more details on the unsuccessful cases of bug fixes suggested by the model. Discussing common patterns or reasons for these failures could provide valuable insights for improving the model.",
                "The paper should provide more details on the limitations of the multi-head graph encoder and the potential issues with this approach.",
                "The paper should provide more details on the limitations and potential issues with the implementation and training process of GRAPHIX.",
                "The paper should provide more details on the limitations and potential issues with the high precision regime for GRAPHIX.",
                "The paper should provide more details on the limitations and potential issues with the use of the Patches in The Wild Java benchmark for evaluation.",
                "The paper should provide more details on the limitations and potential issues with the proposed future directions.",
                "The paper should provide more details on the limitations and potential issues with the ablation studies comparing the performance of GRAPHIX with other models.",
                "The paper should provide more details on the limitations and potential issues with the use of code change data to address the lack of large human-labeled datasets.",
                "The paper should provide more specific details about how the parameters for the multi-head graph encoder were chosen. This includes any optimization or experimentation process involved. This information is crucial for understanding the rationale behind the chosen parameters and for ensuring the reproducibility of the work.",
                "The paper should provide more details about the Abstract Syntax Description Language (ASDL) and how it guides the decoder in maintaining the syntax of the code during the editing process. This information is crucial for understanding the functioning of the decoder and the role of the ASDL in it.",
                "The paper should provide more information on how the size of the sub-tree to be deleted is randomly selected in the deleted sub-tree reconstruction pre-training strategy. This includes any specific method or distribution used. This information is crucial for understanding the pre-training strategy and for ensuring the reproducibility of the work.",
                "The paper should provide more details about the beam search of size 5 used during inference in the Patches in The Wild Java benchmark. This includes how the size of the beam search was chosen. This information is crucial for understanding the inference process and for ensuring the reproducibility of the work.",
                "The paper should provide more information on why the authors think GRAPHIX is insensitive to the naming of variables, types, and APIs. This could include any additional experiments or analyses conducted to support this claim. This information is crucial for understanding the robustness of GRAPHIX.",
                "The paper should provide more details about how the baselines used for comparison were modified for the comparison. This includes how HOPPITY was modified to handle Java source code and to add a sub-tree. This information is crucial for understanding the comparison and for ensuring the reproducibility of the work.",
                "The paper should provide more details about the hyperparameters used in the implementation and experimental setup. This includes any learning rates, batch sizes, number of epochs, etc. This information is crucial for understanding the implementation and experimental setup and for ensuring the reproducibility of the work."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 2,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "Quantitative evaluations show only marginal improvement at best in comparison to the large pre-trained models.",
                    "predicted_comment": "Comparison with Other Models: The paper claims that GRAPHIX is competitive with other state-of-the-art pre-trained Transformer models despite using fewer parameters. However, a detailed comparison or analysis supporting this claim is missing. It would be beneficial to include a thorough comparison of GRAPHIX with these models, detailing the performance on the same benchmark and using the same metrics. This would provide a clearer picture of how GRAPHIX stands in comparison to these models and validate the claim of competitive performance with fewer parameters."
                },
                {
                    "reference_id": 4,
                    "predicted_id": 1,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The pre-training regime, which is presented as a contribution of this paper, does not seem to produce markedly stronger results. It might be valid to apply the same pre-training regime to some of the other models for a fair comparison.",
                    "predicted_comment": "Impact of Pre-training Strategy: The paper introduces a novel pre-training strategy, deleted sub-tree reconstruction, which is claimed to enrich the model with implicit knowledge of program structures from unlabeled source code. However, a detailed analysis of the impact of this strategy on the performance of GRAPHIX is missing. It would be beneficial to include an analysis or study showing how this pre-training strategy contributes to the performance of GRAPHIX. This could be done by comparing the performance of GRAPHIX with and without the pre-training strategy on the same benchmark and using the same metrics."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper does not convincingly demonstrate model novelty or significant improvement to the state-of-the-art on the provided baseline.",
                    "predicted_comment": "Comparison with Other Models: The paper claims that GRAPHIX is competitive with other state-of-the-art pre-trained Transformer models despite using fewer parameters. However, a detailed comparison or analysis supporting this claim is missing. It would be beneficial to include a thorough comparison of GRAPHIX with these models, detailing the performance on the same benchmark and using the same metrics. This would provide a clearer picture of how GRAPHIX stands in comparison to these models and validate the claim of competitive performance with fewer parameters."
                },
                {
                    "reference_id": 4,
                    "predicted_id": 1,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "The contribution of pretraining seems quite slim; performance only increases a little on the Small dataset and actually decreases on the Medium data. This needs more analysis.",
                    "predicted_comment": "Impact of Pre-training Strategy: The paper introduces a novel pre-training strategy, deleted sub-tree reconstruction, which is claimed to enrich the model with implicit knowledge of program structures from unlabeled source code. However, a detailed analysis of the impact of this strategy on the performance of GRAPHIX is missing. It would be beneficial to include an analysis or study showing how this pre-training strategy contributes to the performance of GRAPHIX. This could be done by comparing the performance of GRAPHIX with and without the pre-training strategy on the same benchmark and using the same metrics."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper lacks a comprehensive comparison with prior work, making the contributions over previous work hard to assess.",
                    "predicted_comment": "Comparison with Other Models: The paper claims that GRAPHIX is competitive with other state-of-the-art pre-trained Transformer models despite using fewer parameters. However, a detailed comparison or analysis supporting this claim is missing. It would be beneficial to include a thorough comparison of GRAPHIX with these models, detailing the performance on the same benchmark and using the same metrics. This would provide a clearer picture of how GRAPHIX stands in comparison to these models and validate the claim of competitive performance with fewer parameters."
                }
            ],
            "novelty_impact": [
                {
                    "reference_id": 5,
                    "predicted_id": 8,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The qualitative evaluation does not yield clear conclusions. It would be beneficial to provide evidence that 'GRAPHIX demonstrates strong inductive biases in learning complex bug-fix patterns' or some conclusion on the kinds of bugs that GRAPHIX tends to fix.",
                    "predicted_comment": "The paper should provide more details on the unsuccessful cases of bug fixes suggested by the model. Discussing common patterns or reasons for these failures could provide valuable insights for improving the model."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 14,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper lacks a comprehensive comparison with prior work, making the contributions over previous work hard to assess.",
                    "predicted_comment": "The paper should provide more details on the limitations and potential issues with the ablation studies comparing the performance of GRAPHIX with other models."
                }
            ],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 2,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "Quantitative evaluations show only marginal improvement at best in comparison to the large pre-trained models.",
                    "predicted_comment": "Comparison with Other Models: The paper claims that GRAPHIX is competitive with other state-of-the-art pre-trained Transformer models despite using fewer parameters. However, a detailed comparison or analysis supporting this claim is missing. It would be beneficial to include a thorough comparison of GRAPHIX with these models, detailing the performance on the same benchmark and using the same metrics. This would provide a clearer picture of how GRAPHIX stands in comparison to these models and validate the claim of competitive performance with fewer parameters."
                },
                {
                    "reference_id": 4,
                    "predicted_id": 1,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The pre-training regime, which is presented as a contribution of this paper, does not seem to produce markedly stronger results. It might be valid to apply the same pre-training regime to some of the other models for a fair comparison.",
                    "predicted_comment": "Impact of Pre-training Strategy: The paper introduces a novel pre-training strategy, deleted sub-tree reconstruction, which is claimed to enrich the model with implicit knowledge of program structures from unlabeled source code. However, a detailed analysis of the impact of this strategy on the performance of GRAPHIX is missing. It would be beneficial to include an analysis or study showing how this pre-training strategy contributes to the performance of GRAPHIX. This could be done by comparing the performance of GRAPHIX with and without the pre-training strategy on the same benchmark and using the same metrics."
                },
                {
                    "reference_id": 5,
                    "predicted_id": 8,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The qualitative evaluation does not yield clear conclusions. It would be beneficial to provide evidence that 'GRAPHIX demonstrates strong inductive biases in learning complex bug-fix patterns' or some conclusion on the kinds of bugs that GRAPHIX tends to fix.",
                    "predicted_comment": "The paper should provide more details on the unsuccessful cases of bug fixes suggested by the model. Discussing common patterns or reasons for these failures could provide valuable insights for improving the model."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper does not convincingly demonstrate model novelty or significant improvement to the state-of-the-art on the provided baseline.",
                    "predicted_comment": "Comparison with Other Models: The paper claims that GRAPHIX is competitive with other state-of-the-art pre-trained Transformer models despite using fewer parameters. However, a detailed comparison or analysis supporting this claim is missing. It would be beneficial to include a thorough comparison of GRAPHIX with these models, detailing the performance on the same benchmark and using the same metrics. This would provide a clearer picture of how GRAPHIX stands in comparison to these models and validate the claim of competitive performance with fewer parameters."
                },
                {
                    "reference_id": 4,
                    "predicted_id": 1,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "The contribution of pretraining seems quite slim; performance only increases a little on the Small dataset and actually decreases on the Medium data. This needs more analysis.",
                    "predicted_comment": "Impact of Pre-training Strategy: The paper introduces a novel pre-training strategy, deleted sub-tree reconstruction, which is claimed to enrich the model with implicit knowledge of program structures from unlabeled source code. However, a detailed analysis of the impact of this strategy on the performance of GRAPHIX is missing. It would be beneficial to include an analysis or study showing how this pre-training strategy contributes to the performance of GRAPHIX. This could be done by comparing the performance of GRAPHIX with and without the pre-training strategy on the same benchmark and using the same metrics."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper lacks a comprehensive comparison with prior work, making the contributions over previous work hard to assess.",
                    "predicted_comment": "Comparison with Other Models: The paper claims that GRAPHIX is competitive with other state-of-the-art pre-trained Transformer models despite using fewer parameters. However, a detailed comparison or analysis supporting this claim is missing. It would be beneficial to include a thorough comparison of GRAPHIX with these models, detailing the performance on the same benchmark and using the same metrics. This would provide a clearer picture of how GRAPHIX stands in comparison to these models and validate the claim of competitive performance with fewer parameters."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 14,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper lacks a comprehensive comparison with prior work, making the contributions over previous work hard to assess.",
                    "predicted_comment": "The paper should provide more details on the limitations and potential issues with the ablation studies comparing the performance of GRAPHIX with other models."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 27,
                "n_generated": 2,
                "n_total_aligns": 5,
                "recall": 0.16964285714285715,
                "precision": 0.5,
                "pseudo_jaccard": 0.15865384615384615,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 27,
                "n_generated": 14,
                "n_total_aligns": 2,
                "recall": 0.06696428571428571,
                "precision": 0.03571428571428571,
                "pseudo_jaccard": 0.024404761904761905,
                "specificity_more": 0.5,
                "specificity_less": 0.0,
                "specificity_same": 0.5
            },
            "clarity_reproducibility": {
                "n_reference": 27,
                "n_generated": 7,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 27,
                "n_generated": 23,
                "n_total_aligns": 7,
                "recall": 0.20535714285714285,
                "precision": 0.06521739130434782,
                "pseudo_jaccard": 0.05494869248593181,
                "specificity_more": 0.8571428571428571,
                "specificity_less": 0.0,
                "specificity_same": 0.14285714285714285
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.42857142857142855,
                    "precision": 1.0,
                    "pseudo_jaccard": 0.38461538461538464,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.25,
                    "precision": 1.0,
                    "pseudo_jaccard": 0.25,
                    "n_real": 8
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.14285714285714285,
                    "precision": 0.07142857142857142,
                    "pseudo_jaccard": 0.05,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.125,
                    "precision": 0.07142857142857142,
                    "pseudo_jaccard": 0.047619047619047616,
                    "n_real": 8
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                }
            ],
            "all": [
                {
                    "recall": 0.5714285714285714,
                    "precision": 0.13043478260869565,
                    "pseudo_jaccard": 0.1320754716981132,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.25,
                    "precision": 0.13043478260869565,
                    "pseudo_jaccard": 0.08771929824561403,
                    "n_real": 8
                }
            ]
        }
    },
    {
        "doc_id": "nLb60uXd6Np",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The quantitative results are good, but not revolutionary. The model does well on the crystal classification task, but this setting is described as 'not difficult'. It performs slightly worse than GemNet on molecular force regression. On the backmapping task, it is only compared against a naive transformer, as opposed to another rotation invariant model. Another instance of the proposed model outperforming a strong baseline on a difficult task would significantly improve the case the paper is making.",
                "While one of the advantages of the proposed method is scaling to arbitrary rank, the experiments are all limited to pairwise attention (not counting the current key element). It would be interesting to demonstrate the effect of scaling up the rank, especially as this is noted as a potential advantage of the GemNet baseline.",
                "Group representation-based approaches have not been compared against."
            ],
            [
                "The details of the four functions that make up the attention mechanism are vague. More elaboration on the technical details, such as what are rotation-invariant geometric quantities for tuples, is needed to improve readability.",
                "The paper lacks a mathematical proof of rotational equivariance.",
                "The paper does not provide information on the efficiency of the proposed method, especially when compared with other works.",
                "The paper does not include experiments testing rotation equivariance. It is recommended to have an experimental test for the rotation equivariance by comparing with other works, such as rotation equivariance achieved via data augmentation.",
                "The paper does not clarify whether the proposed method can be extended to larger point clouds. It is also unclear what the definition of 'small point clouds' is. An experiment on 3D point cloud classification, such as the popularly used Modelnet 40, is recommended."
            ],
            [
                "In Table 1, the proposed model performs worse than the baseline GemNet-Q. The authors should consider adjusting their method to incorporate properties such as the use of quadruplets atoms, the incorporation of energy, or a better architecture, in order to perform a fair comparison with this baseline."
            ],
            [
                "The authors do not clearly describe how the geometric product helps to achieve the rotation-invariant attributes. This should be explained in the main manuscript since this is one of the main contributions.",
                "Geometric products do not appear in both model architectures and appendix B. The authors need to explicitly describe how it is implemented.",
                "The interpretation of the linear combination in geometric algebra is not trivial. The paper lacks the interpretation, justification, and discussion about the attention mechanism in geometric algebra.",
                "The proposed architecture is limited to 'small point clouds' without clear explanation. If the statement about the computational complexity is the reason, the connection to it should be clarified.",
                "The authors defined four functions, V, M, J, and S. However, another function R is defined later. Why do the authors distinguish R from the others? Are the four functions cover everything sufficiently? Justification is required.",
                "The authors stated that GemNets can operate on quadruplets of atoms but the proposed network worked only in pairs of bonds from the central atom. If this is the limitation of the proposed network, clear statement and discussion are required. Moreover, a discussion is appreciated whether the proposed network has technical difficulties to include the force label."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "Crystal Structure Identification: The method used to determine the \"nearest neighbors\" is not clearly stated in the paper. This could potentially introduce bias into the experiment. It would be beneficial to use a standard method for determining nearest neighbors in point clouds, such as k-nearest neighbors (k-NN) or a radius-based method, and to clearly state this method in the paper.",
                "Molecular Force Regression: It is not clear how the authors ensured that the force field was conservative. This is crucial for the validity of the results. The authors should clarify this in the paper. One way to verify this could be to conduct an additional experiment where they calculate the work done by the force field around a closed loop and verify that it is zero, which is a condition for conservative fields.",
                "Protein Coarse-Grain Backmapping: The method used to incorporate local geometry information into the coarse-grained bead representations is not clearly stated in the paper. This is important for the accuracy of the model. The authors should clarify how they incorporated this information. A potential improvement could be to use a method that explicitly takes into account local geometry, such as a geometric deep learning method, and to clearly state this method in the paper."
            ],
            "novelty_impact": [
                "The paper presents a novel approach to geometric deep learning, but it could benefit from a more detailed explanation of how the rotation-and permutation-equivariant architectures were specifically applied in the tasks of crystal structure identification, molecular force regression, and backmapping of coarse-graining operators.",
                "The paper mentions the use of Protein Data Bank (PDB) entries for the coarse-graining task, but it would be helpful to provide more context about these entries and why they were chosen.",
                "The integration of geometric algebra and attention mechanisms in the development of the deep learning architectures is a key aspect of the paper, but it could be explained in more detail.",
                "The paper compares the performance of the models in crystal structure identification to other existing models or methods, but it would be beneficial to provide more context on these comparisons.",
                "The paper mentions potential methods to avoid polynomial scaling that are left for future work. It would be helpful to elaborate on the implications of these potential methods and why they were not explored in the current study.",
                "The paper could benefit from a more detailed discussion on the limitations of the study and potential areas for future research."
            ],
            "clarity_reproducibility": [
                "The paper should provide specific details on the initial learning rate, the batch size, and any regularization techniques used in the training process. These details are crucial for the reproducibility of the experiments.",
                "The criteria for the selection of the 19 protein structures from the Protein Data Bank (PDB) used for the coarse-graining task should be explicitly stated. This will ensure the reproducibility and validity of the results.",
                "More details are needed on the methods for reducing the set of products according to the edges of a specified graph, the Voronoi diagram of the point cloud, or by randomly sampling tuples of points. This will enhance the understanding of the paper's methods.",
                "The paper should provide specific examples of \"invariant\" and \"covariant\" attributes in the proposed architecture or the experiments. This will provide clarity on these important concepts."
            ],
            "all": [
                "Crystal Structure Identification: The method used to determine the \"nearest neighbors\" is not clearly stated in the paper. This could potentially introduce bias into the experiment. It would be beneficial to use a standard method for determining nearest neighbors in point clouds, such as k-nearest neighbors (k-NN) or a radius-based method, and to clearly state this method in the paper.",
                "Molecular Force Regression: It is not clear how the authors ensured that the force field was conservative. This is crucial for the validity of the results. The authors should clarify this in the paper. One way to verify this could be to conduct an additional experiment where they calculate the work done by the force field around a closed loop and verify that it is zero, which is a condition for conservative fields.",
                "Protein Coarse-Grain Backmapping: The method used to incorporate local geometry information into the coarse-grained bead representations is not clearly stated in the paper. This is important for the accuracy of the model. The authors should clarify how they incorporated this information. A potential improvement could be to use a method that explicitly takes into account local geometry, such as a geometric deep learning method, and to clearly state this method in the paper.",
                "The paper presents a novel approach to geometric deep learning, but it could benefit from a more detailed explanation of how the rotation-and permutation-equivariant architectures were specifically applied in the tasks of crystal structure identification, molecular force regression, and backmapping of coarse-graining operators.",
                "The paper mentions the use of Protein Data Bank (PDB) entries for the coarse-graining task, but it would be helpful to provide more context about these entries and why they were chosen.",
                "The integration of geometric algebra and attention mechanisms in the development of the deep learning architectures is a key aspect of the paper, but it could be explained in more detail.",
                "The paper compares the performance of the models in crystal structure identification to other existing models or methods, but it would be beneficial to provide more context on these comparisons.",
                "The paper mentions potential methods to avoid polynomial scaling that are left for future work. It would be helpful to elaborate on the implications of these potential methods and why they were not explored in the current study.",
                "The paper could benefit from a more detailed discussion on the limitations of the study and potential areas for future research.",
                "The paper should provide specific details on the initial learning rate, the batch size, and any regularization techniques used in the training process. These details are crucial for the reproducibility of the experiments.",
                "The criteria for the selection of the 19 protein structures from the Protein Data Bank (PDB) used for the coarse-graining task should be explicitly stated. This will ensure the reproducibility and validity of the results.",
                "More details are needed on the methods for reducing the set of products according to the edges of a specified graph, the Voronoi diagram of the point cloud, or by randomly sampling tuples of points. This will enhance the understanding of the paper's methods.",
                "The paper should provide specific examples of \"invariant\" and \"covariant\" attributes in the proposed architecture or the experiments. This will provide clarity on these important concepts."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [],
            "clarity_reproducibility": [],
            "all": []
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 15,
                "n_generated": 3,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 15,
                "n_generated": 6,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "clarity_reproducibility": {
                "n_reference": 15,
                "n_generated": 4,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 15,
                "n_generated": 13,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                }
            ]
        }
    },
    {
        "doc_id": "cVak2hs06z",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The motivation for CnC is centered on the class-conditional alignment loss. There is even a bound on the worst-group loss in terms of the average-group and class-conditional alignment loss. So why not replace step 2, and instead fine-tune the model using L_avg_group + L_alignment? Or even just train models from scratch with this loss. It would be good to compare to these. If CnC is simply more empirically successful than this alternative, then it would be good to see this.",
                "In a related vein to the previous question, how much is CnC buying us as compared to the usual supervised contrastive training? It would be good to see SupCon as a baseline in Table 1. This seems an important baseline, since the main idea of CnC is to pull items from the same class together in feature space, which is also done using SupCon.  The main (even only?) difference is the hard positive/negative sampling approach of CnC."
            ],
            [
                "The observation of aligning representation for improving model robustness is not novel. There are a lot of work with similar ideas in domain generalization/adaptation literature, e.g., [1], [2]. There\u2019s also a recent work [3] that applies contrastive learning for doing so. A more comprehensive discussion for these related work needs to be included.",
                "The assumption of Theorem 3.1 is not well explained and motivated. In particular, the assumption that \u201cthe loss function l(x; y) is 1-Lipschitz in x and bounded from above by one.\u201d seems to be necessary and simplify the proof a lot, but does not hold for typical losses like cross-entropy for classification and MSE for regression.",
                "Using ERM prediction as the group label is not convincing enough, and it is not clear how it would affect the contrastive part. It could be interesting to more extensively analyze how the label prediction affects the improvement given by the contrastive method, probably using a scientific setup where the label prediction is controlled.",
                "For the contrastive part, the current empirical comparison obfuscates the advantage on its own. To decouple it from the effect of wrong group prediction, it is important to compare in the setting where group labels are available, i.e., GDRO vs GDRO + contrastive. Also, there could be a lot of choices of negative selections but only one is used without sufficient explanations, it would be great to include more explanation or compare with some other possible choices as an ablation study.",
                "Though the proposed contrastive method leads to improved worst-group performance, it seems to decrease the average-case performance compared to baselines."
            ],
            [
                "The proof provided in the paper may not be intuitively useful, particularly due to the bound B on the weight matrix. The weight matrix and the representations are learned jointly, and equivalent predictions could be achieved by scaling the weight matrix down and the representations up. Additionally, the Lipschitz and boundedness constraints on the loss functions do not seem to apply in any of the settings explored experimentally.",
                "Section 5.2 is unclear, particularly regarding the role of ERM 'predicting the sensitive attribute'. It is not clear how ERM's predictions of the sensitive attribute play into the CNC algorithm.",
                "Some important training details, such as the clustering-based prediction from the first step ERM model, are buried in the appendix. These details seem crucial to the functioning of the method and should be discussed in the main body, possibly with an ablation study."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "Benchmark Comparison Experiment: Please ensure that the existing methods chosen for comparison are truly representative of the state-of-the-art and that they were implemented under the same conditions as the proposed method to ensure a fair comparison.",
                "Ablation Study: Consider testing all possible combinations of the two stages in the ablation study. This would help to understand the individual and combined contributions of each stage.",
                "Representation Analysis: Please provide more details on how the representations were visualized or quantified. The choice of visualization or quantification method could potentially influence the results.",
                "Theoretical Analysis: If possible, consider having the theoretical results independently verified. Independent verification would strengthen the validity of these results.",
                "Robustness Test: Please provide more details on how the spurious correlations were introduced into the dataset. The method of introduction could potentially influence the results.",
                "Generalization Test: Please ensure that a variety of datasets were used, including those from different domains or with different types of spurious correlations. This would help to test the generalizability of the method."
            ],
            "novelty_impact": [
                "The paper does a good job of identifying the problem of spurious correlations in machine learning models and proposing a novel method, Correct-N-Contrast (CNC), to address this issue. However, it would be helpful to provide a more detailed explanation of how the CNC method works, particularly how it improves the alignment of representations within each class and how it uses empirical risk minimization (ERM) models to infer group information.",
                "The authors should clarify how the CNC method differentiates between groups without explicit training group labels. While the use of ERM model predictions to infer group information is mentioned, a more detailed explanation of this process would be beneficial.",
                "The paper mentions the use of two-sided contrastive sampling and additional intrinsic hard positive/negative mining techniques in the CNC method. However, these techniques are not explained in detail. Providing a more thorough explanation of these techniques and how they contribute to the effectiveness of the CNC method would strengthen the paper.",
                "The authors mention that the CNC method significantly improves worst-group performance over existing methods on popular benchmarks. However, the specific benchmarks used for comparison are not mentioned. Providing this information would give readers a better understanding of the performance of the CNC method.",
                "The authors discuss the limitations of their current theoretical analysis and propose future work to provide a better understanding of the alignment induced by CNC in the context of spurious correlations. However, the specific limitations of the current analysis and the potential directions for future work are not clearly outlined. Providing more details on these points would be beneficial.",
                "The paper mentions the use of distributionally robust optimization (DRO) and other methods to improve the accessibility of robust training methods. However, these methods are not explained in detail, and it is not clear how they compare to the CNC method. Providing a more thorough comparison of these methods would strengthen the paper.",
                "The authors discuss the similarity of their work to Invariant Risk Minimization (IRM) and other related works in domain-invariant learning. However, the specific similarities and differences are not clearly outlined. Providing a more detailed comparison of these methods would be beneficial.",
                "The paper could benefit from a more detailed discussion of the experimental results, including the specific improvements achieved by the CNC method over prior state-of-the-art methods and the specific benchmarks used for comparison."
            ],
            "clarity_reproducibility": [
                "Please provide specific details about the hyperparameters for the Empirical Risk Minimization (ERM) model. This information is crucial for reproducing the experiments and understanding the performance of the model.",
                "Please provide specific details about how the datasets were preprocessed and split into training, validation, and test sets. This information is essential for reproducing the experiments and comparing results.",
                "Please provide a comprehensive comparison between CNC and standard supervised contrastive learning. This comparison is important for understanding the unique contributions of the CNC method and its advantages over existing methods.",
                "Please include a reproducibility statement and contrastive algorithm design details. This information is crucial for reproducing the experiments and understanding the implementation of the CNC method.",
                "The alignment loss formula includes symbols that are not explained in the summary. Please provide clear explanations for all symbols and terms used in formulas and equations to ensure that readers can understand them."
            ],
            "all": [
                "Benchmark Comparison Experiment: Please ensure that the existing methods chosen for comparison are truly representative of the state-of-the-art and that they were implemented under the same conditions as the proposed method to ensure a fair comparison.",
                "Ablation Study: Consider testing all possible combinations of the two stages in the ablation study. This would help to understand the individual and combined contributions of each stage.",
                "Representation Analysis: Please provide more details on how the representations were visualized or quantified. The choice of visualization or quantification method could potentially influence the results.",
                "Theoretical Analysis: If possible, consider having the theoretical results independently verified. Independent verification would strengthen the validity of these results.",
                "Robustness Test: Please provide more details on how the spurious correlations were introduced into the dataset. The method of introduction could potentially influence the results.",
                "Generalization Test: Please ensure that a variety of datasets were used, including those from different domains or with different types of spurious correlations. This would help to test the generalizability of the method.",
                "The paper does a good job of identifying the problem of spurious correlations in machine learning models and proposing a novel method, Correct-N-Contrast (CNC), to address this issue. However, it would be helpful to provide a more detailed explanation of how the CNC method works, particularly how it improves the alignment of representations within each class and how it uses empirical risk minimization (ERM) models to infer group information.",
                "The authors should clarify how the CNC method differentiates between groups without explicit training group labels. While the use of ERM model predictions to infer group information is mentioned, a more detailed explanation of this process would be beneficial.",
                "The paper mentions the use of two-sided contrastive sampling and additional intrinsic hard positive/negative mining techniques in the CNC method. However, these techniques are not explained in detail. Providing a more thorough explanation of these techniques and how they contribute to the effectiveness of the CNC method would strengthen the paper.",
                "The authors mention that the CNC method significantly improves worst-group performance over existing methods on popular benchmarks. However, the specific benchmarks used for comparison are not mentioned. Providing this information would give readers a better understanding of the performance of the CNC method.",
                "The authors discuss the limitations of their current theoretical analysis and propose future work to provide a better understanding of the alignment induced by CNC in the context of spurious correlations. However, the specific limitations of the current analysis and the potential directions for future work are not clearly outlined. Providing more details on these points would be beneficial.",
                "The paper mentions the use of distributionally robust optimization (DRO) and other methods to improve the accessibility of robust training methods. However, these methods are not explained in detail, and it is not clear how they compare to the CNC method. Providing a more thorough comparison of these methods would strengthen the paper.",
                "The authors discuss the similarity of their work to Invariant Risk Minimization (IRM) and other related works in domain-invariant learning. However, the specific similarities and differences are not clearly outlined. Providing a more detailed comparison of these methods would be beneficial.",
                "The paper could benefit from a more detailed discussion of the experimental results, including the specific improvements achieved by the CNC method over prior state-of-the-art methods and the specific benchmarks used for comparison.",
                "Please provide specific details about the hyperparameters for the Empirical Risk Minimization (ERM) model. This information is crucial for reproducing the experiments and understanding the performance of the model.",
                "Please provide specific details about how the datasets were preprocessed and split into training, validation, and test sets. This information is essential for reproducing the experiments and comparing results.",
                "Please provide a comprehensive comparison between CNC and standard supervised contrastive learning. This comparison is important for understanding the unique contributions of the CNC method and its advantages over existing methods.",
                "Please include a reproducibility statement and contrastive algorithm design details. This information is crucial for reproducing the experiments and understanding the implementation of the CNC method.",
                "The alignment loss formula includes symbols that are not explained in the summary. Please provide clear explanations for all symbols and terms used in formulas and equations to ensure that readers can understand them."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [
                {
                    "reference_id": 1,
                    "predicted_id": 7,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "Section 5.2 is unclear, particularly regarding the role of ERM 'predicting the sensitive attribute'. It is not clear how ERM's predictions of the sensitive attribute play into the CNC algorithm.",
                    "predicted_comment": "The authors should clarify how the CNC method differentiates between groups without explicit training group labels. While the use of ERM model predictions to infer group information is mentioned, a more detailed explanation of this process would be beneficial."
                }
            ],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 1,
                    "predicted_id": 7,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "Section 5.2 is unclear, particularly regarding the role of ERM 'predicting the sensitive attribute'. It is not clear how ERM's predictions of the sensitive attribute play into the CNC algorithm.",
                    "predicted_comment": "The authors should clarify how the CNC method differentiates between groups without explicit training group labels. While the use of ERM model predictions to infer group information is mentioned, a more detailed explanation of this process would be beneficial."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 10,
                "n_generated": 6,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 10,
                "n_generated": 8,
                "n_total_aligns": 1,
                "recall": 0.1111111111111111,
                "precision": 0.041666666666666664,
                "pseudo_jaccard": 0.03333333333333333,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 10,
                "n_generated": 5,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 10,
                "n_generated": 19,
                "n_total_aligns": 1,
                "recall": 0.1111111111111111,
                "precision": 0.017543859649122806,
                "pseudo_jaccard": 0.015873015873015872,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.125,
                    "pseudo_jaccard": 0.1,
                    "n_real": 3
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.05263157894736842,
                    "pseudo_jaccard": 0.047619047619047616,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "2234Pp-9ikZ",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper lacks conceptual innovation. While the results are solid, there is little insight about the behavior of the method. No ablations are performed beyond the comparison with NAGO, making it hard to assess the importance of the individual components.",
                "The paper does not include results on ImageNet.",
                "The results are compared with models that use other teacher architectures, making it hard to determine if the improvements are due to an improved teacher or a better knowledge distillation method.",
                "The claim that a standard neural architecture search would produce architectures sampled from the same distributions and therefore not be efficient in exploring the space of students is insufficiently explored.",
                "The paper does not provide comparisons with other architecture search algorithms for knowledge distillation or surrogate functions, therefore the introduction of a new framework may not be warranted under the claims of the authors."
            ],
            [
                "The application of knowledge distillation (KD) into network architecture generator optimization (NAGO) appears to be naively done, suggesting that the proposed method is incremental and the contribution is limited.",
                "The differences between the proposed method and existing methods are not clearly highlighted.",
                "The comparison of results in Table 1 is not fair. The KD used in this paper used better teachers following the convention, but the competitor KD-LSR and SKD in the table are self-distillation methods, making the comparison meaningless.",
                "On the CIFAR100 dataset, the paper inaccurately reports that CRD used ShuffleNetV1, which has more parameters, while the original paper used WRN-40-2 as a teacher and trained the student of WRN-16-2.",
                "On the MIT67 dataset, AutoKD used the fine-tuned teacher, which is much beneficial to KD in terms of performance, while VID used an ImageNet-pretrained model for transfer learning.",
                "On the CIFAR10 dataset, the compared models have fewer parameters than that of NAGO for AutoKD, making it hard to say that AutoKD outperforms them.",
                "The experimental results are unconvincing. For example, as weight is zero in Figure 2, the accuracies in the table should show consistent performance but are deviated with respect to temperature.",
                "The accuracies of NAGO in Figure 4 appear low compared to the other results in the paper, which needs clarification.",
                "The use of KD on NAS introduces additional computational cost, but this is not clearly compared quantitatively."
            ],
            [
                "The paper's main difference from existing KD-NAS approaches is the search for a family of architectures, which mainly takes advantage of the generator in NAGO. This reduces the novelty and contribution of the work.",
                "The purpose of some experiments is unclear. It would be more beneficial to compare AutoKD with earlier KD-NAS approaches instead of NAGO to show the impact of KD on NAS.",
                "The paper does not clearly articulate how the family of student architectures can benefit knowledge distillation. It is suggested that the performance gains may result from the ensemble of networks.",
                "The comparisons shown in Figure 7 do not seem to make much sense.",
                "The logic and results shown in the visualization of Figure 6 are not clear."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "Ablation Studies: The paper could benefit from ablation studies to test the performance of AutoKD without the NAS component, or with different KD methods. This would help identify the individual contributions of different components of the proposed method and could strengthen the paper's claims.",
                "Statistical Analysis: The paper should include statistical analysis to ensure that any differences in performance are statistically significant. This would provide a more rigorous evaluation of the results and would give readers more confidence in the results.",
                "Sensitivity Analysis: The paper should conduct sensitivity analysis to understand how changes in the hyperparameters of AutoKD affect its performance. This would help understand the robustness of the method to changes in its parameters and could strengthen the paper's claims."
            ],
            "novelty_impact": [
                "The paper could benefit from providing more detailed information on the computational cost of AutoKD. This is crucial for understanding its practicality, especially for deployment on small devices.",
                "The paper's focus on vision tasks leaves a gap in understanding how AutoKD performs with other types of data. It would be beneficial to see experiments conducted on a variety of datasets, including text and audio, to understand the versatility of AutoKD.",
                "The use of traditional KD loss and its comparison with other loss functions is not clear. It would be helpful to see a comparison of AutoKD's performance when using different types of loss functions.",
                "The impact of reduced model size on interpretability is an important aspect that seems to be overlooked. If the reduced model size affects interpretability, it could limit the practical applications of AutoKD.",
                "It would be beneficial to understand how multi-fidelity correlation is measured and how it impacts the overall performance of AutoKD.",
                "The potential loss in the richness of the model's representations due to the reduction in parameters is a concern. This could affect the model's ability to capture complex patterns in the data.",
                "The lack of information on how AutoKD handles overfitting is a significant gap. Overfitting is a common issue in machine learning, and it's crucial to understand how this is addressed in AutoKD.",
                "The definition of an \"optimal\" student architecture seems to be subjective and could vary depending on the specific application or dataset. It would be beneficial to have a more concrete definition or criteria for what constitutes an \"optimal\" student architecture.",
                "The comparison of AutoKD with other KD methods is helpful, but it would be more informative to see a comparison with other automated approaches specifically. This would provide a clearer picture of where AutoKD stands in the field of automated knowledge distillation."
            ],
            "clarity_reproducibility": [
                "The paper should provide more details about the Bayesian Optimization and Neural Architecture Search methods used in AutoKD, specifically how the surrogate model is updated based on the student performances. This would help readers understand the methodology better and reproduce the work.",
                "The paper should clearly define 'sample efficiency' and explain how it is measured. It should also provide more details on how the comparison was made that showed AutoKD is up to 20x more sample efficient than alternative NAS-based KD solutions. This would provide more context and clarity to the claims made in the paper.",
                "The paper should provide specific details about the hardware and software used in the experimental setup for the sake of reproducibility. This would help other researchers replicate the experiments and validate the results.",
                "The paper should provide more details on how the multi-fidelity correlation is measured, specifically how the correlation coefficient is calculated. This would help readers understand the evaluation metrics better.",
                "The paper should provide specific details about how the performance of the large state-of-the-art models is measured. This would provide more clarity on the benchmarks used in the paper.",
                "The paper should provide more details on how the size of the networks in the NAGO search space is determined and whether there is a way to adjust the size of these networks for different tasks or datasets. This would provide more insight into the flexibility and adaptability of the proposed method.",
                "The paper should discuss any potential limitations or drawbacks of AutoKD. This would provide a more balanced view of the proposed method and help readers understand its potential challenges and areas for future improvement."
            ],
            "all": [
                "Ablation Studies: The paper could benefit from ablation studies to test the performance of AutoKD without the NAS component, or with different KD methods. This would help identify the individual contributions of different components of the proposed method and could strengthen the paper's claims.",
                "Statistical Analysis: The paper should include statistical analysis to ensure that any differences in performance are statistically significant. This would provide a more rigorous evaluation of the results and would give readers more confidence in the results.",
                "Sensitivity Analysis: The paper should conduct sensitivity analysis to understand how changes in the hyperparameters of AutoKD affect its performance. This would help understand the robustness of the method to changes in its parameters and could strengthen the paper's claims.",
                "The paper could benefit from providing more detailed information on the computational cost of AutoKD. This is crucial for understanding its practicality, especially for deployment on small devices.",
                "The paper's focus on vision tasks leaves a gap in understanding how AutoKD performs with other types of data. It would be beneficial to see experiments conducted on a variety of datasets, including text and audio, to understand the versatility of AutoKD.",
                "The use of traditional KD loss and its comparison with other loss functions is not clear. It would be helpful to see a comparison of AutoKD's performance when using different types of loss functions.",
                "The impact of reduced model size on interpretability is an important aspect that seems to be overlooked. If the reduced model size affects interpretability, it could limit the practical applications of AutoKD.",
                "It would be beneficial to understand how multi-fidelity correlation is measured and how it impacts the overall performance of AutoKD.",
                "The potential loss in the richness of the model's representations due to the reduction in parameters is a concern. This could affect the model's ability to capture complex patterns in the data.",
                "The lack of information on how AutoKD handles overfitting is a significant gap. Overfitting is a common issue in machine learning, and it's crucial to understand how this is addressed in AutoKD.",
                "The definition of an \"optimal\" student architecture seems to be subjective and could vary depending on the specific application or dataset. It would be beneficial to have a more concrete definition or criteria for what constitutes an \"optimal\" student architecture.",
                "The comparison of AutoKD with other KD methods is helpful, but it would be more informative to see a comparison with other automated approaches specifically. This would provide a clearer picture of where AutoKD stands in the field of automated knowledge distillation.",
                "The paper should provide more details about the Bayesian Optimization and Neural Architecture Search methods used in AutoKD, specifically how the surrogate model is updated based on the student performances. This would help readers understand the methodology better and reproduce the work.",
                "The paper should clearly define 'sample efficiency' and explain how it is measured. It should also provide more details on how the comparison was made that showed AutoKD is up to 20x more sample efficient than alternative NAS-based KD solutions. This would provide more context and clarity to the claims made in the paper.",
                "The paper should provide specific details about the hardware and software used in the experimental setup for the sake of reproducibility. This would help other researchers replicate the experiments and validate the results.",
                "The paper should provide more details on how the multi-fidelity correlation is measured, specifically how the correlation coefficient is calculated. This would help readers understand the evaluation metrics better.",
                "The paper should provide specific details about how the performance of the large state-of-the-art models is measured. This would provide more clarity on the benchmarks used in the paper.",
                "The paper should provide more details on how the size of the networks in the NAGO search space is determined and whether there is a way to adjust the size of these networks for different tasks or datasets. This would provide more insight into the flexibility and adaptability of the proposed method.",
                "The paper should discuss any potential limitations or drawbacks of AutoKD. This would provide a more balanced view of the proposed method and help readers understand its potential challenges and areas for future improvement."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "The paper lacks conceptual innovation. While the results are solid, there is little insight about the behavior of the method. No ablations are performed beyond the comparison with NAGO, making it hard to assess the importance of the individual components.",
                    "predicted_comment": "Ablation Studies: The paper could benefit from ablation studies to test the performance of AutoKD without the NAS component, or with different KD methods. This would help identify the individual contributions of different components of the proposed method and could strengthen the paper's claims."
                }
            ],
            "novelty_impact": [
                {
                    "reference_id": 4,
                    "predicted_id": 11,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "The paper does not provide comparisons with other architecture search algorithms for knowledge distillation or surrogate functions, therefore the introduction of a new framework may not be warranted under the claims of the authors.",
                    "predicted_comment": "The comparison of AutoKD with other KD methods is helpful, but it would be more informative to see a comparison with other automated approaches specifically. This would provide a clearer picture of where AutoKD stands in the field of automated knowledge distillation."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 11,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The results are compared with models that use other teacher architectures, making it hard to determine if the improvements are due to an improved teacher or a better knowledge distillation method.",
                    "predicted_comment": "The comparison of AutoKD with other KD methods is helpful, but it would be more informative to see a comparison with other automated approaches specifically. This would provide a clearer picture of where AutoKD stands in the field of automated knowledge distillation."
                }
            ],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 4,
                    "predicted_id": 11,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "The paper does not provide comparisons with other architecture search algorithms for knowledge distillation or surrogate functions, therefore the introduction of a new framework may not be warranted under the claims of the authors.",
                    "predicted_comment": "The comparison of AutoKD with other KD methods is helpful, but it would be more informative to see a comparison with other automated approaches specifically. This would provide a clearer picture of where AutoKD stands in the field of automated knowledge distillation."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "The paper lacks conceptual innovation. While the results are solid, there is little insight about the behavior of the method. No ablations are performed beyond the comparison with NAGO, making it hard to assess the importance of the individual components.",
                    "predicted_comment": "Ablation Studies: The paper could benefit from ablation studies to test the performance of AutoKD without the NAS component, or with different KD methods. This would help identify the individual contributions of different components of the proposed method and could strengthen the paper's claims."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 11,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The results are compared with models that use other teacher architectures, making it hard to determine if the improvements are due to an improved teacher or a better knowledge distillation method.",
                    "predicted_comment": "The comparison of AutoKD with other KD methods is helpful, but it would be more informative to see a comparison with other automated approaches specifically. This would provide a clearer picture of where AutoKD stands in the field of automated knowledge distillation."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 19,
                "n_generated": 3,
                "n_total_aligns": 1,
                "recall": 0.06666666666666667,
                "precision": 0.1111111111111111,
                "pseudo_jaccard": 0.047619047619047616,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 19,
                "n_generated": 9,
                "n_total_aligns": 2,
                "recall": 0.13333333333333333,
                "precision": 0.037037037037037035,
                "pseudo_jaccard": 0.04,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 19,
                "n_generated": 7,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 19,
                "n_generated": 19,
                "n_total_aligns": 3,
                "recall": 0.19999999999999998,
                "precision": 0.03508771929824561,
                "pseudo_jaccard": 0.03875968992248062,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.2,
                    "precision": 0.3333333333333333,
                    "pseudo_jaccard": 0.14285714285714285,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 9
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.4,
                    "precision": 0.1111111111111111,
                    "pseudo_jaccard": 0.12,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 9
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 9
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                }
            ],
            "all": [
                {
                    "recall": 0.6,
                    "precision": 0.10526315789473684,
                    "pseudo_jaccard": 0.11627906976744186,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 9
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                }
            ]
        }
    },
    {
        "doc_id": "KPEFXR1HdIo",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The authors should report the wall clock time to demonstrate the time performance of the fine-grained model.",
                "The authors should clarify the biggest time step the simulation can deal with, as the current time step is relatively small (0.001s).",
                "The authors should demonstrate how this method can scale to more complex topologies. All the experiments are currently on one square sheet, but it would be beneficial to see if the method can simulate something like a T-shirt or dress, which is more relevant to design and fabrication."
            ],
            [
                "The paper should include a discussion comparing this work and the latest work on differentiable cloth simulation in a mesh-based view, as mentioned in reference [1].",
                "The statement 'Previous research (Liang et al., 2019; Du et al., 2021) suffers from this problem, which unfortunately leads to learning implausible parameter values' is technically inaccurate. Leveraging prior knowledge for physical parameters is now a necessary step for system identification tasks.",
                "The motivation for using a yarn-level simulator is unclear. The experiments are too coarse to show the advantages of more accurate collision handling and detailed presentation, which are the reasons for proposing a yarn-level simulation. A more valid explanation for motivation or a set of better results that are more complex than 25*25 is expected.",
                "The manuscript should clarify why the authors chose the yarn-based method, how they plan to use this simulator, and where they see the simulator fits in. The motivation for using yarn-level, which is originally designed for fine details but applied on over-simplified models, is not convincing."
            ],
            [
                "The authors identify a major shortcoming of previous simulation models, namely the use of discontinuous force terms, and engineer alternative models that approximate the terms from domain literature but have the added benefit of being smooth everywhere.",
                "The paper raises a question about the non-linear stick-slip force term leading to oscillatory behavior, which could potentially lead to free energy being trapped in entropy and therefore thermal loss.",
                "The paper raises a question about why differences between inter/intra parameters (Table 2) and ground truth increase as the grid size increases.",
                "In the Comparison video, it is difficult to spot any significant difference between the different weaves, which is a weakness of the paper.",
                "The paper lacks real-world experiments, and the range of simulated experiments is not very wide, making it difficult to judge the expressivity / gamut of the model.",
                "The paper acknowledges that neither the specific problem nor the general idea of differentiable simulation is new, and it is unclear whether the specific solutions introduced in the paper generalize to other problems."
            ],
            [
                "The paper does not comment on the computational cost and comparisons to baseline methods. It would be beneficial to provide such information in the main paper, especially when compared with non-differentiable simulator [Cirio et al., 2014] and prior work on differentiable cloth simulation [Liang et al., In NeurIPS 2019].",
                "The quantitative results need to be presented in a clearer manner. For example, it would be good to show the material prediction error in percentage in Table 2-4 (rather than the absolute value). It would be beneficial to include the algorithm (refer to Section 3.1 in [Liang et al. 2019]) in the main paper.",
                "It is unclear whether the proposed differentiable simulation is deterministic or not? Whether the material estimation is sensitive to initialization? In this case, please consider reporting the standard deviation for multiple runs of the same setting.",
                "It is unclear what happens if the parameter prior was not provided to the proposed method in the control learning experiment. It would be beneficial to provide such ablation studies and discuss the results in the rebuttal."
            ],
            [
                "The design process of the model is heuristic and lacks analysis or ablation study to justify the design choices made.",
                "The paper does not discuss how the method performs as a function of the magnitude of the forces. Additional experiments should be conducted to illustrate this.",
                "There is a discrepancy between the claim in paragraph-2 of section 4.2.1 and the data presented in Fig-6 and Table-4. The paper claims that the error reduces by around 100 times when the training data increase from 5 frames to 25 frames, but the figures show only a 10-fold reduction.",
                "The experiments focus only on simulated woven cloth, and the models are heavily designed. This raises questions about the generalizability of the proposed method to other materials and weaving structures."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper claims that the proposed model is more explicable, has higher data efficiency, generates more accurate predictions, and is faster in control learning compared to other models. However, the paper lacks a detailed comparison with other existing models to substantiate these claims. It would be beneficial to include a more comprehensive comparison with existing models, detailing the aspects of explicability, data efficiency, prediction accuracy, and speed in control learning. This would provide a clearer picture of the advantages of the proposed model over existing ones.",
                "The paper claims that the proposed model can closely approximate the Stribeck effect while maintaining differentiability. However, a detailed analysis to support this claim is missing. It would be helpful to include an analysis showing how closely the proposed model approximates the Stribeck effect while maintaining differentiability. This would provide more evidence to support the claim and strengthen the paper.",
                "The paper could benefit from a more detailed discussion on the limitations of the proposed model and potential areas for future work. This would provide a more balanced view of the work and help guide future research in this area."
            ],
            "novelty_impact": [
                "The paper presents a novel differentiable physics model for fabrics, which is a significant contribution to the field. However, the authors should provide explicit validation for the forces in the new differentiable shear model to ensure that these forces accurately represent the physical phenomena they're supposed to model.",
                "The authors should provide more information about how the penalty energy for yarn-to-yarn collisions was determined. Without this information, it's difficult to assess the validity of this aspect of the model.",
                "The authors should provide more information about how the ground truth cloth states were determined. This information is crucial for understanding the accuracy of the model's predictions.",
                "The authors should provide more information about how they determined the methods for evaluating the effectiveness and data efficiency of their yarn-level differentiable fabric simulator. This information is crucial for understanding the robustness of the model's performance.",
                "The authors should provide more information about how they made the comparisons to a sheet-level simulator and a Bayesian optimization model. This information is crucial for understanding the relative performance of the model.",
                "The authors should provide more information about how they incorporated the complex interactions between the forces into their model. This information is crucial for understanding the complexity and realism of the model.",
                "The authors should provide more information about how they plan to extend their current model to other materials. This information is crucial for understanding the potential applicability and scalability of the model.",
                "The authors should provide more information about potential sources of error or uncertainty in the mathematical derivations for the model. This information is crucial for understanding the robustness of the model's mathematical foundation.",
                "The authors should consider potential variations in the physical properties and forces acting on a material due to factors like temperature, humidity, and wear and tear. This consideration is crucial for understanding the model's applicability to real-world scenarios.",
                "The authors should consider potential complexities in the representation of the physical properties and forces acting on a material with mathematical equations. This consideration is crucial for understanding the model's ability to accurately represent physical phenomena."
            ],
            "clarity_reproducibility": [
                "The paper could benefit from a more detailed explanation of the methods used to model individual yarn physics and interactions. While the paper mentions the use of differentiable forces and a fine-grained model, it does not provide a clear explanation of how these methods are implemented.",
                "The concept of differentiable forces could be explained more clearly. The paper mentions that these forces facilitate gradient-based learning, but it does not provide a clear definition of differentiable forces or explain how they facilitate learning.",
                "The paper should provide a clearer explanation of the physical parameters used in the model. While the paper mentions parameters such as the damping coefficient, spring force, breakaway force, and shear stiffness, it does not clearly define these parameters or explain how they are measured.",
                "The paper should provide more information on the complex physical structures and materials incorporated in the model. While the paper mentions the incorporation of these elements, it does not provide a detailed explanation of what these structures and materials are or how they are incorporated.",
                "The paper should provide a clearer explanation of how the model's ability to capture subtle dynamics is evaluated. While the paper mentions that the model's ability to capture these dynamics is evaluated through simulations, it does not provide specific metrics or criteria for this evaluation.",
                "The paper should provide more information on the experimental settings. This includes details on the equipment used, the specifications of the materials, and other implementation details.",
                "The paper should provide more information on any hyperparameters in the model, including how they were chosen and tuned.",
                "The paper should provide a clear explanation of how to reproduce the experiments and the model. This includes providing all necessary code, data, and instructions.",
                "The paper should provide a clearer explanation of any background concepts that need to be understood to fully grasp the methods used in the paper.",
                "While the paper is generally well-organized, it could benefit from clearer explanations of the methods, experimental settings, and key concepts."
            ],
            "all": [
                "The paper claims that the proposed model is more explicable, has higher data efficiency, generates more accurate predictions, and is faster in control learning compared to other models. However, the paper lacks a detailed comparison with other existing models to substantiate these claims. It would be beneficial to include a more comprehensive comparison with existing models, detailing the aspects of explicability, data efficiency, prediction accuracy, and speed in control learning. This would provide a clearer picture of the advantages of the proposed model over existing ones.",
                "The paper claims that the proposed model can closely approximate the Stribeck effect while maintaining differentiability. However, a detailed analysis to support this claim is missing. It would be helpful to include an analysis showing how closely the proposed model approximates the Stribeck effect while maintaining differentiability. This would provide more evidence to support the claim and strengthen the paper.",
                "The paper could benefit from a more detailed discussion on the limitations of the proposed model and potential areas for future work. This would provide a more balanced view of the work and help guide future research in this area.",
                "The paper presents a novel differentiable physics model for fabrics, which is a significant contribution to the field. However, the authors should provide explicit validation for the forces in the new differentiable shear model to ensure that these forces accurately represent the physical phenomena they're supposed to model.",
                "The authors should provide more information about how the penalty energy for yarn-to-yarn collisions was determined. Without this information, it's difficult to assess the validity of this aspect of the model.",
                "The authors should provide more information about how the ground truth cloth states were determined. This information is crucial for understanding the accuracy of the model's predictions.",
                "The authors should provide more information about how they determined the methods for evaluating the effectiveness and data efficiency of their yarn-level differentiable fabric simulator. This information is crucial for understanding the robustness of the model's performance.",
                "The authors should provide more information about how they made the comparisons to a sheet-level simulator and a Bayesian optimization model. This information is crucial for understanding the relative performance of the model.",
                "The authors should provide more information about how they incorporated the complex interactions between the forces into their model. This information is crucial for understanding the complexity and realism of the model.",
                "The authors should provide more information about how they plan to extend their current model to other materials. This information is crucial for understanding the potential applicability and scalability of the model.",
                "The authors should provide more information about potential sources of error or uncertainty in the mathematical derivations for the model. This information is crucial for understanding the robustness of the model's mathematical foundation.",
                "The authors should consider potential variations in the physical properties and forces acting on a material due to factors like temperature, humidity, and wear and tear. This consideration is crucial for understanding the model's applicability to real-world scenarios.",
                "The authors should consider potential complexities in the representation of the physical properties and forces acting on a material with mathematical equations. This consideration is crucial for understanding the model's ability to accurately represent physical phenomena.",
                "The paper could benefit from a more detailed explanation of the methods used to model individual yarn physics and interactions. While the paper mentions the use of differentiable forces and a fine-grained model, it does not provide a clear explanation of how these methods are implemented.",
                "The concept of differentiable forces could be explained more clearly. The paper mentions that these forces facilitate gradient-based learning, but it does not provide a clear definition of differentiable forces or explain how they facilitate learning.",
                "The paper should provide a clearer explanation of the physical parameters used in the model. While the paper mentions parameters such as the damping coefficient, spring force, breakaway force, and shear stiffness, it does not clearly define these parameters or explain how they are measured.",
                "The paper should provide more information on the complex physical structures and materials incorporated in the model. While the paper mentions the incorporation of these elements, it does not provide a detailed explanation of what these structures and materials are or how they are incorporated.",
                "The paper should provide a clearer explanation of how the model's ability to capture subtle dynamics is evaluated. While the paper mentions that the model's ability to capture these dynamics is evaluated through simulations, it does not provide specific metrics or criteria for this evaluation.",
                "The paper should provide more information on the experimental settings. This includes details on the equipment used, the specifications of the materials, and other implementation details.",
                "The paper should provide more information on any hyperparameters in the model, including how they were chosen and tuned.",
                "The paper should provide a clear explanation of how to reproduce the experiments and the model. This includes providing all necessary code, data, and instructions.",
                "The paper should provide a clearer explanation of any background concepts that need to be understood to fully grasp the methods used in the paper.",
                "While the paper is generally well-organized, it could benefit from clearer explanations of the methods, experimental settings, and key concepts."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper does not comment on the computational cost and comparisons to baseline methods. It would be beneficial to provide such information in the main paper, especially when compared with non-differentiable simulator [Cirio et al., 2014] and prior work on differentiable cloth simulation [Liang et al., In NeurIPS 2019].",
                    "predicted_comment": "The paper claims that the proposed model is more explicable, has higher data efficiency, generates more accurate predictions, and is faster in control learning compared to other models. However, the paper lacks a detailed comparison with other existing models to substantiate these claims. It would be beneficial to include a more comprehensive comparison with existing models, detailing the aspects of explicability, data efficiency, prediction accuracy, and speed in control learning. This would provide a clearer picture of the advantages of the proposed model over existing ones."
                }
            ],
            "novelty_impact": [
                {
                    "reference_id": 3,
                    "predicted_id": 9,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The experiments focus only on simulated woven cloth, and the models are heavily designed. This raises questions about the generalizability of the proposed method to other materials and weaving structures.",
                    "predicted_comment": "The authors should provide more information about how they plan to extend their current model to other materials. This information is crucial for understanding the potential applicability and scalability of the model."
                }
            ],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper does not comment on the computational cost and comparisons to baseline methods. It would be beneficial to provide such information in the main paper, especially when compared with non-differentiable simulator [Cirio et al., 2014] and prior work on differentiable cloth simulation [Liang et al., In NeurIPS 2019].",
                    "predicted_comment": "The paper claims that the proposed model is more explicable, has higher data efficiency, generates more accurate predictions, and is faster in control learning compared to other models. However, the paper lacks a detailed comparison with other existing models to substantiate these claims. It would be beneficial to include a more comprehensive comparison with existing models, detailing the aspects of explicability, data efficiency, prediction accuracy, and speed in control learning. This would provide a clearer picture of the advantages of the proposed model over existing ones."
                },
                {
                    "reference_id": 3,
                    "predicted_id": 9,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The experiments focus only on simulated woven cloth, and the models are heavily designed. This raises questions about the generalizability of the proposed method to other materials and weaving structures.",
                    "predicted_comment": "The authors should provide more information about how they plan to extend their current model to other materials. This information is crucial for understanding the potential applicability and scalability of the model."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 21,
                "n_generated": 3,
                "n_total_aligns": 1,
                "recall": 0.05,
                "precision": 0.06666666666666667,
                "pseudo_jaccard": 0.03333333333333333,
                "specificity_more": 0.0,
                "specificity_less": 0.0,
                "specificity_same": 1.0
            },
            "novelty_impact": {
                "n_reference": 21,
                "n_generated": 10,
                "n_total_aligns": 1,
                "recall": 0.05,
                "precision": 0.02,
                "pseudo_jaccard": 0.015384615384615385,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 21,
                "n_generated": 10,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 21,
                "n_generated": 23,
                "n_total_aligns": 2,
                "recall": 0.1,
                "precision": 0.017391304347826087,
                "pseudo_jaccard": 0.015384615384615385,
                "specificity_more": 0.5,
                "specificity_less": 0.0,
                "specificity_same": 0.5
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.25,
                    "precision": 0.3333333333333333,
                    "pseudo_jaccard": 0.16666666666666666,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.25,
                    "precision": 0.1,
                    "pseudo_jaccard": 0.07692307692307693,
                    "n_real": 4
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.25,
                    "precision": 0.043478260869565216,
                    "pseudo_jaccard": 0.038461538461538464,
                    "n_real": 4
                },
                {
                    "recall": 0.25,
                    "precision": 0.043478260869565216,
                    "pseudo_jaccard": 0.038461538461538464,
                    "n_real": 4
                }
            ]
        }
    },
    {
        "doc_id": "HyxLRTVKPH",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper does not clearly demonstrate the advantages of using a linear scaling schedule over other schedules. For example, step-based schedules outperform the proposed method in 2 of the 4 tasks in Table 2, and Poly and Cosine schedules perform better in some budgets in Figure 2.",
                "The comparisons in Figure 2 and Table 2 are not convincing without considering the variance of different trials. It is unclear whether the advantage is due to the learning rate schedule or randomness. The paper should report the mean and variance for multiple trials and ideally perform a significance test.",
                "The paper should include other learning rate schedules in Table 2. As shown in Figure 2, step-based schedule is not among the top three schedules for CIFAR-10.",
                "As shown in Table 3, the proposed method requires waiting until the end of training to get the best performing model, while step-based schedule can find the best model around 90% training. This means early stopping is not possible for linear scaling based schedule, which could be useful when the training budget is large enough and evaluation is cheap.",
                "The paper lacks a deeper understanding of why a linear learning rate schedule might work better. The authors attempt to provide an explanation from the relationship between learning rate and gradient magnitudes, but no clear conclusion is given. The paper should investigate when no weight decay is used and whether there is any correlation between the learning rate and gradient norms.",
                "The paper does not specify the learning rate decay unit for the linear schedule. Is it decaying per epoch or per mini-batch? If epoch-based learning rate decay is used, it is essentially step-based learning rate decay with many steps.",
                "The paper should investigate when the linear schedule starts to outperform step-based decay in terms of epochs or iterations. The difference between two schedules for small batch size at the early stage of training might be less in comparison with large batch training, especially when the number of iterations is enough.",
                "The paper only compares different schedules with a single initial learning rate. Different initial learning rates may result in different behaviors. The paper should consider scenarios where the initial learning rate is small, as there might not be much difference for different schedules in such cases.",
                "The authors' rebuttal did not fully address the concern about the lack of early stopping in the proposed method, which could limit its usage, especially for Hyperparameter Optimization (HPO).",
                "The authors' rebuttal did not fully address the concern about the lack of understanding of why the linear scaling schedule is better."
            ],
            [
                "The paper lacks a sound theory explaining why linear decay or other smooth decay methods work well.",
                "The novelty of the methodology proposed by the authors might be limited, as Mishkin et al. have already experimented with linear decay."
            ],
            [
                "The paper makes unclear claims about the phenomena of gradient magnitude vanishing and its effectiveness. For example, in section 5, the authors state 'We call this \u201cvanishing gradient\u201d phenomenon budgeted convergence. This correlation suggests that decaying schedules to near-zero rates (and using BAC) may be more effective than early stopping.' This claim appears to be an overreach as the paper merely shows gradient magnitude decreasing with learning rate.",
                "The paper does not perform extensive comparisons for commonly used algorithms like Adam, which are key motivating use cases for budget-aware training, such as providing approximations for problems like neural architecture search and hyperparameter tuning."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper presents a novel and interesting approach to budgeted training in machine learning. However, it lacks an ablation study to understand the contribution of each component of the proposed method. An ablation study would allow for a more detailed understanding of the contribution of each component, particularly the proposed linear decay schedule. It would help to isolate the effects of individual components and understand their impact on the overall performance. Without it, it's difficult to determine which aspects of the proposed method are most critical to its success. Therefore, I recommend conducting an ablation study and including the results in the experimental section of the paper. This would strengthen the paper by providing deeper insights into the workings of the proposed method and potentially leading to further improvements."
            ],
            "novelty_impact": [
                "The paper could benefit from a discussion on how the approach could be extended to other types of resources, such as memory or computational power. This could make the work more applicable to a wider range of scenarios.",
                "The authors should provide a more detailed discussion on what happens if the gradient does not vanish at the end of each allowed budget. This could help readers understand the potential risks and how to mitigate them.",
                "The conditions under which the linear schedule performs less well should be clarified in the paper. This would guide readers on when to use the linear schedule and when to use other schedules.",
                "While the diversity of the models and datasets used in the experiments is commendable, the authors should discuss whether their approach would work as well with other types of models and datasets not included in their experiments.",
                "The authors should explore how the concept of budgeted training for neural architecture search could apply to other areas of machine learning. This could broaden the impact of their work.",
                "The use of Spearman's rank correlation coefficient to measure the rank prediction capabilities is appropriate. However, the authors should discuss the limitations of this metric and how it might affect their results.",
                "The authors' finding that limiting the number of iterations can significantly outperform offline data subsampling strategies is interesting. However, they should discuss the potential drawbacks of this approach, such as whether it could lead to overfitting.",
                "While the implementation details provided are specific to certain tasks, the authors should provide guidance on how to adapt their approach to other tasks. This could make their work more useful to a wider audience.",
                "The authors' conclusion that smooth-decaying schedules can achieve much better performance under budgeted training is based on empirical evidence. However, a theoretical analysis could strengthen their conclusion. They should consider providing a theoretical analysis in future work.",
                "The authors should discuss other factors that could potentially impact the performance of budgeted training. This could provide a more comprehensive view of the challenges and solutions in budgeted training."
            ],
            "clarity_reproducibility": [
                "The paper could benefit from a more detailed explanation or a step-by-step guide on how the learning rate schedule is adjusted according to the given budget. This would make it easier for readers to understand and implement the proposed method.",
                "The concept of budgeted convergence, while interesting, seems to be based on empirical observations. It would be beneficial if the authors could provide any theoretical justifications or mathematical proofs supporting this concept.",
                "While the experiments seem to be well-designed, it would be helpful if the authors could provide more details about the specific criteria used to evaluate the performance of the models. For example, what metrics were used? How were these metrics calculated?",
                "The ranking of different architectures in the context of neural architecture search is a key aspect of the paper. It would be beneficial if the authors could provide more details about the specific metrics used for this ranking.",
                "The discussion of learning rate schedules is informative, but it would be helpful if the authors could provide more details about how these schedules are implemented. For example, what are the specific equations or algorithms used to calculate the learning rates?",
                "The use of data subsampling as a strategy for budgeted training is mentioned, but no details are provided. It would be beneficial if the authors could provide more information about this strategy.",
                "The experimental setup is well-described, but it would be helpful if the authors could provide more details about the specific models and datasets used. For example, what are the characteristics of these models and datasets? Why were they chosen for the experiments?",
                "The discussion of learning rates in convex optimization is mentioned, but no details are provided. It would be beneficial if the authors could provide more information about this topic.",
                "The paper mentions additional implementation details for various tasks, but these details are not provided in the main text. It would be helpful if the authors could provide these details in the main text of the paper, rather than in an appendix."
            ],
            "all": [
                "The paper presents a novel and interesting approach to budgeted training in machine learning. However, it lacks an ablation study to understand the contribution of each component of the proposed method. An ablation study would allow for a more detailed understanding of the contribution of each component, particularly the proposed linear decay schedule. It would help to isolate the effects of individual components and understand their impact on the overall performance. Without it, it's difficult to determine which aspects of the proposed method are most critical to its success. Therefore, I recommend conducting an ablation study and including the results in the experimental section of the paper. This would strengthen the paper by providing deeper insights into the workings of the proposed method and potentially leading to further improvements.",
                "The paper could benefit from a discussion on how the approach could be extended to other types of resources, such as memory or computational power. This could make the work more applicable to a wider range of scenarios.",
                "The authors should provide a more detailed discussion on what happens if the gradient does not vanish at the end of each allowed budget. This could help readers understand the potential risks and how to mitigate them.",
                "The conditions under which the linear schedule performs less well should be clarified in the paper. This would guide readers on when to use the linear schedule and when to use other schedules.",
                "While the diversity of the models and datasets used in the experiments is commendable, the authors should discuss whether their approach would work as well with other types of models and datasets not included in their experiments.",
                "The authors should explore how the concept of budgeted training for neural architecture search could apply to other areas of machine learning. This could broaden the impact of their work.",
                "The use of Spearman's rank correlation coefficient to measure the rank prediction capabilities is appropriate. However, the authors should discuss the limitations of this metric and how it might affect their results.",
                "The authors' finding that limiting the number of iterations can significantly outperform offline data subsampling strategies is interesting. However, they should discuss the potential drawbacks of this approach, such as whether it could lead to overfitting.",
                "While the implementation details provided are specific to certain tasks, the authors should provide guidance on how to adapt their approach to other tasks. This could make their work more useful to a wider audience.",
                "The authors' conclusion that smooth-decaying schedules can achieve much better performance under budgeted training is based on empirical evidence. However, a theoretical analysis could strengthen their conclusion. They should consider providing a theoretical analysis in future work.",
                "The authors should discuss other factors that could potentially impact the performance of budgeted training. This could provide a more comprehensive view of the challenges and solutions in budgeted training.",
                "The paper could benefit from a more detailed explanation or a step-by-step guide on how the learning rate schedule is adjusted according to the given budget. This would make it easier for readers to understand and implement the proposed method.",
                "The concept of budgeted convergence, while interesting, seems to be based on empirical observations. It would be beneficial if the authors could provide any theoretical justifications or mathematical proofs supporting this concept.",
                "While the experiments seem to be well-designed, it would be helpful if the authors could provide more details about the specific criteria used to evaluate the performance of the models. For example, what metrics were used? How were these metrics calculated?",
                "The ranking of different architectures in the context of neural architecture search is a key aspect of the paper. It would be beneficial if the authors could provide more details about the specific metrics used for this ranking.",
                "The discussion of learning rate schedules is informative, but it would be helpful if the authors could provide more details about how these schedules are implemented. For example, what are the specific equations or algorithms used to calculate the learning rates?",
                "The use of data subsampling as a strategy for budgeted training is mentioned, but no details are provided. It would be beneficial if the authors could provide more information about this strategy.",
                "The experimental setup is well-described, but it would be helpful if the authors could provide more details about the specific models and datasets used. For example, what are the characteristics of these models and datasets? Why were they chosen for the experiments?",
                "The discussion of learning rates in convex optimization is mentioned, but no details are provided. It would be beneficial if the authors could provide more information about this topic.",
                "The paper mentions additional implementation details for various tasks, but these details are not provided in the main text. It would be helpful if the authors could provide these details in the main text of the paper, rather than in an appendix."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [
                {
                    "reference_id": 0,
                    "predicted_id": 9,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper lacks a sound theory explaining why linear decay or other smooth decay methods work well.",
                    "predicted_comment": "The authors' conclusion that smooth-decaying schedules can achieve much better performance under budgeted training is based on empirical evidence. However, a theoretical analysis could strengthen their conclusion. They should consider providing a theoretical analysis in future work."
                }
            ],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 0,
                    "predicted_id": 9,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper lacks a sound theory explaining why linear decay or other smooth decay methods work well.",
                    "predicted_comment": "The authors' conclusion that smooth-decaying schedules can achieve much better performance under budgeted training is based on empirical evidence. However, a theoretical analysis could strengthen their conclusion. They should consider providing a theoretical analysis in future work."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 14,
                "n_generated": 1,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 14,
                "n_generated": 10,
                "n_total_aligns": 1,
                "recall": 0.16666666666666666,
                "precision": 0.03333333333333333,
                "pseudo_jaccard": 0.030303030303030304,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 14,
                "n_generated": 9,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 14,
                "n_generated": 20,
                "n_total_aligns": 1,
                "recall": 0.16666666666666666,
                "precision": 0.016666666666666666,
                "pseudo_jaccard": 0.015873015873015872,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 10
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 10
                },
                {
                    "recall": 0.5,
                    "precision": 0.1,
                    "pseudo_jaccard": 0.09090909090909091,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 10
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 10
                },
                {
                    "recall": 0.5,
                    "precision": 0.05,
                    "pseudo_jaccard": 0.047619047619047616,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                }
            ]
        }
    },
    {
        "doc_id": "giit4HdDNa",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "No theoretical or empirical evidence is presented that confirms that the training speed of N-CODE is significantly lower compared to NODE.",
                "While performance seems generally higher, the limitations of the proposed N-CODE method are not clear.",
                "The claim of better performance in less time is not fully supported by the results."
            ],
            [
                "The supervised classification part of the paper is highly related to the 'Data Controlled Neural ODEs' section in Massaroli et al (2020b), a paper that has been cited in the related work section but not addressed in the main content. The problems tackled and the results are highly related to those of Massaroli et al (2020b). This issue needs to be addressed sufficiently.",
                "The unsupervised section needs more work. The experiments and tables can be described more effectively. For example, it would be preferable to clearly explain which row section 5.3 refers to in Figure 7, and which row corresponds to section 5.4.",
                "The paper claims that the replacement of a linear layer with a Neural CODE improves image reconstruction quality. This should mean that all layers in the encoder can be replaced to give better encoding, taking care of dimensionality (such as in Normalizing Flows). However, this paper uses a typical neural network for image reconstruction and generation. This suggests that the majority of heavy-lifting is done by the decoder, hence the Neural CODE is more amenable to warping the latent space suitable for the decoder. For image generation as well, the latent space has been designed so that the decoder can produce nice images, this is not necessarily a win for the Neural CODE.",
                "To make the case for Neural CODE, especially for images, higher resolution images need to be tackled, since it is in higher dimensions that the success of the advancements in the methods listed in Figure 7 lies."
            ],
            [
                "The authors should investigate how the model performance changes as f grows. Learning the parameters of a neural network via another neural network(s) is a very challenging problem, and it would be beneficial to see this verified or not.",
                "The connection with control theory can be made clearer. There is very little reference to Pontryagin's maximum principle and the link is not visible.",
                "The authors should test vanilla NODE on experiments 5.3 and 5.4. It would be interesting to see if the improvement of N-CODE is significant when NODE is latent."
            ],
            [
                "The paper is closely related to other works that have also focused on making the weights of neural ODEs time dependent. A more thorough discussion of the differences between these models is needed to better understand where the novelty/contribution comes in.",
                "The closed loop model, which is the main innovation, is thoroughly discussed in the paper. However, most experiments are performed with the open loop model, which is very similar to other models already proposed in the literature. This puts into question whether the full closed loop model is actually useful in practice.",
                "There is no evaluation of the number of functions evaluations in the paper. According to the abstract, the model trains faster so it seems important to include this information in the paper.",
                "The authors claim that the model achieves state of the art reconstruction on CIFAR10. However, reconstruction doesn\u2019t really make sense as a task (since the identity function would be optimal in this case). This claim is questionable and should be removed."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "Comparative Analysis: The paper could benefit from a more detailed comparative analysis with other existing models. This would help to gauge the effectiveness of the proposed model in a broader context. (Suggested by Agent 2)",
                "Discussion of Limitations: The authors should discuss any potential limitations or shortcomings of their model. This is a common practice in scientific papers to provide a balanced view of the work. (Suggested by Agent 2)",
                "Replicability: The authors should ensure that the experiments are described in enough detail that they could be replicated by another researcher. This includes details about the model architecture, training procedure, and any hyperparameters used. (Suggested by Agent 3)",
                "Robustness: The authors should demonstrate the robustness of the results across a variety of datasets and tasks. The more robust the results, the more confident we can be in the generalizability of the findings. (Suggested by Agent 3)",
                "Statistical Significance: The authors should ensure that the differences in performance between N-CODE and NODEs or a vanilla autoencoder are statistically significant. If not, the results may be due to random chance rather than a true difference in performance. (Suggested by Agent 3)",
                "Ablation Studies: The authors should conduct ablation studies that show the contribution of each component of N-CODE to its performance. This can help to understand which components are most important for the improved performance. (Suggested by Agent 3)",
                "Comparison to State-of-the-Art: The authors should compare N-CODE to the current state-of-the-art models for the tasks it is evaluated on. If it does not outperform or at least match the performance of the state-of-the-art, this could be a potential shortcoming. (Suggested by Agent 3)",
                "Interpretability: The authors should discuss whether N-CODE maintains or improves interpretability compared to other models. This is especially important in fields where understanding the model's decisions is crucial. (Suggested by Agent 3)",
                "Computational Efficiency: The authors should discuss the computational efficiency of N-CODE. If N-CODE is significantly more computationally expensive than other models, this could limit its practical applicability. (Suggested by Agent 3)"
            ],
            "novelty_impact": [
                "The paper should provide empirical evidence or theoretical justification to support the claim that Neural Ordinary Differential Equations (NODEs) have representational limitations due to their static weight parametrization. This is a significant claim that needs to be substantiated.",
                "The workings of the trainable map in Neurally-Controlled ODEs (N-CODE) that governs the dynamic variables from initial or current activation state need to be explained in more detail. This is a key part of the proposed solution and its lack of clarity could undermine the paper's claims.",
                "The paper should provide more details about the tasks used for the comparison where N-CODE outperforms NODEs in supervised learning tasks. This includes the nature of these tasks and the metrics used to evaluate performance.",
                "The claim that N-CODE leads to state-of-the-art image reconstruction on the CIFAR-10 dataset when applied to an image autoencoder needs to be substantiated with a direct comparison with other state-of-the-art methods.",
                "The paper should provide more details or examples to illustrate how the model that combines autoencoders and generative normalizing flows works in practice, specifically how the latent generative flow is data-dependent and parameterized by the encoder output.",
                "The claim that a simple linear homogeneous differential system is more expressive than a linear layer for shaping the latent representation needs to be supported with specific metrics or criteria for this evaluation.",
                "The paper should clarify how the decrease in the Frechet Inception Distance (FID) of the sampled images with an increasing number of components in the mixture was measured and what it implies about the quality of the generated images.",
                "The paper should provide specific details on how their approach differs from other recent generative techniques and what specific future work they are considering.",
                "The paper should provide specific details on the applications they are considering for their model.",
                "The paper should provide specific details on how their work relates to Neural ODEs, Optimal Control, and Hypernetworks, and what contributions they are making to these fields."
            ],
            "clarity_reproducibility": [
                "The paper should provide more specific details about the trainable map from initial or current activation state in the N-CODE module. This would make it easier for others to understand and implement the N-CODE module.",
                "The paper should explicitly mention the specific metrics used to measure training speed and testing accuracy in supervised learning. This would make it easier for others to reproduce the results.",
                "The paper should provide more specific details about how the control perspective is applied to the image autoencoder in the unsupervised learning section.",
                "The paper should provide more specific details about how the sparse prediction strategy is implemented. This would make it easier for others to understand and implement this strategy.",
                "The paper should provide more specific details about how the learning rate is reduced by half every time the loss plateaus during training. This would make it easier for others to reproduce the training process.",
                "The paper should provide more specific details about how the Frechet Inception Distance (FID) is calculated. This would make it easier for others to understand and reproduce the results.",
                "The paper should provide more details about the optimal encoding control formulation. This would make it easier for others to understand and reproduce the results.",
                "The paper should provide more specific details about the similarities with fast-synaptic modulation systems observed in neuroscience. This would make it easier for others to understand the relevance of these similarities to the N-CODE module."
            ],
            "all": [
                "Comparative Analysis: The paper could benefit from a more detailed comparative analysis with other existing models. This would help to gauge the effectiveness of the proposed model in a broader context. (Suggested by Agent 2)",
                "Discussion of Limitations: The authors should discuss any potential limitations or shortcomings of their model. This is a common practice in scientific papers to provide a balanced view of the work. (Suggested by Agent 2)",
                "Replicability: The authors should ensure that the experiments are described in enough detail that they could be replicated by another researcher. This includes details about the model architecture, training procedure, and any hyperparameters used. (Suggested by Agent 3)",
                "Robustness: The authors should demonstrate the robustness of the results across a variety of datasets and tasks. The more robust the results, the more confident we can be in the generalizability of the findings. (Suggested by Agent 3)",
                "Statistical Significance: The authors should ensure that the differences in performance between N-CODE and NODEs or a vanilla autoencoder are statistically significant. If not, the results may be due to random chance rather than a true difference in performance. (Suggested by Agent 3)",
                "Ablation Studies: The authors should conduct ablation studies that show the contribution of each component of N-CODE to its performance. This can help to understand which components are most important for the improved performance. (Suggested by Agent 3)",
                "Comparison to State-of-the-Art: The authors should compare N-CODE to the current state-of-the-art models for the tasks it is evaluated on. If it does not outperform or at least match the performance of the state-of-the-art, this could be a potential shortcoming. (Suggested by Agent 3)",
                "Interpretability: The authors should discuss whether N-CODE maintains or improves interpretability compared to other models. This is especially important in fields where understanding the model's decisions is crucial. (Suggested by Agent 3)",
                "Computational Efficiency: The authors should discuss the computational efficiency of N-CODE. If N-CODE is significantly more computationally expensive than other models, this could limit its practical applicability. (Suggested by Agent 3)",
                "The paper should provide empirical evidence or theoretical justification to support the claim that Neural Ordinary Differential Equations (NODEs) have representational limitations due to their static weight parametrization. This is a significant claim that needs to be substantiated.",
                "The workings of the trainable map in Neurally-Controlled ODEs (N-CODE) that governs the dynamic variables from initial or current activation state need to be explained in more detail. This is a key part of the proposed solution and its lack of clarity could undermine the paper's claims.",
                "The paper should provide more details about the tasks used for the comparison where N-CODE outperforms NODEs in supervised learning tasks. This includes the nature of these tasks and the metrics used to evaluate performance.",
                "The claim that N-CODE leads to state-of-the-art image reconstruction on the CIFAR-10 dataset when applied to an image autoencoder needs to be substantiated with a direct comparison with other state-of-the-art methods.",
                "The paper should provide more details or examples to illustrate how the model that combines autoencoders and generative normalizing flows works in practice, specifically how the latent generative flow is data-dependent and parameterized by the encoder output.",
                "The claim that a simple linear homogeneous differential system is more expressive than a linear layer for shaping the latent representation needs to be supported with specific metrics or criteria for this evaluation.",
                "The paper should clarify how the decrease in the Frechet Inception Distance (FID) of the sampled images with an increasing number of components in the mixture was measured and what it implies about the quality of the generated images.",
                "The paper should provide specific details on how their approach differs from other recent generative techniques and what specific future work they are considering.",
                "The paper should provide specific details on the applications they are considering for their model.",
                "The paper should provide specific details on how their work relates to Neural ODEs, Optimal Control, and Hypernetworks, and what contributions they are making to these fields.",
                "The paper should provide more specific details about the trainable map from initial or current activation state in the N-CODE module. This would make it easier for others to understand and implement the N-CODE module.",
                "The paper should explicitly mention the specific metrics used to measure training speed and testing accuracy in supervised learning. This would make it easier for others to reproduce the results.",
                "The paper should provide more specific details about how the control perspective is applied to the image autoencoder in the unsupervised learning section.",
                "The paper should provide more specific details about how the sparse prediction strategy is implemented. This would make it easier for others to understand and implement this strategy.",
                "The paper should provide more specific details about how the learning rate is reduced by half every time the loss plateaus during training. This would make it easier for others to reproduce the training process.",
                "The paper should provide more specific details about how the Frechet Inception Distance (FID) is calculated. This would make it easier for others to understand and reproduce the results.",
                "The paper should provide more details about the optimal encoding control formulation. This would make it easier for others to understand and reproduce the results.",
                "The paper should provide more specific details about the similarities with fast-synaptic modulation systems observed in neuroscience. This would make it easier for others to understand the relevance of these similarities to the N-CODE module."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 1,
                    "predicted_id": 1,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "While performance seems generally higher, the limitations of the proposed N-CODE method are not clear.",
                    "predicted_comment": "Discussion of Limitations: The authors should discuss any potential limitations or shortcomings of their model. This is a common practice in scientific papers to provide a balanced view of the work. (Suggested by Agent 2)"
                }
            ],
            "novelty_impact": [
                {
                    "reference_id": 2,
                    "predicted_id": 14,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper claims that the replacement of a linear layer with a Neural CODE improves image reconstruction quality. This should mean that all layers in the encoder can be replaced to give better encoding, taking care of dimensionality (such as in Normalizing Flows). However, this paper uses a typical neural network for image reconstruction and generation. This suggests that the majority of heavy-lifting is done by the decoder, hence the Neural CODE is more amenable to warping the latent space suitable for the decoder. For image generation as well, the latent space has been designed so that the decoder can produce nice images, this is not necessarily a win for the Neural CODE.",
                    "predicted_comment": "The claim that a simple linear homogeneous differential system is more expressive than a linear layer for shaping the latent representation needs to be supported with specific metrics or criteria for this evaluation."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 18,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper is closely related to other works that have also focused on making the weights of neural ODEs time dependent. A more thorough discussion of the differences between these models is needed to better understand where the novelty/contribution comes in.",
                    "predicted_comment": "The paper should provide specific details on how their work relates to Neural ODEs, Optimal Control, and Hypernetworks, and what contributions they are making to these fields."
                }
            ],
            "clarity_reproducibility": [
                {
                    "reference_id": 2,
                    "predicted_id": 20,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "There is no evaluation of the number of functions evaluations in the paper. According to the abstract, the model trains faster so it seems important to include this information in the paper.",
                    "predicted_comment": "The paper should explicitly mention the specific metrics used to measure training speed and testing accuracy in supervised learning. This would make it easier for others to reproduce the results."
                }
            ],
            "all": [
                {
                    "reference_id": 1,
                    "predicted_id": 1,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "While performance seems generally higher, the limitations of the proposed N-CODE method are not clear.",
                    "predicted_comment": "Discussion of Limitations: The authors should discuss any potential limitations or shortcomings of their model. This is a common practice in scientific papers to provide a balanced view of the work. (Suggested by Agent 2)"
                },
                {
                    "reference_id": 2,
                    "predicted_id": 14,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper claims that the replacement of a linear layer with a Neural CODE improves image reconstruction quality. This should mean that all layers in the encoder can be replaced to give better encoding, taking care of dimensionality (such as in Normalizing Flows). However, this paper uses a typical neural network for image reconstruction and generation. This suggests that the majority of heavy-lifting is done by the decoder, hence the Neural CODE is more amenable to warping the latent space suitable for the decoder. For image generation as well, the latent space has been designed so that the decoder can produce nice images, this is not necessarily a win for the Neural CODE.",
                    "predicted_comment": "The claim that a simple linear homogeneous differential system is more expressive than a linear layer for shaping the latent representation needs to be supported with specific metrics or criteria for this evaluation."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 18,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper is closely related to other works that have also focused on making the weights of neural ODEs time dependent. A more thorough discussion of the differences between these models is needed to better understand where the novelty/contribution comes in.",
                    "predicted_comment": "The paper should provide specific details on how their work relates to Neural ODEs, Optimal Control, and Hypernetworks, and what contributions they are making to these fields."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 20,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "There is no evaluation of the number of functions evaluations in the paper. According to the abstract, the model trains faster so it seems important to include this information in the paper.",
                    "predicted_comment": "The paper should explicitly mention the specific metrics used to measure training speed and testing accuracy in supervised learning. This would make it easier for others to reproduce the results."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 14,
                "n_generated": 9,
                "n_total_aligns": 1,
                "recall": 0.08333333333333333,
                "precision": 0.027777777777777776,
                "pseudo_jaccard": 0.022727272727272728,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 14,
                "n_generated": 10,
                "n_total_aligns": 2,
                "recall": 0.125,
                "precision": 0.05,
                "pseudo_jaccard": 0.038461538461538464,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 14,
                "n_generated": 8,
                "n_total_aligns": 1,
                "recall": 0.0625,
                "precision": 0.03125,
                "pseudo_jaccard": 0.022727272727272728,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "all": {
                "n_reference": 14,
                "n_generated": 27,
                "n_total_aligns": 4,
                "recall": 0.2708333333333333,
                "precision": 0.037037037037037035,
                "pseudo_jaccard": 0.034195402298850576,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.1111111111111111,
                    "pseudo_jaccard": 0.09090909090909091,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.25,
                    "precision": 0.1,
                    "pseudo_jaccard": 0.07692307692307693,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.25,
                    "precision": 0.1,
                    "pseudo_jaccard": 0.07692307692307693,
                    "n_real": 4
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.25,
                    "precision": 0.125,
                    "pseudo_jaccard": 0.09090909090909091,
                    "n_real": 4
                }
            ],
            "all": [
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.037037037037037035,
                    "pseudo_jaccard": 0.034482758620689655,
                    "n_real": 3
                },
                {
                    "recall": 0.25,
                    "precision": 0.037037037037037035,
                    "pseudo_jaccard": 0.03333333333333333,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.5,
                    "precision": 0.07407407407407407,
                    "pseudo_jaccard": 0.06896551724137931,
                    "n_real": 4
                }
            ]
        }
    },
    {
        "doc_id": "b7ZRqEFXdQ",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper lacks novelty as it combines existing procedures such as relativistic discriminator, Gumbel-Softmax approximation for categorical distribution, and features matching to improve upon vanilla sequence generation networks.",
                "The generated sentences by the GAN, especially the lengthy ones, are not semantically meaningful as illustrated in Table 8.",
                "The Mean Distance Alignment (MDA) and the Mean Square alignment (MSA) used for features distribution alignment encode the same matching up to a power 2, and it's unclear why they lead to different empirical results.",
                "The paper should consider aligning the overall distributions of the latent representations by considering metrics such as MMD or Wasserstein distance.",
                "The paper should clarify earlier that the used features are extracted from the discrimination network and from which layer of the discrimination network the features are extracted.",
                "The findings of human evaluation are not unequivocal as MSA and MDA achieve higher scores than the real sentences, and the best model with MSA is not preferred due to a lack of diversity and quality."
            ],
            [
                "The authors claim that the use of Gumbel-softmax in GAN training is under-explored, but this is not clear. There are other methods using Gumbel-softmax, such as Gu et al. 2019, and similar softmax with temperature annealing. The authors need to clearly distinguish between using Gumbel-softmax and other smoothed softmax methods.",
                "The authors missed some related work, such as DialogWAE (Gu et al. 2019) and ARAML (Ke et al. 2019). In particular, DialogWAE uses GAN and Gumbel-softmax for text generation, even though it focuses on dialog generation.",
                "To verify mode collapse issues, the authors should consider using Self-BLUE in addition to BLUE scores as a metric to evaluate the diversity.",
                "The novelty of the paper might be incremental. It seems that the novelty comes from using feature statistics alignment. To emphasize the contribution of feature statistics, the authors should consider comparing between the latent feature visualization with and without FSA, in addition to the ablation study."
            ],
            [
                "Although the proposed model is thoroughly evaluated and empirically effective, it is not very different from existing methods, except for the Feature Statistics Alignment (FSA). The application of FSA in this context might be novel; however, the proposed approach seems to be a simple combination of two existing approaches. Therefore, the novelty of the model is limited."
            ],
            [
                "In Table 2, the LSTM model gets NLL lower than the real data, which is a clear evidence of overfitting.",
                "In Table 2, the NLL_{gen} metric, used in SAL (Zhou et. al, 2020) to evaluate the diversity of the generator, is missing without explanation.",
                "In Table 3, the BLEU metric used is the BLEU(F) metric from SAL (Zhou et. al, 2020), which evaluates the generated sentences using the test set as a reference and thus cannot show the diversity of examples.",
                "In Table 3, the BLEU(B) metric from SAL (Zhou et. al, 2020) is missing. This metric evaluates the test set using the generated sentences as a reference, so it can detect mode collapse of a generative model.",
                "The evaluation metrics used in the paper prefer overfitted models, i.e., if the model can remember one example from the training set and repeat that sentence, it can get a very high score.",
                "The authors need to provide more evidence on the diversity of the generated sentences, for example by showing the NLL_{gen} metric on the synthetic dataset, and BLEU(B) metric on real datasets."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper lacks an ablation study to demonstrate the effectiveness of each component of the proposed method. This is a significant shortcoming as it prevents readers from understanding the contribution of each component to the overall performance. To address this issue, the authors should conduct an ablation study where they systematically remove or alter each component of the proposed method and measure the impact on performance.",
                "The paper only compares the proposed method with other GAN-based methods. This limits the scope of the evaluation and prevents a comprehensive understanding of the proposed method's performance relative to the broader field. To address this issue, the authors should expand their comparison to include a wider range of existing methods, including non-GAN-based methods. This would provide a more comprehensive view of the proposed method's performance and make the results more relevant to researchers or practitioners who might be using non-GAN-based methods."
            ],
            "novelty_impact": [
                "The paper should provide a more detailed comparison between the proposed model and existing GANs, discussing the specific advantages and potential drawbacks of the proposed model.",
                "The paper should discuss how the model would perform on other types of datasets, especially those that might have different characteristics than the ones used in the study.",
                "The paper should discuss the limitations of the negative log-likelihood (NLL) and BLEU score metrics and how they might affect the interpretation of the results.",
                "The paper should provide statistical significance values to support the claim that the proposed model outperforms other models.",
                "The paper should explicitly discuss any limitations or assumptions that could potentially affect the generalizability of the results.",
                "The paper should provide more details on why not pretraining the discriminator but only pretraining the generator using MLE for a few epochs helps in addressing the issues of mode collapse and training instability in GANs.",
                "The paper should provide information on the computational requirements of the proposed model.",
                "The paper should provide more details on the performance of the model on sequences of different lengths.",
                "The paper should discuss the potential applications of the model in more detail, including potential challenges and how the proposed model could address them.",
                "The paper should provide information on whether the code for the model is available and, if not, discuss the reasons for this."
            ],
            "clarity_reproducibility": [
                "The paper could provide more details on how the Feature Statistics Alignment (FSA) paradigm forces the mean statistics of the fake data distribution to approach that of real data as close as possible. The specific techniques or methods used could be explained more clearly.",
                "The paper could elaborate on how the relativistic discriminator compares the fake and real distributions from two different aspects. The specific aspects and how they are used in the comparison could be explained more clearly.",
                "The paper could provide more details on how the softmax temperature \u03c4 in the Gumbel-Softmax distribution encourages the generator to explore different options when it is high and tends to exploit during training when it is low.",
                "The paper could provide more details on how the BLEU score, NLL of the generator, and human evaluation via crowdsourcing were used to assess the quality and diversity of the generated sequences. The specific procedures followed and how the results were interpreted could be explained more clearly.",
                "The paper could provide more details on the specific requirements of the task that should be considered when choosing between the LSTM and RMC generators. The specific requirements and how they impact the choice of generator could be explained more clearly.",
                "The paper could discuss potential improvements or future directions for the proposed framework in more detail. The potential for utilizing higher-order statistics and extending the proposed model to conditional text generation, such as text style transfer, could be explored further.",
                "The paper could provide more details on why the specific parameters of the Adam optimizer, the learning rates for the generator and discriminator, and the batch sizes for both synthetic and real datasets were chosen. The experiments or considerations that led to these choices could be explained more clearly.",
                "The sections of the paper that require a deeper understanding of the underlying concepts could be improved for clarity. The concepts could be explained in more detail and in a way that is easier to understand for readers who are not familiar with them.",
                "The paper could discuss any potential limitations or weaknesses of the proposed framework. This would provide a more balanced view of the work and help readers understand the potential challenges and areas for improvement."
            ],
            "all": [
                "The paper lacks an ablation study to demonstrate the effectiveness of each component of the proposed method. This is a significant shortcoming as it prevents readers from understanding the contribution of each component to the overall performance. To address this issue, the authors should conduct an ablation study where they systematically remove or alter each component of the proposed method and measure the impact on performance.",
                "The paper only compares the proposed method with other GAN-based methods. This limits the scope of the evaluation and prevents a comprehensive understanding of the proposed method's performance relative to the broader field. To address this issue, the authors should expand their comparison to include a wider range of existing methods, including non-GAN-based methods. This would provide a more comprehensive view of the proposed method's performance and make the results more relevant to researchers or practitioners who might be using non-GAN-based methods.",
                "The paper should provide a more detailed comparison between the proposed model and existing GANs, discussing the specific advantages and potential drawbacks of the proposed model.",
                "The paper should discuss how the model would perform on other types of datasets, especially those that might have different characteristics than the ones used in the study.",
                "The paper should discuss the limitations of the negative log-likelihood (NLL) and BLEU score metrics and how they might affect the interpretation of the results.",
                "The paper should provide statistical significance values to support the claim that the proposed model outperforms other models.",
                "The paper should explicitly discuss any limitations or assumptions that could potentially affect the generalizability of the results.",
                "The paper should provide more details on why not pretraining the discriminator but only pretraining the generator using MLE for a few epochs helps in addressing the issues of mode collapse and training instability in GANs.",
                "The paper should provide information on the computational requirements of the proposed model.",
                "The paper should provide more details on the performance of the model on sequences of different lengths.",
                "The paper should discuss the potential applications of the model in more detail, including potential challenges and how the proposed model could address them.",
                "The paper should provide information on whether the code for the model is available and, if not, discuss the reasons for this.",
                "The paper could provide more details on how the Feature Statistics Alignment (FSA) paradigm forces the mean statistics of the fake data distribution to approach that of real data as close as possible. The specific techniques or methods used could be explained more clearly.",
                "The paper could elaborate on how the relativistic discriminator compares the fake and real distributions from two different aspects. The specific aspects and how they are used in the comparison could be explained more clearly.",
                "The paper could provide more details on how the softmax temperature \u03c4 in the Gumbel-Softmax distribution encourages the generator to explore different options when it is high and tends to exploit during training when it is low.",
                "The paper could provide more details on how the BLEU score, NLL of the generator, and human evaluation via crowdsourcing were used to assess the quality and diversity of the generated sequences. The specific procedures followed and how the results were interpreted could be explained more clearly.",
                "The paper could provide more details on the specific requirements of the task that should be considered when choosing between the LSTM and RMC generators. The specific requirements and how they impact the choice of generator could be explained more clearly.",
                "The paper could discuss potential improvements or future directions for the proposed framework in more detail. The potential for utilizing higher-order statistics and extending the proposed model to conditional text generation, such as text style transfer, could be explored further.",
                "The paper could provide more details on why the specific parameters of the Adam optimizer, the learning rates for the generator and discriminator, and the batch sizes for both synthetic and real datasets were chosen. The experiments or considerations that led to these choices could be explained more clearly.",
                "The sections of the paper that require a deeper understanding of the underlying concepts could be improved for clarity. The concepts could be explained in more detail and in a way that is easier to understand for readers who are not familiar with them.",
                "The paper could discuss any potential limitations or weaknesses of the proposed framework. This would provide a more balanced view of the work and help readers understand the potential challenges and areas for improvement."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [
                {
                    "reference_id": 4,
                    "predicted_id": 4,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The evaluation metrics used in the paper prefer overfitted models, i.e., if the model can remember one example from the training set and repeat that sentence, it can get a very high score.",
                    "predicted_comment": "The paper should discuss the limitations of the negative log-likelihood (NLL) and BLEU score metrics and how they might affect the interpretation of the results."
                }
            ],
            "clarity_reproducibility": [
                {
                    "reference_id": 5,
                    "predicted_id": 15,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The authors need to provide more evidence on the diversity of the generated sentences, for example by showing the NLL_{gen} metric on the synthetic dataset, and BLEU(B) metric on real datasets.",
                    "predicted_comment": "The paper could provide more details on how the BLEU score, NLL of the generator, and human evaluation via crowdsourcing were used to assess the quality and diversity of the generated sequences. The specific procedures followed and how the results were interpreted could be explained more clearly."
                }
            ],
            "all": [
                {
                    "reference_id": 5,
                    "predicted_id": 15,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The authors need to provide more evidence on the diversity of the generated sentences, for example by showing the NLL_{gen} metric on the synthetic dataset, and BLEU(B) metric on real datasets.",
                    "predicted_comment": "The paper could provide more details on how the BLEU score, NLL of the generator, and human evaluation via crowdsourcing were used to assess the quality and diversity of the generated sequences. The specific procedures followed and how the results were interpreted could be explained more clearly."
                },
                {
                    "reference_id": 4,
                    "predicted_id": 4,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The evaluation metrics used in the paper prefer overfitted models, i.e., if the model can remember one example from the training set and repeat that sentence, it can get a very high score.",
                    "predicted_comment": "The paper should discuss the limitations of the negative log-likelihood (NLL) and BLEU score metrics and how they might affect the interpretation of the results."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 17,
                "n_generated": 2,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 17,
                "n_generated": 10,
                "n_total_aligns": 1,
                "recall": 0.041666666666666664,
                "precision": 0.025,
                "pseudo_jaccard": 0.016666666666666666,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 17,
                "n_generated": 9,
                "n_total_aligns": 1,
                "recall": 0.041666666666666664,
                "precision": 0.027777777777777776,
                "pseudo_jaccard": 0.017857142857142856,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "all": {
                "n_reference": 17,
                "n_generated": 21,
                "n_total_aligns": 2,
                "recall": 0.08333333333333333,
                "precision": 0.023809523809523808,
                "pseudo_jaccard": 0.02,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.16666666666666666,
                    "precision": 0.1,
                    "pseudo_jaccard": 0.06666666666666667,
                    "n_real": 6
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.16666666666666666,
                    "precision": 0.1111111111111111,
                    "pseudo_jaccard": 0.07142857142857142,
                    "n_real": 6
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.09523809523809523,
                    "pseudo_jaccard": 0.08,
                    "n_real": 6
                }
            ]
        }
    },
    {
        "doc_id": "zCu1BZYCueE",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The experiments focus only on selecting a single initial learning rate, which is less interesting because normally a learning rate scheduler that changes over time is used. The experiment does not consider different types of learning rate schedulers for comparisons.",
                "The learning rate is just a scalar, so a simple grid search could be used to coarsely find a good learning rate and then refine it. This makes the focus on learning rate less impactful.",
                "The work would be more impactful if it demonstrated improvement in other hyper-parameter optimizations such as weight decay and convolution filter size and channels."
            ],
            [
                "The paper lacks comparisons to baseline hyperparameter tuning setups. For example, in Figure 3, the best final validation and/or test accuracies achieved by autoHyper and by random search for the same number of trials are not shown.",
                "The paper does not compare the proposed algorithm to existing state-of-the-art methods, which would be useful for a solid paper.",
                "The suggested initial learning rates (LRs) seem problematic to compare to. It would be useful to highlight how the suggested initial LRs were tuned.",
                "The suggested initial LRs were tuned using less trials than the proposed algorithm, and using a grid search algorithm that may be worse than random search.",
                "The suggested initial LRs were for a different model than the models considered in the experiments here, which means they could serve as a poor baseline.",
                "The ranges of values tuned over in Wilson 2017 seem to be quite close to several of the values proposed by autoHyper, meaning that perhaps repeating the same grid search could be competitive.",
                "In the cases where the autoHyper values are not close, they are sometimes outside the range considered by Wilson 2017, which could bias the results towards the proposed algorithm.",
                "Several recent works have shown that when tuning optimizers one needs to be careful to report the ranges used, as changing the hyperparameter ranges can drastically affect experimental results.",
                "Figure 1a would be much more informative if you showed the entire training trajectory, including past the first 5 epochs, to see if the selected learning rates actually generalize noticeably better.",
                "In your conclusion you discuss that you could extend your tuning algorithm to multiple hyperparameters, and I believe in order to truly demonstrate its capabilities this would be required."
            ],
            [
                "The algorithm has an implicit assumption that the response function is monotonously decreasing. The paper should clarify what happens if the response function does not decrease, and how the range of the search area is chosen in such cases.",
                "The algorithm introduces new hyperparameters, such as alpha and the 'rate of change plateaus'. The paper should explain how these hyperparameters are chosen and how they influence the result.",
                "The paper should provide a reasonable explanation for why the initial learning rate influences the testing accuracy, not just the convergence rate for training loss.",
                "The paper only compares the algorithm with the baseline. The authors should discuss if there are any other works about tuning the initial learning rate, or if this paper is the first of its kind.",
                "The reviewer suggests that it may be more convincing if the whole parameters are chosen together, not just the initial learning rate."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper could benefit from conducting multiple trials of learning rates to account for random initialization variations. This would help to ensure the robustness of the results and provide a more accurate representation of the performance of the autoHyper algorithm.",
                "A detailed analysis of the response surface model used by autoHyper would be beneficial. This would provide a deeper understanding of why the method works and could potentially lead to further improvements in the algorithm.",
                "Conducting experiments under different conditions, such as different initializations, different noise levels in the function evaluations, and different computational budgets, would help to ensure the robustness of the method. This would also provide a more comprehensive evaluation of the performance of the autoHyper algorithm.",
                "If not already done, the authors should consider comparing the time taken by autoHyper and other existing HPO methods on various network architectures. This would provide a more concrete validation of the claim of time efficiency.",
                "The authors should ensure that they have conducted experiments on a wide variety of models, optimizers, and datasets. If the variety is not wide enough, it might limit the generalizability of the results.",
                "If not already done, the authors should consider conducting an ablation study where they gradually increase the number of hyperparameters being optimized simultaneously. This would provide a clearer picture of the scalability of the method."
            ],
            "novelty_impact": [
                "The paper lacks a detailed theoretical basis for using the low-rank factorization of the convolution weights of intermediate layers for optimizing the initial learning rate. The authors should provide a more detailed explanation or justification for this choice, and how it fits into the related literature.",
                "The paper does not mention any hyperparameters that need to be set specifically for the low-rank factorization method. This is a potential red flag as almost all optimization methods require some form of hyperparameter tuning. The authors should clarify this point.",
                "The authors' claim that their method generalizes well across different models, optimizers, and datasets is not well-supported by the evidence provided in the paper. The authors should conduct additional experiments to validate this claim, including testing their method on other types of neural networks and other types of data.",
                "The paper lacks specific details on the computational resources used in their experiments and how the computational cost of their method scales with the complexity of the model and the size of the dataset. The authors should provide more detailed information on this point.",
                "The authors' claim that their method revealed \"blind spots\" in their HPO is not well-explained. The authors should provide more details on what these \"blind spots\" are and how their method revealed them.",
                "The paper lacks a direct comparison with other state-of-the-art methods for HPO. The authors should conduct additional experiments to compare their method with other state-of-the-art methods.",
                "The paper lacks specific details on how they ensured the consistency of their results. The authors should provide more detailed information on this point, including the number of runs for each experiment and the use of statistical tests to compare the results."
            ],
            "clarity_reproducibility": [],
            "all": [
                "The paper could benefit from conducting multiple trials of learning rates to account for random initialization variations. This would help to ensure the robustness of the results and provide a more accurate representation of the performance of the autoHyper algorithm.",
                "A detailed analysis of the response surface model used by autoHyper would be beneficial. This would provide a deeper understanding of why the method works and could potentially lead to further improvements in the algorithm.",
                "Conducting experiments under different conditions, such as different initializations, different noise levels in the function evaluations, and different computational budgets, would help to ensure the robustness of the method. This would also provide a more comprehensive evaluation of the performance of the autoHyper algorithm.",
                "If not already done, the authors should consider comparing the time taken by autoHyper and other existing HPO methods on various network architectures. This would provide a more concrete validation of the claim of time efficiency.",
                "The authors should ensure that they have conducted experiments on a wide variety of models, optimizers, and datasets. If the variety is not wide enough, it might limit the generalizability of the results.",
                "If not already done, the authors should consider conducting an ablation study where they gradually increase the number of hyperparameters being optimized simultaneously. This would provide a clearer picture of the scalability of the method.",
                "The paper lacks a detailed theoretical basis for using the low-rank factorization of the convolution weights of intermediate layers for optimizing the initial learning rate. The authors should provide a more detailed explanation or justification for this choice, and how it fits into the related literature.",
                "The paper does not mention any hyperparameters that need to be set specifically for the low-rank factorization method. This is a potential red flag as almost all optimization methods require some form of hyperparameter tuning. The authors should clarify this point.",
                "The authors' claim that their method generalizes well across different models, optimizers, and datasets is not well-supported by the evidence provided in the paper. The authors should conduct additional experiments to validate this claim, including testing their method on other types of neural networks and other types of data.",
                "The paper lacks specific details on the computational resources used in their experiments and how the computational cost of their method scales with the complexity of the model and the size of the dataset. The authors should provide more detailed information on this point.",
                "The authors' claim that their method revealed \"blind spots\" in their HPO is not well-explained. The authors should provide more details on what these \"blind spots\" are and how their method revealed them.",
                "The paper lacks a direct comparison with other state-of-the-art methods for HPO. The authors should conduct additional experiments to compare their method with other state-of-the-art methods.",
                "The paper lacks specific details on how they ensured the consistency of their results. The authors should provide more detailed information on this point, including the number of runs for each experiment and the use of statistical tests to compare the results."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 9,
                    "predicted_id": 5,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "In your conclusion you discuss that you could extend your tuning algorithm to multiple hyperparameters, and I believe in order to truly demonstrate its capabilities this would be required.",
                    "predicted_comment": "If not already done, the authors should consider conducting an ablation study where they gradually increase the number of hyperparameters being optimized simultaneously. This would provide a clearer picture of the scalability of the method."
                },
                {
                    "reference_id": 3,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The suggested initial LRs were tuned using less trials than the proposed algorithm, and using a grid search algorithm that may be worse than random search.",
                    "predicted_comment": "The paper could benefit from conducting multiple trials of learning rates to account for random initialization variations. This would help to ensure the robustness of the results and provide a more accurate representation of the performance of the autoHyper algorithm."
                },
                {
                    "reference_id": 4,
                    "predicted_id": 5,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The reviewer suggests that it may be more convincing if the whole parameters are chosen together, not just the initial learning rate.",
                    "predicted_comment": "If not already done, the authors should consider conducting an ablation study where they gradually increase the number of hyperparameters being optimized simultaneously. This would provide a clearer picture of the scalability of the method."
                }
            ],
            "novelty_impact": [
                {
                    "reference_id": 1,
                    "predicted_id": 11,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper does not compare the proposed algorithm to existing state-of-the-art methods, which would be useful for a solid paper.",
                    "predicted_comment": "The paper lacks a direct comparison with other state-of-the-art methods for HPO. The authors should conduct additional experiments to compare their method with other state-of-the-art methods."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 11,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper lacks comparisons to baseline hyperparameter tuning setups. For example, in Figure 3, the best final validation and/or test accuracies achieved by autoHyper and by random search for the same number of trials are not shown.",
                    "predicted_comment": "The paper lacks a direct comparison with other state-of-the-art methods for HPO. The authors should conduct additional experiments to compare their method with other state-of-the-art methods."
                },
                {
                    "reference_id": 3,
                    "predicted_id": 11,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper only compares the algorithm with the baseline. The authors should discuss if there are any other works about tuning the initial learning rate, or if this paper is the first of its kind.",
                    "predicted_comment": "The paper lacks a direct comparison with other state-of-the-art methods for HPO. The authors should conduct additional experiments to compare their method with other state-of-the-art methods."
                }
            ],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 9,
                    "predicted_id": 5,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "In your conclusion you discuss that you could extend your tuning algorithm to multiple hyperparameters, and I believe in order to truly demonstrate its capabilities this would be required.",
                    "predicted_comment": "If not already done, the authors should consider conducting an ablation study where they gradually increase the number of hyperparameters being optimized simultaneously. This would provide a clearer picture of the scalability of the method."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 11,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper does not compare the proposed algorithm to existing state-of-the-art methods, which would be useful for a solid paper.",
                    "predicted_comment": "The paper lacks a direct comparison with other state-of-the-art methods for HPO. The authors should conduct additional experiments to compare their method with other state-of-the-art methods."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 11,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper lacks comparisons to baseline hyperparameter tuning setups. For example, in Figure 3, the best final validation and/or test accuracies achieved by autoHyper and by random search for the same number of trials are not shown.",
                    "predicted_comment": "The paper lacks a direct comparison with other state-of-the-art methods for HPO. The authors should conduct additional experiments to compare their method with other state-of-the-art methods."
                },
                {
                    "reference_id": 3,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The suggested initial LRs were tuned using less trials than the proposed algorithm, and using a grid search algorithm that may be worse than random search.",
                    "predicted_comment": "The paper could benefit from conducting multiple trials of learning rates to account for random initialization variations. This would help to ensure the robustness of the results and provide a more accurate representation of the performance of the autoHyper algorithm."
                },
                {
                    "reference_id": 3,
                    "predicted_id": 11,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper only compares the algorithm with the baseline. The authors should discuss if there are any other works about tuning the initial learning rate, or if this paper is the first of its kind.",
                    "predicted_comment": "The paper lacks a direct comparison with other state-of-the-art methods for HPO. The authors should conduct additional experiments to compare their method with other state-of-the-art methods."
                },
                {
                    "reference_id": 4,
                    "predicted_id": 5,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The reviewer suggests that it may be more convincing if the whole parameters are chosen together, not just the initial learning rate.",
                    "predicted_comment": "If not already done, the authors should consider conducting an ablation study where they gradually increase the number of hyperparameters being optimized simultaneously. This would provide a clearer picture of the scalability of the method."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 18,
                "n_generated": 6,
                "n_total_aligns": 3,
                "recall": 0.13333333333333333,
                "precision": 0.16666666666666666,
                "pseudo_jaccard": 0.08095238095238096,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 18,
                "n_generated": 7,
                "n_total_aligns": 3,
                "recall": 0.13333333333333333,
                "precision": 0.09523809523809523,
                "pseudo_jaccard": 0.06256109481915934,
                "specificity_more": 0.3333333333333333,
                "specificity_less": 0.0,
                "specificity_same": 0.6666666666666666
            },
            "clarity_reproducibility": {
                "n_reference": 18,
                "n_generated": 0,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 18,
                "n_generated": 13,
                "n_total_aligns": 6,
                "recall": 0.26666666666666666,
                "precision": 0.12820512820512822,
                "pseudo_jaccard": 0.10149572649572651,
                "specificity_more": 0.6666666666666666,
                "specificity_less": 0.0,
                "specificity_same": 0.3333333333333333
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.2,
                    "precision": 0.3333333333333333,
                    "pseudo_jaccard": 0.14285714285714285,
                    "n_real": 10
                },
                {
                    "recall": 0.2,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.1,
                    "n_real": 5
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.2,
                    "precision": 0.14285714285714285,
                    "pseudo_jaccard": 0.0967741935483871,
                    "n_real": 10
                },
                {
                    "recall": 0.2,
                    "precision": 0.14285714285714285,
                    "pseudo_jaccard": 0.09090909090909091,
                    "n_real": 5
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 10
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.4,
                    "precision": 0.23076923076923078,
                    "pseudo_jaccard": 0.1794871794871795,
                    "n_real": 10
                },
                {
                    "recall": 0.4,
                    "precision": 0.15384615384615385,
                    "pseudo_jaccard": 0.125,
                    "n_real": 5
                }
            ]
        }
    },
    {
        "doc_id": "rrWeE9ZDw_",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper claims that the learned object-centric abstraction can be transferred to a new task with the same object type, but it does not define how 'same' is with respect to the same object type in the new task. For example, an apple on a table and an apple on a tree are semantically the same but have different preconditions for the 'pick' action. This suggests that the object-centric abstractions are still more task-specific, even for semantically identical objects.",
                "The paper suggests using Problem-Specific Instantiation to solve the problem of same-type objects having different preconditions in different scenarios. However, this process is similar to manually adjusting the planning domain to adapt to a different scenario, which does not solve the fundamental limitation of the human-defined planning domain.",
                "The learned symbolic predicates of specific object types are not general enough to be applied across different tasks. The proposed object-centric abstraction only groups similar conditions it observed, but does not abstract the preconditions of why an action could be performed. For example, if an apple is placed at different heights, the proposed method will lead to different apple object types as they have different preconditions. If the apple is placed even higher--a condition out of the previously seen cases, the proposed method cannot solve this problem as it does not abstract the 'reachable' concept for the picking action."
            ],
            [
                "The paper claims that the proposed method results in being able to obtain a successful agent for long-term planning with considerably fewer environment interactions, but does not provide a quantitative evaluation against a baseline to demonstrate this reduction in environment interactions.",
                "The paper's comparison with prior works like Kaiser et al. (2020) and Hafner et al. (2021) may not be fair, as the presented method assumes that environment observations are pre-factored and pre-processed into object components, which is a non-trivial task that might explain the sample inefficiency of earlier methods.",
                "The paper should either significantly reduce the prominence of the sample efficiency claims or provide a fair experimental comparison against a baseline that has access to the same factored, pre-processed observations and abstract options as the presented technique."
            ],
            [
                "The paper should clarify why a grouping based on effect distributions alone is preferred over one that also considers the pre-condition.",
                "The paper should provide more details on how it is determined what objects/operators require grounding.",
                "The paper should explain how the clustering proceeds for X, including how X is partitioned and how the result from clustering X is connected to clustering S.",
                "The paper should clarify how much domain knowledge is injected in the method for integrating task-specific information in the learned representations.",
                "The paper should provide more details on the steps needed to achieve the result in the Craft environment experiment, including how the need for having identities is identified, how it is determined which objects require ids, and how the id is connected to the object.",
                "The paper should discuss the limitations of the approach, including the assumptions made such as the frame assumption, that options only affect the object they act upon, and that the world comes pre-segmented according to objects.",
                "The paper should clarify how the grounding is achieved and what knowledge is assumed to arrive at the presented results."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper claims that the learned representations can be transferred between tasks that share the same types of objects, resulting in agents that require fewer samples to learn a model of a new task. However, the paper does not provide a detailed analysis or specific experiments to support this claim. To strengthen this claim, the authors should consider adding an experiment specifically designed to evaluate the transferability of learned operators. This could involve training the model on one task, then transferring the learned operators to a new task and measuring the performance.",
                "The paper does not provide a detailed comparison of their method with other existing methods. Comparisons are crucial in scientific papers to demonstrate the relative strengths and weaknesses of the proposed method. Without these comparisons, it is difficult to assess the novelty and effectiveness of the proposed method. The authors should consider identifying a few key methods in the field that also aim to learn object-centric representations or that also claim transferability. They could then compare their method with these existing methods in terms of performance on the tasks, the quality of the learned representations, and the number of samples required to learn a model of a new task."
            ],
            "novelty_impact": [
                "The paper presents a novel method for autonomously learning an object-centric representation of a continuous and high-dimensional environment that is suitable for planning. However, the concept of \"object-centric\" could be better illustrated with more concrete examples or case studies.",
                "The motivation behind the work is clear and significant - to improve sample efficiency in reinforcement learning by building a compact, high-level model. However, the potential bias or limitations that might be introduced by augmenting the state space with problem-specific information should be explored further.",
                "The goals of the paper are well-defined, but the limitation of the object-centric approach in tasks where the dynamics cannot be fully described by the state of the objects should be discussed more explicitly.",
                "The key findings of the paper are interesting and potentially impactful. However, the paper should provide specific details on how the preprocessing steps affect the performance of the model.",
                "The paper fits into the related literature by proposing a method that learns the type system, predicates, and high-level operators from pixel data. However, the paper should provide more details on why the DBSCAN clustering algorithm and an SVM with Platt scaling were chosen over other methods.",
                "The paper builds and expands on the work in the field by demonstrating the application of the proposed method to Minecraft tasks and a Crafting domain. However, the authors should provide more evidence to support the generalizability of the method, perhaps by applying their method to a wider range of tasks or domains.",
                "The paper should propose a specific metric or method for measuring the quality of the PPDDL operators. This would allow for a more objective evaluation of their approach.",
                "The paper should provide a more rigorous evaluation of the transferability of the learned abstractions, perhaps by conducting experiments that specifically test the transferability of the abstractions.",
                "The authors should justify their choice of tasks more thoroughly. If these tasks are not representative of the range of tasks the method is intended to handle, the authors should explain why they were chosen and discuss how their results might generalize to other tasks.",
                "The authors should provide a more precise definition or measure for \"fewer\" to allow for a more objective evaluation of their claim."
            ],
            "clarity_reproducibility": [
                "The paper could benefit from providing specific metrics for measuring the transferability of the method between tasks that share the same types of objects. This would help readers understand the effectiveness of the method in different contexts.",
                "The paper should elaborate more on the concept of model-based methods in reinforcement learning and how the proposed method extends these. This would provide more background information and make the paper more accessible to readers unfamiliar with these methods.",
                "The paper should provide more details about the five-step process for learning abstract representations of tasks in different environments. This would help readers understand the process and its applicability to different tasks.",
                "The paper should provide specific metrics or benchmarks for measuring the transferability of the learned abstractions to additional procedurally-generated Minecraft tasks. This would help readers evaluate the effectiveness of the learned abstractions.",
                "The paper should provide more details about the method that learns the type system, predicates, and high-level operators from pixel data. This would help readers understand the method and its potential applications.",
                "The paper should provide more details about the visualization of operators in the context of Minecraft and the potential errors that can occur when constructing these abstract representations. This would help readers understand the challenges and limitations of the method.",
                "The paper should provide more details about the quality of the PPDDL operators and how errors in learning can result in imperfect operators. This would help readers understand the potential issues and how they can be addressed.",
                "The paper should provide more information to replicate the experimental settings, including any hyperparameters, equipment and material specifications, or other implementation details. This would help other researchers replicate the study and build upon the work."
            ],
            "all": [
                "The paper claims that the learned representations can be transferred between tasks that share the same types of objects, resulting in agents that require fewer samples to learn a model of a new task. However, the paper does not provide a detailed analysis or specific experiments to support this claim. To strengthen this claim, the authors should consider adding an experiment specifically designed to evaluate the transferability of learned operators. This could involve training the model on one task, then transferring the learned operators to a new task and measuring the performance.",
                "The paper does not provide a detailed comparison of their method with other existing methods. Comparisons are crucial in scientific papers to demonstrate the relative strengths and weaknesses of the proposed method. Without these comparisons, it is difficult to assess the novelty and effectiveness of the proposed method. The authors should consider identifying a few key methods in the field that also aim to learn object-centric representations or that also claim transferability. They could then compare their method with these existing methods in terms of performance on the tasks, the quality of the learned representations, and the number of samples required to learn a model of a new task.",
                "The paper presents a novel method for autonomously learning an object-centric representation of a continuous and high-dimensional environment that is suitable for planning. However, the concept of \"object-centric\" could be better illustrated with more concrete examples or case studies.",
                "The motivation behind the work is clear and significant - to improve sample efficiency in reinforcement learning by building a compact, high-level model. However, the potential bias or limitations that might be introduced by augmenting the state space with problem-specific information should be explored further.",
                "The goals of the paper are well-defined, but the limitation of the object-centric approach in tasks where the dynamics cannot be fully described by the state of the objects should be discussed more explicitly.",
                "The key findings of the paper are interesting and potentially impactful. However, the paper should provide specific details on how the preprocessing steps affect the performance of the model.",
                "The paper fits into the related literature by proposing a method that learns the type system, predicates, and high-level operators from pixel data. However, the paper should provide more details on why the DBSCAN clustering algorithm and an SVM with Platt scaling were chosen over other methods.",
                "The paper builds and expands on the work in the field by demonstrating the application of the proposed method to Minecraft tasks and a Crafting domain. However, the authors should provide more evidence to support the generalizability of the method, perhaps by applying their method to a wider range of tasks or domains.",
                "The paper should propose a specific metric or method for measuring the quality of the PPDDL operators. This would allow for a more objective evaluation of their approach.",
                "The paper should provide a more rigorous evaluation of the transferability of the learned abstractions, perhaps by conducting experiments that specifically test the transferability of the abstractions.",
                "The authors should justify their choice of tasks more thoroughly. If these tasks are not representative of the range of tasks the method is intended to handle, the authors should explain why they were chosen and discuss how their results might generalize to other tasks.",
                "The authors should provide a more precise definition or measure for \"fewer\" to allow for a more objective evaluation of their claim.",
                "The paper could benefit from providing specific metrics for measuring the transferability of the method between tasks that share the same types of objects. This would help readers understand the effectiveness of the method in different contexts.",
                "The paper should elaborate more on the concept of model-based methods in reinforcement learning and how the proposed method extends these. This would provide more background information and make the paper more accessible to readers unfamiliar with these methods.",
                "The paper should provide more details about the five-step process for learning abstract representations of tasks in different environments. This would help readers understand the process and its applicability to different tasks.",
                "The paper should provide specific metrics or benchmarks for measuring the transferability of the learned abstractions to additional procedurally-generated Minecraft tasks. This would help readers evaluate the effectiveness of the learned abstractions.",
                "The paper should provide more details about the method that learns the type system, predicates, and high-level operators from pixel data. This would help readers understand the method and its potential applications.",
                "The paper should provide more details about the visualization of operators in the context of Minecraft and the potential errors that can occur when constructing these abstract representations. This would help readers understand the challenges and limitations of the method.",
                "The paper should provide more details about the quality of the PPDDL operators and how errors in learning can result in imperfect operators. This would help readers understand the potential issues and how they can be addressed.",
                "The paper should provide more information to replicate the experimental settings, including any hyperparameters, equipment and material specifications, or other implementation details. This would help other researchers replicate the study and build upon the work."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 2,
                    "predicted_id": 1,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper should either significantly reduce the prominence of the sample efficiency claims or provide a fair experimental comparison against a baseline that has access to the same factored, pre-processed observations and abstract options as the presented technique.",
                    "predicted_comment": "The paper does not provide a detailed comparison of their method with other existing methods. Comparisons are crucial in scientific papers to demonstrate the relative strengths and weaknesses of the proposed method. Without these comparisons, it is difficult to assess the novelty and effectiveness of the proposed method. The authors should consider identifying a few key methods in the field that also aim to learn object-centric representations or that also claim transferability. They could then compare their method with these existing methods in terms of performance on the tasks, the quality of the learned representations, and the number of samples required to learn a model of a new task."
                }
            ],
            "novelty_impact": [],
            "clarity_reproducibility": [
                {
                    "reference_id": 6,
                    "predicted_id": 16,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper should clarify how the grounding is achieved and what knowledge is assumed to arrive at the presented results.",
                    "predicted_comment": "The paper should provide more details about the method that learns the type system, predicates, and high-level operators from pixel data. This would help readers understand the method and its potential applications."
                }
            ],
            "all": [
                {
                    "reference_id": 2,
                    "predicted_id": 1,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper should either significantly reduce the prominence of the sample efficiency claims or provide a fair experimental comparison against a baseline that has access to the same factored, pre-processed observations and abstract options as the presented technique.",
                    "predicted_comment": "The paper does not provide a detailed comparison of their method with other existing methods. Comparisons are crucial in scientific papers to demonstrate the relative strengths and weaknesses of the proposed method. Without these comparisons, it is difficult to assess the novelty and effectiveness of the proposed method. The authors should consider identifying a few key methods in the field that also aim to learn object-centric representations or that also claim transferability. They could then compare their method with these existing methods in terms of performance on the tasks, the quality of the learned representations, and the number of samples required to learn a model of a new task."
                },
                {
                    "reference_id": 6,
                    "predicted_id": 16,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper should clarify how the grounding is achieved and what knowledge is assumed to arrive at the presented results.",
                    "predicted_comment": "The paper should provide more details about the method that learns the type system, predicates, and high-level operators from pixel data. This would help readers understand the method and its potential applications."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 13,
                "n_generated": 2,
                "n_total_aligns": 1,
                "recall": 0.1111111111111111,
                "precision": 0.16666666666666666,
                "pseudo_jaccard": 0.08333333333333333,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 13,
                "n_generated": 10,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "clarity_reproducibility": {
                "n_reference": 13,
                "n_generated": 8,
                "n_total_aligns": 1,
                "recall": 0.047619047619047616,
                "precision": 0.041666666666666664,
                "pseudo_jaccard": 0.023809523809523808,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "all": {
                "n_reference": 13,
                "n_generated": 20,
                "n_total_aligns": 2,
                "recall": 0.15873015873015872,
                "precision": 0.03333333333333333,
                "pseudo_jaccard": 0.027972027972027972,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.5,
                    "pseudo_jaccard": 0.25,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.14285714285714285,
                    "precision": 0.125,
                    "pseudo_jaccard": 0.07142857142857142,
                    "n_real": 7
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.05,
                    "pseudo_jaccard": 0.045454545454545456,
                    "n_real": 3
                },
                {
                    "recall": 0.14285714285714285,
                    "precision": 0.05,
                    "pseudo_jaccard": 0.038461538461538464,
                    "n_real": 7
                }
            ]
        }
    },
    {
        "doc_id": "EG5Pgd7-MY",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The contributions of the paper are not clearly stated in the introduction section, making it difficult to identify the main contributions.",
                "The proposed framework, which connects membership inference attacks with binary hypothesis testing, lacks novelty as it has been previously proposed in other works.",
                "The paper heavily relies on the assumption introduced in equation (1) to characterize the outcome of the algorithm obtained via SGD training, but no valid proof or formal justification for this assumption is provided.",
                "The paper lacks numerical results and comparisons with competing methods for building attacks in the literature.",
                "The tradeoffs between Type I and Type II error are not always reported.",
                "There is a lack of diverse datasets and relevant architectures to validate the results."
            ],
            [
                "The paper does not clearly explain how having multiple specific forms of attacks can provide a comprehensive picture of whether an algorithm is private or not.",
                "The paper does not provide any justification as to why their set of attacks are 'complete'.",
                "The paper's plans are cryptic and each page has many ambiguous sentences with unclear goals.",
                "The paper does not clearly justify why they pick certain criteria and focus on them.",
                "The paper's definition of privacy is an 'average case, computational, indistinguishability-based' variation of DP, which is not standard DP.",
                "The first attack writes probabilities P(theta|D) as if we are aware of the learning algorithm, which is an assumption that needs to be clarified.",
                "The paper's assertion that 'The population data used for constructing the attack algorithm, and evaluating the inference game, need to be similar, in distribution, to the training data' needs proof.",
                "The paper's claim that 'By violating this principle, we might overestimate the privacy loss' is incorrect as the whole point of DP is that the adversary might have arbitrary auxiliary information.",
                "The paper's statement 'The adversary knows the underlying data distribution' is unclear.",
                "The paper's use of pi(z) in Equation (4) is questionable as z should be independent of theta and D in that case.",
                "The paper does not specify the learner in the experiments of the main body.",
                "The paper needs a much more detailed and justified approach, arguing for 'complete' attacks that at least 'capture known attack techniques so far'.",
                "The paper needs to be much clearer in its criteria that it proposes (in addition to the success rate of the attacks) to be part of the 'report' on privacy."
            ],
            [
                "Many of the attacks proposed could still be inefficient for auditing purposes as one might need to train 1000 reference models to perform the attacks. The numbers are also only high for models that are extremely overfitting. The paper should discuss how to audit models that are well-generalized.",
                "The comparison and difference between this work and prior work is vaguely discussed. There is no detailed comparison with [1], which also formalized MIA with an indistinguishability based framework. The hypothesis testing framework in this work is very similar to the threshold adversary in [1]. The authors should discuss the differences in detail. There is also no comparison to white-box models (e.g. [2]). The authors should discuss how this compares to the proposed framework.",
                "No connection is made between this work and differential privacy, although the indistinguishability setup is highly related. The authors should discuss how the framework translates to privacy loss in DP."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "**Limited Dataset Variety**: The experiments are conducted on three datasets: Purchase100, CIFAR100, and MNIST. While these are commonly used datasets, they may not cover all the possible scenarios where membership inference attacks can be applied. It would be beneficial to include more diverse datasets, including those from different domains (e.g., text, audio, medical, etc.), to ensure the robustness of the results.",
                "**Lack of Baseline Comparisons**: The paper compares different attack strategies but does not mention any comparison with baseline or state-of-the-art methods. This makes it difficult to understand how much improvement the proposed methods offer over existing methods. Please include comparisons with baseline or state-of-the-art methods in your experiments.",
                "**Lack of Ablation Studies**: The paper does not seem to conduct ablation studies to understand the contribution of different components of the proposed methods. For example, it would be interesting to see how much the performance of the proposed methods would drop if some components were removed. Please include ablation studies in your experiments.",
                "**Lack of Analysis on Different Model Architectures**: The paper does not provide details on the types of models used in the experiments. Different model architectures can have different vulnerabilities to membership inference attacks. It would be beneficial to conduct experiments on a variety of model architectures to understand their impact on the performance of the proposed methods.",
                "**Lack of Analysis on Different Privacy Techniques**: The paper discusses the use of membership inference attacks to measure the privacy loss on models trained with differentially private algorithms. However, it does not provide details on the types of differentially private algorithms used in the experiments. Different privacy techniques can have different impacts on the performance of membership inference attacks. It would be beneficial to conduct experiments on a variety of privacy techniques to understand their impact on the performance of the proposed methods."
            ],
            "novelty_impact": [
                "Consider discussing any limitations or potential issues with using the AUC score as the primary performance metric. While the AUC score is a common metric for classification problems, it might not be the best choice for all scenarios.",
                "Provide theoretical justification or proof to support the claim of capturing a precise approximation of privacy loss in models. Empirical evaluation alone might not be sufficient to validate such a claim.",
                "Discuss how the inherent randomness in differentially private algorithms is handled. The inherent randomness in differentially private algorithms could potentially bias the results of the membership inference attacks.",
                "Address the issue of the multiple testing problem in the hypothesis testing framework. This could lead to an inflated type I error rate, which could undermine the validity of the results.",
                "Discuss the generalizability of the results to other types of data. The datasets used for the empirical evaluation might not be representative of all types of data that machine learning models might encounter in real-world applications.",
                "Discuss any potential challenges or limitations with using the method in practice. There could be significant challenges or limitations that have not been considered."
            ],
            "clarity_reproducibility": [
                "The paper could provide more explicit details about the implicit assumptions and simplifications made in prior work that the proposed hypothesis testing framework aims to explain.",
                "The paper could elaborate more on the characteristics and potential errors of each type of membership inference attack (Attack P, Attack R, Attack D, and Attack L).",
                "The paper could provide more specific details on how the new attack algorithms derived from the proposed framework achieve a high AUC score.",
                "The paper could provide more information about the experimental settings, such as the specific configurations of the target models and the datasets used, any preprocessing steps, and specific hyperparameters used in the experiments.",
                "The paper could explain more clearly how the performance of different attacks is measured and compared, and what metrics are used.",
                "The paper could explain more clearly how membership inference attacks are used to measure privacy loss in models trained with differentially private algorithms, and how the empirical performance of these attacks is used to provide lower bounds on the privacy guarantees of these algorithms.",
                "The paper could provide a brief explanation of any background concepts or terms that it assumes the reader is familiar with."
            ],
            "all": [
                "**Limited Dataset Variety**: The experiments are conducted on three datasets: Purchase100, CIFAR100, and MNIST. While these are commonly used datasets, they may not cover all the possible scenarios where membership inference attacks can be applied. It would be beneficial to include more diverse datasets, including those from different domains (e.g., text, audio, medical, etc.), to ensure the robustness of the results.",
                "**Lack of Baseline Comparisons**: The paper compares different attack strategies but does not mention any comparison with baseline or state-of-the-art methods. This makes it difficult to understand how much improvement the proposed methods offer over existing methods. Please include comparisons with baseline or state-of-the-art methods in your experiments.",
                "**Lack of Ablation Studies**: The paper does not seem to conduct ablation studies to understand the contribution of different components of the proposed methods. For example, it would be interesting to see how much the performance of the proposed methods would drop if some components were removed. Please include ablation studies in your experiments.",
                "**Lack of Analysis on Different Model Architectures**: The paper does not provide details on the types of models used in the experiments. Different model architectures can have different vulnerabilities to membership inference attacks. It would be beneficial to conduct experiments on a variety of model architectures to understand their impact on the performance of the proposed methods.",
                "**Lack of Analysis on Different Privacy Techniques**: The paper discusses the use of membership inference attacks to measure the privacy loss on models trained with differentially private algorithms. However, it does not provide details on the types of differentially private algorithms used in the experiments. Different privacy techniques can have different impacts on the performance of membership inference attacks. It would be beneficial to conduct experiments on a variety of privacy techniques to understand their impact on the performance of the proposed methods.",
                "Consider discussing any limitations or potential issues with using the AUC score as the primary performance metric. While the AUC score is a common metric for classification problems, it might not be the best choice for all scenarios.",
                "Provide theoretical justification or proof to support the claim of capturing a precise approximation of privacy loss in models. Empirical evaluation alone might not be sufficient to validate such a claim.",
                "Discuss how the inherent randomness in differentially private algorithms is handled. The inherent randomness in differentially private algorithms could potentially bias the results of the membership inference attacks.",
                "Address the issue of the multiple testing problem in the hypothesis testing framework. This could lead to an inflated type I error rate, which could undermine the validity of the results.",
                "Discuss the generalizability of the results to other types of data. The datasets used for the empirical evaluation might not be representative of all types of data that machine learning models might encounter in real-world applications.",
                "Discuss any potential challenges or limitations with using the method in practice. There could be significant challenges or limitations that have not been considered.",
                "The paper could provide more explicit details about the implicit assumptions and simplifications made in prior work that the proposed hypothesis testing framework aims to explain.",
                "The paper could elaborate more on the characteristics and potential errors of each type of membership inference attack (Attack P, Attack R, Attack D, and Attack L).",
                "The paper could provide more specific details on how the new attack algorithms derived from the proposed framework achieve a high AUC score.",
                "The paper could provide more information about the experimental settings, such as the specific configurations of the target models and the datasets used, any preprocessing steps, and specific hyperparameters used in the experiments.",
                "The paper could explain more clearly how the performance of different attacks is measured and compared, and what metrics are used.",
                "The paper could explain more clearly how membership inference attacks are used to measure privacy loss in models trained with differentially private algorithms, and how the empirical performance of these attacks is used to provide lower bounds on the privacy guarantees of these algorithms.",
                "The paper could provide a brief explanation of any background concepts or terms that it assumes the reader is familiar with."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 3,
                    "predicted_id": 1,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "The paper lacks numerical results and comparisons with competing methods for building attacks in the literature.",
                    "predicted_comment": "**Lack of Baseline Comparisons**: The paper compares different attack strategies but does not mention any comparison with baseline or state-of-the-art methods. This makes it difficult to understand how much improvement the proposed methods offer over existing methods. Please include comparisons with baseline or state-of-the-art methods in your experiments."
                },
                {
                    "reference_id": 5,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "There is a lack of diverse datasets and relevant architectures to validate the results.",
                    "predicted_comment": "**Limited Dataset Variety**: The experiments are conducted on three datasets: Purchase100, CIFAR100, and MNIST. While these are commonly used datasets, they may not cover all the possible scenarios where membership inference attacks can be applied. It would be beneficial to include more diverse datasets, including those from different domains (e.g., text, audio, medical, etc.), to ensure the robustness of the results."
                }
            ],
            "novelty_impact": [
                {
                    "reference_id": 4,
                    "predicted_id": 8,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The tradeoffs between Type I and Type II error are not always reported.",
                    "predicted_comment": "Address the issue of the multiple testing problem in the hypothesis testing framework. This could lead to an inflated type I error rate, which could undermine the validity of the results."
                }
            ],
            "clarity_reproducibility": [
                {
                    "reference_id": 12,
                    "predicted_id": 15,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper needs to be much clearer in its criteria that it proposes (in addition to the success rate of the attacks) to be part of the 'report' on privacy.",
                    "predicted_comment": "The paper could explain more clearly how the performance of different attacks is measured and compared, and what metrics are used."
                }
            ],
            "all": [
                {
                    "reference_id": 3,
                    "predicted_id": 1,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "The paper lacks numerical results and comparisons with competing methods for building attacks in the literature.",
                    "predicted_comment": "**Lack of Baseline Comparisons**: The paper compares different attack strategies but does not mention any comparison with baseline or state-of-the-art methods. This makes it difficult to understand how much improvement the proposed methods offer over existing methods. Please include comparisons with baseline or state-of-the-art methods in your experiments."
                },
                {
                    "reference_id": 5,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "There is a lack of diverse datasets and relevant architectures to validate the results.",
                    "predicted_comment": "**Limited Dataset Variety**: The experiments are conducted on three datasets: Purchase100, CIFAR100, and MNIST. While these are commonly used datasets, they may not cover all the possible scenarios where membership inference attacks can be applied. It would be beneficial to include more diverse datasets, including those from different domains (e.g., text, audio, medical, etc.), to ensure the robustness of the results."
                },
                {
                    "reference_id": 4,
                    "predicted_id": 8,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The tradeoffs between Type I and Type II error are not always reported.",
                    "predicted_comment": "Address the issue of the multiple testing problem in the hypothesis testing framework. This could lead to an inflated type I error rate, which could undermine the validity of the results."
                },
                {
                    "reference_id": 12,
                    "predicted_id": 15,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper needs to be much clearer in its criteria that it proposes (in addition to the success rate of the attacks) to be part of the 'report' on privacy.",
                    "predicted_comment": "The paper could explain more clearly how the performance of different attacks is measured and compared, and what metrics are used."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 22,
                "n_generated": 5,
                "n_total_aligns": 2,
                "recall": 0.1111111111111111,
                "precision": 0.13333333333333333,
                "pseudo_jaccard": 0.07407407407407407,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 22,
                "n_generated": 6,
                "n_total_aligns": 1,
                "recall": 0.05555555555555555,
                "precision": 0.05555555555555555,
                "pseudo_jaccard": 0.030303030303030304,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 22,
                "n_generated": 7,
                "n_total_aligns": 1,
                "recall": 0.025641025641025644,
                "precision": 0.047619047619047616,
                "pseudo_jaccard": 0.017543859649122806,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "all": {
                "n_reference": 22,
                "n_generated": 18,
                "n_total_aligns": 4,
                "recall": 0.1923076923076923,
                "precision": 0.07407407407407407,
                "pseudo_jaccard": 0.05873015873015872,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.4,
                    "pseudo_jaccard": 0.2222222222222222,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 13
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.16666666666666666,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.09090909090909091,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 13
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.07692307692307693,
                    "precision": 0.14285714285714285,
                    "pseudo_jaccard": 0.05263157894736842,
                    "n_real": 13
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "all": [
                {
                    "recall": 0.5,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.14285714285714285,
                    "n_real": 6
                },
                {
                    "recall": 0.07692307692307693,
                    "precision": 0.05555555555555555,
                    "pseudo_jaccard": 0.03333333333333333,
                    "n_real": 13
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "DILxQP08O3B",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper doesn't fully address why DETR performs better than FasterRCNN features. It's not clear why DETR is doing better than Faster-RCNN especially when the detection accuracy of DETR is also better than Faster RCNN.",
                "The method for obtaining the appearance features from Faster RCNN based method is not clear. It's not explained how it's different from the approach taken by Bottom-Up, Top-Down paper in which 2048-dim appearance features are extracted for each visual region.",
                "The experimental setup isn't fully reflective of the object goal navigation task. The experiments are conducted in AI2 thor scenes which only contain one room. It's not clear how this method will perform when evaluated on significantly more complicated environments like Matterport / Gibson.",
                "The navigation task is simplified by discretizing into a grid. Single room environments and discrete grids simplify a lot of navigation-related challenges and the authors don't discuss how the proposed architecture will generalize to more complex object navigation tasks.",
                "The use of spatial embeddings as well as appearance embedding isn't all that surprising. Existing work including Du et al. uses bounding box coordinates to help learn spatial associations between objects.",
                "It's not clear how the agent learns long-term associations between objects across multiple frames. The proposed architecture puts all the burden of learning these long-term object relationships across multiple frames on the LSTM policy since the VT only learns association within a single frame."
            ],
            [
                "The paper adopts the Transformer and adapts it for the navigation problem, but does not propose a new architecture or model.",
                "A similar usage of the Transformer has already been demonstrated in the vision-and-language navigation task. The referenced paper also shows that pre-training of navigation tasks using Transformers can help to boost performance."
            ],
            [
                "The writing in the approach section isn't very clear. It would be better to define clear notations for all the features/descriptors and use such notations in the figure. The current writing uses various terms like 'instance feature', 'global feature', 'positional-global', 'spatial feature', 'spatial-enhanced', which are confusing.",
                "The details in Fig.2 are not sufficient and it is not as informative as the detailed version (Fig.4) in Appendix. These two figures are not consistent, for example, the 'add' symbol for positional enhancement is missing in Fig.4.",
                "In Sec.4.2, it is not clear how the reduction of the channel dimension of a high-level activation map from D to a smaller dimension d is done.",
                "The terms $h$, 'w', and $L$ are used inconsistently in the paper, causing confusion.",
                "The statement in Sec.4.1 about Faster RCNN being blamed for scale-sensitive is not fully convincing. It is unclear what scale-sensitive means here and why DETR doesn't suffer from it.",
                "The statement about 'early stopping' in Sec.4.4 is not fully convincing. It is unclear why the transformer-based representation learner suffers from 'early stopping'.",
                "The SPL formulation in Sec.5.1 seems to be incorrect. The success indicator seems to be missing.",
                "It is unclear why global features are not also added into the transformer encoder."
            ],
            [
                "The introduction of the paper is difficult to understand as it discusses details such as the difference between DETR and Faster RCNN or difficulty of training the transformers without providing a high-level overview of the proposed method.",
                "The paper does not provide a satisfactory explanation for why nothing is learned without the imitation pre-training.",
                "The paper should run the method on other frameworks which include slightly larger scenes, such as RoboTHOR, to test if the method generalizes to those as well."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "Robustness Test: The paper could benefit from a robustness test to evaluate the performance of VTNet under various conditions. This would provide a more comprehensive understanding of how well VTNet performs under different lighting conditions, object densities, or navigation complexities. We recommend conducting additional experiments to test the robustness of VTNet under these conditions and including the results in the paper.",
                "Statistical Significance Test: The paper lacks a statistical significance test on the results of the experiments. This test is crucial to ensure that the differences observed in the performance of VTNet and other methods are not due to random chance. We recommend conducting a statistical significance test on your results and including the results in the paper. This would provide more confidence in the results and the claims made in the paper."
            ],
            "novelty_impact": [
                "The paper should explicitly state the assumptions about the environment in which the VTNet operates. This would help readers understand the applicability of VTNet in real-world scenarios.",
                "More details should be provided on how the pre-training scheme handles variations in visual representations, such as different lighting conditions, object occlusions, or changes in object appearances. This would strengthen the robustness of VTNet.",
                "It should be clarified whether the other methods compared with VTNet were trained in the same conditions. This would ensure a fair comparison of the methods.",
                "Additional metrics that measure the system's robustness to unexpected obstacles should be considered. While success rate and SPL are useful, they do not capture the entire picture of VTNet's performance.",
                "The ablation study should be extended to include scenarios where multiple components are removed or modified at the same time. This would provide a more comprehensive understanding of the interactions between the components of VTNet.",
                "A more direct comparison of the learning speed between VTNet and other methods would strengthen the conclusion that VTNet can expedite navigation policy learning.",
                "VTNet should be tested in a wider variety of environments. This would better represent the variety of real-world scenarios that VTNet might encounter and strengthen the claim that VTNet significantly outperforms state-of-the-art methods in unseen testing environments."
            ],
            "clarity_reproducibility": [
                "The paper should provide specific details about how the object detector DETR transforms encoded features to detection results. This information is crucial for understanding how the spatial-enhanced local descriptor is calculated.",
                "The paper should provide specific details about how the global observation is divided into multiple regions based on spatial layouts for the positional global descriptor. This information is crucial for understanding how the positional global descriptor is calculated.",
                "The paper should provide specific details about the human instructions used in the warming-up process of the pre-training scheme. This information is crucial for understanding how the VT learns to encode directional signals.",
                "The paper should provide specific details about the size of the LSTM network used in the training and evaluation protocols. This information is crucial for reproducing the work.",
                "The paper should provide specific details about the content of each room in the AI2-Thor environment. This information is crucial for understanding the experimental settings and for reproducing the work.",
                "The paper should provide specific details about the conditions or settings in the AI2-Thor environment. This information is crucial for understanding the experimental settings and for reproducing the work.",
                "The paper should provide specific details about how the length of each episode and its optimal path are determined for calculating the success rate and SPL. This information is crucial for evaluating the model's performance and for comparing it with other methods.",
                "The paper should provide specific details on the settings or conditions that are kept constant across all methods in the comparison. This information is crucial for comparing the performance of VTNet with other methods.",
                "The paper should provide specific details about the variant and ablation study. This information is crucial for understanding the impact of each component in VTNet.",
                "The paper should mention any specific software or hardware requirements for implementing VTNet. This information is crucial for reproducing the work."
            ],
            "all": [
                "Robustness Test: The paper could benefit from a robustness test to evaluate the performance of VTNet under various conditions. This would provide a more comprehensive understanding of how well VTNet performs under different lighting conditions, object densities, or navigation complexities. We recommend conducting additional experiments to test the robustness of VTNet under these conditions and including the results in the paper.",
                "Statistical Significance Test: The paper lacks a statistical significance test on the results of the experiments. This test is crucial to ensure that the differences observed in the performance of VTNet and other methods are not due to random chance. We recommend conducting a statistical significance test on your results and including the results in the paper. This would provide more confidence in the results and the claims made in the paper.",
                "The paper should explicitly state the assumptions about the environment in which the VTNet operates. This would help readers understand the applicability of VTNet in real-world scenarios.",
                "More details should be provided on how the pre-training scheme handles variations in visual representations, such as different lighting conditions, object occlusions, or changes in object appearances. This would strengthen the robustness of VTNet.",
                "It should be clarified whether the other methods compared with VTNet were trained in the same conditions. This would ensure a fair comparison of the methods.",
                "Additional metrics that measure the system's robustness to unexpected obstacles should be considered. While success rate and SPL are useful, they do not capture the entire picture of VTNet's performance.",
                "The ablation study should be extended to include scenarios where multiple components are removed or modified at the same time. This would provide a more comprehensive understanding of the interactions between the components of VTNet.",
                "A more direct comparison of the learning speed between VTNet and other methods would strengthen the conclusion that VTNet can expedite navigation policy learning.",
                "VTNet should be tested in a wider variety of environments. This would better represent the variety of real-world scenarios that VTNet might encounter and strengthen the claim that VTNet significantly outperforms state-of-the-art methods in unseen testing environments.",
                "The paper should provide specific details about how the object detector DETR transforms encoded features to detection results. This information is crucial for understanding how the spatial-enhanced local descriptor is calculated.",
                "The paper should provide specific details about how the global observation is divided into multiple regions based on spatial layouts for the positional global descriptor. This information is crucial for understanding how the positional global descriptor is calculated.",
                "The paper should provide specific details about the human instructions used in the warming-up process of the pre-training scheme. This information is crucial for understanding how the VT learns to encode directional signals.",
                "The paper should provide specific details about the size of the LSTM network used in the training and evaluation protocols. This information is crucial for reproducing the work.",
                "The paper should provide specific details about the content of each room in the AI2-Thor environment. This information is crucial for understanding the experimental settings and for reproducing the work.",
                "The paper should provide specific details about the conditions or settings in the AI2-Thor environment. This information is crucial for understanding the experimental settings and for reproducing the work.",
                "The paper should provide specific details about how the length of each episode and its optimal path are determined for calculating the success rate and SPL. This information is crucial for evaluating the model's performance and for comparing it with other methods.",
                "The paper should provide specific details on the settings or conditions that are kept constant across all methods in the comparison. This information is crucial for comparing the performance of VTNet with other methods.",
                "The paper should provide specific details about the variant and ablation study. This information is crucial for understanding the impact of each component in VTNet.",
                "The paper should mention any specific software or hardware requirements for implementing VTNet. This information is crucial for reproducing the work."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [
                {
                    "reference_id": 3,
                    "predicted_id": 8,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The navigation task is simplified by discretizing into a grid. Single room environments and discrete grids simplify a lot of navigation-related challenges and the authors don't discuss how the proposed architecture will generalize to more complex object navigation tasks.",
                    "predicted_comment": "VTNet should be tested in a wider variety of environments. This would better represent the variety of real-world scenarios that VTNet might encounter and strengthen the claim that VTNet significantly outperforms state-of-the-art methods in unseen testing environments."
                }
            ],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 3,
                    "predicted_id": 8,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The navigation task is simplified by discretizing into a grid. Single room environments and discrete grids simplify a lot of navigation-related challenges and the authors don't discuss how the proposed architecture will generalize to more complex object navigation tasks.",
                    "predicted_comment": "VTNet should be tested in a wider variety of environments. This would better represent the variety of real-world scenarios that VTNet might encounter and strengthen the claim that VTNet significantly outperforms state-of-the-art methods in unseen testing environments."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 19,
                "n_generated": 2,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 19,
                "n_generated": 7,
                "n_total_aligns": 1,
                "recall": 0.041666666666666664,
                "precision": 0.03571428571428571,
                "pseudo_jaccard": 0.020833333333333332,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 19,
                "n_generated": 10,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 19,
                "n_generated": 19,
                "n_total_aligns": 1,
                "recall": 0.041666666666666664,
                "precision": 0.013157894736842105,
                "pseudo_jaccard": 0.010416666666666666,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.16666666666666666,
                    "precision": 0.14285714285714285,
                    "pseudo_jaccard": 0.08333333333333333,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "all": [
                {
                    "recall": 0.16666666666666666,
                    "precision": 0.05263157894736842,
                    "pseudo_jaccard": 0.041666666666666664,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "fmOOI2a3tQP",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper does not clearly demonstrate if any insight is gained from the theoretical analysis, as the derivation of value and sample complexity bounds for approximate bisimulation was already performed in Gelada et al. 2019.",
                "The empirical gains are modest and it's unclear if they are due to the image representation learning component of the loss or other aspects of the method.",
                "The paper uses a very small number of training tasks - only 4. It's unclear if using more training tasks would improve the generalization of single-task methods like DeepMDP.",
                "There seems to be a missing baseline - a generic multi-task algorithm that conditions on the environment ID but does use the bisimulation loss to help process the image observations.",
                "It's unclear why there's a gap between your method in the multi-task experiments but not in the meta-RL ones.",
                "It's not explained why HiP-BMDP handles the sticky observation setting better than the baselines.",
                "The paper does not convincingly demonstrate that the structural assumptions on the MDP used by the proposed algorithm yield performance improvement."
            ],
            []
        ],
        "generated_comments": {
            "experiments_proofs": [
                "Depth of Analysis: While the authors have conducted a thorough analysis of their proposed methods, there might be room for a more in-depth analysis to provide a comprehensive understanding of the results and their implications. The authors should consider providing more detailed insights into the results and fully validating the claims of the paper.",
                "Number of Baselines: The authors have compared their method with several baselines. However, using a variety of baselines can help ensure that the results are not specific to a particular comparison and can generalize across different settings. The authors should consider using more baselines for comparison in their experiments.",
                "Hypothesis Support: The authors hypothesize that one of the baselines, Distral-Ensemble, behaves poorly because it cannot leverage a shared global dynamics model. This could potentially be seen as a shortcoming if the authors did not provide sufficient evidence to support this hypothesis. The authors should consider providing more evidence or conducting additional experiments to support this hypothesis.",
                "Limitations: The authors note that Meta-RL techniques are too time-intensive to train on pixel observations directly, which could be a limitation of the study. The authors should consider discussing this limitation in more detail and possibly suggesting ways to overcome it in future work."
            ],
            "novelty_impact": [
                "The paper presents a novel framework that combines the concepts of Hidden-Parameter Markov Decision Processes (HiP-MDPs) and Block MDPs to improve sample efficiency in multi-task reinforcement learning (MTRL) and meta-reinforcement learning (Meta-RL) settings. This is a significant contribution to the field. However, the authors should provide a comparison of the sample efficiency of their proposed method with other existing methods in a controlled setting to support their claim of improved sample efficiency.",
                "The authors assume that the task-specific hidden parameter \u03b8 captures the task-specific dynamics. This is a crucial assumption that underpins their method. However, they do not provide explicit validation for this assumption. They should provide this validation to ensure the validity of their method.",
                "The authors define task and state similarity in terms of the difference in the hidden parameter \u03b8 and the state representations. While this is not necessarily a problem, it could make it difficult to compare their method with other methods that use different measures of task and state similarity. The authors should compare their measures of task and state similarity with those used by other methods.",
                "The authors provide a theoretical proof for the \"Transfer bound\" formula. This is a significant contribution. However, they should also test this formula in different scenarios to ensure its robustness.",
                "The authors use the Wasserstein distance to measure the distance between hidden parameters. However, they do not provide a detailed explanation or justification for this choice. They should provide this explanation or justification to allow readers to evaluate the appropriateness of this measure.",
                "The authors have conducted experiments in a variety of environments. This is commendable. However, they should also test their method in a wider variety of environments to ensure its generalizability.",
                "The authors claim that their method is robust to changes in environment dynamics. However, they do not provide specific details about the significance of these changes. They should provide these details to support their claim.",
                "The authors recommend a different approach for PCGrad. They should compare the performance of their recommended approach with the traditional approach in a controlled setting. This would provide a more direct comparison of the two approaches.",
                "The authors claim that their model adapts quickly in unseen environments. However, they do not specify the number of unseen environments they tested their model in. They should provide this information to support their claim.",
                "The authors do not discuss the limitations of their proposed method. They should provide a discussion of these limitations to provide a balanced view of their method."
            ],
            "clarity_reproducibility": [
                "The paper could provide more details about the HiP-BMDP model, specifically how it works and what makes it different from other models. This would help readers better understand the unique contributions of this work.",
                "The paper could provide more information about the encoder that maps observations from state space to a learned, latent representation. This could include details about the type of encoder used, how it learns the latent representation, and how this process contributes to the overall performance of the model.",
                "The paper could provide more detailed explanations of the concepts of Transfer bound and Sample Complexity, including how they are calculated and what they represent. This would help readers better understand these key performance metrics.",
                "The paper could provide more information about the experimental setup, specifically the environments from the Deepmind Control Suite (DMC) that were used. This could include details about how these environments were selected and what characteristics they have.",
                "The paper could provide more information about the different hyperparameters used for different environments in the MetaRL algorithm. This could include details about what these hyperparameters are and how they were determined.",
                "The paper could provide more information about the different \u03b1 values used for different environments. This could include details about what these \u03b1 values represent and how they were determined.",
                "The paper could provide more information about the Distral-Ensemble baseline, specifically what it is and why it was unable to leverage a shared global dynamics model.",
                "The paper could provide more information about the performance of the transition model in an unseen environment by only adapting the task parameter \u03b8. This could include details about how this process works and what impact it has on the model's performance.",
                "The paper could provide more details about the mathematical proofs and theorems related to value bounds, expected error bounds, and additional results and proofs for HiP-BMDP results. This could include more detailed explanations of these proofs and theorems and how they support the paper's claims.",
                "The paper could provide more information about the implementation details of the Meta-RL algorithm and the HiP-MDP setting. This could include details about the specific steps taken to implement these algorithms and any challenges that were encountered."
            ],
            "all": [
                "Depth of Analysis: While the authors have conducted a thorough analysis of their proposed methods, there might be room for a more in-depth analysis to provide a comprehensive understanding of the results and their implications. The authors should consider providing more detailed insights into the results and fully validating the claims of the paper.",
                "Number of Baselines: The authors have compared their method with several baselines. However, using a variety of baselines can help ensure that the results are not specific to a particular comparison and can generalize across different settings. The authors should consider using more baselines for comparison in their experiments.",
                "Hypothesis Support: The authors hypothesize that one of the baselines, Distral-Ensemble, behaves poorly because it cannot leverage a shared global dynamics model. This could potentially be seen as a shortcoming if the authors did not provide sufficient evidence to support this hypothesis. The authors should consider providing more evidence or conducting additional experiments to support this hypothesis.",
                "Limitations: The authors note that Meta-RL techniques are too time-intensive to train on pixel observations directly, which could be a limitation of the study. The authors should consider discussing this limitation in more detail and possibly suggesting ways to overcome it in future work.",
                "The paper presents a novel framework that combines the concepts of Hidden-Parameter Markov Decision Processes (HiP-MDPs) and Block MDPs to improve sample efficiency in multi-task reinforcement learning (MTRL) and meta-reinforcement learning (Meta-RL) settings. This is a significant contribution to the field. However, the authors should provide a comparison of the sample efficiency of their proposed method with other existing methods in a controlled setting to support their claim of improved sample efficiency.",
                "The authors assume that the task-specific hidden parameter \u03b8 captures the task-specific dynamics. This is a crucial assumption that underpins their method. However, they do not provide explicit validation for this assumption. They should provide this validation to ensure the validity of their method.",
                "The authors define task and state similarity in terms of the difference in the hidden parameter \u03b8 and the state representations. While this is not necessarily a problem, it could make it difficult to compare their method with other methods that use different measures of task and state similarity. The authors should compare their measures of task and state similarity with those used by other methods.",
                "The authors provide a theoretical proof for the \"Transfer bound\" formula. This is a significant contribution. However, they should also test this formula in different scenarios to ensure its robustness.",
                "The authors use the Wasserstein distance to measure the distance between hidden parameters. However, they do not provide a detailed explanation or justification for this choice. They should provide this explanation or justification to allow readers to evaluate the appropriateness of this measure.",
                "The authors have conducted experiments in a variety of environments. This is commendable. However, they should also test their method in a wider variety of environments to ensure its generalizability.",
                "The authors claim that their method is robust to changes in environment dynamics. However, they do not provide specific details about the significance of these changes. They should provide these details to support their claim.",
                "The authors recommend a different approach for PCGrad. They should compare the performance of their recommended approach with the traditional approach in a controlled setting. This would provide a more direct comparison of the two approaches.",
                "The authors claim that their model adapts quickly in unseen environments. However, they do not specify the number of unseen environments they tested their model in. They should provide this information to support their claim.",
                "The authors do not discuss the limitations of their proposed method. They should provide a discussion of these limitations to provide a balanced view of their method.",
                "The paper could provide more details about the HiP-BMDP model, specifically how it works and what makes it different from other models. This would help readers better understand the unique contributions of this work.",
                "The paper could provide more information about the encoder that maps observations from state space to a learned, latent representation. This could include details about the type of encoder used, how it learns the latent representation, and how this process contributes to the overall performance of the model.",
                "The paper could provide more detailed explanations of the concepts of Transfer bound and Sample Complexity, including how they are calculated and what they represent. This would help readers better understand these key performance metrics.",
                "The paper could provide more information about the experimental setup, specifically the environments from the Deepmind Control Suite (DMC) that were used. This could include details about how these environments were selected and what characteristics they have.",
                "The paper could provide more information about the different hyperparameters used for different environments in the MetaRL algorithm. This could include details about what these hyperparameters are and how they were determined.",
                "The paper could provide more information about the different \u03b1 values used for different environments. This could include details about what these \u03b1 values represent and how they were determined.",
                "The paper could provide more information about the Distral-Ensemble baseline, specifically what it is and why it was unable to leverage a shared global dynamics model.",
                "The paper could provide more information about the performance of the transition model in an unseen environment by only adapting the task parameter \u03b8. This could include details about how this process works and what impact it has on the model's performance.",
                "The paper could provide more details about the mathematical proofs and theorems related to value bounds, expected error bounds, and additional results and proofs for HiP-BMDP results. This could include more detailed explanations of these proofs and theorems and how they support the paper's claims.",
                "The paper could provide more information about the implementation details of the Meta-RL algorithm and the HiP-MDP setting. This could include details about the specific steps taken to implement these algorithms and any challenges that were encountered."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [],
            "clarity_reproducibility": [
                {
                    "reference_id": 0,
                    "predicted_id": 22,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper does not clearly demonstrate if any insight is gained from the theoretical analysis, as the derivation of value and sample complexity bounds for approximate bisimulation was already performed in Gelada et al. 2019.",
                    "predicted_comment": "The paper could provide more details about the mathematical proofs and theorems related to value bounds, expected error bounds, and additional results and proofs for HiP-BMDP results. This could include more detailed explanations of these proofs and theorems and how they support the paper's claims."
                }
            ],
            "all": [
                {
                    "reference_id": 0,
                    "predicted_id": 22,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper does not clearly demonstrate if any insight is gained from the theoretical analysis, as the derivation of value and sample complexity bounds for approximate bisimulation was already performed in Gelada et al. 2019.",
                    "predicted_comment": "The paper could provide more details about the mathematical proofs and theorems related to value bounds, expected error bounds, and additional results and proofs for HiP-BMDP results. This could include more detailed explanations of these proofs and theorems and how they support the paper's claims."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 7,
                "n_generated": 4,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 7,
                "n_generated": 10,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "clarity_reproducibility": {
                "n_reference": 7,
                "n_generated": 10,
                "n_total_aligns": 1,
                "recall": 0.07142857142857142,
                "precision": 0.05,
                "pseudo_jaccard": 0.03125,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "all": {
                "n_reference": 7,
                "n_generated": 24,
                "n_total_aligns": 1,
                "recall": 0.07142857142857142,
                "precision": 0.020833333333333332,
                "pseudo_jaccard": 0.016666666666666666,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 0
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 0
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.14285714285714285,
                    "precision": 0.1,
                    "pseudo_jaccard": 0.0625,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 0
                }
            ],
            "all": [
                {
                    "recall": 0.14285714285714285,
                    "precision": 0.041666666666666664,
                    "pseudo_jaccard": 0.03333333333333333,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 0
                }
            ]
        }
    },
    {
        "doc_id": "hbzCPZEIUU",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The comparison in the experiment section is weak without any reference to existing prior arts such as Chen et al., Fine-grained representation learning and recognition by exploiting hierarchical semantic embedding, ACM-MM 2018.",
                "There is a significant performance gap between the proposed method and the latest methods when considering the 5 experiment datasets.",
                "The baseline performance in the experiments shows considerable gaps to standard baseline training without bells and whistles, raising questions about its solidity.",
                "The performance gain of the proposed method diminishes very quickly on bigger datasets such as Tiny ImageNet, and the results on ImageNet are not provided.",
                "The proposed method depends on a pre-defined semantic hierarchical graph rather than a learned one, which potentially limits the technical value of this work.",
                "There is concern about the selection of initial radius $R_0$ and its decay policy, as these parameters should be dataset dependent due to different numbers of categories and the densities of class distributions.",
                "Forcing a fixed radius does not seem as reasonable as allowing a learnable radius with soft regularization."
            ],
            [
                "The empirical evaluation is relatively weak and the evaluation metric does not well reflect the advantages of hierarchically modelling the label space. It would be more informative to incorporate the classification accuracy of the super-classes.",
                "More experiments that analyze the difference in feature distributions between normally trained neural networks and the hierarchically trained neural networks are needed. An intuitive visualization of the feature space would be of great interest.",
                "Important ablation studies to justify some heuristic designs are necessary. For example, the effect of the hyperparameter in the radius decay on performance is crucial. Evaluations should be conducted on what happens if no sphericity constraint is applied, or if no radius decay is used.",
                "The empirical evaluation does not convincingly demonstrate the usefulness of modeling the hierarchical label space, especially in experiments on CIFAR-100 and Tiny-ImageNet. More discussions and insights on this issue would be useful.",
                "The usefulness of the hierarchical label structure should be evaluated and verified. A simple way to evaluate it is to use some random assignment or simple K-means assignments for the super-classes. If using the ground truth hierarchical structure consistently outperforms the random or K-means super-class assignment, then it can be argued that incorporating the ground truth hierarchical label structure is indeed useful."
            ],
            [
                "Section 2 needs more specific definitions for terms such as 'classifier' and 'separators'. For example, in equation (2), Wp and Wpi are called classifiers. Are these hyperplanes?",
                "In Definition 1, it would be helpful to provide early examples of P and L to make it easier to interpret the matrix H.",
                "The notation for Delta in equation (8) could be confusing as it may remind an operator on H in equation (9). Consider using a different notation.",
                "In equation (9), it is unclear whether deltas in the same subtree are required or observed to be in roughly the same direction.",
                "In Section 3, the claim that no hyperparameters are added seems incorrect. Initial radius R0, radius decay parameter, and the organization of classes could all be considered additional hyperparameters.",
                "In practice, defining mutually exclusive class partitions to form the required class tree in Figure 1 can be non-trivial or even impossible. The authors should discuss how different class hierarchies affect the classification accuracy, for example, in Table 2."
            ],
            [
                "The third paragraph of Section 1 is unclear about why Euclidean distance is not sufficient for learning with hierarchical regularization.",
                "The paper does not clearly explain whether the whole model follows end-to-end training or stage-wise training.",
                "In Section 2.2, the definition of 'separator' is unclear and the sentence about classifiers identifying dog breeds being similar is confusing.",
                "The paper does not provide a clear rationale or related work for the use of radius decay in Section 3.1.",
                "The optimization method used for learning other layers is not specified.",
                "The paper does not provide a clear explanation or justification for the construction of the spherical fully-connected layer.",
                "The paper does not present the hierarchical structure in a visually appealing or clear way.",
                "The paper does not clearly explain whether images from datasets Dogs and Tiny-Imagenet are part of ImageNet or if the classes in the two datasets are included in the set of ImageNet classes.",
                "The definition of 'plain networks' in Section 4.1 is unclear.",
                "The paper does not justify its claim of 'high efficiency' in Section 4.2.1.",
                "The paper does not formally compare Riemannian gradient and 'projected gradient' in Figure 2 right.",
                "The paper does not discuss how the proposed method may work if classes do not follow a tree hierarchy.",
                "The explanation of why 'Euclidean distance is not sufficient for learning such a hierarchical regularization' in the rebuttal is not intuitive or concrete.",
                "The paper does not clearly explain whether other methods suffer severely from the choice of learning rate and scheduler.",
                "The blue line in Page-2 is confusing and the logic of why diversity reduces over-fitting is unclear.",
                "The tSNE visuals in Figure 5 are not adequately analyzed."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper lacks a clear comparison with other existing methods that also incorporate hierarchical information into the network. This comparison is crucial to demonstrate the superiority or novelty of the proposed method. We suggest that the authors include a comparison with other similar methods using the same datasets and evaluation metrics for a fair comparison.",
                "The paper does not provide an ablation study to show the individual contribution of each component of the proposed method. An ablation study would provide insights into the contribution of each component to the overall performance. We recommend that the authors conduct an ablation study to understand the importance of each component and how they contribute to the overall performance improvement.",
                "The paper does not discuss the limitations of the proposed method. Every method has its limitations, and discussing them helps to provide a balanced view of the method and also suggests directions for future work. We suggest that the authors include a section discussing the limitations of their proposed method and potential ways to overcome these limitations in future work."
            ],
            "novelty_impact": [],
            "clarity_reproducibility": [
                "The paper should provide the exact mathematical formulation of the regularization term that constrains the distance between children nodes and parent nodes. This is a crucial detail for understanding and implementing the proposed regularization method.",
                "The paper should explain the specific process of how the Wordnet hierarchy was used to build the hierarchy for the (Tiny) Imagenet dataset. This information is necessary for replicating the experiments with this dataset.",
                "The paper should detail the calculation and implementation of the radius decay in the network. This is a key aspect of the proposed method and its absence makes it difficult to understand and implement the method fully.",
                "The paper should specify the baseline methods used for comparison in the numerical experiments. Without this information, it is difficult to evaluate the relative performance of the proposed method.",
                "The paper should describe the process of how the results were analyzed and interpreted, including any statistical tests used and how accuracy improvements were quantified and compared across different methods and datasets. This makes it challenging to understand the significance and impact of the results."
            ],
            "all": [
                "The paper lacks a clear comparison with other existing methods that also incorporate hierarchical information into the network. This comparison is crucial to demonstrate the superiority or novelty of the proposed method. We suggest that the authors include a comparison with other similar methods using the same datasets and evaluation metrics for a fair comparison.",
                "The paper does not provide an ablation study to show the individual contribution of each component of the proposed method. An ablation study would provide insights into the contribution of each component to the overall performance. We recommend that the authors conduct an ablation study to understand the importance of each component and how they contribute to the overall performance improvement.",
                "The paper does not discuss the limitations of the proposed method. Every method has its limitations, and discussing them helps to provide a balanced view of the method and also suggests directions for future work. We suggest that the authors include a section discussing the limitations of their proposed method and potential ways to overcome these limitations in future work.",
                "The paper should provide the exact mathematical formulation of the regularization term that constrains the distance between children nodes and parent nodes. This is a crucial detail for understanding and implementing the proposed regularization method.",
                "The paper should explain the specific process of how the Wordnet hierarchy was used to build the hierarchy for the (Tiny) Imagenet dataset. This information is necessary for replicating the experiments with this dataset.",
                "The paper should detail the calculation and implementation of the radius decay in the network. This is a key aspect of the proposed method and its absence makes it difficult to understand and implement the method fully.",
                "The paper should specify the baseline methods used for comparison in the numerical experiments. Without this information, it is difficult to evaluate the relative performance of the proposed method.",
                "The paper should describe the process of how the results were analyzed and interpreted, including any statistical tests used and how accuracy improvements were quantified and compared across different methods and datasets. This makes it challenging to understand the significance and impact of the results."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 3,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The empirical evaluation does not convincingly demonstrate the usefulness of modeling the hierarchical label space, especially in experiments on CIFAR-100 and Tiny-ImageNet. More discussions and insights on this issue would be useful.",
                    "predicted_comment": "The paper lacks a clear comparison with other existing methods that also incorporate hierarchical information into the network. This comparison is crucial to demonstrate the superiority or novelty of the proposed method. We suggest that the authors include a comparison with other similar methods using the same datasets and evaluation metrics for a fair comparison."
                }
            ],
            "novelty_impact": [],
            "clarity_reproducibility": [
                {
                    "reference_id": 5,
                    "predicted_id": 5,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "There is concern about the selection of initial radius $R_0$ and its decay policy, as these parameters should be dataset dependent due to different numbers of categories and the densities of class distributions.",
                    "predicted_comment": "The paper should detail the calculation and implementation of the radius decay in the network. This is a key aspect of the proposed method and its absence makes it difficult to understand and implement the method fully."
                }
            ],
            "all": [
                {
                    "reference_id": 5,
                    "predicted_id": 5,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "There is concern about the selection of initial radius $R_0$ and its decay policy, as these parameters should be dataset dependent due to different numbers of categories and the densities of class distributions.",
                    "predicted_comment": "The paper should detail the calculation and implementation of the radius decay in the network. This is a key aspect of the proposed method and its absence makes it difficult to understand and implement the method fully."
                },
                {
                    "reference_id": 3,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The empirical evaluation does not convincingly demonstrate the usefulness of modeling the hierarchical label space, especially in experiments on CIFAR-100 and Tiny-ImageNet. More discussions and insights on this issue would be useful.",
                    "predicted_comment": "The paper lacks a clear comparison with other existing methods that also incorporate hierarchical information into the network. This comparison is crucial to demonstrate the superiority or novelty of the proposed method. We suggest that the authors include a comparison with other similar methods using the same datasets and evaluation metrics for a fair comparison."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 34,
                "n_generated": 3,
                "n_total_aligns": 1,
                "recall": 0.05,
                "precision": 0.08333333333333333,
                "pseudo_jaccard": 0.03571428571428571,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 34,
                "n_generated": 0,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "clarity_reproducibility": {
                "n_reference": 34,
                "n_generated": 5,
                "n_total_aligns": 1,
                "recall": 0.03571428571428571,
                "precision": 0.05,
                "pseudo_jaccard": 0.022727272727272728,
                "specificity_more": 0.0,
                "specificity_less": 0.0,
                "specificity_same": 1.0
            },
            "all": {
                "n_reference": 34,
                "n_generated": 8,
                "n_total_aligns": 2,
                "recall": 0.08571428571428572,
                "precision": 0.0625,
                "pseudo_jaccard": 0.03869047619047619,
                "specificity_more": 0.5,
                "specificity_less": 0.0,
                "specificity_same": 0.5
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.2,
                    "precision": 0.3333333333333333,
                    "pseudo_jaccard": 0.14285714285714285,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 16
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 16
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.14285714285714285,
                    "precision": 0.2,
                    "pseudo_jaccard": 0.09090909090909091,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 16
                }
            ],
            "all": [
                {
                    "recall": 0.14285714285714285,
                    "precision": 0.125,
                    "pseudo_jaccard": 0.07142857142857142,
                    "n_real": 7
                },
                {
                    "recall": 0.2,
                    "precision": 0.125,
                    "pseudo_jaccard": 0.08333333333333333,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 16
                }
            ]
        }
    },
    {
        "doc_id": "ab7lBP7Fb60",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The proposed method is not compatible with federated averaging, which is a significant limitation in federated learning due to the communication bottleneck characteristic of federated settings.",
                "The paper does not compare the proposed method to prior work on group fairness in the centralized setting, such as the work by Zafar et al., (2017). An in-depth experimental evaluation of this aspect is needed to motivate the proposed approach.",
                "The proposed method may be susceptible to a potential privacy violation, as the statistics shared at every round include the number of individuals per protected attribute for every client. This could be exploited to learn an unfair model with respect to certain sensitive attributes.",
                "The paper incorrectly characterizes prior fair federated learning approaches, such as the work by Li et al. (2019), as individual fairness methods. These methods enforce uniform performance across all devices, which is neither individual nor group fairness, but a novel notion tailored to the federated learning setting."
            ],
            [
                "The paper is missing important literature review, including methods that enforce group fairness in Federated Learning such as 'Fair and Consistent Federated Learning' by Cui et al. 2021. These could have been considered as a baseline for comparing their method.",
                "The paper does not include enough baselines for Fair Federated Learning to compare against. Even if some methods do not satisfy privacy considerations, they should still be included for the reader to understand how the proposed method compares against such methods, especially given that the results are not promising. Some baselines to consider include Cui et al or Tran et al.",
                "The authors need to justify some of the incompatibility results shown in previous work between fairness and privacy.",
                "Algorithm 1 needs improvement. It is unclear what the clients do, what 'UpdateMultiplier', 'UpdateParameters', and 'Calculate noiseScale' mean.",
                "The results are not promising for predictive parity among groups. There is also a reduction in accuracy. The authors should discuss why this is the case. Including the suggested baselines could provide a sense of what other methods achieve in terms of loss in accuracy, etc.",
                "The results for the central setting should be included in Table 2."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The authors should consider conducting additional experiments to explore the impact of different cohort sizes on the performance of the FPFL algorithm. This could provide valuable insights into the scalability of the algorithm and its applicability in real-world scenarios where the number of users may vary significantly. The authors should also provide a detailed analysis of how different cohort sizes affect the performance of the algorithm, including the impact on accuracy, fairness, and sensitivity to DP noise."
            ],
            "novelty_impact": [
                "The paper presents a novel approach to enforcing fairness in private federated learning, which is a significant contribution to the field. However, the paper could benefit from providing more details on how the False Negative Rate (FNR) parity and Accuracy parity are calculated and used in the context of the FPFL algorithm. This would help readers understand how the algorithm ensures fairness.",
                "The paper should clarify how the tolerance for function variation between groups and the overall population is determined. This is a key parameter in the algorithm, and its determination could significantly impact the algorithm's performance and fairness.",
                "The paper should provide specific details on how the sensitivity of the FPFL algorithm to DP noise was measured. Understanding this sensitivity is crucial for assessing the algorithm's robustness and reliability.",
                "The paper should provide more details on how the additional experiments simulating the effect of chalk on a blackboard contribute to the evaluation of the FPFL algorithm's performance and fairness.",
                "The paper should discuss how the results might generalize to other types of data or tasks. This would help readers understand the potential applications and limitations of the FPFL algorithm.",
                "While the paper suggests that increasing the cohort size or user participation can mitigate the issue of sensitivity to DP noise, it should also discuss potential solutions or mitigation strategies for the issue of differential privacy disproportionately degrading the performance of the models on under-represented groups.",
                "The paper should provide more details on the advantages and disadvantages of using the FPFL algorithm over the MMDM algorithm, especially in terms of their performance, fairness, and privacy guarantees."
            ],
            "clarity_reproducibility": [
                "The paper could provide more details about the FPFL algorithm and the modified method of differential multipliers (MMDM). Specifically, the steps involved in the FPFL algorithm and how it enforces group fairness in PFL could be explained more clearly. The paper could also provide more information about MMDM and how it is extended to empirical risk minimization with fairness constraints.",
                "The paper could elaborate on the concept of group fairness. While the paper discusses group fairness in the context of machine learning models, it could provide a more detailed definition and explanation of how it is measured.",
                "The paper could provide more specific details about the Adult and FEMNIST datasets used in the experiments. For example, how were they prepared for the experiments? The paper could also provide more information about the FederatedSGD and FPFL models used in the experiments. What are their architectures and how were they trained?",
                "The paper could provide more details about the image modifications and the purpose of these modifications. The paper could also provide more details about the convolutional network used in the experiments, such as the specific layer configurations and activation functions. The paper could also provide more information about the specific values of the learning rate, Lagrange multipliers, damping parameter, cohort sizes, and clipping bound used in the experiments. How were these values chosen? How were the experiments repeated with different cohort sizes? What were the specific cohort sizes used and how were they chosen? How were the results compared? What metrics were used to evaluate the performance of the models?"
            ],
            "all": [
                "The authors should consider conducting additional experiments to explore the impact of different cohort sizes on the performance of the FPFL algorithm. This could provide valuable insights into the scalability of the algorithm and its applicability in real-world scenarios where the number of users may vary significantly. The authors should also provide a detailed analysis of how different cohort sizes affect the performance of the algorithm, including the impact on accuracy, fairness, and sensitivity to DP noise.",
                "The paper presents a novel approach to enforcing fairness in private federated learning, which is a significant contribution to the field. However, the paper could benefit from providing more details on how the False Negative Rate (FNR) parity and Accuracy parity are calculated and used in the context of the FPFL algorithm. This would help readers understand how the algorithm ensures fairness.",
                "The paper should clarify how the tolerance for function variation between groups and the overall population is determined. This is a key parameter in the algorithm, and its determination could significantly impact the algorithm's performance and fairness.",
                "The paper should provide specific details on how the sensitivity of the FPFL algorithm to DP noise was measured. Understanding this sensitivity is crucial for assessing the algorithm's robustness and reliability.",
                "The paper should provide more details on how the additional experiments simulating the effect of chalk on a blackboard contribute to the evaluation of the FPFL algorithm's performance and fairness.",
                "The paper should discuss how the results might generalize to other types of data or tasks. This would help readers understand the potential applications and limitations of the FPFL algorithm.",
                "While the paper suggests that increasing the cohort size or user participation can mitigate the issue of sensitivity to DP noise, it should also discuss potential solutions or mitigation strategies for the issue of differential privacy disproportionately degrading the performance of the models on under-represented groups.",
                "The paper should provide more details on the advantages and disadvantages of using the FPFL algorithm over the MMDM algorithm, especially in terms of their performance, fairness, and privacy guarantees.",
                "The paper could provide more details about the FPFL algorithm and the modified method of differential multipliers (MMDM). Specifically, the steps involved in the FPFL algorithm and how it enforces group fairness in PFL could be explained more clearly. The paper could also provide more information about MMDM and how it is extended to empirical risk minimization with fairness constraints.",
                "The paper could elaborate on the concept of group fairness. While the paper discusses group fairness in the context of machine learning models, it could provide a more detailed definition and explanation of how it is measured.",
                "The paper could provide more specific details about the Adult and FEMNIST datasets used in the experiments. For example, how were they prepared for the experiments? The paper could also provide more information about the FederatedSGD and FPFL models used in the experiments. What are their architectures and how were they trained?",
                "The paper could provide more details about the image modifications and the purpose of these modifications. The paper could also provide more details about the convolutional network used in the experiments, such as the specific layer configurations and activation functions. The paper could also provide more information about the specific values of the learning rate, Lagrange multipliers, damping parameter, cohort sizes, and clipping bound used in the experiments. How were these values chosen? How were the experiments repeated with different cohort sizes? What were the specific cohort sizes used and how were they chosen? How were the results compared? What metrics were used to evaluate the performance of the models?"
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [],
            "clarity_reproducibility": [
                {
                    "reference_id": 3,
                    "predicted_id": 8,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "Algorithm 1 needs improvement. It is unclear what the clients do, what 'UpdateMultiplier', 'UpdateParameters', and 'Calculate noiseScale' mean.",
                    "predicted_comment": "The paper could provide more details about the FPFL algorithm and the modified method of differential multipliers (MMDM). Specifically, the steps involved in the FPFL algorithm and how it enforces group fairness in PFL could be explained more clearly. The paper could also provide more information about MMDM and how it is extended to empirical risk minimization with fairness constraints."
                }
            ],
            "all": [
                {
                    "reference_id": 3,
                    "predicted_id": 8,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "Algorithm 1 needs improvement. It is unclear what the clients do, what 'UpdateMultiplier', 'UpdateParameters', and 'Calculate noiseScale' mean.",
                    "predicted_comment": "The paper could provide more details about the FPFL algorithm and the modified method of differential multipliers (MMDM). Specifically, the steps involved in the FPFL algorithm and how it enforces group fairness in PFL could be explained more clearly. The paper could also provide more information about MMDM and how it is extended to empirical risk minimization with fairness constraints."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 10,
                "n_generated": 1,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 10,
                "n_generated": 7,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "clarity_reproducibility": {
                "n_reference": 10,
                "n_generated": 4,
                "n_total_aligns": 1,
                "recall": 0.08333333333333333,
                "precision": 0.125,
                "pseudo_jaccard": 0.05555555555555555,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "all": {
                "n_reference": 10,
                "n_generated": 12,
                "n_total_aligns": 1,
                "recall": 0.08333333333333333,
                "precision": 0.041666666666666664,
                "pseudo_jaccard": 0.029411764705882353,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.16666666666666666,
                    "precision": 0.25,
                    "pseudo_jaccard": 0.1111111111111111,
                    "n_real": 6
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.16666666666666666,
                    "precision": 0.08333333333333333,
                    "pseudo_jaccard": 0.058823529411764705,
                    "n_real": 6
                }
            ]
        }
    },
    {
        "doc_id": "rsf1z-JSj87",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The performance of the proposed model is promising, but still below the baselines.",
                "The claim of the model being end-to-end is misleading as the character-based model is not performing well, and the phoneme-based model is not truly end-to-end, as the grapheme-to-phoneme (g2p) part is not trained jointly.",
                "The reason behind using an adversarial loss is not adequately explained in the paper. A few lines before section 2.1 would help clarify that.",
                "It's not clear which dataset was used in the experiment. If it is a private dataset, please state it clearly."
            ],
            [
                "The paper does not provide a comparison over the computational cost or model size. This is particularly important because the proposed model is non-autoregressive and may be capable of real-time inference.",
                "The paper does not use widely accepted benchmark datasets. A more direct comparison would be beneficial.",
                "The proposed aligner module doesn't seem to be very useful compared with the attention-based aligner as seen in the ablation study (Table 1): there is only a very small improvement from 3.551 to 3.559 MOS. More explanation is needed."
            ],
            [
                "The elegance in the architecture is overshadowed by the complicated training algorithm, making the method look a bit heuristic and harder to reproduce.",
                "The authors should provide brief guidelines for readers trying to reimplement EATS, such as how to tune the hyperparameters.",
                "EATS performs slightly worse than GAN-TTS, which does not quite show the benefit of end-to-end training. It would be worthwhile to expand the discussion a bit by showing further experiments that demonstrate the potential benefit of end-to-end training.",
                "Without a clearly interpretable hidden representation, it is harder to have direct control over prosody. The authors should discuss how prosody control would be possible under the end-to-end framework."
            ],
            [
                "The paper lacks a comparison with Tacotron 2 in Table 1. This would be informative as all the baselines are trained on aligned data while Tacotron is a legitimate contender for EATS as it can be trained on the same data.",
                "The authors claim that their method is simpler because the training is in one stage. However, given the large number of losses and components in their model, with their respective hyper-parameters to tune, the simplicity argument is not entirely convincing.",
                "The performance of the transformer attention is surprisingly poor, particularly in the audio samples where the output for this model is garbage towards the end of the signal. The authors should provide an explanation for this."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "Diversity of Datasets: It would be beneficial if the authors could test their model on a more diverse range of datasets, including different languages, accents, and speaking styles. This would help to demonstrate the generalizability of the model.",
                "Comparison with Baselines: While the authors have compared their model with state-of-the-art TTS systems, it would also be beneficial to compare with simpler baseline models. This can help to demonstrate the incremental improvement offered by their model.",
                "Detailed Analysis of Ablation Studies: A more detailed analysis of the ablation studies would be useful. For example, how much does each component contribute to the overall performance? Are there any components that are more important than others?",
                "In-depth Analysis of Failure Cases: Including an in-depth analysis of the cases where the model fails or performs poorly could provide valuable insights for future improvements.",
                "Evaluation of Robustness: Evaluating the robustness of the model to various perturbations, such as noise in the input data or changes in the speaking rate or pitch, would be interesting.",
                "Evaluation of the Adversarial Approach: A more detailed evaluation of the claim that the adversarial approach can learn from a relatively weak supervisory signal would be beneficial. For example, how does the performance change as the strength of the supervisory signal is varied?"
            ],
            "novelty_impact": [
                "The paper could benefit from providing specific details on how the model would handle non-North American accents or other languages. This could broaden the applicability of the model.",
                "The paper should discuss how the model would handle languages or dialects that the phonemizer may not support. This could improve the model's performance on diverse datasets.",
                "The paper should discuss the potential limitations of the MOS and FDSD metrics in capturing all aspects of speech synthesis quality. This could prevent an overestimation of the model's performance.",
                "The paper should provide a direct comparison of the aligner architecture with other architectures. This would allow readers to better assess the claimed efficiency of the aligner.",
                "The paper should discuss how the model handles local variations in the speech signal, given the sensitivity of dynamic time warping to such variations.",
                "The paper should discuss the potential challenges of using adversarial feedback and domain-specific loss functions in training the system. This could help others in replicating the results or extending the model.",
                "The paper should discuss the computational requirements of the EATS model in more detail. This could help potential users assess the applicability of the model in their specific, potentially resource-constrained, environments.",
                "The paper should propose solutions to the challenges in automatic quantitative evaluation of text-to-speech models. This could increase the model's usefulness in practical applications.",
                "The paper should discuss how EATS could be integrated with existing TTS systems or used in real-world applications. This could increase the impact of the work.",
                "The paper should discuss potential limitations of using a transformer-based attention aligner baseline. This could help others in replicating the results or extending the model."
            ],
            "clarity_reproducibility": [],
            "all": [
                "Diversity of Datasets: It would be beneficial if the authors could test their model on a more diverse range of datasets, including different languages, accents, and speaking styles. This would help to demonstrate the generalizability of the model.",
                "Comparison with Baselines: While the authors have compared their model with state-of-the-art TTS systems, it would also be beneficial to compare with simpler baseline models. This can help to demonstrate the incremental improvement offered by their model.",
                "Detailed Analysis of Ablation Studies: A more detailed analysis of the ablation studies would be useful. For example, how much does each component contribute to the overall performance? Are there any components that are more important than others?",
                "In-depth Analysis of Failure Cases: Including an in-depth analysis of the cases where the model fails or performs poorly could provide valuable insights for future improvements.",
                "Evaluation of Robustness: Evaluating the robustness of the model to various perturbations, such as noise in the input data or changes in the speaking rate or pitch, would be interesting.",
                "Evaluation of the Adversarial Approach: A more detailed evaluation of the claim that the adversarial approach can learn from a relatively weak supervisory signal would be beneficial. For example, how does the performance change as the strength of the supervisory signal is varied?",
                "The paper could benefit from providing specific details on how the model would handle non-North American accents or other languages. This could broaden the applicability of the model.",
                "The paper should discuss how the model would handle languages or dialects that the phonemizer may not support. This could improve the model's performance on diverse datasets.",
                "The paper should discuss the potential limitations of the MOS and FDSD metrics in capturing all aspects of speech synthesis quality. This could prevent an overestimation of the model's performance.",
                "The paper should provide a direct comparison of the aligner architecture with other architectures. This would allow readers to better assess the claimed efficiency of the aligner.",
                "The paper should discuss how the model handles local variations in the speech signal, given the sensitivity of dynamic time warping to such variations.",
                "The paper should discuss the potential challenges of using adversarial feedback and domain-specific loss functions in training the system. This could help others in replicating the results or extending the model.",
                "The paper should discuss the computational requirements of the EATS model in more detail. This could help potential users assess the applicability of the model in their specific, potentially resource-constrained, environments.",
                "The paper should propose solutions to the challenges in automatic quantitative evaluation of text-to-speech models. This could increase the model's usefulness in practical applications.",
                "The paper should discuss how EATS could be integrated with existing TTS systems or used in real-world applications. This could increase the impact of the work.",
                "The paper should discuss potential limitations of using a transformer-based attention aligner baseline. This could help others in replicating the results or extending the model."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [
                {
                    "reference_id": 0,
                    "predicted_id": 12,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "The paper does not provide a comparison over the computational cost or model size. This is particularly important because the proposed model is non-autoregressive and may be capable of real-time inference.",
                    "predicted_comment": "The paper should discuss the computational requirements of the EATS model in more detail. This could help potential users assess the applicability of the model in their specific, potentially resource-constrained, environments."
                }
            ],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 0,
                    "predicted_id": 12,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "The paper does not provide a comparison over the computational cost or model size. This is particularly important because the proposed model is non-autoregressive and may be capable of real-time inference.",
                    "predicted_comment": "The paper should discuss the computational requirements of the EATS model in more detail. This could help potential users assess the applicability of the model in their specific, potentially resource-constrained, environments."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 14,
                "n_generated": 6,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 14,
                "n_generated": 10,
                "n_total_aligns": 1,
                "recall": 0.08333333333333333,
                "precision": 0.025,
                "pseudo_jaccard": 0.020833333333333332,
                "specificity_more": 0.0,
                "specificity_less": 0.0,
                "specificity_same": 1.0
            },
            "clarity_reproducibility": {
                "n_reference": 14,
                "n_generated": 0,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 14,
                "n_generated": 16,
                "n_total_aligns": 1,
                "recall": 0.08333333333333333,
                "precision": 0.015625,
                "pseudo_jaccard": 0.013888888888888888,
                "specificity_more": 0.0,
                "specificity_less": 0.0,
                "specificity_same": 1.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.1,
                    "pseudo_jaccard": 0.08333333333333333,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.0625,
                    "pseudo_jaccard": 0.05555555555555555,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ]
        }
    }
]