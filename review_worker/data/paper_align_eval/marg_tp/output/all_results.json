[
    {
        "doc_id": "B1x8anVFPr",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The authors should have performed multiple runs of their experiments and shown the standard deviations across runs to make their results more convincing.",
                "The authors claim that using Pre-LN rather than Post-LN transformers leads to faster convergence, but the models converge to the same result. This work is incremental as Wang et al. have already shown the benefits of Pre-LN rather than Post-LN transformers for machine translation.",
                "The paper is very long for an incremental improvement.",
                "It would be good to see repeats for Figure 2 and the standard deviations for Figure 3.",
                "Without reading the appendix it is not clear where the assumption that W^Q and W^K are zero is used. Making some connection with how this assumption relates to the lemmas would be useful.",
                "In Lemma two, the authors are comparing the magnitudes for the input in the Pre-LN and the output in the Post-LN transformer according to how x_{l,i}^post and x_{l, i}^pre are defined in Table 1.",
                "In Figure 3(b) the gradients are clearly decreasing with the number of layers, are there any comments on this? In the limit this could cause vanishing gradients?",
                "Figure 2 and 4 appear to contradict. Is the difference a result of using RAdam? If so, this should be made very clear. If not, why are the results contradictory?"
            ],
            [
                "The novelty of the paper is limited as Pre-LN placement has been used in prior work.",
                "There are concerns about the relation between the analytic investigation of the gradient norms and the empirical results that are presented.",
                "The theoretical results do not clearly demonstrate that there is a problem with Post-LN and that this problem is fixed by switching to Pre-LN.",
                "The paper does not clearly explain the relationship between the theoretical results and the empirical study.",
                "The paper does not adequately explain the scaling of the gradient changes throughout the layers, which is examined in appendix section F.",
                "The connection between the expression that scales in the upper bound and the actual gradient norm is tenuous and there are multiple places where the argument could break down.",
                "The paper does not verify the sqrt(2/3) scaling on the data from the empirical study that is shown.",
                "The paper does not explain where the expectation near the bottom of page 20, which is approximately 2/3, comes from."
            ]
        ],
        "generated_comments": {
            "all": []
        },
        "alignments": {
            "all": []
        },
        "metrics": {
            "all": {
                "n_reference": 16,
                "n_generated": 0,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                }
            ]
        }
    },
    {
        "doc_id": "o2UwRc8fbXI",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The paper does not clearly state the main claims. The proposed method does not demonstrate a speed advantage or a clear performance boost.",
                "The reason for coupling the proposed method with CapsGCN is unclear, especially if the RL module is intended to tune the depth and width of convolutional layers.",
                "The paper does not clearly differentiate the proposed method from Neural Architecture Search (NAS).",
                "The motivations and experimental results presented in the paper are weak."
            ],
            [
                "The use of Reinforcement Learning (RL) during the training process to search for and modify the neural structures may create stability issues. The paper should provide some theoretical justifications or 'proofs' of convergence, or at the minimum, provide some empirical analysis on the stability of the training process.",
                "The evaluation of the proposed Adaptive Graph Capsule Convolutional Networks (AdaGCCN) could be more thorough. The paper should include ablation studies to show how effective the RL is in adapting the network structure, and discuss the runtime complexity, among other things."
            ],
            [
                "The motivations for the paper are not well discussed.",
                "The experimental methodology used in the paper is not well detailed, making it difficult to reproduce the reported results.",
                "The authors' claims are not always supported by the experimental evidence.",
                "The authors do not provide information about how the models considered during the experiments were validated. The policy used to validate the hyperparameters of a model highly impacts the model's performance.",
                "The authors do not report the standard deviation of the accuracies in table 5. This is crucial to correctly assess the model performance.",
                "The results in Table 5 suggest that using the AM to optimize the D and W does not justify the increase of complexity due to the use of this external model.",
                "The results reported in Table 5 are outperformed by results published in the literature for Enzymes datasets and in NCI.",
                "The performance of the model in terms of memory consumption is not completely discussed. Using AM seems to significantly impact the memory consumption and/or the training time of the model.",
                "The authors state that adjusting the model structure through an RL-based searching process improves the explainability of the model. This point should be further discussed with empirical results."
            ],
            [
                "The capsule graph neural network section (2.2) is somewhat shallow and could benefit from a more detailed introduction to the topics discussed to make the paper more self-contained.",
                "The paper lacks ablation experiments on the epoch sliding window size and choice of search space for D and W. It's also unclear how the method would perform without the capsule components, i.e. is the assistant module strategy practically useful for other architectures.",
                "There are concerns about reporting test accuracy throughout training in figure 3, as models could be overfit to the test set, rendering the results potentially incomparable to previous works.",
                "No standard deviations are reported in the results in table 2, and generally only marginal improvements are observed despite the increase in runtime complexity of the proposed method.",
                "The paper lacks meaningful runtime comparisons between the proposed method and the various previous works without the assistant module. For example, a comparison to a parallel cross validation version of CapsGNN in figure 4 is suggested.",
                "In the discussion, the authors claim the proposed approach improves the explainability of the model, but it's not clear how that is the case. The authors should either elaborate on this or exclude it from the paper.",
                "The purpose of using dynamic routing in capsule networks is to adaptively modify the network connectivity based on the input. Therefore, it might be possible to reduce the search space by only considering depth D and not width W, since dynamic routing already takes care of 'pruning' W. The authors should address this point."
            ]
        ],
        "generated_comments": {
            "all": [
                "The introduction provides a good overview of the limitations of existing methods, such as the latent information loss problem in Graph Convolutional Networks (GCNs) and the static nature of GCNs. However, it could be helpful to elaborate on why these limitations are significant and how they impact the performance of GCNs.",
                "The methodology section provides a detailed explanation of the assistant module and its use of reinforcement learning. However, it might be beneficial to include a simplified summary or a visual representation to aid reader understanding.",
                "The paper provides a detailed explanation of how the model parameters are shared when the depth and width are updated. However, it might be helpful to include a visual representation or a step-by-step walkthrough of this process to aid reader understanding.",
                "The paper provides a good explanation of how the model handles computation efficiency, but it lacks a detailed explanation of how it handles overfitting. It would be beneficial to include more information on this aspect.",
                "The 'Evaluation' and 'AdagcCN Performance' sections provide a comprehensive analysis of the results, including detailed explanations of the figures and tables. However, it might be beneficial to include a summary of the key findings at the end of each subsection to help readers quickly grasp the main points.",
                "The 'Discussion' section provides a thorough analysis of the results and their implications. However, it might be beneficial to include a subsection specifically dedicated to potential future research directions based on the paper's findings.",
                "The paper does discuss the limitations of the study, such as the fact that some datasets either exceed the GPU memory capacity or cost too much time in training. However, it might be beneficial to discuss these limitations in more detail and suggest potential solutions."
            ]
        },
        "alignments": {
            "all": [
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The motivations for the paper are not well discussed.",
                    "predicted_comment": "The introduction provides a good overview of the limitations of existing methods, such as the latent information loss problem in Graph Convolutional Networks (GCNs) and the static nature of GCNs. However, it could be helpful to elaborate on why these limitations are significant and how they impact the performance of GCNs."
                },
                {
                    "reference_id": 7,
                    "predicted_id": 6,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The performance of the model in terms of memory consumption is not completely discussed. Using AM seems to significantly impact the memory consumption and/or the training time of the model.",
                    "predicted_comment": "The paper does discuss the limitations of the study, such as the fact that some datasets either exceed the GPU memory capacity or cost too much time in training. However, it might be beneficial to discuss these limitations in more detail and suggest potential solutions."
                }
            ]
        },
        "metrics": {
            "all": {
                "n_reference": 22,
                "n_generated": 7,
                "n_total_aligns": 2,
                "recall": 0.05555555555555555,
                "precision": 0.07142857142857142,
                "pseudo_jaccard": 0.03571428571428571,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.2222222222222222,
                    "precision": 0.2857142857142857,
                    "pseudo_jaccard": 0.14285714285714285,
                    "n_real": 9
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                }
            ]
        }
    },
    {
        "doc_id": "tJCwZBHm-jW",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The novelty of the paper is limited as the idea of transforming a 2D image pre-train backbone into a 3D version has already been used in other 2D-3D domains. The paper needs to place extra emphasis on discussing why it works and conducting ablation studies.",
                "The discussion section provides some experiments, but the results seem quite random (figure 4). The explanation for why 'shape representation' are better transferred is not convincing.",
                "The paper does not discuss if there is a difference on which axis to inflate (x, y, or z) when designing inflation. This is different to video since 3 axis is symmetric.",
                "The paper does not compare the parameter size in section 4.1. This raises concerns about the fairness of the comparison with the baseline, as the performance gain could be solely due to the increase in parameters.",
                "In section 4.2, there is a possibility that the ResNet structure is not a good structure to train on point cloud, making it quite bad on scratch training. The training method for pointnet++ is also questionable."
            ],
            [
                "The paper's visualizations do not adequately explain why the transfer works or what information can be transferred.",
                "The conclusion on page 8 that 'shape representations are better transferred from image to point-cloud' cannot be inferred from overall dataset performances. The term 'shape representations' is also not clearly defined."
            ],
            [
                "The paper lacks comparisons between results from datasets of different sizes in Table 1. It is unclear whether a larger dataset would lead to better performance.",
                "The paper does not include results of state-of-the-art task specific methods for comparison. It would be beneficial to compare with different self-supervised learning methods, such as Foldingnet and Pointcontrast.",
                "The paper does not provide a theoretical analysis to explain why inflating 2D filters to 3D is reasonable, given the significant domain gap between 2D and 3D datasets."
            ],
            [
                "The idea of inflating model weight from 2D to 3D is not new, as shown in the work of Carreira and Zisserman.",
                "The effectiveness of pretrained weight would be more convincing if the author could do further ablation study to initialize new model with expanding subsets of layers and exploring the effect on model performance. This would indicate which part of the pretrained model is really useful in this transferred learning setting."
            ],
            [
                "The paper is missing detailed information, making it difficult to follow the process. For example, in Table 1, the performances of pointnet++ and the performances are listed, but it's unclear how the inflation in point net++ is performed.",
                "The authors only perform experiments on classification and segmentation tasks. The performance on detection tasks should also be evaluated.",
                "The paper does not clearly address why the proposed method of inflating 2D CNN of image for 3D pointcloud works."
            ]
        ],
        "generated_comments": {
            "all": [
                "Please provide a detailed discussion on any challenges faced during the training process and how they were overcome. Include specific strategies or techniques used to address these challenges."
            ]
        },
        "alignments": {
            "all": []
        },
        "metrics": {
            "all": {
                "n_reference": 15,
                "n_generated": 1,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "H1enKkrFDB",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The role of the partitioning index in the proposed algorithm is unclear, as it does not appear to be used in the experiments. The authors should clarify its importance and usage.",
                "The empirical evaluation of the normalization technique is insufficient to demonstrate its practical usefulness. The authors should focus more on showing how their regularization can improve generalization in practice.",
                "The authors should provide experiments on at least one additional dataset, such as CIFAR10 or ImageNet, to strengthen their empirical results.",
                "The reported accuracies for ResNet and DenseNet on the CIFAR100 dataset are significantly lower than what is reported in the literature. This issue needs to be resolved.",
                "The current results are based on training with a fixed number of epochs. Instead, the authors should train with a stopping criterion based on the cross-entropy loss on the training set and use the same stopping criterion for all models. They should also add plots showing training and test errors based on the number of epochs."
            ],
            [
                "The paper should characterize the runtime difference between SRN and SN. It is presently unclear how computationally intensive the method is. What is the difference in time per training iteration? The authors should also indicate their hardware setup and overall training time.",
                "Table 1 is confusing as it lacks the test error. If there is a difference in testing error, then this table is misleading. If the test errors are approximately the same, then why should I care if the training error is higher? This would just be a way to decrease the stated \u201cgeneralization gap,\u201d which is not necessarily indicative of a better model.",
                "The paper does not explain what \u201cStable-50\u201d, \u201cSRN-50\u201d, and \u201cSRN-50%\u201d are. This should be explicitly stated so that the reader knows which model corresponds to which settings."
            ],
            [
                "The paper could mention some related older introductory approaches, such as representing linear layers as 'bottlenecks' to enforce low rank explicitly, or solving in the manifold of reduced-rank matrices directly.",
                "The authors target the same stable rank (srank) r=c*min(m,n) for all layers, even though only the sum of sranks is important. It would be interesting to see if there is any observable difference for CNNs with only a few linear layers by lightly deviating from this.",
                "The paper could explore whether the first linear layer typically contributes the majority to the sum of sranks.",
                "The paper could discuss how 'stable rank' concepts might be extended to the convolutional layers. As a starting point, spectral values of the block-circulant matrices corresponding to convolutions have been described in previous work."
            ]
        ],
        "generated_comments": {
            "all": [
                "The introduction could benefit from a more detailed explanation of the Stable Rank and Lipschitz constant. Specifically, it would be helpful to elaborate on how the Stable Rank is a softer version of the rank operator and how it is defined as the ratio of the Frobenius norm to the spectral norm. Additionally, providing more context on how the Lipschitz constant measures the sensitivity of the output with respect to changes in the input could enhance reader understanding.",
                "The methodology is complex and might be difficult for readers without a strong background in mathematics to understand. Consider simplifying the explanation or providing a more intuitive explanation of the method. For instance, a step-by-step breakdown of the solution to the SRN problem as detailed in Theorem 1 could be beneficial. Additionally, consider providing a visual representation of the method to aid understanding.",
                "The results section could benefit from more visual aids, such as graphs or charts, to help illustrate the findings. Specifically, visual representations of the performance of the Stable Rank Normalization Generative Adversarial Network (SRN-GAN) in comparison to other GANs such as SN-GAN, WGAN-GP, and Ortho-GAN could be useful. The technical proof section is quite complex and may be difficult for readers without a strong mathematical background to understand. Consider providing a simplified explanation of the proof of Theorem 1 and the optimal solution to the spectral norm problem.",
                "The technical section could benefit from more explanations or interpretations of the equations and proofs in plain language, a clear statement of the main findings, and visual aids. For example, a layman's explanation of the constraints and the Lagrangian dual variables used in the problem, as well as the concept of stable rank, could be beneficial. Additionally, a clear statement of the main findings from the proof for the optimal spectral normalization and the use of singular values in the problem could enhance reader comprehension.",
                "The conclusion should emphasize the effect of rank on the empirical Lipschitz constants, highlight the local Lipschitz upper-bound for neural networks, summarize the performance and characteristics of various network models, and include the results of GAN objective functions evaluations. Specifically, it would be helpful to summarize the findings from the experiments on WideResNet-28-10, ResNet-110, Densenet-100, VGG19, and AlexNet, as well as the additional experiments on generalization and GAN objective functions."
            ]
        },
        "alignments": {
            "all": []
        },
        "metrics": {
            "all": {
                "n_reference": 12,
                "n_generated": 5,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                }
            ]
        }
    },
    {
        "doc_id": "Qm7R_SdqTpT",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The paper should provide insights on why switching actions at some fixed number of frames beats using the Gaussian variance for FVD.",
                "The paper should discuss the importance of the heuristic of changing states when using GP, and how the performance would be affected if a different threshold is used."
            ],
            [
                "The experimental section could be improved. More details about how the comparison to some baselines was made would be appreciated.",
                "The results for the VRNN model in Figure 4 and 5 do not follow the results in the literature, where it outperforms SVG and SAVP. It's unclear whether this is due to an architectural change, suboptimal hyperparameters, or a different reimplementation.",
                "The VRNN model is missing from some other comparisons such as Table 1.",
                "For SAVP, the results for Figure 4 seem much worse than those reported in the original paper."
            ],
            [
                "The application of Gaussian Processes (GP) in this work is straightforward and naive. Dynamical modeling with GP has been widely investigated since the introduction of Gaussian Process Dynamical Model in NIPs 2005.",
                "The modules of LSTM Frame Generation and GP Frame Generation in Equation (4) are not clearly explained or depicted in Figure 3. It is unclear if these modules correspond to 'D' in Stage 3.",
                "The use of GP to generate images is questionable as GP is more suitable for working in the latent space.",
                "The datasets used in the study are not representative due to their simplicity and experimental nature.",
                "The paper does not demonstrate the utility of the proposed method for high-level research topics such as large-scale action recognition, video captioning, etc."
            ]
        ],
        "generated_comments": {
            "all": [
                "Provide a more detailed explanation of the Diverse Video Generator, including its purpose and how it uses a Gaussian Process to learn priors on future states and maintain a probability distribution over possible futures, in the introduction.",
                "Explain what a Gaussian Process is and why it's suitable for this task in the introduction. Include information on how it models the correlation between past and future states.",
                "Provide more context on how the proposed approach, the Diverse Video Generator, compares to existing methods such as SVG-LP, SAVP, Conditional VRNN, and VideoFlow in the introduction.",
                "In the background section, provide a more detailed comparison of related work to the proposed approach, including how the Diverse Video Generator outperforms the baselines according to the FVD metric.",
                "Provide a clearer explanation of the evaluation metrics used to assess the quality and diversity of the generated frames, including SSIM, PSNR, LPIPS, FVD, and the action classifier for diversity.",
                "Include a brief overview of the Frame Auto-Encoder, LSTM Temporal Dynamics Encoder, and GP Temporal Dynamics Encoder, explaining how they map frames to a latent space, encode the dynamics of an ongoing action sequence, and learn the priors for potential future states, respectively.",
                "Provide a brief summary of the architectures for the encoder and generator networks borrowed from Denton & Fergus (2018).",
                "Provide more explanation or examples of how the \"trigger switch\" in the inference model of the Diverse Video Generator works, including how it uses the variance output from the GP encoder to decide whether to continue an ongoing action or generate a new diverse output.",
                "Provide experimental results or data to support the claim that the model trains better with higher values for \u03bb1, \u03bb2, \u03bb4.",
                "Provide a more intuitive explanation or visual representation of the training objective and the different losses used, including the three frame generation losses and the two dynamics encoder losses.",
                "Provide a more detailed discussion of the ablation studies, including the performance of the different dynamics models (RNN, GRU, LSTM) on the KTH, BAIR, and Human3.6M datasets.",
                "Provide more context on what the baselines are and why the proposed approach outperforms them, including specific performance metrics.",
                "Provide more context on why reconstruction metrics like SSIM and PSNR are not ideal for video prediction and suggest alternatives.",
                "Provide more details on the Gaussian Layer specifics, such as why 40 inducing points were chosen for the variational GP implementation and how GPytorch was used.",
                "Provide more context on why there is a difference in accuracy between the KTH dataset and the human3.6m dataset for the action classifier, including the specific accuracy rates."
            ]
        },
        "alignments": {
            "all": [
                {
                    "reference_id": 0,
                    "predicted_id": 11,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The experimental section could be improved. More details about how the comparison to some baselines was made would be appreciated.",
                    "predicted_comment": "Provide more context on what the baselines are and why the proposed approach outperforms them, including specific performance metrics."
                }
            ]
        },
        "metrics": {
            "all": {
                "n_reference": 11,
                "n_generated": 15,
                "n_total_aligns": 1,
                "recall": 0.08333333333333333,
                "precision": 0.022222222222222223,
                "pseudo_jaccard": 0.018518518518518517,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.25,
                    "precision": 0.06666666666666667,
                    "pseudo_jaccard": 0.05555555555555555,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                }
            ]
        }
    },
    {
        "doc_id": "K5j7D81ABvt",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The paper does not clarify whether 'S_F = S_sTEX' refers to exact string equality or after white space normalization. The authors should specify the exact normalizations and provide the success rate before and after them.",
                "The paper does not discuss the potential impact of using larger GPT models.",
                "The paper does not explore the potential usefulness of unsupervised learning methods, which have recently seen significant improvements.",
                "The term 'fully disambiguated' is used in definition 4.1 without considering the types of variables. In systems with subtypes, the meaning and provability of a statement can change depending on the quantification over different types of numbers."
            ],
            [
                "The current benchmark lacks sufficient training and evaluation data. The lack of training corpora may be addressed by pretraining and building synthetic data.",
                "A larger and high-quality evaluation set is needed to validate any actual progress on this problem. The current evaluation set is too small and covers limited math topics.",
                "The evaluation protocol is unclear. The best evaluation protocol should be checking if S_F belongs to STEX(S_STEX), which is not used in this work. The authors should consider implementing this protocol.",
                "An ablation study on the corresponding contributions of pretraining and synthetic data is needed for the proposed approach."
            ],
            [
                "The paper does not compare the proposed method against other systems listed in section 4 that also aim to automate the formalization process.",
                "The choice of phrasing the task as a translation problem is questionable, as it seems to set up the NMT baselines to perform poorly due to the identity transformation required for much of the document.",
                "The evaluation methodology is confusing, particularly the use of automated procedures to generate some of the data. This raises questions about why these automated methods are not sufficient for the end task.",
                "The results section does not present the results of the baselines, making it difficult to assess their performance relative to the proposed method.",
                "The paper does not provide a clear understanding of the problem setting, what is being learned, and what is failing. This makes it difficult to assess the importance and difficulty of the targeted problem.",
                "The experimental section is short and lacks important comparisons to other work in this area.",
                "The model is used in a very off-the-shelf way and does not treat the problem setting any differently than a standard translation task, which raises questions about the novelty of the modeling contribution.",
                "The paper uses a lot of domain-specific terminology that may be difficult for a general ML audience to understand. For example, the terms 'formal' and 'informal' are used in a way that is not immediately clear.",
                "The paper needs major restructuring and refocusing, more experiments, and more analysis to be considered for acceptance."
            ],
            [
                "The problem definition was not clear. A concrete example demonstrating what exactly an algorithm for the problem gets as input and what is its output is missing.",
                "The description of the dataset was unclear. While there is a formal description, the lack of examples leaves the description at a very abstract level, making it difficult to understand what exactly should be expected in the dataset.",
                "The authors do not show awareness of work in semantic parsing and in style transfer. These works are very important both for the algorithmic approach and for understanding the challenges of evaluation. For example, there is lots of semantic parsing research on transferring text into SQL queries or on solving textual mathematical problems. The connection to relevant ideas and approaches should be part of the introduction of the task."
            ]
        ],
        "generated_comments": {
            "all": []
        },
        "alignments": {
            "all": []
        },
        "metrics": {
            "all": {
                "n_reference": 20,
                "n_generated": 0,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 9
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "rkxZCJrtwS",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The paper should include an argument explaining that accurately estimating Q using Q_phi does not necessarily mean that the gradients of Q_phi will be good approximations of the true gradients of Q. The reviewer suggests referring to Fig 1 of arxiv.org/pdf/1705.07107.pdf for an informative example.",
                "The choice of Euclidean norm in line 10 needs justification. The reviewer suggests considering the importance of the direction of the gradient over the norm and proposes the use of cosine similarity instead.",
                "The paper argues that Deep Reinforcement Learning (DRL) is superior to Model Predictive Control (MPC) due to better exploration. The reviewer suggests exploring the possibility of using simulator gradients to further improve exploration in DRL."
            ],
            [
                "The paper's contribution is empirical in nature, and a promising future direction would be to add derivative information to and evaluate similar variants of newer actor-critic methods such as TD3 and SAC."
            ],
            [
                "The form of gradient given in equation (2) might contain an error. In DDPG, the action is given by a deterministic policy, thus we have $a_t = \\pi(s_t)$ for all $t\\geq 0$. For critic estimation, it seems you are basing on the Bellman equation $ Q(s,a) = r(s,a) + Q(s', \\pi(s'))$, where $s'$ is the next state following $(s,a)$. Then, it seems that Equation (2) is obtained by taking gradient with respect to $(s,a)$. However, I cannot understand what $\\nabla_{\\pi} Q$ stands for. If it is $\\nabla_a Q(s_{i+1}, a_{i+1}) \\cdot \\nabla_s \\pi(s_{i+1}) $, then that makes sense.",
                "Based on the experiments, the proposed method does not always outperform MPC or DDPG, even in a small-scale control problem Mountaincar. Moreover, it seems that the performance is similar to that of the DDPG.",
                "The model-based gradient in equation (2) is defined by only unroll one-step forward by going from $s_i, a_i$ to $s_{i+1}$. It would be interesting to see how the number of unroll steps affect the algorithm, which is a gradient version of TD($\\lambda$)."
            ]
        ],
        "generated_comments": {
            "all": [
                "The introduction does a good job of outlining the limitations of both model-based control algorithms and DRL methods. However, it could further elaborate on the specific scenarios or types of tasks where these limitations become particularly problematic. This would help readers understand the motivation for developing a hybrid method.",
                "The methodology section provides a detailed explanation of the proposed algorithm. However, a visual representation or flowchart of the algorithm could be included to help readers better understand the process, especially those who are less familiar with these concepts.",
                "The paper does mention the DDPG algorithm, but it assumes that the reader is already familiar with it. A brief overview or a reference to a comprehensive source on DDPG would be beneficial for readers who are not familiar with it.",
                "The paper does explain how the algorithm uses true model gradients to improve the efficacy of learned critic models. However, it could further elaborate on why this approach is effective and how it compares to other methods that do not use true model gradients.",
                "The results section provides a comprehensive comparison of the proposed method with several baseline algorithms. However, it might be beneficial to include more quantitative data or statistical analysis, such as confidence intervals or significance tests, to further support the claims made.",
                "The conclusion does mention potential future directions. However, it might be beneficial to provide more specific examples of other DRL algorithms where the proposed mathematical framework could be applied or discuss potential modifications or extensions to the hybrid algorithm."
            ]
        },
        "alignments": {
            "all": []
        },
        "metrics": {
            "all": {
                "n_reference": 7,
                "n_generated": 6,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "xP37gkVKa_0",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The paper's proposed method, LBS, only supports one agent in a two-player game.",
                "The authors mention learning a centralized reward function using VDN, while other agent\u2019s q-value functions are unavailable. It's unclear how this is achieved.",
                "The method for using the total reward expectation to avoid having to unroll episodes until the end of the game is not clear."
            ],
            [
                "The paper needs to address how overestimation is handled when using argmax of $Q(a^i|\\tau^i)$ in n-step rollout, especially considering the large variance in rollout. The use of any variance reduction technique should be clarified.",
                "The experiments are only evaluated on Hanabi, raising concerns about the generalizability of the method. The paper should include at least one experiment evaluated on a different game, such as Leduc, to demonstrate its applicability to different imperfect information games.",
                "The paper needs to provide clear definitions and explanations for terms and concepts used, such as 'BP' on page 1, 'blueprint policies' in section 2.1, and the belief definition in section 3.",
                "Equations 2, 3, and 4 are confusing and need further explanation, as they are crucial to the paper."
            ],
            [
                "The motivation of why Learned Belief Search (LBS) can be challenging and useful for DEC-POMDP or the multi-agent game domains is not convincingly explained.",
                "The experiment settings, problem description, and discussions need more elaboration.",
                "The results in Table 1 and 3 are not discussed in the main text, making it hard to understand the reported numbers. The authors should elaborate on these in their response.",
                "A brief description of the Hanabi game should be included in the Appendix.",
                "Comparisons of different design/hyperparameter choices should be included to help judge the benefit of using learning for belief representation."
            ],
            [
                "The paper's proposed method, LBS, is highly dependent on the well-trained Blue Policy (BP). The importance of BP's contribution to the final performance is unclear.",
                "It is not clear how LBS would perform when based on a broken or flawed BP policy.",
                "The paper does not provide clarity on how LBS would perform when using an unconverging learned belief model or value model.",
                "The choice of the number of factorized private features in the belief model is not clear.",
                "The efficacy of LBS, especially in more complex environments like Contract Bridge, is not convincingly demonstrated despite the provided ablation studies."
            ],
            [
                "More detail is required to explain why multi-agent search is not theoretically sound. If the trained model is only accurate for single agent search, is it not possible to train a model that is compatible with multi-agent search under different settings for max range?",
                "The authors focus on 2-player Hanabi and claim it to be straightforward to extend to any number of players. However, SPARTA itself was computationally limited to single-agent search for these 3-player and up variants. Applying your approach in these experiments would make a stronger case for elevation of the state-of-the-art than the 6-card variant.",
                "The experimental setup requires significantly more details on the hardware used for training, testing and validating.",
                "Have you examined how this would look for more general partially observable settings? In particular, work on POSGs with public observations [Hor\u00e1k, K.; and Bosansk\u00fd, B. 2019. Solving Partially Observable Stochastic Games with Public Observations. In AAAI Conf. on Artificial Intelligence, 2029\u20132036. AAAI Press.]",
                "Are there any other DEC-POMDP settings that you considered? It may be insightful to compare the effects of different feature spaces."
            ]
        ],
        "generated_comments": {
            "all": [
                "The paper could benefit from a more comprehensive discussion on the limitations of the LBS method. Specifically, it would be helpful to delve into how these limitations impact the overall effectiveness of the method and potential solutions that could mitigate these issues. This would provide a more balanced view of the method and could guide future research in this area.",
                "The paper would be strengthened by a more detailed discussion on the future directions of the LBS method. Specifically, it would be beneficial to outline how these future directions could address the limitations of the method and potentially improve its effectiveness. This would provide readers with a clearer understanding of the potential evolution and impact of the LBS method.",
                "To enhance the reproducibility and transparency of your research, it would be beneficial to include a link to the code used in your study, or at least mention where it will be available. This would allow other researchers to validate your results and potentially build upon your work."
            ]
        },
        "alignments": {
            "all": []
        },
        "metrics": {
            "all": {
                "n_reference": 22,
                "n_generated": 3,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                }
            ]
        }
    },
    {
        "doc_id": "-qB7ZgRNRq",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The motivation for the new task proposed in the paper is not clear.",
                "The paper does not compare the proposed model, DDNet, to any other baselines that incorporate the audio information in a different way.",
                "The paper incorrectly claims that the method from Serdyuk et al. (2018) was 'concurrent', despite it being published in 2018.",
                "It is unclear why the Spoken-CoQA dataset includes text transcripts as well as the audio. It would make more sense for the text transcripts to be part of the model solving the dataset."
            ],
            [
                "The work is limited in its scope and is not yet ready to be published at ICLR due to lack of comparisons made to prior approaches, making it hard to assess the merits of the proposed approach.",
                "The authors do not mention any plans of releasing the new dataset described in this work, which is a concern.",
                "It would be interesting to show how F1 scores vary on the test instances as a function of the number of turns in the conversation, and how test F1 scores vary as a function of ASR accuracy of the spoken documents/questions.",
                "The error rate of the ASR system on the spoken documents and spoken questions should be provided to give the reader an idea of the accuracy of the transcriptions fed as input to the student model.",
                "In Table 4, the F1 scores using SDNet are higher for S-CoQA compared to CoQA which is unexpected. The authors should provide an explanation for this.",
                "The reliance of the proposed system on clean text is a significant issue that needs to be addressed."
            ],
            [
                "The paper's contributions and novelty are unclear, and it's uncertain if they warrant acceptance.",
                "The paper's methods, including the cross attention mechanism for speech and text embedding fusion and knowledge distillation for combatting ASR errors, are existing, well-studied methods.",
                "The Spoken-CoQA dataset, which is a significant contribution of the paper, is not made available.",
                "The paper should clarify whether the knowledge distillation and cross attention mechanisms can be combined for better performance.",
                "The paper should address whether the textual input is more useful than the audio input in the Spoken-CoQA dataset, especially considering the audio input is larger and more difficult to process."
            ],
            [
                "The biggest weakness of this paper is in its data construction. When the ASR output is noisy, the spans refer to ghost token positions based on the clean text. This makes the data difficult to learn from and may result in models predicting the wrong span.",
                "The paper falls short of delivering a usable dataset for speech-based conversational QA. The authors should consider returning the audio segments as answers instead of using text-based spans. This might require additional annotation but could make the dataset more meaningful and usable.",
                "The use of a knowledge distillation (KD) layer to address the issue of noisy ASR text is questionable. The KD layer shows the model the gold clean transcription (original passage), which could defeat the purpose of speech-based conversational QA as motivated in this paper."
            ]
        ],
        "generated_comments": {
            "all": [
                "The introduction section provides a good overview of the problem and the proposed solution. However, it could benefit from a more detailed explanation of the Spoken Conversational Question Answering task (SCQA). Specifically, it would be helpful to elaborate on how the SCQA task enables QA systems to model complex dialogues flow given the speech utterances and text corpora, and why this is an important advancement in the field. (Agent 1)",
                "The methodology section introduces the novel unified data distillation approach, DDNet, which is well explained. However, it would be helpful to include more details on how the DDNet fuses audio-text features to reduce the misalignment between automatic speech recognition hypotheses and the reference transcriptions, and how this improves the performance of QA systems in spoken conversational question answering. (Agent 1)",
                "The paper mentions the creation of a new Spoken Conversational Question Answering (Spoken-CoQA) dataset. It would be beneficial to provide more information on how this dataset was assembled, why it is more challenging than single-turn benchmarks, and why it is suitable for evaluating the proposed method. (Agent 1)",
                "The results section could benefit from a more detailed explanation of the statistical methods used to analyze the data. Specifically, it would be helpful to explain how the Exact Match (EM) and F1 score were used for evaluation of model performance, and how the Kullback-Leibler divergence and cross entropy were used in the context of knowledge distillation. This would help readers understand how the conclusions were reached. (Agent 2)",
                "The conclusion section does a good job of summarizing the findings, but it could also discuss potential implications of the research and suggest areas for future study. Specifically, it would be beneficial to discuss how the proposed method, DDNet, can improve the performance of spoken conversational question answering tasks by mitigating ASR errors, and how this could potentially enhance human-machine communication. (Agent 2)"
            ]
        },
        "alignments": {
            "all": [
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The motivation for the new task proposed in the paper is not clear.",
                    "predicted_comment": "The introduction section provides a good overview of the problem and the proposed solution. However, it could benefit from a more detailed explanation of the Spoken Conversational Question Answering task (SCQA). Specifically, it would be helpful to elaborate on how the SCQA task enables QA systems to model complex dialogues flow given the speech utterances and text corpora, and why this is an important advancement in the field. (Agent 1)"
                }
            ]
        },
        "metrics": {
            "all": {
                "n_reference": 18,
                "n_generated": 5,
                "n_total_aligns": 1,
                "recall": 0.0625,
                "precision": 0.05,
                "pseudo_jaccard": 0.03125,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.25,
                    "precision": 0.2,
                    "pseudo_jaccard": 0.125,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "0NQdxInFWT_",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The method is a small extension to the DPS method where the network that selects which rows to samples is conditioned on the existing measurements.",
                "The paper did not compare to any other active sampling strategies. The authors made no effort to replicate existing methods.",
                "The Markov chain example in section 4.1 was hard to follow and more distracting than informative.",
                "The results with active sampling were only marginally better than results with a fixed (learned) sampling strategy.",
                "The method is applicable only to true subsampling problems, not general sensing. That is, one isn't designing the rows of a measurement matrix on the fly but rather selecting which row from an existing matrix (identity in most of the examples) that one would like to sample from.",
                "Because of the nonlinearity in the forward model, equation (9) is not actually proximal gradient descent. There's a sign(F^HD\\circFX) term missing from the (sub) gradient.",
                "The expression A^H(Ax-Y\\circ sign(A(x))) is a subgradient of 1/2|| Y - |A(X)|||^2 but A^H(|Ax|-Y) is not. I would avoid calling (9) projected gradient descent as the 'gradient' isn't really a gradient."
            ],
            [
                "The proposed approach appears to be incremental: ADPS adds a simple sequential update structure (of a context vector) to DPS, which can be described by only two equations (6 and 7). The simplicity of the changes proposed (over DPS) is not a limitation, but it could be accompanied by an in-depth theoretical analysis, a convincing qualitative discussion or extensive experiments demonstrating the practical relevance of the proposed approach.",
                "The paper contributes by extending DPS into an active/sequential learning framework achieving significant performance gains over DPS, mainly on toy problems. However, there is room for more extensive experimental validation, perhaps beyond MRI."
            ],
            [
                "The paper lacks comparison to other active sub-sampling schemes, such as those proposed by Zhang et al., 2019 and Jin et al., 2019. It would be beneficial to see if the proposed method produces better results than these existing methods.",
                "The toy example provided is overly constructed and difficult to understand, which does not improve the quality of the paper.",
                "It is unclear what happens when the MNIST sampling ratio in Figure 3a is further increased. It would be helpful to know if A-DPS consistently outperforms DPS in low sampling ratio regimes."
            ]
        ],
        "generated_comments": {
            "all": [
                "In the introduction, provide a more detailed explanation of the Deep Probabilistic Subsampling (DPS) method, including how DPS integrates subsampling in an end-to-end deep learning model and its limitations that led to the development of A-DPS.",
                "In the methodology section, include a simplified explanation or analogy for the key concepts of DPS and A-DPS, to help readers unfamiliar with the field understand these concepts. Also, provide more context on how each step in the equations contributes to the overall method.",
                "Explain in more detail how the Active Deep Probabilistic Subsampling (A-DPS) method improves upon the DPS method, including a comparison of their performance in various scenarios such as the MNIST classification at high subsampling rates and a discussion of the reasons for the differences in performance.",
                "Provide more specific examples or case studies to illustrate how A-DPS outperforms other sampling pattern selection methods, including a comparison of their performance in the same scenarios and a discussion of the reasons for the differences in performance.",
                "In the conclusion section, provide more context on the computational complexity of A-DPS, including a comparison with DPS, a discussion of the trade-off between computational complexity and adaptation rate, and the implications for practical applications.",
                "Include a more detailed discussion on the potential challenges and opportunities in improving the conditioning of the sampling scheme and exploring potential applications, including a discussion of the current state of the art and potential future directions."
            ]
        },
        "alignments": {
            "all": [
                {
                    "reference_id": 1,
                    "predicted_id": 3,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper did not compare to any other active sampling strategies. The authors made no effort to replicate existing methods.",
                    "predicted_comment": "Provide more specific examples or case studies to illustrate how A-DPS outperforms other sampling pattern selection methods, including a comparison of their performance in the same scenarios and a discussion of the reasons for the differences in performance."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 2,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The proposed approach appears to be incremental: ADPS adds a simple sequential update structure (of a context vector) to DPS, which can be described by only two equations (6 and 7). The simplicity of the changes proposed (over DPS) is not a limitation, but it could be accompanied by an in-depth theoretical analysis, a convincing qualitative discussion or extensive experiments demonstrating the practical relevance of the proposed approach.",
                    "predicted_comment": "Explain in more detail how the Active Deep Probabilistic Subsampling (A-DPS) method improves upon the DPS method, including a comparison of their performance in various scenarios such as the MNIST classification at high subsampling rates and a discussion of the reasons for the differences in performance."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 3,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper contributes by extending DPS into an active/sequential learning framework achieving significant performance gains over DPS, mainly on toy problems. However, there is room for more extensive experimental validation, perhaps beyond MRI.",
                    "predicted_comment": "Provide more specific examples or case studies to illustrate how A-DPS outperforms other sampling pattern selection methods, including a comparison of their performance in the same scenarios and a discussion of the reasons for the differences in performance."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 2,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "It is unclear what happens when the MNIST sampling ratio in Figure 3a is further increased. It would be helpful to know if A-DPS consistently outperforms DPS in low sampling ratio regimes.",
                    "predicted_comment": "Explain in more detail how the Active Deep Probabilistic Subsampling (A-DPS) method improves upon the DPS method, including a comparison of their performance in various scenarios such as the MNIST classification at high subsampling rates and a discussion of the reasons for the differences in performance."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 3,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "The paper lacks comparison to other active sub-sampling schemes, such as those proposed by Zhang et al., 2019 and Jin et al., 2019. It would be beneficial to see if the proposed method produces better results than these existing methods.",
                    "predicted_comment": "Provide more specific examples or case studies to illustrate how A-DPS outperforms other sampling pattern selection methods, including a comparison of their performance in the same scenarios and a discussion of the reasons for the differences in performance."
                }
            ]
        },
        "metrics": {
            "all": {
                "n_reference": 12,
                "n_generated": 6,
                "n_total_aligns": 5,
                "recall": 0.6031746031746031,
                "precision": 0.27777777777777773,
                "pseudo_jaccard": 0.2341269841269841,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.14285714285714285,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.08333333333333333,
                    "n_real": 7
                },
                {
                    "recall": 1.0,
                    "precision": 0.3333333333333333,
                    "pseudo_jaccard": 0.3333333333333333,
                    "n_real": 2
                },
                {
                    "recall": 0.6666666666666666,
                    "precision": 0.3333333333333333,
                    "pseudo_jaccard": 0.2857142857142857,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "-spj8FZD4y2",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The paper lacks a complete theoretical analysis, such as the upper bound/lower bound on the regret of the algorithm.",
                "The algorithm should be explicitly written out with its pseudocode.",
                "The paper states that the communication is one-way from the decision-maker towards the controller, but it is strange not to consider the realization history observed by the decision-maker as communication. The goal of this work and how it differs from existing papers is unclear.",
                "The problem setup is unclear: it is not specified whether the state is i.i.d. sampled from the distribution $P_S$ or not.",
                "The relation between the constraint quantities $(\\rho,R)$ and the discussions in Section 3 is unclear.",
                "The constraints seem to be placed in the posterior probabilities involved in the TS algorithms. It is not clear what would happen if we consider UCB-class algorithms.",
                "The definitions of $D_\\alpha$ and reverse KL divergence $D_{KL}$ are missing. Usually, the original KL divergence is denoted by $D_{KL}$.",
                "In Section 3.3, it states ''If $S$ and $A$ are independent...'', which is not a common assumption in contextual bandits."
            ],
            [
                "There are inconsistencies and imprecisions in the writing, particularly in Section 2, which defines quantities, regrets, and gains that are not analyzed afterwards. Clarification is needed on the definitions and usage of G, \\rho, and R.",
                "The paper does not provide regret bounds. While it may be out of the scope of the paper, some commentary on this would be beneficial.",
                "The algorithm needs to be clearly outlined. While detailed explanations are provided, a pseudo-code would greatly improve readability and understanding.",
                "The paper could be better connected with existing literature on contextual bandit, such as contextual bandits with context uncertainties or Partially Observable MDPs."
            ],
            [
                "The results in the paper are largely adapted from known information-theoretic results, which limits the overall novelty of the work.",
                "The assumption that each agent's state is observed by the decision-maker, but not the agent itself, is unusual. The authors should clarify why they make this assumption.",
                "The practical coding scheme is about sending the state to the corresponding agent, which may not be necessary in practice.",
                "The controller 'decoder' function should have the joint states of all agents as an input to enhance the decoder design."
            ],
            [
                "The paper lacks theoretical performance guarantees for the clustering coding scheme. A characterization of how regret depends on the number of states, the rate constraint, number of actions, etc. would provide a more in depth understanding of the performances of the clustering scheme in various scenarios.",
                "The paper does not discuss how the proposed clustering schemes\u2019 performance decays as the total number of states grows. More states would induce larger clusters and hence larger distortion in the policy defined over the compressed states (compared to the target policy). This is particularly relevant as the paper uses personalized ad placements as a motivating example, where the content owner\u2019s valuation for a user view (i.e. impression) or conversion serves as the 'context' (or state), and may take a wide range of values."
            ]
        ],
        "generated_comments": {
            "all": [
                "Consider providing a more detailed explanation of the Contextual Multi-Armed Bandit (CMAB) problem, particularly how it applies to scenarios where the number of agents and possible actions are large, but the communication budget is limited.",
                "Include more context on the existing literature in the field, particularly on the synergies between Machine Learning (ML) and communication networks, and the need for efficient data compression.",
                "Clarify the concept of 'task-oriented compression', and how it relates to the design of distributed learning algorithms.",
                "Provide a more detailed explanation of the rate-distortion theory, and how it is used in the optimization objective of the RC-CMAB problem.",
                "Provide a more intuitive explanation or example of the KL-divergence as a distortion function, and how it is used in the construction of approximate sampling policies.",
                "Include the equations mentioned in the text, particularly those related to the optimization of the Lagrangian multiplier \u03bb and the computation of the optimal centroids.",
                "Provide a brief overview or reference for the Blahut-Arimoto and Lloyd algorithms, and how they are used in the solution of the RC-CMAB problem and the grouping of states into clusters, respectively.",
                "Provide more details or examples on the implementation of the practical coding scheme based on state reduction, and how it contributes to the performance of the policy obtained through the asymptotic information theoretic formulation.",
                "Provide more details on the setup of the experiments, such as the parameters used and the rationale behind their selection, and how they contribute to the analysis of the rate-distortion function and the performance of different agents in the RC-CMAB problem.",
                "Provide more detailed explanations of the results obtained from the experiments, particularly how they relate to the learning phase of agents and the asymptotic rate bound.",
                "Elaborate on the point that the work can serve as a first step towards understanding the fundamental performance limits of multi-agent decision-making problems under communication constraints, and how the study of the RC-CMAB problem contributes to this understanding.",
                "Provide a brief summary of the mathematical derivations in the annex section in the main text, particularly those related to the optimization of the Lagrangian multiplier \u03bb and the computation of the optimal centroids."
            ]
        },
        "alignments": {
            "all": []
        },
        "metrics": {
            "all": {
                "n_reference": 18,
                "n_generated": 12,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                }
            ]
        }
    },
    {
        "doc_id": "7udZAsEzd60",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The implication of Theorem 4.2 is not fully understood. The size(rho, kappa) critically depends on r, the rank of the weight matrix W. But if we decompose W=UV, where U is d*r and V=r*k, it seems like the VC dimension of HANN is still higher than the number of effective parameters, dr+rk. In this case, what benefit do HANN have over ReLU and sigmoid networks?",
                "The role of the binary activation in HANN is unclear. The critical assumption of Theorem 4.2 is that the weight matrix is low-rank. In this case, can't the layer with low-rank weight be reformulated as two layers with a small bottleneck hidden layer of dimensionality r, which has much smaller number of parameters than the equivalent network? If this is the case, the result seems to be only relevant to the low-rank assumption, not the HANN itself."
            ],
            [
                "The title and introduction of the paper suggest that a more general class of Quantized Neural Networks (QNNs) have small VC dimension, but the model studied is essentially a two-layer NN. This raises doubts about whether any non-toy quantized models will have small VC dimension. An experiment similar to Zhang et al 2017 could easily verify this.",
                "The paper's focus on studying the VC dimension of Neural Networks (NNs) is questionable. The authors should clarify what we are aiming to learn from this, as in practice VC dimensions often provide non-meaningful bounds.",
                "The title and introduction of the paper may overstate the significance of the results, and the motivation for the study may not be strong enough."
            ],
            [
                "The bound on the VC dimension for hyperplane augmented neural networks (HANNs) seems quite large: O(k^r). This seems to be a major limitation in the application of the main result of the paper.",
                "The paper could significantly benefit from a discussion on the large bound on the VC dimension, either explaining more cases where the results are useful or clarifying this limitation.",
                "Using the numbers given in the experiments, with a dataset of size 77904 and k=100, and r=3 seems to result in vacuous bounds on generalization again, which questions the effectiveness of the main result."
            ]
        ],
        "generated_comments": {
            "all": [
                "Provide a more detailed explanation of the 'generalization puzzle' in the introduction. This will help readers understand the problem you are addressing and why it is important.",
                "Provide more context on why hyperplane arrangement neural networks (HANNs) were chosen for the study. This will help readers understand the rationale behind your choice of method.",
                "Elaborate more on how the theory of sample compression schemes is applied in the study. This will help readers understand how your theoretical framework informs your methodology.",
                "Provide more detailed explanations of the experimental results and their significance. This will help readers understand the implications of your findings.",
                "Provide more context for the synthetic and real datasets used in the experiments. This will help readers understand the scope and limitations of your data.",
                "Provide more context or background information on the VC theorem and its relevance to the study. This will help readers understand the theoretical underpinnings of your work.",
                "Support the claim that this is the first time VC theory has been used to analyze the performance of a neural network in the overparametrized regime with more evidence or references. This will help readers understand the novelty of your work.",
                "Provide more details about the unstructured datasets used for benchmarking and why they were chosen. This will help readers understand the validity of your benchmarks.",
                "Provide a more detailed summary of the key findings in the conclusion. This will help readers understand the main takeaways from your work.",
                "Clarify the implications of the No-Free-Lunch Theorem in the context of the study. This will help readers understand the broader theoretical implications of your work.",
                "Provide more context or explanation for the cited works, such as Naslund (2017) and Audibert & Tsybakov (2007). This will help readers understand how your work builds on previous research."
            ]
        },
        "alignments": {
            "all": []
        },
        "metrics": {
            "all": {
                "n_reference": 8,
                "n_generated": 11,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "QmKblFEgQJ",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The empirical performance gains provided by the DIGRAC method are difficult to understand. In many of the ARI graphs on synthetic data, the performance of various methods seems quite close. In the real data, methods overlap in their error bars.",
                "More explanation is needed about when DIGRAC's model for clustering is appropriate for a dataset and when it is not, and how adaptive the model can be in terms of performance despite those considerations."
            ],
            [
                "The practicality of the problem and the method studied in this paper is questionable. The authors mention that 'real data sets with ground-truth flow imbalances are not available to date', so they use normalized imbalance scores to evaluate clustering performance. This approach seems to go against the typical order of how machine learning models are developed.",
                "The experiments in Section 4 do not demonstrate the practical importance of clustering with respect to flow imbalances. As this is an empirical paper, it is necessary to provide solid empirical evidence that showcases the need for clustering with respect to flow imbalances.",
                "The authors use 80% of all nodes for training in their empirical evaluations. This percentage seems too high and it is suggested to consider using a smaller percentage, such as 5%, of all nodes for training."
            ],
            [
                "The theoretical properties of the loss function defined in Eq. (2) and (3) are not clear. The paper should provide a theoretical characterization for this loss and discuss cases where it could lead to degenerate solutions.",
                "The constraints of the optimization problem need to be clearly presented.",
                "The paper should examine how the loss behaves in terms of convergence.",
                "The novelty of the DIMPA algorithm is unclear. The paper should clarify what are the novel parts of this model with respect to previous works.",
                "The baseline models used for comparison could be enhanced. The paper should consider adding models that capture information flow, including algorithms that rely on stochastic blockmodels and a model that is based on directed modularity.",
                "The paper lacks scalability experiments with the running time of the algorithm and its comparison to baseline models."
            ]
        ],
        "generated_comments": {
            "all": [
                "The paper effectively highlights the significance of directionality in node clustering, distinguishing it from traditional methods that emphasize edge density. To enhance understanding, consider incorporating concrete examples or case studies that demonstrate how directionality impacts node clustering in real-world scenarios.",
                "While the DIGRAC method is clearly introduced, the paper could benefit from a more comprehensive discussion on how this method compares and contrasts with other graph neural network frameworks. This would provide readers with a broader understanding of the method's place within the field.",
                "The paper mentions that DIGRAC can incorporate node features and operate without known labels, a significant advantage over existing methods. To enhance clarity, consider providing a more detailed explanation of how DIGRAC achieves this, perhaps through a step-by-step walkthrough or a diagram.",
                "The paper could expand on the potential applications of DIGRAC. While a few examples are mentioned (e.g., analyzing migration patterns, detecting influential social groups), a more detailed exploration of these use cases, as well as the inclusion of additional potential applications, would be beneficial.",
                "The background section provides a comprehensive review of related work. To enhance this section, consider providing a more direct comparison between DIGRAC and these methods, highlighting the unique advantages of DIGRAC. This could be achieved through a comparison table or a detailed discussion.",
                "The methodology section, while detailed, is quite complex due to the inclusion of numerous mathematical equations and terminologies. To aid reader comprehension, consider providing a simplified explanation of the methodology or including visual aids such as diagrams or flowcharts.",
                "The section on Directed Mixed Path Aggregation (DIMPA) is well explained. To enhance understanding, consider providing more context or examples illustrating when and why this method would be used in practice.",
                "The methodology section contains numerous abbreviations. While these are defined upon first use, redefining them occasionally throughout the section could enhance clarity and reader comprehension, given the section's length.",
                "The paper could benefit from a more detailed explanation of the loss functions used in the experiments. While the 'sort' loss function is mentioned as the best performer, a more comprehensive discussion on why this is the case and how the other loss functions differ would be beneficial.",
                "The conclusion mentions future work on detecting the number of clusters and the value of \u03b2. To enhance understanding, consider discussing why these are important factors to consider and how their optimization could potentially improve the model's performance.",
                "The paper mentions that the model can boost its performance when additional label information is available. To enhance clarity, consider providing more context on how this additional label information is used and why it contributes to improved performance.",
                "The conclusion section discusses extending the framework to large scale industrial applications. To enhance this discussion, consider including a discussion on the potential challenges and solutions for scaling the model, including computational requirements, data availability, and model robustness."
            ]
        },
        "alignments": {
            "all": []
        },
        "metrics": {
            "all": {
                "n_reference": 11,
                "n_generated": 12,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                }
            ]
        }
    },
    {
        "doc_id": "b-ny3x071E5",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The name 'matching function' makes the presentation of the method a little bit harder to digest. Since the function is a pseudometric (i.e., the larger it is, the larger the distance from the target), it should really be called with a name that reminds the reader of this nature (e.g., 'mismatch function').",
                "The theoretical results only deal with targets of specific forms and, especially, with $L=1$ only. Ideally, theoretical result with a dependency on $L$ would shed some light on the benefits and limitations of longer bootstrapping horizons.",
                "The connection between the way the bootstrapping target is formed in the method and traditional temporal difference learning needs to be elaborated on. In particular, the grounding role of that subtracted gradient 'nudging the trajectory in a descent direction' is the same as the one of the reward in temporal difference learning; but, while the reward is at the beginning of the trajectory, the grounding is here at the end of the optimization subtrajectory. Is there any mathematical connection beyond the general shared motivation?",
                "When the underlying function is highly nonlinear, there is the risk that the bootstrapping mechanism can lead the optimization process in worse areas of the landscape. For instance, if the function in Figure 1 had a bump/plateaux where $\tilde w$ is, the bootstrapping mechanism would cause more troubles than standard meta-gradients. Why is this not happening in practice?"
            ],
            [
                "The algorithm description in section 3 is very abstract. The paper would benefit from a running example and a dedicated section and pseudo-code describing the algorithm and how it can be instantiated in different experimental settings.",
                "The theoretical analysis is limited to noiseless 1-step target updates.",
                "The experimental evaluation in the multi-task meta-learning setting is limited to only compare with MAML on computer vision applications.",
                "Some engineering / handcrafting is still required by the machine learning practitioner to select what 'target' the meta-learner is going to optimize, as well as the proper 'metric' for the meta-learner to optimize for. Could the authors comment a bit about what heuristics they used when making these decisions? and whether the automation for this process is possible or not?",
                "What would it take to extend the analysis beyond 1-step noiseless target updates?",
                "How does the performance of BMG compare to alternative meta-learning algorithms like R2D2, Meta-OPT-net and prototypical networks? Have the authors experimented with other meta-learning benchmarks beyond image classification?"
            ],
            [
                "Given that you say BMG is compatible with any update function (so long as it is differentiable in the meta-parameters), it would be nice to have some experiments on learned sequence model update rules (e.g. RNN). All current experiments use update rules with a fixed functional form.",
                "I am not putting much weight on section 4 ('Performance Guarantees') given the gap between its assumptions and results vs. what is actually implemented, and the restriction to local optimization."
            ],
            [
                "The paper could benefit from more detailed discussions in certain areas to improve clarity and understanding.",
                "The methodologies of the experiments are difficult to follow, making it hard to link the results back to intuition or interpretation. More detailed descriptions or an algorithm box in the main text could help address this.",
                "The implementation and implications of the experiment on multi-task few-shot learning (Sec 6) are unclear. The authors should clarify the intuition of why a 'hot' expert transforms more information than a 'cold' expert, and why BMG is able to use this to improve performance.",
                "The training procedure for BMG in the multi-task few-shot learning experiment should be formally described in the main text.",
                "The analysis presented in Sec 4 ('Performance Guarantees') is not especially insightful. The conclusion drawn from this section is evident from the experiments themselves."
            ]
        ],
        "generated_comments": {
            "all": [
                "The introduction could benefit from a more detailed explanation of the proposed algorithm and more context on why the current approaches to meta-learning are limited. Specifically, the paper should elaborate on the high computational costs and the assumption that optimizing performance after a certain number of applications of the update rule will yield improved performance for the remainder of the learner's lifetime. The paper should also discuss how these limitations lead to degraded lifetime performance, collapsed exploration, biased learner updates, and poor generalisation performance. Additionally, the paper should provide a clearer explanation of the benefits of the proposed algorithm, including how it allows for the extension of the effective meta-learning horizon without requiring backpropagation through all updates and how it allows for control over the optimisation landscape. The paper should also discuss the potential performance and efficiency gains in multi-task meta-learning that can be achieved with the proposed algorithm. (Agent 1)",
                "The methodology section could benefit from a more structured presentation. More context or background information on the BMG approach, including how it focuses on the effect of bootstrapping and how it compares with the MG approach in an online setup, would be helpful. The paper should also provide more details on the specific tasks used in the experiments, such as the tabular grid-world with two items to collect and the Atari game. The paper should also discuss the experimental setup, including how the BMG approach was tested in two main settings: the Atari game and a multi-task few-shot learning setting. The paper should also discuss how the results were evaluated, including how performance and efficiency were measured. (Agent 3)",
                "The discussion section could benefit from a summary of the key findings at the beginning, including how BMG outperforms MG for a given horizon without backpropagating through all updates. The paper should also include brief explanations of certain concepts, visual representations of certain concepts, such as Figure 2, and a table comparing the computational properties of STACX and BMG. The paper should also provide a brief explanation of the Few-Shot MiniImagenet setup and its significance. The paper should also consolidate the discussion into a single section, include a glossary or definitions of key terms, and discuss the limitations and potential areas for future research, such as exploring other forms of bootstraps. (Agent 5)"
            ]
        },
        "alignments": {
            "all": [
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The algorithm description in section 3 is very abstract. The paper would benefit from a running example and a dedicated section and pseudo-code describing the algorithm and how it can be instantiated in different experimental settings.",
                    "predicted_comment": "The introduction could benefit from a more detailed explanation of the proposed algorithm and more context on why the current approaches to meta-learning are limited. Specifically, the paper should elaborate on the high computational costs and the assumption that optimizing performance after a certain number of applications of the update rule will yield improved performance for the remainder of the learner's lifetime. The paper should also discuss how these limitations lead to degraded lifetime performance, collapsed exploration, biased learner updates, and poor generalisation performance. Additionally, the paper should provide a clearer explanation of the benefits of the proposed algorithm, including how it allows for the extension of the effective meta-learning horizon without requiring backpropagation through all updates and how it allows for control over the optimisation landscape. The paper should also discuss the potential performance and efficiency gains in multi-task meta-learning that can be achieved with the proposed algorithm. (Agent 1)"
                },
                {
                    "reference_id": 1,
                    "predicted_id": 1,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "The methodologies of the experiments are difficult to follow, making it hard to link the results back to intuition or interpretation. More detailed descriptions or an algorithm box in the main text could help address this.",
                    "predicted_comment": "The methodology section could benefit from a more structured presentation. More context or background information on the BMG approach, including how it focuses on the effect of bootstrapping and how it compares with the MG approach in an online setup, would be helpful. The paper should also provide more details on the specific tasks used in the experiments, such as the tabular grid-world with two items to collect and the Atari game. The paper should also discuss the experimental setup, including how the BMG approach was tested in two main settings: the Atari game and a multi-task few-shot learning setting. The paper should also discuss how the results were evaluated, including how performance and efficiency were measured. (Agent 3)"
                },
                {
                    "reference_id": 3,
                    "predicted_id": 1,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The training procedure for BMG in the multi-task few-shot learning experiment should be formally described in the main text.",
                    "predicted_comment": "The methodology section could benefit from a more structured presentation. More context or background information on the BMG approach, including how it focuses on the effect of bootstrapping and how it compares with the MG approach in an online setup, would be helpful. The paper should also provide more details on the specific tasks used in the experiments, such as the tabular grid-world with two items to collect and the Atari game. The paper should also discuss the experimental setup, including how the BMG approach was tested in two main settings: the Atari game and a multi-task few-shot learning setting. The paper should also discuss how the results were evaluated, including how performance and efficiency were measured. (Agent 3)"
                },
                {
                    "reference_id": 0,
                    "predicted_id": 2,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper could benefit from more detailed discussions in certain areas to improve clarity and understanding.",
                    "predicted_comment": "The discussion section could benefit from a summary of the key findings at the beginning, including how BMG outperforms MG for a given horizon without backpropagating through all updates. The paper should also include brief explanations of certain concepts, visual representations of certain concepts, such as Figure 2, and a table comparing the computational properties of STACX and BMG. The paper should also provide a brief explanation of the Few-Shot MiniImagenet setup and its significance. The paper should also consolidate the discussion into a single section, include a glossary or definitions of key terms, and discuss the limitations and potential areas for future research, such as exploring other forms of bootstraps. (Agent 5)"
                },
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper could benefit from more detailed discussions in certain areas to improve clarity and understanding.",
                    "predicted_comment": "The introduction could benefit from a more detailed explanation of the proposed algorithm and more context on why the current approaches to meta-learning are limited. Specifically, the paper should elaborate on the high computational costs and the assumption that optimizing performance after a certain number of applications of the update rule will yield improved performance for the remainder of the learner's lifetime. The paper should also discuss how these limitations lead to degraded lifetime performance, collapsed exploration, biased learner updates, and poor generalisation performance. Additionally, the paper should provide a clearer explanation of the benefits of the proposed algorithm, including how it allows for the extension of the effective meta-learning horizon without requiring backpropagation through all updates and how it allows for control over the optimisation landscape. The paper should also discuss the potential performance and efficiency gains in multi-task meta-learning that can be achieved with the proposed algorithm. (Agent 1)"
                }
            ]
        },
        "metrics": {
            "all": {
                "n_reference": 17,
                "n_generated": 3,
                "n_total_aligns": 5,
                "recall": 0.19166666666666665,
                "precision": 0.3333333333333333,
                "pseudo_jaccard": 0.18125,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.16666666666666666,
                    "precision": 0.3333333333333333,
                    "pseudo_jaccard": 0.125,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.6,
                    "precision": 1.0,
                    "pseudo_jaccard": 0.6,
                    "n_real": 5
                }
            ]
        }
    },
    {
        "doc_id": "uB12zutkXJR",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The paper does not convincingly demonstrate model novelty or significant improvement to the state-of-the-art on the provided baseline.",
                "The model architecture seems very closely based on the HOPPITY baseline, with the primary change being to the multi-head graph encoder. This does not introduce a fundamentally new idea.",
                "Quantitative evaluations show only marginal improvement at best in comparison to the large pre-trained models.",
                "The evaluation omits a comparison to the closely related Yao 2021 model that is cited throughout the paper.",
                "The pre-training regime, which is presented as a contribution of this paper, does not seem to produce markedly stronger results. It might be valid to apply the same pre-training regime to some of the other models for a fair comparison.",
                "The qualitative evaluation does not yield clear conclusions. It would be beneficial to provide evidence that 'GRAPHIX demonstrates strong inductive biases in learning complex bug-fix patterns' or some conclusion on the kinds of bugs that GRAPHIX tends to fix.",
                "The pre-training regime is not particularly novel. The authors suggest deleting elements of subtrees from existing code and reconstructing them, which is not sufficiently novel."
            ],
            [
                "The paper does not clearly explain the relevance of the presented work despite not beating the state of the art. A discussion on this topic is missing.",
                "The anecdotal examples are not compared to the results other approaches give, making it unclear what to make of them.",
                "The claim that the model is more than 10x smaller than 'current large scale sequence models' is misleading as only one of the evaluated baselines, 'BART', is that much larger. The presented approach is even outperformed by CodeT5-small, which is less than 2x as large.",
                "The paper lacks a discussion on the accuracy of detecting bug fixes by filtering for 'fix', 'bug', 'error', and 'exception'.",
                "The reasoning behind having exactly 600 nodes is not explained."
            ],
            [
                "The pre-training task, while claimed to be novel, is not particularly innovative. It involves 'masking sub-trees' on AST, which essentially asks the model to do 'missing code prediction', covering only a small subset of general program repair samples. The pre-training task could be more innovative if more thought was put into generating the pre-training dataset, such as how to generate data for pre-training other edits like copy, remove, and update.",
                "Pre-training is costly and does not provide a significant performance gain (<2%). This issue is more pronounced with medium-sized data. The reviewer suggests two questions for consideration: 1) Why is pre-training only done on 'sub-trees between 2 and 6 descendants'? Could the size be increased to cover longer edit sequences during pre-training? Is it possible to add such an experiment? 2) Could the lack of performance gain be due to the pre-training task only asking the model to predict a sequence of addition operations? Could more diverse tasks be added to allow the model to predict a variety of edits?",
                "The multi-head graph encoder does not seem particularly novel.",
                "The paper does not mention related works such as 'TFix: Learning to Fix Coding Errors with a Text-to-Text Transformer' and 'Learning semantic program embeddings with graph interval neural network'.",
                "The last paragraph in Section 4.1 mentions a single-head 'base' model, which is not mentioned in any experiment in the main paper. The reviewer suggests moving this description into the Appendix or including the 'base' model and its performance in Table 1/2.",
                "In Listing 2 Top, the code is not valid if the highlighted green part is removed. The reviewer asks for clarification on the before and after of that code snippet.",
                "The paper claims 'these observations suggest that the code abstraction may not be necessary'. The reviewer finds this argument weak and disagrees with the initial premise that 'code abstraction is necessary'."
            ],
            [
                "The paper lacks a comprehensive comparison with prior work, making the contributions over previous work hard to assess.",
                "The efficacy of the proposed multihead Graph Neural Network (GNN) scheme is not adequately ablated. The paper needs to include ablations with models of the same size.",
                "There are concerns about the scalability of the model given the need to encode the entire tree at each intermediate step.",
                "The paper needs to provide equal comparisons of Hoppity and one or more deeper non-multihead Graphix models in the 32M parameter domain.",
                "The contribution of pretraining seems quite slim; performance only increases a little on the Small dataset and actually decreases on the Medium data. This needs more analysis.",
                "The paper needs to analyze and discuss the potential scalability problems of reinstantiating and encoding the program graph on every edit, especially on longer chains of edits.",
                "The paper needs to clarify whether the ensemble-of-graphs model is meant to be a contribution in general or just for source code.",
                "The paper needs to provide a more detailed analysis of the ASDL-guided decoder, especially in comparison with a small instantiation of Hoppity."
            ]
        ],
        "generated_comments": {
            "all": [
                "From Agent 1 (Introduction and Background):",
                "1. Provide a more detailed explanation of why current methods are insufficient and how GRAPHIX addresses these shortcomings. This will help readers understand the motivation behind the development of GRAPHIX.",
                "2. Provide more context on how GRAPHIX fits into the broader landscape of automated program repair tools. This will give readers a better understanding of the field and how GRAPHIX contributes to it.",
                "3. Provide a more detailed comparison between other models and GRAPHIX to highlight its unique advantages. This will help readers understand what sets GRAPHIX apart from other models.",
                "4. Provide a clearer explanation of the pre-training strategy for GRAPHIX and why it improves the model's performance. This will help readers understand the technical aspects of GRAPHIX.",
                "5. Elaborate on how GRAPHIX can be extended to work with multiple languages and the challenges associated with it. This will give readers an idea of the potential applications and limitations of GRAPHIX.",
                "From Agent 2 (Pre-training Objective, Experiments, Dataset, Data Processing for GRAPHIX, Baselines and Metrics, Results, Anecdotal Examples, Discussion and Future Work):",
                "1. Provide a more detailed explanation of how the novel pre-training technique for tree-structured data differs from existing methods. This will help readers understand the novelty of the pre-training technique.",
                "2. Elaborate on why the pre-training technique is specific to ASTs with an underlying syntax language and not applicable to arbitrary graph structures. This will help readers understand the limitations of the pre-training technique.",
                "3. Provide a clear justification for the choice of the Patches in the Wild Java bug-fix benchmark for evaluation. This will help readers understand why this particular dataset was chosen.",
                "4. Provide more details on how the CodeSearchNet dataset was processed and prepared for use with the proposed model. This will help readers understand the data preparation process.",
                "5. Provide a clear explanation of why the chosen baselines were selected for comparison. This will help readers understand the rationale behind the choice of baselines.",
                "6. Provide a justification for the choice of the standard top-1 exact match accuracy (EM) as the evaluation metric. This will help readers understand why this particular metric was chosen.",
                "7. Provide more details on how the grammar-based tree decoder works to ensure the grammatical correctness of each edit action. This will help readers understand the technical aspects of GRAPHIX.",
                "From Agent 3 (Encoder and Decoder, Tree Edit Operations, High Precision Regime for GRAPHIX, Ablation Studies, Implementation Details):",
                "1. Include more detailed statistical analysis of the results to provide a clearer picture of the model's performance. This will help readers understand the effectiveness of GRAPHIX.",
                "2. Strengthen the conclusion by discussing potential future work or improvements to the model. This will give readers an idea of the potential future directions of this research.",
                "3. Provide a more detailed discussion of the limitations of the model for a more balanced view of its capabilities. This will help readers understand the limitations of GRAPHIX."
            ]
        },
        "alignments": {
            "all": [
                {
                    "reference_id": 6,
                    "predicted_id": 7,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The pre-training regime is not particularly novel. The authors suggest deleting elements of subtrees from existing code and reconstructing them, which is not sufficiently novel.",
                    "predicted_comment": "1. Provide a more detailed explanation of how the novel pre-training technique for tree-structured data differs from existing methods. This will help readers understand the novelty of the pre-training technique."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 1,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper does not convincingly demonstrate model novelty or significant improvement to the state-of-the-art on the provided baseline.",
                    "predicted_comment": "1. Provide a more detailed explanation of why current methods are insufficient and how GRAPHIX addresses these shortcomings. This will help readers understand the motivation behind the development of GRAPHIX."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 3,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "Quantitative evaluations show only marginal improvement at best in comparison to the large pre-trained models.",
                    "predicted_comment": "3. Provide a more detailed comparison between other models and GRAPHIX to highlight its unique advantages. This will help readers understand what sets GRAPHIX apart from other models."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 3,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper lacks a comprehensive comparison with prior work, making the contributions over previous work hard to assess.",
                    "predicted_comment": "3. Provide a more detailed comparison between other models and GRAPHIX to highlight its unique advantages. This will help readers understand what sets GRAPHIX apart from other models."
                }
            ]
        },
        "metrics": {
            "all": {
                "n_reference": 27,
                "n_generated": 18,
                "n_total_aligns": 4,
                "recall": 0.13839285714285715,
                "precision": 0.05555555555555555,
                "pseudo_jaccard": 0.04409090909090909,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.42857142857142855,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.13636363636363635,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.125,
                    "precision": 0.05555555555555555,
                    "pseudo_jaccard": 0.04,
                    "n_real": 8
                }
            ]
        }
    },
    {
        "doc_id": "nLb60uXd6Np",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The quantitative results are good, but not revolutionary. The model does well on the crystal classification task, but this setting is described as 'not difficult'. It performs slightly worse than GemNet on molecular force regression. On the backmapping task, it is only compared against a naive transformer, as opposed to another rotation invariant model. Another instance of the proposed model outperforming a strong baseline on a difficult task would significantly improve the case the paper is making.",
                "While one of the advantages of the proposed method is scaling to arbitrary rank, the experiments are all limited to pairwise attention (not counting the current key element). It would be interesting to demonstrate the effect of scaling up the rank, especially as this is noted as a potential advantage of the GemNet baseline.",
                "Group representation-based approaches have not been compared against."
            ],
            [
                "The details of the four functions that make up the attention mechanism are vague. More elaboration on the technical details, such as what are rotation-invariant geometric quantities for tuples, is needed to improve readability.",
                "The paper lacks a mathematical proof of rotational equivariance.",
                "The paper does not provide information on the efficiency of the proposed method, especially when compared with other works.",
                "The paper does not include experiments testing rotation equivariance. It is recommended to have an experimental test for the rotation equivariance by comparing with other works, such as rotation equivariance achieved via data augmentation.",
                "The paper does not clarify whether the proposed method can be extended to larger point clouds. It is also unclear what the definition of 'small point clouds' is. An experiment on 3D point cloud classification, such as the popularly used Modelnet 40, is recommended."
            ],
            [
                "In Table 1, the proposed model performs worse than the baseline GemNet-Q. The authors should consider adjusting their method to incorporate properties such as the use of quadruplets atoms, the incorporation of energy, or a better architecture, in order to perform a fair comparison with this baseline."
            ],
            [
                "The authors do not clearly describe how the geometric product helps to achieve the rotation-invariant attributes. This should be explained in the main manuscript since this is one of the main contributions.",
                "Geometric products do not appear in both model architectures and appendix B. The authors need to explicitly describe how it is implemented.",
                "The interpretation of the linear combination in geometric algebra is not trivial. The paper lacks the interpretation, justification, and discussion about the attention mechanism in geometric algebra.",
                "The proposed architecture is limited to 'small point clouds' without clear explanation. If the statement about the computational complexity is the reason, the connection to it should be clarified.",
                "The authors defined four functions, V, M, J, and S. However, another function R is defined later. Why do the authors distinguish R from the others? Are the four functions cover everything sufficiently? Justification is required.",
                "The authors stated that GemNets can operate on quadruplets of atoms but the proposed network worked only in pairs of bonds from the central atom. If this is the limitation of the proposed network, clear statement and discussion are required. Moreover, a discussion is appreciated whether the proposed network has technical difficulties to include the force label."
            ]
        ],
        "generated_comments": {
            "all": [
                "The abstract provides a clear and concise overview of the paper's content, including the problem being addressed, the proposed solution, and its potential applications. However, it could benefit from a brief mention of the results or findings to give readers a complete picture of the study. Specifically, it would be helpful to include a summary of the performance of the proposed architectures on the sample problems in physics, chemistry, and biology.",
                "The background section does a good job of explaining the concept of geometric deep learning and its applications. However, it could provide more context on why this approach is necessary or advantageous over other deep learning methods. Specifically, a comparison with traditional deep learning methods in terms of performance and applicability would be beneficial.",
                "The paper mentions several applications of geometric deep learning, such as in physics, chemistry, and biology. It would be helpful to provide more specific examples or case studies to illustrate these applications. Including real-world examples where geometric deep learning has been successfully applied would strengthen this section.",
                "The paper discusses the use of geometric algebra and attention mechanisms in the proposed architecture. However, it could provide more explanation or background on these concepts for readers who may not be familiar with them. A brief introduction to geometric algebra and attention mechanisms, along with references for further reading, would be beneficial.",
                "The related work section provides a comprehensive review of existing methods. However, it could benefit from a more explicit comparison of these methods with the proposed architecture, highlighting the unique contributions of this study. A table comparing the features and performance of the proposed architecture with existing methods would be a useful addition.",
                "The paper provides a detailed explanation of the geometric algebra attention scheme and its application in different fields. However, it would be beneficial to include a more simplified explanation or a visual representation for readers who are not familiar with geometric algebra. A diagram illustrating the geometric algebra attention scheme would be a helpful addition.",
                "The paper mentions the use of multilayer perceptrons with a hidden width of 64 units, but it does not provide a justification for this choice. It would be helpful to include a brief explanation or reference to support this decision. A discussion on how the hidden width was chosen and its impact on the performance of the model would be beneficial.",
                "The paper mentions the use of different functions (V, S, R, M, J) in the geometric algebra attention models, but it does not clearly explain how these functions are chosen or optimized. More information on this would be beneficial. A section discussing the selection and optimization of these functions would strengthen the methodology.",
                "The paper provides Python code under the MIT license, which is commendable for reproducibility. However, it would be helpful to include a brief overview or explanation of the code in the main text for readers who may not be familiar with Python or coding in general. A walkthrough of the main parts of the code would be a useful addition.",
                "The paper mentions the use of the freud Python library for finding the nearest neighbors of each particle. It would be helpful to include a brief explanation of why this specific library was chosen and how it contributes to the methodology. A discussion on the advantages of the freud library over other options would be beneficial.",
                "The paper mentions training the models for up to 800 epochs using the adam optimizer, but it does not provide a justification for these choices. It would be helpful to include a brief explanation or reference to support these decisions. A discussion on how the number of epochs and the optimizer were chosen and their impact on the performance of the model would be beneficial.",
                "The paper mentions using the training set error to characterize model performance due to the resolution of the structural refinement algorithms being large compared to the capacity of the models. However, it would be beneficial to include a discussion on the limitations of this approach and how it might affect the overall results. A section discussing the potential biases and errors introduced by this approach would strengthen the paper.",
                "The paper discusses the use of an architecture that generates output distributions for systems at nonzero temperature, but disregards this for simplicity. It would be helpful to provide some insight into how this simplification might impact the results and whether it could potentially introduce any bias. A discussion on the implications of this simplification would be beneficial.",
                "The paper lists several PDB entries used for the coarse-graining task but does not provide any context or explanation as to why these specific entries were chosen. Including this information would strengthen the paper by providing more transparency about the methodology. A section discussing the selection criteria for the PDB entries would be a useful addition.",
                "The paper mentions potential extensions for reducing the set of products to avoid polynomial scaling, but leaves these for future work. It would be beneficial to discuss the potential impact of these extensions on the results. A discussion on how these extensions could improve the performance and scalability of the model would be a useful addition."
            ]
        },
        "alignments": {
            "all": [
                {
                    "reference_id": 4,
                    "predicted_id": 7,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "The authors defined four functions, V, M, J, and S. However, another function R is defined later. Why do the authors distinguish R from the others? Are the four functions cover everything sufficiently? Justification is required.",
                    "predicted_comment": "The paper mentions the use of different functions (V, S, R, M, J) in the geometric algebra attention models, but it does not clearly explain how these functions are chosen or optimized. More information on this would be beneficial. A section discussing the selection and optimization of these functions would strengthen the methodology."
                }
            ]
        },
        "metrics": {
            "all": {
                "n_reference": 15,
                "n_generated": 15,
                "n_total_aligns": 1,
                "recall": 0.041666666666666664,
                "precision": 0.016666666666666666,
                "pseudo_jaccard": 0.0125,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.16666666666666666,
                    "precision": 0.06666666666666667,
                    "pseudo_jaccard": 0.05,
                    "n_real": 6
                }
            ]
        }
    },
    {
        "doc_id": "cVak2hs06z",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The motivation for CnC is centered on the class-conditional alignment loss. There is even a bound on the worst-group loss in terms of the average-group and class-conditional alignment loss. So why not replace step 2, and instead fine-tune the model using L_avg_group + L_alignment? Or even just train models from scratch with this loss. It would be good to compare to these. If CnC is simply more empirically successful than this alternative, then it would be good to see this.",
                "In a related vein to the previous question, how much is CnC buying us as compared to the usual supervised contrastive training? It would be good to see SupCon as a baseline in Table 1. This seems an important baseline, since the main idea of CnC is to pull items from the same class together in feature space, which is also done using SupCon.  The main (even only?) difference is the hard positive/negative sampling approach of CnC."
            ],
            [
                "The observation of aligning representation for improving model robustness is not novel. There are a lot of work with similar ideas in domain generalization/adaptation literature, e.g., [1], [2]. There\u2019s also a recent work [3] that applies contrastive learning for doing so. A more comprehensive discussion for these related work needs to be included.",
                "The assumption of Theorem 3.1 is not well explained and motivated. In particular, the assumption that \u201cthe loss function l(x; y) is 1-Lipschitz in x and bounded from above by one.\u201d seems to be necessary and simplify the proof a lot, but does not hold for typical losses like cross-entropy for classification and MSE for regression.",
                "Using ERM prediction as the group label is not convincing enough, and it is not clear how it would affect the contrastive part. It could be interesting to more extensively analyze how the label prediction affects the improvement given by the contrastive method, probably using a scientific setup where the label prediction is controlled.",
                "For the contrastive part, the current empirical comparison obfuscates the advantage on its own. To decouple it from the effect of wrong group prediction, it is important to compare in the setting where group labels are available, i.e., GDRO vs GDRO + contrastive. Also, there could be a lot of choices of negative selections but only one is used without sufficient explanations, it would be great to include more explanation or compare with some other possible choices as an ablation study.",
                "Though the proposed contrastive method leads to improved worst-group performance, it seems to decrease the average-case performance compared to baselines."
            ],
            [
                "The proof provided in the paper may not be intuitively useful, particularly due to the bound B on the weight matrix. The weight matrix and the representations are learned jointly, and equivalent predictions could be achieved by scaling the weight matrix down and the representations up. Additionally, the Lipschitz and boundedness constraints on the loss functions do not seem to apply in any of the settings explored experimentally.",
                "Section 5.2 is unclear, particularly regarding the role of ERM 'predicting the sensitive attribute'. It is not clear how ERM's predictions of the sensitive attribute play into the CNC algorithm.",
                "Some important training details, such as the clustering-based prediction from the first step ERM model, are buried in the appendix. These details seem crucial to the functioning of the method and should be discussed in the main body, possibly with an ablation study."
            ]
        ],
        "generated_comments": {
            "all": [
                "Please provide a more comprehensive explanation of spurious correlations. Specifically, it would be helpful to understand their definition in the context of this study, why they pose a challenge in machine learning models, and the potential consequences of not addressing them in the model design.",
                "It would be beneficial to provide a more layman-friendly explanation of the CNC mechanism. Specifically, how does it work to improve model robustness and why is it expected to be effective in mitigating the impact of spurious correlations? This could include a step-by-step walkthrough of the process or a simplified analogy.",
                "Could you provide a comparison of the Correct-N-Contrast (CNC) method with other methods such as Empirical Risk Minimization (ERM) and Group Distributionally Robust Optimization (GDRO)? Specifically, it would be helpful to understand the unique features of CNC and why it is expected to provide improved performance over these other methods.",
                "Could you provide more specifics about the experimental setup? This could include the size and nature of the dataset used, the specific task the model was trained to perform, and a comparison of CNC's performance with other methods. This would help readers understand the practical implications and effectiveness of CNC.",
                "It would be helpful to provide a more detailed roadmap of the rest of the paper. This could include a brief summary of each section, the key points or findings to be discussed, and how each part contributes to the overall objective of the study.",
                "Please elaborate on the specific model architectures used in the second stage of the procedure, including the LeNet-5 CNN, ResNet-50 CNN, and BERT model. Additionally, provide more insight into the choice of training hyperparameters such as batch size, temperature, contrastive weight, optimizer, learning rate, momentum, and weight decay. Discuss the results of this stage in relation to the worst-group and average test set results reported in Table 1. Furthermore, could you delve deeper into the process of validating the design choices of CNC and explain why ERM-guided contrastive sampling is crucial for improving worst-group accuracy?"
            ]
        },
        "alignments": {
            "all": [
                {
                    "reference_id": 2,
                    "predicted_id": 5,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "Some important training details, such as the clustering-based prediction from the first step ERM model, are buried in the appendix. These details seem crucial to the functioning of the method and should be discussed in the main body, possibly with an ablation study.",
                    "predicted_comment": "Please elaborate on the specific model architectures used in the second stage of the procedure, including the LeNet-5 CNN, ResNet-50 CNN, and BERT model. Additionally, provide more insight into the choice of training hyperparameters such as batch size, temperature, contrastive weight, optimizer, learning rate, momentum, and weight decay. Discuss the results of this stage in relation to the worst-group and average test set results reported in Table 1. Furthermore, could you delve deeper into the process of validating the design choices of CNC and explain why ERM-guided contrastive sampling is crucial for improving worst-group accuracy?"
                }
            ]
        },
        "metrics": {
            "all": {
                "n_reference": 10,
                "n_generated": 6,
                "n_total_aligns": 1,
                "recall": 0.1111111111111111,
                "precision": 0.05555555555555555,
                "pseudo_jaccard": 0.041666666666666664,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.125,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "2234Pp-9ikZ",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The paper lacks conceptual innovation. While the results are solid, there is little insight about the behavior of the method. No ablations are performed beyond the comparison with NAGO, making it hard to assess the importance of the individual components.",
                "The paper does not include results on ImageNet.",
                "The results are compared with models that use other teacher architectures, making it hard to determine if the improvements are due to an improved teacher or a better knowledge distillation method.",
                "The claim that a standard neural architecture search would produce architectures sampled from the same distributions and therefore not be efficient in exploring the space of students is insufficiently explored.",
                "The paper does not provide comparisons with other architecture search algorithms for knowledge distillation or surrogate functions, therefore the introduction of a new framework may not be warranted under the claims of the authors."
            ],
            [
                "The application of knowledge distillation (KD) into network architecture generator optimization (NAGO) appears to be naively done, suggesting that the proposed method is incremental and the contribution is limited.",
                "The differences between the proposed method and existing methods are not clearly highlighted.",
                "The comparison of results in Table 1 is not fair. The KD used in this paper used better teachers following the convention, but the competitor KD-LSR and SKD in the table are self-distillation methods, making the comparison meaningless.",
                "On the CIFAR100 dataset, the paper inaccurately reports that CRD used ShuffleNetV1, which has more parameters, while the original paper used WRN-40-2 as a teacher and trained the student of WRN-16-2.",
                "On the MIT67 dataset, AutoKD used the fine-tuned teacher, which is much beneficial to KD in terms of performance, while VID used an ImageNet-pretrained model for transfer learning.",
                "On the CIFAR10 dataset, the compared models have fewer parameters than that of NAGO for AutoKD, making it hard to say that AutoKD outperforms them.",
                "The experimental results are unconvincing. For example, as weight is zero in Figure 2, the accuracies in the table should show consistent performance but are deviated with respect to temperature.",
                "The accuracies of NAGO in Figure 4 appear low compared to the other results in the paper, which needs clarification.",
                "The use of KD on NAS introduces additional computational cost, but this is not clearly compared quantitatively."
            ],
            [
                "The paper's main difference from existing KD-NAS approaches is the search for a family of architectures, which mainly takes advantage of the generator in NAGO. This reduces the novelty and contribution of the work.",
                "The purpose of some experiments is unclear. It would be more beneficial to compare AutoKD with earlier KD-NAS approaches instead of NAGO to show the impact of KD on NAS.",
                "The paper does not clearly articulate how the family of student architectures can benefit knowledge distillation. It is suggested that the performance gains may result from the ensemble of networks.",
                "The comparisons shown in Figure 7 do not seem to make much sense.",
                "The logic and results shown in the visualization of Figure 6 are not clear."
            ]
        ],
        "generated_comments": {
            "all": [
                "The introduction could provide a more comprehensive explanation of the challenges associated with deploying large deep learning models on small devices. This should include the memory and computational resource requirements of these models, the inefficiencies of the hardware of target devices in supporting all operations used by state-of-the-art architectures, and the cost implications of large-scale deployment. This will help to underscore the significance of the problem that AutoKD is addressing in the field of deep learning.",
                "The methodology section should include a comparison with other existing solutions such as pruning-based methods, quantization methods, low-rank factorization approaches, and other Knowledge Distillation (KD) methods. This comparison should highlight the advantages of AutoKD over these methods, particularly in terms of its ability to find the optimal student architecture. This will help to position AutoKD within the broader context of model compression techniques and underscore its unique contributions.",
                "The explanation of how AutoKD uses Bayesian Optimization, Neural Architecture Search, and Knowledge Distillation could be more accessible. Consider including a simplified explanation or analogy for readers who are not as familiar with these concepts. This will help to make the paper more inclusive and understandable to a wider audience.",
                "The paper could provide a more detailed explanation of how AutoKD manages to emulate the performance of large models with smaller students. This should include specific examples of how AutoKD uses a fraction of the memory and parameters of the larger models to achieve similar performance. This will help to highlight the efficiency and effectiveness of AutoKD.",
                "The paper should explain why AutoKD, despite outperforming other methods, uses a larger number of parameters for CIFAR10. This should include a discussion of the default networks in the NAGO search space and how AutoKD manages to outperform more advanced KD approaches while requiring fewer parameters. This will help to provide a more balanced view of the performance and efficiency of AutoKD.",
                "The conclusion section should discuss potential limitations of AutoKD and areas for future research. This should include a discussion of how to fully exploit the distribution of the optimal student family and the possibility of fine-tuning the network distribution to obtain an even better performing model. This will help to provide a more comprehensive view of the potential and future direction of AutoKD."
            ]
        },
        "alignments": {
            "all": [
                {
                    "reference_id": 5,
                    "predicted_id": 4,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "On the CIFAR10 dataset, the compared models have fewer parameters than that of NAGO for AutoKD, making it hard to say that AutoKD outperforms them.",
                    "predicted_comment": "The paper should explain why AutoKD, despite outperforming other methods, uses a larger number of parameters for CIFAR10. This should include a discussion of the default networks in the NAGO search space and how AutoKD manages to outperform more advanced KD approaches while requiring fewer parameters. This will help to provide a more balanced view of the performance and efficiency of AutoKD."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 1,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The differences between the proposed method and existing methods are not clearly highlighted.",
                    "predicted_comment": "The methodology section should include a comparison with other existing solutions such as pruning-based methods, quantization methods, low-rank factorization approaches, and other Knowledge Distillation (KD) methods. This comparison should highlight the advantages of AutoKD over these methods, particularly in terms of its ability to find the optimal student architecture. This will help to position AutoKD within the broader context of model compression techniques and underscore its unique contributions."
                }
            ]
        },
        "metrics": {
            "all": {
                "n_reference": 19,
                "n_generated": 6,
                "n_total_aligns": 2,
                "recall": 0.07407407407407407,
                "precision": 0.1111111111111111,
                "pseudo_jaccard": 0.05128205128205129,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.2222222222222222,
                    "precision": 0.3333333333333333,
                    "pseudo_jaccard": 0.15384615384615385,
                    "n_real": 9
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                }
            ]
        }
    },
    {
        "doc_id": "KPEFXR1HdIo",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The authors should report the wall clock time to demonstrate the time performance of the fine-grained model.",
                "The authors should clarify the biggest time step the simulation can deal with, as the current time step is relatively small (0.001s).",
                "The authors should demonstrate how this method can scale to more complex topologies. All the experiments are currently on one square sheet, but it would be beneficial to see if the method can simulate something like a T-shirt or dress, which is more relevant to design and fabrication."
            ],
            [
                "The paper should include a discussion comparing this work and the latest work on differentiable cloth simulation in a mesh-based view, as mentioned in reference [1].",
                "The statement 'Previous research (Liang et al., 2019; Du et al., 2021) suffers from this problem, which unfortunately leads to learning implausible parameter values' is technically inaccurate. Leveraging prior knowledge for physical parameters is now a necessary step for system identification tasks.",
                "The motivation for using a yarn-level simulator is unclear. The experiments are too coarse to show the advantages of more accurate collision handling and detailed presentation, which are the reasons for proposing a yarn-level simulation. A more valid explanation for motivation or a set of better results that are more complex than 25*25 is expected.",
                "The manuscript should clarify why the authors chose the yarn-based method, how they plan to use this simulator, and where they see the simulator fits in. The motivation for using yarn-level, which is originally designed for fine details but applied on over-simplified models, is not convincing."
            ],
            [
                "The authors identify a major shortcoming of previous simulation models, namely the use of discontinuous force terms, and engineer alternative models that approximate the terms from domain literature but have the added benefit of being smooth everywhere.",
                "The paper raises a question about the non-linear stick-slip force term leading to oscillatory behavior, which could potentially lead to free energy being trapped in entropy and therefore thermal loss.",
                "The paper raises a question about why differences between inter/intra parameters (Table 2) and ground truth increase as the grid size increases.",
                "In the Comparison video, it is difficult to spot any significant difference between the different weaves, which is a weakness of the paper.",
                "The paper lacks real-world experiments, and the range of simulated experiments is not very wide, making it difficult to judge the expressivity / gamut of the model.",
                "The paper acknowledges that neither the specific problem nor the general idea of differentiable simulation is new, and it is unclear whether the specific solutions introduced in the paper generalize to other problems."
            ],
            [
                "The paper does not comment on the computational cost and comparisons to baseline methods. It would be beneficial to provide such information in the main paper, especially when compared with non-differentiable simulator [Cirio et al., 2014] and prior work on differentiable cloth simulation [Liang et al., In NeurIPS 2019].",
                "The quantitative results need to be presented in a clearer manner. For example, it would be good to show the material prediction error in percentage in Table 2-4 (rather than the absolute value). It would be beneficial to include the algorithm (refer to Section 3.1 in [Liang et al. 2019]) in the main paper.",
                "It is unclear whether the proposed differentiable simulation is deterministic or not? Whether the material estimation is sensitive to initialization? In this case, please consider reporting the standard deviation for multiple runs of the same setting.",
                "It is unclear what happens if the parameter prior was not provided to the proposed method in the control learning experiment. It would be beneficial to provide such ablation studies and discuss the results in the rebuttal."
            ],
            [
                "The design process of the model is heuristic and lacks analysis or ablation study to justify the design choices made.",
                "The paper does not discuss how the method performs as a function of the magnitude of the forces. Additional experiments should be conducted to illustrate this.",
                "There is a discrepancy between the claim in paragraph-2 of section 4.2.1 and the data presented in Fig-6 and Table-4. The paper claims that the error reduces by around 100 times when the training data increase from 5 frames to 25 frames, but the figures show only a 10-fold reduction.",
                "The experiments focus only on simulated woven cloth, and the models are heavily designed. This raises questions about the generalizability of the proposed method to other materials and weaving structures."
            ]
        ],
        "generated_comments": {
            "all": [
                "Provide a clear explanation of how the new differentiable fabrics model is different from existing models. (Agent 1)",
                "Provide a detailed explanation of how the differentiable forces in the model, such as static friction, damping, and the Stribeck effect, are differentiable and why this is important for the simulation. (Agent 1)",
                "Provide a detailed explanation of the comparison with the most similar work and traditional Bayesian optimization on inverse problems and the results. (Agent 1)",
                "Provide a detailed explanation of the new differentiable yarn-to-yarn friction model and its importance. (Agent 1)",
                "Provide more details on how the new differentiable shear model was validated. (Agent 2)",
                "Provide more information on why Stochastic Gradient Descent and 70 epochs were chosen for training. (Agent 2)",
                "Provide more details on how the model's effectiveness in learning meaningful physical parameters was measured. (Agent 2)",
                "Provide more information on why the reinforcement learning baseline model was chosen for comparison. (Agent 2)",
                "Provide more details on how the method can be readily extended for general composite materials with mesh structures. (Agent 2)",
                "Provide more details about the unique features of the yarn-level differentiable fabric simulator and how it differs from existing fabric simulators. (Agent 3)",
                "Discuss more about the challenges anticipated in pursuing fabric models on other composite materials such as metal meshes in the future. (Agent 3)",
                "Provide more details about why the sheet-level simulator (Narain et al., 2012) was chosen for comparison. (Agent 3)",
                "Provide more details about how the valid ranges for parameter learning are determined. (Agent 3)",
                "Provide more details about why the EoL discretization (Sueda et al., 2011) was chosen, such as the advantages it offers over other discretization methods and how it contributes to the overall performance of the model. (Agent 3)",
                "Provide more details about why Bayesian Optimization was chosen. (Agent 3)",
                "Provide more intuitive explanations or visualizations to help readers understand the complex mathematical equations. (Agent 4)",
                "Clarify how the derivatives of the general mass matrix are used in the overall simulation. (Agent 4)",
                "Provide more details about the comparison with similar work and traditional Bayesian optimization. (Agent 4)",
                "Provide a clear explanation of the mathematical symbols and equations used in the \"General Mass Matrix\" section. (Agent 5)",
                "Explain the significance of the \"General Mass Matrix\" section in the context of the new differentiable fabric simulator. (Agent 5)",
                "Consider adding visual aids, such as diagrams or graphs, to illustrate the differentiable forces and the new differentiable yarn-to-yarn friction model. (Agent 5)",
                "Provide examples or case studies that illustrate how the new differentiable fabric simulator can be applied in real-world situations. (Agent 5)",
                "Provide a glossary or a brief explanation of the complex mathematical equations and terms. (Agent 6)",
                "Provide more justification for the choice of different forces and models. (Agent 6)",
                "Include more information about the comparisons with similar work and traditional Bayesian optimization. (Agent 6)",
                "Provide more information about the new differentiable yarn-to-yarn friction model. (Agent 6)",
                "Provide more details about the collision handling method. (Agent 7)",
                "Provide more information about how the weight matrix and constraint parameters are determined, such as the criteria used to set these parameters and how they affect the simulation results. (Agent 7)",
                "Explain what each term represents in the equations presented in the section on derivatives of the simulator. (Agent 7)",
                "Provide information about the results of the simulations and how the model was validated. (Agent 7)"
            ]
        },
        "alignments": {
            "all": [
                {
                    "reference_id": 2,
                    "predicted_id": 9,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The motivation for using a yarn-level simulator is unclear. The experiments are too coarse to show the advantages of more accurate collision handling and detailed presentation, which are the reasons for proposing a yarn-level simulation. A more valid explanation for motivation or a set of better results that are more complex than 25*25 is expected.",
                    "predicted_comment": "Provide more details about the unique features of the yarn-level differentiable fabric simulator and how it differs from existing fabric simulators. (Agent 3)"
                },
                {
                    "reference_id": 0,
                    "predicted_id": 23,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The design process of the model is heuristic and lacks analysis or ablation study to justify the design choices made.",
                    "predicted_comment": "Provide more justification for the choice of different forces and models. (Agent 6)"
                },
                {
                    "reference_id": 3,
                    "predicted_id": 8,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The experiments focus only on simulated woven cloth, and the models are heavily designed. This raises questions about the generalizability of the proposed method to other materials and weaving structures.",
                    "predicted_comment": "Provide more details on how the method can be readily extended for general composite materials with mesh structures. (Agent 2)"
                }
            ]
        },
        "metrics": {
            "all": {
                "n_reference": 21,
                "n_generated": 30,
                "n_total_aligns": 3,
                "recall": 0.15,
                "precision": 0.02,
                "pseudo_jaccard": 0.018560606060606062,
                "specificity_more": 0.6666666666666666,
                "specificity_less": 0.0,
                "specificity_same": 0.3333333333333333
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.25,
                    "precision": 0.03333333333333333,
                    "pseudo_jaccard": 0.030303030303030304,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.5,
                    "precision": 0.06666666666666667,
                    "pseudo_jaccard": 0.0625,
                    "n_real": 4
                }
            ]
        }
    },
    {
        "doc_id": "HyxLRTVKPH",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The paper does not clearly demonstrate the advantages of using a linear scaling schedule over other schedules. For example, step-based schedules outperform the proposed method in 2 of the 4 tasks in Table 2, and Poly and Cosine schedules perform better in some budgets in Figure 2.",
                "The comparisons in Figure 2 and Table 2 are not convincing without considering the variance of different trials. It is unclear whether the advantage is due to the learning rate schedule or randomness. The paper should report the mean and variance for multiple trials and ideally perform a significance test.",
                "The paper should include other learning rate schedules in Table 2. As shown in Figure 2, step-based schedule is not among the top three schedules for CIFAR-10.",
                "As shown in Table 3, the proposed method requires waiting until the end of training to get the best performing model, while step-based schedule can find the best model around 90% training. This means early stopping is not possible for linear scaling based schedule, which could be useful when the training budget is large enough and evaluation is cheap.",
                "The paper lacks a deeper understanding of why a linear learning rate schedule might work better. The authors attempt to provide an explanation from the relationship between learning rate and gradient magnitudes, but no clear conclusion is given. The paper should investigate when no weight decay is used and whether there is any correlation between the learning rate and gradient norms.",
                "The paper does not specify the learning rate decay unit for the linear schedule. Is it decaying per epoch or per mini-batch? If epoch-based learning rate decay is used, it is essentially step-based learning rate decay with many steps.",
                "The paper should investigate when the linear schedule starts to outperform step-based decay in terms of epochs or iterations. The difference between two schedules for small batch size at the early stage of training might be less in comparison with large batch training, especially when the number of iterations is enough.",
                "The paper only compares different schedules with a single initial learning rate. Different initial learning rates may result in different behaviors. The paper should consider scenarios where the initial learning rate is small, as there might not be much difference for different schedules in such cases.",
                "The authors' rebuttal did not fully address the concern about the lack of early stopping in the proposed method, which could limit its usage, especially for Hyperparameter Optimization (HPO).",
                "The authors' rebuttal did not fully address the concern about the lack of understanding of why the linear scaling schedule is better."
            ],
            [
                "The paper lacks a sound theory explaining why linear decay or other smooth decay methods work well.",
                "The novelty of the methodology proposed by the authors might be limited, as Mishkin et al. have already experimented with linear decay."
            ],
            [
                "The paper makes unclear claims about the phenomena of gradient magnitude vanishing and its effectiveness. For example, in section 5, the authors state 'We call this \u201cvanishing gradient\u201d phenomenon budgeted convergence. This correlation suggests that decaying schedules to near-zero rates (and using BAC) may be more effective than early stopping.' This claim appears to be an overreach as the paper merely shows gradient magnitude decreasing with learning rate.",
                "The paper does not perform extensive comparisons for commonly used algorithms like Adam, which are key motivating use cases for budget-aware training, such as providing approximations for problems like neural architecture search and hyperparameter tuning."
            ]
        ],
        "generated_comments": {
            "all": [
                "Elaborate on why resource-constrained training is a significant issue in machine learning, particularly in the context of hyper-parameter tuning and neural architecture search. Discuss the practical implications of limited computational resources.",
                "Provide more context on how budgeted training fits into the broader landscape of machine learning research. Discuss the key question under this setting: 'given a dataset, algorithm, and fixed resource budget, what is the best achievable performance?'",
                "Include more specific examples of how resource constraints affect practical applications of machine learning. Discuss the implications of these constraints on the performance of machine learning models.",
                "Expand on the discussion of previous work on learning rate schedules. Discuss the properties of different schedules such as the poly schedule, cosine schedule, and htd schedule, and how they compare to the proposed linear schedule.",
                "Discuss the implications of the point that the globally optimal solution may not be achievable in the budgeted setting. Discuss the concept of 'budgeted convergence' and its implications on the effectiveness of decaying schedules to near-zero rates.",
                "Include more detailed explanations or examples of budget-aware schedules. Discuss how these schedules are aware of the budget constraints and how they control the stage of optimization.",
                "Provide more detail on how the linear schedule was derived and why it works better than other schedules. Discuss the properties of the linear schedule that make it outperform other schedules under almost all budgets.",
                "Provide more information on other types of schedules, such as the poly schedule and the cosine schedule, and why they were chosen for comparison. Discuss the properties of these schedules and how they compare to the linear schedule.",
                "Include a more detailed explanation of the experiments conducted, including the specific settings used and why they were chosen. Discuss the number of times each configuration is repeated, the models used, and the validation accuracy obtained.",
                "Explain why the CIFAR-10 dataset and other vision benchmarks were chosen and how they are relevant to the study. Discuss the tasks performed on these datasets and how the results contribute to the study.",
                "Include a more detailed discussion on why certain schedules (like linear or cosine) perform better than others (like step decay) in the context of budgeted training. Discuss the results obtained from the experiments and how they support this conclusion.",
                "Provide more information on why 21 out of 100 random architectures could not be trained. Discuss the implications of this finding on the effectiveness of different learning rate schedules.",
                "Include a more comprehensive conclusion that summarizes the key findings and their implications for the field of neural architecture search. Discuss the implications of the findings on the use of learning rate schedules in budgeted training.",
                "Include a discussion on potential future work, such as how the proposed method could be improved or other learning rate schedules that could be explored. Discuss the potential impact of these improvements or explorations on the field of machine learning."
            ]
        },
        "alignments": {
            "all": [
                {
                    "reference_id": 4,
                    "predicted_id": 6,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper lacks a deeper understanding of why a linear learning rate schedule might work better. The authors attempt to provide an explanation from the relationship between learning rate and gradient magnitudes, but no clear conclusion is given. The paper should investigate when no weight decay is used and whether there is any correlation between the learning rate and gradient norms.",
                    "predicted_comment": "Provide more detail on how the linear schedule was derived and why it works better than other schedules. Discuss the properties of the linear schedule that make it outperform other schedules under almost all budgets."
                },
                {
                    "reference_id": 9,
                    "predicted_id": 6,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The authors' rebuttal did not fully address the concern about the lack of understanding of why the linear scaling schedule is better.",
                    "predicted_comment": "Provide more detail on how the linear schedule was derived and why it works better than other schedules. Discuss the properties of the linear schedule that make it outperform other schedules under almost all budgets."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 6,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper lacks a sound theory explaining why linear decay or other smooth decay methods work well.",
                    "predicted_comment": "Provide more detail on how the linear schedule was derived and why it works better than other schedules. Discuss the properties of the linear schedule that make it outperform other schedules under almost all budgets."
                }
            ]
        },
        "metrics": {
            "all": {
                "n_reference": 14,
                "n_generated": 14,
                "n_total_aligns": 3,
                "recall": 0.2333333333333333,
                "precision": 0.047619047619047616,
                "pseudo_jaccard": 0.044444444444444446,
                "specificity_more": 0.6666666666666666,
                "specificity_less": 0.0,
                "specificity_same": 0.3333333333333333
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.2,
                    "precision": 0.07142857142857142,
                    "pseudo_jaccard": 0.06666666666666667,
                    "n_real": 10
                },
                {
                    "recall": 0.5,
                    "precision": 0.07142857142857142,
                    "pseudo_jaccard": 0.06666666666666667,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                }
            ]
        }
    },
    {
        "doc_id": "giit4HdDNa",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "No theoretical or empirical evidence is presented that confirms that the training speed of N-CODE is significantly lower compared to NODE.",
                "While performance seems generally higher, the limitations of the proposed N-CODE method are not clear.",
                "The claim of better performance in less time is not fully supported by the results."
            ],
            [
                "The supervised classification part of the paper is highly related to the 'Data Controlled Neural ODEs' section in Massaroli et al (2020b), a paper that has been cited in the related work section but not addressed in the main content. The problems tackled and the results are highly related to those of Massaroli et al (2020b). This issue needs to be addressed sufficiently.",
                "The unsupervised section needs more work. The experiments and tables can be described more effectively. For example, it would be preferable to clearly explain which row section 5.3 refers to in Figure 7, and which row corresponds to section 5.4.",
                "The paper claims that the replacement of a linear layer with a Neural CODE improves image reconstruction quality. This should mean that all layers in the encoder can be replaced to give better encoding, taking care of dimensionality (such as in Normalizing Flows). However, this paper uses a typical neural network for image reconstruction and generation. This suggests that the majority of heavy-lifting is done by the decoder, hence the Neural CODE is more amenable to warping the latent space suitable for the decoder. For image generation as well, the latent space has been designed so that the decoder can produce nice images, this is not necessarily a win for the Neural CODE.",
                "To make the case for Neural CODE, especially for images, higher resolution images need to be tackled, since it is in higher dimensions that the success of the advancements in the methods listed in Figure 7 lies."
            ],
            [
                "The authors should investigate how the model performance changes as f grows. Learning the parameters of a neural network via another neural network(s) is a very challenging problem, and it would be beneficial to see this verified or not.",
                "The connection with control theory can be made clearer. There is very little reference to Pontryagin's maximum principle and the link is not visible.",
                "The authors should test vanilla NODE on experiments 5.3 and 5.4. It would be interesting to see if the improvement of N-CODE is significant when NODE is latent."
            ],
            [
                "The paper is closely related to other works that have also focused on making the weights of neural ODEs time dependent. A more thorough discussion of the differences between these models is needed to better understand where the novelty/contribution comes in.",
                "The closed loop model, which is the main innovation, is thoroughly discussed in the paper. However, most experiments are performed with the open loop model, which is very similar to other models already proposed in the literature. This puts into question whether the full closed loop model is actually useful in practice.",
                "There is no evaluation of the number of functions evaluations in the paper. According to the abstract, the model trains faster so it seems important to include this information in the paper.",
                "The authors claim that the model achieves state of the art reconstruction on CIFAR10. However, reconstruction doesn\u2019t really make sense as a task (since the identity function would be optimal in this case). This claim is questionable and should be removed."
            ]
        ],
        "generated_comments": {
            "all": [
                "The paper presents an original modulation system for continuous-time neural feature transformations that theoretically relates to optimal control, which is a significant contribution to the field. However, the paper could benefit from a more detailed discussion on the practical implications of this system. Specifically, it would be helpful to discuss how the dynamic shaping of the transformation module, achieved by augmenting the network with a trained control inference mechanism, improves the representational power of autoencoders. Additionally, the paper could explore the potential of this system in areas such as recurrent neural networks and robotics.",
                "The paper mentions that the system can be applied in the context of unsupervised image representation learning. It would be helpful to provide more concrete examples or case studies to illustrate this application.",
                "In the conclusion section, the paper mentions several avenues for future research. It would be beneficial to elaborate on these points to give readers a clearer understanding of the potential future developments in this field. Specifically, the authors could provide more details on how they plan to investigate the robustness and generalization properties of the proposed modulation system, explore its similarities with fast-synaptic modulation systems observed in neuroscience, and test its application in natural applications such as recurrent neural networks and robotics. Additionally, the authors could discuss the potential connection between their system and the theory of bifurcations in dynamical systems and neuroscience.",
                "The introduction does a good job of setting up the problem and the limitations of existing solutions. However, it could benefit from a clearer explanation of the proposed solution, N-CODE. The current explanation is quite technical and may be difficult for readers unfamiliar with the topic to understand. It would be helpful to provide a more layman-friendly explanation of how N-CODE increases the expressivity of continuous-time neural nets by using tools from control theory, how it learns a family of vector fields parameterized by data, and how the transformation of the input space is no longer constrained to be a homeomorphism, allowing the system to easily 'tear' apart the two annulus classes without directly lifting the data space to a higher dimension.",
                "The methodology section provides a detailed explanation of N-CODE and how it improves upon NODEs. However, it could benefit from more visual aids to help illustrate the concepts being discussed. For instance, a diagram showing the structure of N-CODE, including the mapping of the input space to control weights and how these interact with neural activity to steer model dynamics, could be beneficial. A comparison of the single, time-varying vector field learned by previous continuous-time methods and the family of vector fields learned by N-CODE could also be included. Visual representations of the transformation of the input space, showing how it is no longer constrained to be a homeomorphism with N-CODE, would be helpful. Additionally, diagrams illustrating the architecture of the encoder and decoder, as well as the flow of data through the system, and graphs showing the results of the experiments, such as the decrease in the Frechet Inception Distance (FID) with increasing number of components in the mixture, could be added.",
                "The section on open and closed-loop controllers is a bit confusing. It might be helpful to provide more context or examples to help readers understand these concepts. For instance, the paper could explain how in open-loop control, the mapping \u03b3 in equation 4 is used as a controller, mapping the input space X to \u0398 so that f is conditioned on x0 but not necessarily on x(t) for t > 0, allowing each initial value x0 to evolve according to its own learned flow. This allows for particle trajectories to evolve more freely than within a single flow that must account for the whole data-distribution. In contrast, in closed-loop control, a differentiable mapping g outputs the time-dependent control weights \u03b8(t) given the state of the variable x(t), resulting in a module describing a time-varying transformation \u03a6\u03b8(t)(x0, t). The presence of closed-loop feedback might result in interesting dynamical regimes such as stability, chaos, etc. More examples or diagrams illustrating these concepts could be beneficial.",
                "The training section is well-written and provides a clear explanation of how N-CODE is trained. However, it might be helpful to provide more information on how the loss function was chosen and why it is suitable for this task. For instance, the paper could explain how the generalized loss function integrates a cost over some interval [0, T], allowing for a more comprehensive evaluation of the model's performance over the entire trajectory, not just the final state of the system. This loss formulation is more general than in previous works as it can be any Lebesgue-measurable function, encompassing discrete time point penalization or regularization of activations or weights over the whole trajectory. Additionally, the paper could discuss why the parameters of the encoder and decoder module were trained for minimizing the mean-squared error (MSE) on CIFAR-10 and CelebA datasets, or alternatively the Kullback-Leibler divergence between the data distribution and the output of the decoder for the MNIST dataset. The choice of loss function is likely due to the nature of the task, which involves image autoencoding and generation. MSE is a common loss function for regression problems, and it measures the average squared difference between the estimated values and the actual value. On the other hand, the Kullback-Leibler divergence measures how one probability distribution diverges from a second, expected probability distribution, which is suitable for tasks involving probability distributions such as in the case of the MNIST dataset."
            ]
        },
        "alignments": {
            "all": []
        },
        "metrics": {
            "all": {
                "n_reference": 14,
                "n_generated": 7,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                }
            ]
        }
    },
    {
        "doc_id": "b7ZRqEFXdQ",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The paper lacks novelty as it combines existing procedures such as relativistic discriminator, Gumbel-Softmax approximation for categorical distribution, and features matching to improve upon vanilla sequence generation networks.",
                "The generated sentences by the GAN, especially the lengthy ones, are not semantically meaningful as illustrated in Table 8.",
                "The Mean Distance Alignment (MDA) and the Mean Square alignment (MSA) used for features distribution alignment encode the same matching up to a power 2, and it's unclear why they lead to different empirical results.",
                "The paper should consider aligning the overall distributions of the latent representations by considering metrics such as MMD or Wasserstein distance.",
                "The paper should clarify earlier that the used features are extracted from the discrimination network and from which layer of the discrimination network the features are extracted.",
                "The findings of human evaluation are not unequivocal as MSA and MDA achieve higher scores than the real sentences, and the best model with MSA is not preferred due to a lack of diversity and quality."
            ],
            [
                "The authors claim that the use of Gumbel-softmax in GAN training is under-explored, but this is not clear. There are other methods using Gumbel-softmax, such as Gu et al. 2019, and similar softmax with temperature annealing. The authors need to clearly distinguish between using Gumbel-softmax and other smoothed softmax methods.",
                "The authors missed some related work, such as DialogWAE (Gu et al. 2019) and ARAML (Ke et al. 2019). In particular, DialogWAE uses GAN and Gumbel-softmax for text generation, even though it focuses on dialog generation.",
                "To verify mode collapse issues, the authors should consider using Self-BLUE in addition to BLUE scores as a metric to evaluate the diversity.",
                "The novelty of the paper might be incremental. It seems that the novelty comes from using feature statistics alignment. To emphasize the contribution of feature statistics, the authors should consider comparing between the latent feature visualization with and without FSA, in addition to the ablation study."
            ],
            [
                "Although the proposed model is thoroughly evaluated and empirically effective, it is not very different from existing methods, except for the Feature Statistics Alignment (FSA). The application of FSA in this context might be novel; however, the proposed approach seems to be a simple combination of two existing approaches. Therefore, the novelty of the model is limited."
            ],
            [
                "In Table 2, the LSTM model gets NLL lower than the real data, which is a clear evidence of overfitting.",
                "In Table 2, the NLL_{gen} metric, used in SAL (Zhou et. al, 2020) to evaluate the diversity of the generator, is missing without explanation.",
                "In Table 3, the BLEU metric used is the BLEU(F) metric from SAL (Zhou et. al, 2020), which evaluates the generated sentences using the test set as a reference and thus cannot show the diversity of examples.",
                "In Table 3, the BLEU(B) metric from SAL (Zhou et. al, 2020) is missing. This metric evaluates the test set using the generated sentences as a reference, so it can detect mode collapse of a generative model.",
                "The evaluation metrics used in the paper prefer overfitted models, i.e., if the model can remember one example from the training set and repeat that sentence, it can get a very high score.",
                "The authors need to provide more evidence on the diversity of the generated sentences, for example by showing the NLL_{gen} metric on the synthetic dataset, and BLEU(B) metric on real datasets."
            ]
        ],
        "generated_comments": {
            "all": [
                "While the paper does discuss the challenges faced by GANs, it would be beneficial to provide more context on why these challenges are significant and how they impact the performance of GANs. This would help readers understand the importance of the research.",
                "The paper provides an explanation of how FSA works, but it would be helpful to provide a more detailed explanation, perhaps with diagrams or examples, to help readers better understand this concept.",
                "The paper discusses previous approaches to addressing the challenges faced by GANs, but it would be useful to provide more detail on why these approaches were not sufficient. This could include specific examples or case studies.",
                "The paper provides some details on how the Feature Statistics Alignment and Gumbel-Softmax relaxation techniques are implemented in the model, but it would be helpful to provide more details, perhaps with diagrams or examples, to help readers better understand these techniques.",
                "The paper provides some details on the training procedure, specifically how the Gumbel-Softmax temperature and batch size were fine-tuned, but it would be helpful to provide more details, perhaps with diagrams or examples, to help readers better understand this process.",
                "The paper discusses why certain approaches were unsuccessful, but it would be useful to provide more details on how these unsuccessful approaches affected the overall performance of the model. This could include specific examples or case studies.",
                "Revise and restructure the results and conclusion section to improve its clarity and coherence."
            ]
        },
        "alignments": {
            "all": []
        },
        "metrics": {
            "all": {
                "n_reference": 17,
                "n_generated": 7,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                }
            ]
        }
    },
    {
        "doc_id": "zCu1BZYCueE",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The experiments focus only on selecting a single initial learning rate, which is less interesting because normally a learning rate scheduler that changes over time is used. The experiment does not consider different types of learning rate schedulers for comparisons.",
                "The learning rate is just a scalar, so a simple grid search could be used to coarsely find a good learning rate and then refine it. This makes the focus on learning rate less impactful.",
                "The work would be more impactful if it demonstrated improvement in other hyper-parameter optimizations such as weight decay and convolution filter size and channels."
            ],
            [
                "The paper lacks comparisons to baseline hyperparameter tuning setups. For example, in Figure 3, the best final validation and/or test accuracies achieved by autoHyper and by random search for the same number of trials are not shown.",
                "The paper does not compare the proposed algorithm to existing state-of-the-art methods, which would be useful for a solid paper.",
                "The suggested initial learning rates (LRs) seem problematic to compare to. It would be useful to highlight how the suggested initial LRs were tuned.",
                "The suggested initial LRs were tuned using less trials than the proposed algorithm, and using a grid search algorithm that may be worse than random search.",
                "The suggested initial LRs were for a different model than the models considered in the experiments here, which means they could serve as a poor baseline.",
                "The ranges of values tuned over in Wilson 2017 seem to be quite close to several of the values proposed by autoHyper, meaning that perhaps repeating the same grid search could be competitive.",
                "In the cases where the autoHyper values are not close, they are sometimes outside the range considered by Wilson 2017, which could bias the results towards the proposed algorithm.",
                "Several recent works have shown that when tuning optimizers one needs to be careful to report the ranges used, as changing the hyperparameter ranges can drastically affect experimental results.",
                "Figure 1a would be much more informative if you showed the entire training trajectory, including past the first 5 epochs, to see if the selected learning rates actually generalize noticeably better.",
                "In your conclusion you discuss that you could extend your tuning algorithm to multiple hyperparameters, and I believe in order to truly demonstrate its capabilities this would be required."
            ],
            [
                "The algorithm has an implicit assumption that the response function is monotonously decreasing. The paper should clarify what happens if the response function does not decrease, and how the range of the search area is chosen in such cases.",
                "The algorithm introduces new hyperparameters, such as alpha and the 'rate of change plateaus'. The paper should explain how these hyperparameters are chosen and how they influence the result.",
                "The paper should provide a reasonable explanation for why the initial learning rate influences the testing accuracy, not just the convergence rate for training loss.",
                "The paper only compares the algorithm with the baseline. The authors should discuss if there are any other works about tuning the initial learning rate, or if this paper is the first of its kind.",
                "The reviewer suggests that it may be more convincing if the whole parameters are chosen together, not just the initial learning rate."
            ]
        ],
        "generated_comments": {
            "all": [
                "The introduction could benefit from a more detailed explanation of the current methodologies and their limitations in Hyper-Parameter Optimization (HPO) for Deep Neural Network (DNN) training. This would provide a clearer context for the study and help readers understand the motivation behind the proposed method. (Agent 1)",
                "The methodology section introduces a new response surface model based on low-rank factorization of convolutional weights in a CNN. A more simplified explanation or analogy could help readers without a strong background in the field to understand the concept. Additionally, providing a comparison with existing models could highlight the unique aspects of the proposed model. (Agent 1)",
                "A more detailed discussion on the choice of grid space and rate of change functions in the autoHyper algorithm, and how they affect the final generated learning rate, would be beneficial. This could include an explanation of how different choices might impact the performance of the algorithm. (Agent 1)",
                "Including a discussion on the limitations of the proposed method, autoHyper, would provide a more balanced view of the work. This could include potential issues with scalability, computational cost, or applicability to different types of datasets or network architectures. (Agent 2)",
                "A brief discussion on how the method's susceptibility to random initialization variations due to only trialing learning rates once could impact the results and any potential ways to mitigate this issue would be helpful. This could include a discussion of potential strategies for multiple trials or different initialization methods. (Agent 2)",
                "A clearer explanation of the results in the conclusion section, specifically the impact of the proposed method on the performance of various network architectures and optimizers, could be summarized more succinctly. This could include a summary table or graph showing the performance improvement for each combination of architecture, optimizer, and dataset. (Agent 2)"
            ]
        },
        "alignments": {
            "all": [
                {
                    "reference_id": 1,
                    "predicted_id": 2,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "The algorithm introduces new hyperparameters, such as alpha and the 'rate of change plateaus'. The paper should explain how these hyperparameters are chosen and how they influence the result.",
                    "predicted_comment": "A more detailed discussion on the choice of grid space and rate of change functions in the autoHyper algorithm, and how they affect the final generated learning rate, would be beneficial. This could include an explanation of how different choices might impact the performance of the algorithm. (Agent 1)"
                },
                {
                    "reference_id": 3,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper only compares the algorithm with the baseline. The authors should discuss if there are any other works about tuning the initial learning rate, or if this paper is the first of its kind.",
                    "predicted_comment": "The introduction could benefit from a more detailed explanation of the current methodologies and their limitations in Hyper-Parameter Optimization (HPO) for Deep Neural Network (DNN) training. This would provide a clearer context for the study and help readers understand the motivation behind the proposed method. (Agent 1)"
                }
            ]
        },
        "metrics": {
            "all": {
                "n_reference": 18,
                "n_generated": 6,
                "n_total_aligns": 2,
                "recall": 0.13333333333333333,
                "precision": 0.1111111111111111,
                "pseudo_jaccard": 0.07407407407407407,
                "specificity_more": 0.5,
                "specificity_less": 0.0,
                "specificity_same": 0.5
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 10
                },
                {
                    "recall": 0.4,
                    "precision": 0.3333333333333333,
                    "pseudo_jaccard": 0.2222222222222222,
                    "n_real": 5
                }
            ]
        }
    },
    {
        "doc_id": "rrWeE9ZDw_",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The paper claims that the learned object-centric abstraction can be transferred to a new task with the same object type, but it does not define how 'same' is with respect to the same object type in the new task. For example, an apple on a table and an apple on a tree are semantically the same but have different preconditions for the 'pick' action. This suggests that the object-centric abstractions are still more task-specific, even for semantically identical objects.",
                "The paper suggests using Problem-Specific Instantiation to solve the problem of same-type objects having different preconditions in different scenarios. However, this process is similar to manually adjusting the planning domain to adapt to a different scenario, which does not solve the fundamental limitation of the human-defined planning domain.",
                "The learned symbolic predicates of specific object types are not general enough to be applied across different tasks. The proposed object-centric abstraction only groups similar conditions it observed, but does not abstract the preconditions of why an action could be performed. For example, if an apple is placed at different heights, the proposed method will lead to different apple object types as they have different preconditions. If the apple is placed even higher--a condition out of the previously seen cases, the proposed method cannot solve this problem as it does not abstract the 'reachable' concept for the picking action."
            ],
            [
                "The paper claims that the proposed method results in being able to obtain a successful agent for long-term planning with considerably fewer environment interactions, but does not provide a quantitative evaluation against a baseline to demonstrate this reduction in environment interactions.",
                "The paper's comparison with prior works like Kaiser et al. (2020) and Hafner et al. (2021) may not be fair, as the presented method assumes that environment observations are pre-factored and pre-processed into object components, which is a non-trivial task that might explain the sample inefficiency of earlier methods.",
                "The paper should either significantly reduce the prominence of the sample efficiency claims or provide a fair experimental comparison against a baseline that has access to the same factored, pre-processed observations and abstract options as the presented technique."
            ],
            [
                "The paper should clarify why a grouping based on effect distributions alone is preferred over one that also considers the pre-condition.",
                "The paper should provide more details on how it is determined what objects/operators require grounding.",
                "The paper should explain how the clustering proceeds for X, including how X is partitioned and how the result from clustering X is connected to clustering S.",
                "The paper should clarify how much domain knowledge is injected in the method for integrating task-specific information in the learned representations.",
                "The paper should provide more details on the steps needed to achieve the result in the Craft environment experiment, including how the need for having identities is identified, how it is determined which objects require ids, and how the id is connected to the object.",
                "The paper should discuss the limitations of the approach, including the assumptions made such as the frame assumption, that options only affect the object they act upon, and that the world comes pre-segmented according to objects.",
                "The paper should clarify how the grounding is achieved and what knowledge is assumed to arrive at the presented results."
            ]
        ],
        "generated_comments": {
            "all": []
        },
        "alignments": {
            "all": []
        },
        "metrics": {
            "all": {
                "n_reference": 13,
                "n_generated": 0,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                }
            ]
        }
    },
    {
        "doc_id": "EG5Pgd7-MY",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The contributions of the paper are not clearly stated in the introduction section, making it difficult to identify the main contributions.",
                "The proposed framework, which connects membership inference attacks with binary hypothesis testing, lacks novelty as it has been previously proposed in other works.",
                "The paper heavily relies on the assumption introduced in equation (1) to characterize the outcome of the algorithm obtained via SGD training, but no valid proof or formal justification for this assumption is provided.",
                "The paper lacks numerical results and comparisons with competing methods for building attacks in the literature.",
                "The tradeoffs between Type I and Type II error are not always reported.",
                "There is a lack of diverse datasets and relevant architectures to validate the results."
            ],
            [
                "The paper does not clearly explain how having multiple specific forms of attacks can provide a comprehensive picture of whether an algorithm is private or not.",
                "The paper does not provide any justification as to why their set of attacks are 'complete'.",
                "The paper's plans are cryptic and each page has many ambiguous sentences with unclear goals.",
                "The paper does not clearly justify why they pick certain criteria and focus on them.",
                "The paper's definition of privacy is an 'average case, computational, indistinguishability-based' variation of DP, which is not standard DP.",
                "The first attack writes probabilities P(theta|D) as if we are aware of the learning algorithm, which is an assumption that needs to be clarified.",
                "The paper's assertion that 'The population data used for constructing the attack algorithm, and evaluating the inference game, need to be similar, in distribution, to the training data' needs proof.",
                "The paper's claim that 'By violating this principle, we might overestimate the privacy loss' is incorrect as the whole point of DP is that the adversary might have arbitrary auxiliary information.",
                "The paper's statement 'The adversary knows the underlying data distribution' is unclear.",
                "The paper's use of pi(z) in Equation (4) is questionable as z should be independent of theta and D in that case.",
                "The paper does not specify the learner in the experiments of the main body.",
                "The paper needs a much more detailed and justified approach, arguing for 'complete' attacks that at least 'capture known attack techniques so far'.",
                "The paper needs to be much clearer in its criteria that it proposes (in addition to the success rate of the attacks) to be part of the 'report' on privacy."
            ],
            [
                "Many of the attacks proposed could still be inefficient for auditing purposes as one might need to train 1000 reference models to perform the attacks. The numbers are also only high for models that are extremely overfitting. The paper should discuss how to audit models that are well-generalized.",
                "The comparison and difference between this work and prior work is vaguely discussed. There is no detailed comparison with [1], which also formalized MIA with an indistinguishability based framework. The hypothesis testing framework in this work is very similar to the threshold adversary in [1]. The authors should discuss the differences in detail. There is also no comparison to white-box models (e.g. [2]). The authors should discuss how this compares to the proposed framework.",
                "No connection is made between this work and differential privacy, although the indistinguishability setup is highly related. The authors should discuss how the framework translates to privacy loss in DP."
            ]
        ],
        "generated_comments": {
            "all": [
                "From Agent 1:\n1. Could the authors expand on the concept of membership inference attacks in the introduction? Specifically, it would be helpful to understand their implications for privacy in more detail.\n2. In the methodology section, could the authors provide more detailed explanations or examples of the new framework for understanding the relationship between the success of membership inference attacks and information leakage from machine learning models?\n3. Could the authors provide more details about the approximations derived from the same hypothesis test formulation and explain why they were chosen?\n4. It would be beneficial if the authors could discuss the limitations of their methodology.",
                "From Agent 2:\n1. Could the authors provide more detailed explanations for the equations used in the attacks? This would help readers understand the mathematical basis of the attacks.\n2. The results section could be enhanced with more visual aids, such as graphs or charts, to help illustrate the results of the attacks.\n3. A more detailed comparison of the different attacks would be beneficial. This could include a discussion of the strengths and weaknesses of each attack.",
                "From Agent 3:\n1. It would be helpful if the authors could provide more context on how their findings compare to previous research in the field. This could include a discussion of how their results build on or differ from previous studies.\n2. The discussion could benefit from a more detailed explanation of the implications of the results. This could include a discussion of the potential impact of the results on the field of privacy in machine learning.\n3. The authors should consider discussing potential limitations of their study and how these might be addressed in future research.",
                "From Agent 4:\n1. The conclusion could benefit from a more detailed summary of the key findings from the empirical analysis of membership inference attacks on differentially private algorithms.\n2. The authors could provide a brief description of the external resources linked in the paper and how they relate to the study.\n3. The authors could elaborate more on the simulation of the DP adversary and its benefits in measuring leakage through the model in the conclusion."
            ]
        },
        "alignments": {
            "all": [
                {
                    "reference_id": 3,
                    "predicted_id": 1,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper lacks numerical results and comparisons with competing methods for building attacks in the literature.",
                    "predicted_comment": "From Agent 2:\n1. Could the authors provide more detailed explanations for the equations used in the attacks? This would help readers understand the mathematical basis of the attacks.\n2. The results section could be enhanced with more visual aids, such as graphs or charts, to help illustrate the results of the attacks.\n3. A more detailed comparison of the different attacks would be beneficial. This could include a discussion of the strengths and weaknesses of each attack."
                }
            ]
        },
        "metrics": {
            "all": {
                "n_reference": 22,
                "n_generated": 4,
                "n_total_aligns": 1,
                "recall": 0.05555555555555555,
                "precision": 0.08333333333333333,
                "pseudo_jaccard": 0.037037037037037035,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.16666666666666666,
                    "precision": 0.25,
                    "pseudo_jaccard": 0.1111111111111111,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 13
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "DILxQP08O3B",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The paper doesn't fully address why DETR performs better than FasterRCNN features. It's not clear why DETR is doing better than Faster-RCNN especially when the detection accuracy of DETR is also better than Faster RCNN.",
                "The method for obtaining the appearance features from Faster RCNN based method is not clear. It's not explained how it's different from the approach taken by Bottom-Up, Top-Down paper in which 2048-dim appearance features are extracted for each visual region.",
                "The experimental setup isn't fully reflective of the object goal navigation task. The experiments are conducted in AI2 thor scenes which only contain one room. It's not clear how this method will perform when evaluated on significantly more complicated environments like Matterport / Gibson.",
                "The navigation task is simplified by discretizing into a grid. Single room environments and discrete grids simplify a lot of navigation-related challenges and the authors don't discuss how the proposed architecture will generalize to more complex object navigation tasks.",
                "The use of spatial embeddings as well as appearance embedding isn't all that surprising. Existing work including Du et al. uses bounding box coordinates to help learn spatial associations between objects.",
                "It's not clear how the agent learns long-term associations between objects across multiple frames. The proposed architecture puts all the burden of learning these long-term object relationships across multiple frames on the LSTM policy since the VT only learns association within a single frame."
            ],
            [
                "The paper adopts the Transformer and adapts it for the navigation problem, but does not propose a new architecture or model.",
                "A similar usage of the Transformer has already been demonstrated in the vision-and-language navigation task. The referenced paper also shows that pre-training of navigation tasks using Transformers can help to boost performance."
            ],
            [
                "The writing in the approach section isn't very clear. It would be better to define clear notations for all the features/descriptors and use such notations in the figure. The current writing uses various terms like 'instance feature', 'global feature', 'positional-global', 'spatial feature', 'spatial-enhanced', which are confusing.",
                "The details in Fig.2 are not sufficient and it is not as informative as the detailed version (Fig.4) in Appendix. These two figures are not consistent, for example, the 'add' symbol for positional enhancement is missing in Fig.4.",
                "In Sec.4.2, it is not clear how the reduction of the channel dimension of a high-level activation map from D to a smaller dimension d is done.",
                "The terms $h$, 'w', and $L$ are used inconsistently in the paper, causing confusion.",
                "The statement in Sec.4.1 about Faster RCNN being blamed for scale-sensitive is not fully convincing. It is unclear what scale-sensitive means here and why DETR doesn't suffer from it.",
                "The statement about 'early stopping' in Sec.4.4 is not fully convincing. It is unclear why the transformer-based representation learner suffers from 'early stopping'.",
                "The SPL formulation in Sec.5.1 seems to be incorrect. The success indicator seems to be missing.",
                "It is unclear why global features are not also added into the transformer encoder."
            ],
            [
                "The introduction of the paper is difficult to understand as it discusses details such as the difference between DETR and Faster RCNN or difficulty of training the transformers without providing a high-level overview of the proposed method.",
                "The paper does not provide a satisfactory explanation for why nothing is learned without the imitation pre-training.",
                "The paper should run the method on other frameworks which include slightly larger scenes, such as RoboTHOR, to test if the method generalizes to those as well."
            ]
        ],
        "generated_comments": {
            "all": [
                "The introduction could benefit from a more detailed explanation of how the Visual Transformer Network (VTNet) differs from previous methods. Specifically, it would be helpful to highlight how VTNet exploits relationships among all object instances in a scene and emphasizes the spatial locations of objects and image regions, which are crucial for learning directional navigation signals. This would provide readers with a clearer understanding of the unique aspects of VTNet.",
                "The methodology section might benefit from more visual aids or diagrams to help readers understand the spatial-enhanced local descriptor and the positional global descriptor.",
                "Provide more justification for why the pre-training scheme for the VT is necessary and how it improves the performance of the VT. Specifically, it would be helpful to explain how the pre-training scheme addresses the difficulty of training deep Visual Transformers, especially when the supervision signals are weak. This could include a discussion of how the pre-training scheme associates the visual representations with navigation signals, thus facilitating navigation policy learning.",
                "Provide more context on why the specific methods were chosen for comparison with the proposed method (VTNet). Specifically, it would be helpful to explain that these methods represent a range of different approaches to visual navigation and were chosen to provide a comprehensive comparison of VTNet's performance against both traditional and state-of-the-art methods.",
                "The conclusion section could benefit from a more detailed discussion on the limitations of the proposed method and potential future work.",
                "Provide more information on how the threshold distance for VTNet to reach targets was determined and why it was set at 1.5m.",
                "Provide more context on why certain visual transformer architectures were chosen for assessment and how these choices impact the results. Specifically, it would be helpful to explain that the choice of architectures was driven by a need to balance complexity and performance, and to discuss the impact of varying the number of encoder and decoder layers in the transformer on the performance of VTNet."
            ]
        },
        "alignments": {
            "all": [
                {
                    "reference_id": 1,
                    "predicted_id": 2,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "A similar usage of the Transformer has already been demonstrated in the vision-and-language navigation task. The referenced paper also shows that pre-training of navigation tasks using Transformers can help to boost performance.",
                    "predicted_comment": "Provide more justification for why the pre-training scheme for the VT is necessary and how it improves the performance of the VT. Specifically, it would be helpful to explain how the pre-training scheme addresses the difficulty of training deep Visual Transformers, especially when the supervision signals are weak. This could include a discussion of how the pre-training scheme associates the visual representations with navigation signals, thus facilitating navigation policy learning."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "The introduction of the paper is difficult to understand as it discusses details such as the difference between DETR and Faster RCNN or difficulty of training the transformers without providing a high-level overview of the proposed method.",
                    "predicted_comment": "The introduction could benefit from a more detailed explanation of how the Visual Transformer Network (VTNet) differs from previous methods. Specifically, it would be helpful to highlight how VTNet exploits relationships among all object instances in a scene and emphasizes the spatial locations of objects and image regions, which are crucial for learning directional navigation signals. This would provide readers with a clearer understanding of the unique aspects of VTNet."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 2,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper does not provide a satisfactory explanation for why nothing is learned without the imitation pre-training.",
                    "predicted_comment": "Provide more justification for why the pre-training scheme for the VT is necessary and how it improves the performance of the VT. Specifically, it would be helpful to explain how the pre-training scheme addresses the difficulty of training deep Visual Transformers, especially when the supervision signals are weak. This could include a discussion of how the pre-training scheme associates the visual representations with navigation signals, thus facilitating navigation policy learning."
                }
            ]
        },
        "metrics": {
            "all": {
                "n_reference": 19,
                "n_generated": 7,
                "n_total_aligns": 3,
                "recall": 0.29166666666666663,
                "precision": 0.10714285714285714,
                "pseudo_jaccard": 0.09375,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.5,
                    "precision": 0.14285714285714285,
                    "pseudo_jaccard": 0.125,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                },
                {
                    "recall": 0.6666666666666666,
                    "precision": 0.2857142857142857,
                    "pseudo_jaccard": 0.25,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "fmOOI2a3tQP",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The paper does not clearly demonstrate if any insight is gained from the theoretical analysis, as the derivation of value and sample complexity bounds for approximate bisimulation was already performed in Gelada et al. 2019.",
                "The empirical gains are modest and it's unclear if they are due to the image representation learning component of the loss or other aspects of the method.",
                "The paper uses a very small number of training tasks - only 4. It's unclear if using more training tasks would improve the generalization of single-task methods like DeepMDP.",
                "There seems to be a missing baseline - a generic multi-task algorithm that conditions on the environment ID but does use the bisimulation loss to help process the image observations.",
                "It's unclear why there's a gap between your method in the multi-task experiments but not in the meta-RL ones.",
                "It's not explained why HiP-BMDP handles the sticky observation setting better than the baselines.",
                "The paper does not convincingly demonstrate that the structural assumptions on the MDP used by the proposed algorithm yield performance improvement."
            ],
            []
        ],
        "generated_comments": {
            "all": [
                "The paper frequently uses technical terms and complex equations, particularly in sections B.1 and B.2, which may be difficult for readers without a strong background in the field to understand. It would be beneficial to include a glossary of key terms, simpler explanations of the model and its components, and more accessible explanations or examples for the mathematical proofs and equations.",
                "The paper could benefit from a more detailed discussion of the results. For instance, it would be helpful to understand why HiP-BMDP outperforms other baselines in all environments during training time. The paper suggests that the success of HiP-BMDP over other baselines cannot be attributed to task embeddings alone, but it does not provide a detailed explanation for this.",
                "The paper mentions that the performance of all models deteriorates when evaluated on interpolation/extrapolation environments. It would be beneficial to delve deeper into the reasons behind this observation. The paper mentions that the gap between the HiP-BMDP model and other baselines widens in these environments, indicating that the proposed approach is relatively more robust to changes in environment dynamics, but it does not provide a detailed explanation for this.",
                "The paper mentions computing average reward over 10 episodes as an evaluation metric. It would be helpful to provide more detail on this metric and any others used to evaluate the performance of the models, including how they were calculated and why they were chosen.",
                "The paper discusses value bounds and expected error bounds in sections B.1 and B.2, but it's unclear how these concepts relate to the overall goals of the research. Providing more context or explanation would help readers understand the implications of these results. The paper provides theoretical analysis and proofs for the HiP-MDP and HiP-BMDP settings, showing improvements in sample complexity over prior work, but it does not clearly explain how these results relate to the overall goals of the research.",
                "The conclusion does a good job of summarizing the findings of the paper, but it could benefit from a more explicit discussion of the implications of these findings for future research. The paper concludes by advocating for the HiP-BMDP framework to address the multi-task reinforcement learning setting and discusses potential future work, but it could provide a more explicit discussion of these points.",
                "The paper mentions several different algorithms and techniques, but it does not always clearly explain how these are related to each other and to the overall goals of the research. This could be clarified in the conclusion or in a separate section. The paper discusses several different algorithms and techniques, including HiP-BMDP, HiP-MDP, Distral, PCGrad, GradNorm, and others, but it does not always provide a clear explanation of how these are related to each other and to the overall goals of the research."
            ]
        },
        "alignments": {
            "all": [
                {
                    "reference_id": 0,
                    "predicted_id": 4,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "The paper does not clearly demonstrate if any insight is gained from the theoretical analysis, as the derivation of value and sample complexity bounds for approximate bisimulation was already performed in Gelada et al. 2019.",
                    "predicted_comment": "The paper discusses value bounds and expected error bounds in sections B.1 and B.2, but it's unclear how these concepts relate to the overall goals of the research. Providing more context or explanation would help readers understand the implications of these results. The paper provides theoretical analysis and proofs for the HiP-MDP and HiP-BMDP settings, showing improvements in sample complexity over prior work, but it does not clearly explain how these results relate to the overall goals of the research."
                }
            ]
        },
        "metrics": {
            "all": {
                "n_reference": 7,
                "n_generated": 7,
                "n_total_aligns": 1,
                "recall": 0.07142857142857142,
                "precision": 0.07142857142857142,
                "pseudo_jaccard": 0.038461538461538464,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.14285714285714285,
                    "precision": 0.14285714285714285,
                    "pseudo_jaccard": 0.07692307692307693,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 0
                }
            ]
        }
    },
    {
        "doc_id": "hbzCPZEIUU",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The comparison in the experiment section is weak without any reference to existing prior arts such as Chen et al., Fine-grained representation learning and recognition by exploiting hierarchical semantic embedding, ACM-MM 2018.",
                "There is a significant performance gap between the proposed method and the latest methods when considering the 5 experiment datasets.",
                "The baseline performance in the experiments shows considerable gaps to standard baseline training without bells and whistles, raising questions about its solidity.",
                "The performance gain of the proposed method diminishes very quickly on bigger datasets such as Tiny ImageNet, and the results on ImageNet are not provided.",
                "The proposed method depends on a pre-defined semantic hierarchical graph rather than a learned one, which potentially limits the technical value of this work.",
                "There is concern about the selection of initial radius $R_0$ and its decay policy, as these parameters should be dataset dependent due to different numbers of categories and the densities of class distributions.",
                "Forcing a fixed radius does not seem as reasonable as allowing a learnable radius with soft regularization."
            ],
            [
                "The empirical evaluation is relatively weak and the evaluation metric does not well reflect the advantages of hierarchically modelling the label space. It would be more informative to incorporate the classification accuracy of the super-classes.",
                "More experiments that analyze the difference in feature distributions between normally trained neural networks and the hierarchically trained neural networks are needed. An intuitive visualization of the feature space would be of great interest.",
                "Important ablation studies to justify some heuristic designs are necessary. For example, the effect of the hyperparameter in the radius decay on performance is crucial. Evaluations should be conducted on what happens if no sphericity constraint is applied, or if no radius decay is used.",
                "The empirical evaluation does not convincingly demonstrate the usefulness of modeling the hierarchical label space, especially in experiments on CIFAR-100 and Tiny-ImageNet. More discussions and insights on this issue would be useful.",
                "The usefulness of the hierarchical label structure should be evaluated and verified. A simple way to evaluate it is to use some random assignment or simple K-means assignments for the super-classes. If using the ground truth hierarchical structure consistently outperforms the random or K-means super-class assignment, then it can be argued that incorporating the ground truth hierarchical label structure is indeed useful."
            ],
            [
                "Section 2 needs more specific definitions for terms such as 'classifier' and 'separators'. For example, in equation (2), Wp and Wpi are called classifiers. Are these hyperplanes?",
                "In Definition 1, it would be helpful to provide early examples of P and L to make it easier to interpret the matrix H.",
                "The notation for Delta in equation (8) could be confusing as it may remind an operator on H in equation (9). Consider using a different notation.",
                "In equation (9), it is unclear whether deltas in the same subtree are required or observed to be in roughly the same direction.",
                "In Section 3, the claim that no hyperparameters are added seems incorrect. Initial radius R0, radius decay parameter, and the organization of classes could all be considered additional hyperparameters.",
                "In practice, defining mutually exclusive class partitions to form the required class tree in Figure 1 can be non-trivial or even impossible. The authors should discuss how different class hierarchies affect the classification accuracy, for example, in Table 2."
            ],
            [
                "The third paragraph of Section 1 is unclear about why Euclidean distance is not sufficient for learning with hierarchical regularization.",
                "The paper does not clearly explain whether the whole model follows end-to-end training or stage-wise training.",
                "In Section 2.2, the definition of 'separator' is unclear and the sentence about classifiers identifying dog breeds being similar is confusing.",
                "The paper does not provide a clear rationale or related work for the use of radius decay in Section 3.1.",
                "The optimization method used for learning other layers is not specified.",
                "The paper does not provide a clear explanation or justification for the construction of the spherical fully-connected layer.",
                "The paper does not present the hierarchical structure in a visually appealing or clear way.",
                "The paper does not clearly explain whether images from datasets Dogs and Tiny-Imagenet are part of ImageNet or if the classes in the two datasets are included in the set of ImageNet classes.",
                "The definition of 'plain networks' in Section 4.1 is unclear.",
                "The paper does not justify its claim of 'high efficiency' in Section 4.2.1.",
                "The paper does not formally compare Riemannian gradient and 'projected gradient' in Figure 2 right.",
                "The paper does not discuss how the proposed method may work if classes do not follow a tree hierarchy.",
                "The explanation of why 'Euclidean distance is not sufficient for learning such a hierarchical regularization' in the rebuttal is not intuitive or concrete.",
                "The paper does not clearly explain whether other methods suffer severely from the choice of learning rate and scheduler.",
                "The blue line in Page-2 is confusing and the logic of why diversity reduces over-fitting is unclear.",
                "The tSNE visuals in Figure 5 are not adequately analyzed."
            ]
        ],
        "generated_comments": {
            "all": [
                "Please elaborate on the proposed solution, specifically how it incorporates hierarchical information into deep neural network architectures. How does this differ from existing methods?",
                "Could you provide a brief description of each dataset used in the study? Please include why each dataset is relevant to the study and how it was used.",
                "For each technique referenced in the paper, please include a brief explanation or reference for readers who may not be familiar with them.",
                "Could you provide a clearer outline of the paper's structure? This could include a brief summary of each section or a flowchart of the paper's organization.",
                "Discuss how the proposed technique can be combined with other strategies. Please provide examples of how this could be done and the potential benefits.",
                "Could you provide more context or references to previous works that have attempted similar approaches? This could help readers understand the novelty of your work.",
                "The concepts of spherical constraints and Riemannian optimization are complex. Could you provide more intuitive explanations or visualizations to help readers understand these concepts?",
                "Could you provide more justification or explanation for the choice of the 'simple strategy called radius decay'? How does this strategy contribute to the overall performance of the method?",
                "Please provide more details on how the optimization strategies were implemented. How do they compare to each other in terms of performance and complexity?",
                "Could you provide a more detailed discussion on the impact of the radius decay on the test accuracy? How does changing the radius decay value affect the performance of the method?",
                "Could you elaborate on why the number of levels in the hierarchy tree affects the accuracy? What is the optimal number of levels and why?",
                "Could you provide more information on how the Resnet18 architecture with Riemannian gradient descent was used to optimize the spherical fully-connected layer? How does this compare to other optimization methods?"
            ]
        },
        "alignments": {
            "all": [
                {
                    "reference_id": 5,
                    "predicted_id": 9,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "There is concern about the selection of initial radius $R_0$ and its decay policy, as these parameters should be dataset dependent due to different numbers of categories and the densities of class distributions.",
                    "predicted_comment": "Could you provide a more detailed discussion on the impact of the radius decay on the test accuracy? How does changing the radius decay value affect the performance of the method?"
                },
                {
                    "reference_id": 5,
                    "predicted_id": 7,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "There is concern about the selection of initial radius $R_0$ and its decay policy, as these parameters should be dataset dependent due to different numbers of categories and the densities of class distributions.",
                    "predicted_comment": "Could you provide more justification or explanation for the choice of the 'simple strategy called radius decay'? How does this strategy contribute to the overall performance of the method?"
                },
                {
                    "reference_id": 5,
                    "predicted_id": 10,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "In practice, defining mutually exclusive class partitions to form the required class tree in Figure 1 can be non-trivial or even impossible. The authors should discuss how different class hierarchies affect the classification accuracy, for example, in Table 2.",
                    "predicted_comment": "Could you elaborate on why the number of levels in the hierarchy tree affects the accuracy? What is the optimal number of levels and why?"
                },
                {
                    "reference_id": 10,
                    "predicted_id": 11,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper does not formally compare Riemannian gradient and 'projected gradient' in Figure 2 right.",
                    "predicted_comment": "Could you provide more information on how the Resnet18 architecture with Riemannian gradient descent was used to optimize the spherical fully-connected layer? How does this compare to other optimization methods?"
                },
                {
                    "reference_id": 3,
                    "predicted_id": 7,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper does not provide a clear rationale or related work for the use of radius decay in Section 3.1.",
                    "predicted_comment": "Could you provide more justification or explanation for the choice of the 'simple strategy called radius decay'? How does this strategy contribute to the overall performance of the method?"
                },
                {
                    "reference_id": 4,
                    "predicted_id": 8,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The optimization method used for learning other layers is not specified.",
                    "predicted_comment": "Please provide more details on how the optimization strategies were implemented. How do they compare to each other in terms of performance and complexity?"
                }
            ]
        },
        "metrics": {
            "all": {
                "n_reference": 34,
                "n_generated": 12,
                "n_total_aligns": 6,
                "recall": 0.12425595238095238,
                "precision": 0.125,
                "pseudo_jaccard": 0.0661344537815126,
                "specificity_more": 0.8333333333333334,
                "specificity_less": 0.0,
                "specificity_same": 0.16666666666666666
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.14285714285714285,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.08571428571428572,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.16666666666666666,
                    "precision": 0.08333333333333333,
                    "pseudo_jaccard": 0.058823529411764705,
                    "n_real": 6
                },
                {
                    "recall": 0.1875,
                    "precision": 0.25,
                    "pseudo_jaccard": 0.12,
                    "n_real": 16
                }
            ]
        }
    },
    {
        "doc_id": "ab7lBP7Fb60",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The proposed method is not compatible with federated averaging, which is a significant limitation in federated learning due to the communication bottleneck characteristic of federated settings.",
                "The paper does not compare the proposed method to prior work on group fairness in the centralized setting, such as the work by Zafar et al., (2017). An in-depth experimental evaluation of this aspect is needed to motivate the proposed approach.",
                "The proposed method may be susceptible to a potential privacy violation, as the statistics shared at every round include the number of individuals per protected attribute for every client. This could be exploited to learn an unfair model with respect to certain sensitive attributes.",
                "The paper incorrectly characterizes prior fair federated learning approaches, such as the work by Li et al. (2019), as individual fairness methods. These methods enforce uniform performance across all devices, which is neither individual nor group fairness, but a novel notion tailored to the federated learning setting."
            ],
            [
                "The paper is missing important literature review, including methods that enforce group fairness in Federated Learning such as 'Fair and Consistent Federated Learning' by Cui et al. 2021. These could have been considered as a baseline for comparing their method.",
                "The paper does not include enough baselines for Fair Federated Learning to compare against. Even if some methods do not satisfy privacy considerations, they should still be included for the reader to understand how the proposed method compares against such methods, especially given that the results are not promising. Some baselines to consider include Cui et al or Tran et al.",
                "The authors need to justify some of the incompatibility results shown in previous work between fairness and privacy.",
                "Algorithm 1 needs improvement. It is unclear what the clients do, what 'UpdateMultiplier', 'UpdateParameters', and 'Calculate noiseScale' mean.",
                "The results are not promising for predictive parity among groups. There is also a reduction in accuracy. The authors should discuss why this is the case. Including the suggested baselines could provide a sense of what other methods achieve in terms of loss in accuracy, etc.",
                "The results for the central setting should be included in Table 2."
            ]
        ],
        "generated_comments": {
            "all": [
                "In the introduction, it would be helpful to provide a more detailed explanation of how the FPFL algorithm works to mitigate unfairness. Specifically, how does it adapt the MMDM to empirical loss minimization with fairness constraints?",
                "In the methodology section, please clarify how the concepts of private federated learning, the MMDM algorithm, and group fairness are integrated in the proposed algorithm. How does the algorithm enforce fairness in private federated learning?",
                "It would be beneficial to provide examples or case studies demonstrating how the proposed algorithm can be tailored to enforce different group fairness metrics. How can the algorithm be adapted to different fairness constraints?",
                "Please provide examples or case studies demonstrating the capability of the proposed algorithm to consider any number of attributes determining the groups. How does the algorithm handle multiple attributes?",
                "The results section could benefit from more visual aids such as graphs or charts to better illustrate the performance of the MMDM algorithm and FPFL. This would help readers understand the impact of clipping and DP on under-represented groups and the performance of the FPFL algorithm compared to other models.",
                "The authors should provide more context for the results, such as how they compare to previous studies or expected outcomes.",
                "The authors should clarify the implications of the results for the field of machine learning and fairness enforcement.",
                "The discussion section could benefit from a more detailed analysis of the results and their implications. For example, how does the performance of the FPFL algorithm deteriorate for deeper networks due to the noise being large enough to sometimes mistake the sign of the constraints' gradient?",
                "The authors should discuss the limitations of their study in more detail, including the sensitivity of the FPFL algorithm to DP noise.",
                "The authors should discuss potential future research directions based on their findings.",
                "The conclusion could benefit from a more detailed summary of the results and their implications. For example, how does the problem of group fairness in private federated learning relate to the proposed solution of adapting the MMDM to empirical loss minimization with fairness constraints?",
                "The authors should consider discussing potential applications of their findings in the conclusion.",
                "In the remaining sections, all technical terms and acronyms should be clearly defined when they are first introduced."
            ]
        },
        "alignments": {
            "all": []
        },
        "metrics": {
            "all": {
                "n_reference": 10,
                "n_generated": 13,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                }
            ]
        }
    },
    {
        "doc_id": "rsf1z-JSj87",
        "method": "gpt_generic_multi_agent",
        "reference_comments": [
            [
                "The performance of the proposed model is promising, but still below the baselines.",
                "The claim of the model being end-to-end is misleading as the character-based model is not performing well, and the phoneme-based model is not truly end-to-end, as the grapheme-to-phoneme (g2p) part is not trained jointly.",
                "The reason behind using an adversarial loss is not adequately explained in the paper. A few lines before section 2.1 would help clarify that.",
                "It's not clear which dataset was used in the experiment. If it is a private dataset, please state it clearly."
            ],
            [
                "The paper does not provide a comparison over the computational cost or model size. This is particularly important because the proposed model is non-autoregressive and may be capable of real-time inference.",
                "The paper does not use widely accepted benchmark datasets. A more direct comparison would be beneficial.",
                "The proposed aligner module doesn't seem to be very useful compared with the attention-based aligner as seen in the ablation study (Table 1): there is only a very small improvement from 3.551 to 3.559 MOS. More explanation is needed."
            ],
            [
                "The elegance in the architecture is overshadowed by the complicated training algorithm, making the method look a bit heuristic and harder to reproduce.",
                "The authors should provide brief guidelines for readers trying to reimplement EATS, such as how to tune the hyperparameters.",
                "EATS performs slightly worse than GAN-TTS, which does not quite show the benefit of end-to-end training. It would be worthwhile to expand the discussion a bit by showing further experiments that demonstrate the potential benefit of end-to-end training.",
                "Without a clearly interpretable hidden representation, it is harder to have direct control over prosody. The authors should discuss how prosody control would be possible under the end-to-end framework."
            ],
            [
                "The paper lacks a comparison with Tacotron 2 in Table 1. This would be informative as all the baselines are trained on aligned data while Tacotron is a legitimate contender for EATS as it can be trained on the same data.",
                "The authors claim that their method is simpler because the training is in one stage. However, given the large number of losses and components in their model, with their respective hyper-parameters to tune, the simplicity argument is not entirely convincing.",
                "The performance of the transformer attention is surprisingly poor, particularly in the audio samples where the output for this model is garbage towards the end of the signal. The authors should provide an explanation for this."
            ]
        ],
        "generated_comments": {
            "all": [
                "Provide a detailed comparison of the proposed EATS model with the state of the art in TTS, highlighting the unique features and improvements of the EATS model.",
                "Elaborate on the operation of the EATS model on pure text and raw phoneme input sequences. Discuss the role of the phonemizer tool in improving sample quality and the significance of pre-and post-padding with a special silence token.",
                "Provide an in-depth explanation of the use of adversarial feedback in the form of a spectrogram discriminator in the EATS model. Discuss the architecture of the discriminator and its modifications to suit the resolution of the spectrogram inputs. Also, elaborate on the use of the soft dynamic time warping (DTW) procedure for the spectrogram prediction loss and its impact on training time.",
                "Elaborate on the use of adversarial feedback and domain-specific loss functions in training the TTS system.",
                "Provide more explanation on the significance of the main contributions to the field of TTS.",
                "Provide more context or background information on the Dynamic Time Warping and Aligner Length Loss methods.",
                "Provide a more detailed explanation of how the phonemizer tool works and why it improves sample quality.",
                "Include more visual aids, such as diagrams or flowcharts, to help readers better understand the processes being described.",
                "Provide a clear explanation of how the parameters (e.g., \u03c4, \u03bb pred, \u03bb length) were chosen or optimized.",
                "Provide a more detailed explanation of the adversarial approach used in the model.",
                "Include a comparison of the EATS model's performance with other existing models.",
                "Include a discussion on how the phonemizer for text preprocessing and the transformer-based attention aligner baseline contributed to the overall results.",
                "Provide a more in-depth analysis of the variation in alignment and the multispeaker results.",
                "Include a summary of the key findings from the comparison of TTS methods and how they relate to the EATS model.",
                "Include a brief explanation or commentary on the code snippets and pseudocode in the text.",
                "Include a discussion on the limitations of the Mean Opinion Scores (MOS) and Fr\u00e9chet DeepSpeech Distances (FDSD) metrics.",
                "Include a discussion on potential directions for future research on automatic quantitative evaluation of text-to-speech models."
            ]
        },
        "alignments": {
            "all": [
                {
                    "reference_id": 2,
                    "predicted_id": 2,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The reason behind using an adversarial loss is not adequately explained in the paper. A few lines before section 2.1 would help clarify that.",
                    "predicted_comment": "Provide an in-depth explanation of the use of adversarial feedback in the form of a spectrogram discriminator in the EATS model. Discuss the architecture of the discriminator and its modifications to suit the resolution of the spectrogram inputs. Also, elaborate on the use of the soft dynamic time warping (DTW) procedure for the spectrogram prediction loss and its impact on training time."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 9,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The reason behind using an adversarial loss is not adequately explained in the paper. A few lines before section 2.1 would help clarify that.",
                    "predicted_comment": "Provide a more detailed explanation of the adversarial approach used in the model."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 8,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The authors should provide brief guidelines for readers trying to reimplement EATS, such as how to tune the hyperparameters.",
                    "predicted_comment": "Provide a clear explanation of how the parameters (e.g., \u03c4, \u03bb pred, \u03bb length) were chosen or optimized."
                }
            ]
        },
        "metrics": {
            "all": {
                "n_reference": 14,
                "n_generated": 17,
                "n_total_aligns": 3,
                "recall": 0.125,
                "precision": 0.044117647058823525,
                "pseudo_jaccard": 0.03173076923076923,
                "specificity_more": 0.6666666666666666,
                "specificity_less": 0.0,
                "specificity_same": 0.3333333333333333
            }
        },
        "metrics_for_reviews": {
            "all": [
                {
                    "recall": 0.25,
                    "precision": 0.11764705882352941,
                    "pseudo_jaccard": 0.07692307692307693,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.25,
                    "precision": 0.058823529411764705,
                    "pseudo_jaccard": 0.05,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ]
        }
    }
]