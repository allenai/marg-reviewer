[
    {
        "doc_id": "B1x8anVFPr",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The authors should have performed multiple runs of their experiments and shown the standard deviations across runs to make their results more convincing.",
                "The authors claim that using Pre-LN rather than Post-LN transformers leads to faster convergence, but the models converge to the same result. This work is incremental as Wang et al. have already shown the benefits of Pre-LN rather than Post-LN transformers for machine translation.",
                "The paper is very long for an incremental improvement.",
                "It would be good to see repeats for Figure 2 and the standard deviations for Figure 3.",
                "Without reading the appendix it is not clear where the assumption that W^Q and W^K are zero is used. Making some connection with how this assumption relates to the lemmas would be useful.",
                "In Lemma two, the authors are comparing the magnitudes for the input in the Pre-LN and the output in the Post-LN transformer according to how x_{l,i}^post and x_{l, i}^pre are defined in Table 1.",
                "In Figure 3(b) the gradients are clearly decreasing with the number of layers, are there any comments on this? In the limit this could cause vanishing gradients?",
                "Figure 2 and 4 appear to contradict. Is the difference a result of using RAdam? If so, this should be made very clear. If not, why are the results contradictory?"
            ],
            [
                "The novelty of the paper is limited as Pre-LN placement has been used in prior work.",
                "There are concerns about the relation between the analytic investigation of the gradient norms and the empirical results that are presented.",
                "The theoretical results do not clearly demonstrate that there is a problem with Post-LN and that this problem is fixed by switching to Pre-LN.",
                "The paper does not clearly explain the relationship between the theoretical results and the empirical study.",
                "The paper does not adequately explain the scaling of the gradient changes throughout the layers, which is examined in appendix section F.",
                "The connection between the expression that scales in the upper bound and the actual gradient norm is tenuous and there are multiple places where the argument could break down.",
                "The paper does not verify the sqrt(2/3) scaling on the data from the empirical study that is shown.",
                "The paper does not explain where the expectation near the bottom of page 20, which is approximately 2/3, comes from."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper acknowledges the sensitivity of the validation accuracy to random seeds and has taken steps to address this issue by repeating experiments with different random seeds. However, the paper does not provide specific details on the range of random seeds used or the statistical analysis of the results from these multiple runs. Providing these details would strengthen the paper by giving readers a better understanding of the robustness of the results. For example, the authors could report the range of random seeds used and the average and standard deviation of the results.",
                "The paper mentions a gap between the BLEU score and the validation loss when the model is not well-trained, which is attributed to the BLEU score being defined on the translation results generated by step-wise decoding. This is a significant issue that affects the interpretation of the results. However, the paper does not provide a clear explanation of why this gap occurs and how it affects the interpretation of the results. It would be helpful if the authors could provide more explanation on this issue. For example, they could discuss the impact of the learning rate warm-up stage on this gap, as it seems to have a significant effect on both the BLEU score and the validation loss."
            ],
            "novelty_impact": [
                "The paper provides a novel perspective on the Transformer architecture, focusing on the role of layer normalization and the learning rate warm-up stage. The authors' proposal to place layer normalization inside the residual blocks, creating the Pre-LN Transformer, is innovative and could have significant implications for the training of Transformer models. However, it would be helpful if the authors could provide more detail about the comparison between the Post-LN and Pre-LN Transformers, particularly in terms of their performance when stacking more layers. Additionally, the authors could clarify the explanation of the method and conduct additional experiments or analyses to further support their claims.",
                "While the authors provide a clear motivation for their work, the goals of the paper could be more explicitly defined. Specifically, the authors could provide more explicit hypotheses at the beginning of the paper, stating what they expect to find regarding the role of the learning rate warm-up stage and the impact of layer normalization positioning. Additionally, providing more background information on the Post-LN and Pre-LN Transformers, as well as a more detailed explanation of the warm-up stage and its significance in the training process, would help a broader audience understand the goals of the paper. Finally, a summary or overview of the proof of Lemma 3 in simpler terms, as well as a brief explanation of how the experimental settings and datasets contribute to the overall goals of the paper, would further clarify the goals of the paper.",
                "The paper presents its key findings effectively and supports them with empirical evidence. However, the paper could be strengthened by including detailed visualizations or quantitative analyses of the gradients at initialization and during training for both the original and modified Transformer models. These could provide more insight into the behavior of the models during training, helping readers understand why the modified architecture is easier and faster to train and why it allows the learning rate warm-up stage to be safely removed. For example, visualizations could show how the gradients evolve over time for different layers in the models, and quantitative analyses could compare the magnitude and variance of the gradients for the original and modified models.",
                "The authors reference important previous work, such as Vaswani et al., 2017 and Liu et al., 2019a, but could do a better job of explaining how their work expands on these studies. Specifically, the authors should clarify how their results differ from those of Vaswani et al., 2017 and Liu et al., 2019a, and why these differences are significant. This would help readers understand the novelty and significance of the authors' work.",
                "The authors' claim that their modified Transformer model does not require a learning rate warm-up stage is significant and well-justified by their experiments. However, the paper could benefit from a more detailed discussion of the conditions under which the experiments were conducted. Specifically, it would be helpful to know more about the batch sizes used in the experiments. Additionally, while the paper does include comparisons of the model with and without a warm-up stage, it would strengthen the authors' claim if they could provide more detailed comparisons under a wider variety of conditions. For example, the authors could conduct additional experiments using different optimizers, learning rates, and batch sizes, and discuss how these conditions affect the performance of the model with and without a warm-up stage.",
                "The authors have provided a thorough investigation into the importance of the learning rate warm-up stage in training the Transformer model using Adam and SGD optimizers. Given the specific focus of the study, the choice of optimizers seems appropriate. However, it would be interesting for future work to explore how the modified model performs with other popular optimizers, as this could provide additional insights into the generalizability of the findings and the robustness of the proposed architecture.",
                "While the authors propose a modified Transformer architecture that locates the layer normalization inside the residual blocks, which is easier and faster to train, the paper does not discuss any potential trade-offs or downsides of this approach. It would be beneficial for the authors to discuss potential trade-offs such as the impact on model performance when the learning rate warm-up stage is removed, or the potential for overfitting with a faster training time. Discussing these trade-offs would provide a more balanced view of the work and help readers understand the full implications of the proposed approach.",
                "The authors' plan to further investigate strategies of positioning the layer normalization in the Transformer architecture is promising. However, they should provide some preliminary insights or hypotheses to guide this research in future work. For example, they could discuss potential strategies for positioning the layer normalization in other architectures, or hypothesize about the impact of different positioning strategies on the learning rate warm-up stage. This would provide a clearer direction for future research and could potentially lead to more efficient training methods for Transformer models.",
                "The inclusion of mathematical proofs of several lemmas related to the Transformer model is a strength of the paper. However, it would be beneficial if the authors could provide a more explicit connection between these lemmas and the performance differences between the Post-LN and Pre-LN Transformer models. For instance, the authors could discuss how the lemmas support the observations about the scale of the gradients and hidden states in the models, and how these factors contribute to the need for a learning rate warm-up stage in the Post-LN Transformer and the faster training of the Pre-LN Transformer.",
                "While the authors' findings are significant and applicable to Transformer-based models, and they have used standard benchmarks for their experiments, it would be beneficial to see the modified model tested on a wider range of tasks and datasets. This would provide a more comprehensive assessment of its generalizability. The authors should also discuss the potential generalizability of their model in the paper, as this is a key aspect of its value."
            ],
            "clarity_reproducibility": [
                "The paper provides some explanations of the Post-LN and Pre-LN Transformer architectures, but these could be expanded upon for clarity. Specifically, the paper could benefit from a more detailed comparison of the two architectures, highlighting the differences in their layer normalization and residual block structures. Additionally, the paper mentions that the Pre-LN Transformer is easier and faster to train, but does not provide sufficient evidence or explanation to support this claim. It would be helpful if the authors could elaborate on why the lack of a learning rate warm-up stage in the Pre-LN Transformer contributes to its ease and speed of training, and how this compares to the Post-LN Transformer's need for a learning rate warm-up stage.",
                "The paper provides some details about the removal of the learning rate warm-up stage in the Pre-LN Transformer, such as the motivation for its removal and the results of experiments showing that it can be safely removed. However, it would be helpful to provide more specific details about the alternative approach used in place of the warm-up stage. For example, the paper mentions that a linear learning rate decay starting from 3e \u22124 was used for the Pre-LN BERT, and specific learning rates and decay schedules were used for the IWSLT14 De-En task and the WMT14 En-De task. Providing more details about these approaches, such as why they were chosen and how they compare to the warm-up stage, would help readers understand the training process of the Pre-LN Transformer and why it does not require a learning rate warm-up stage.",
                "The paper lacks detailed information about the parameter initialization setting and theoretical findings related to the initialization of the Transformer. This information is crucial for understanding the learning rate warm-up stage in training the Post-LN Transformer and its relation to the position of the layer normalization. Specifically, the initialization of the parameters could significantly impact the optimization process and the necessity of the learning rate warm-up stage. Therefore, providing more detailed information about the parameter initialization setting and theoretical findings related to the initialization could help readers better understand why the learning rate warm-up stage is essential and how it is related to the position of the layer normalization.",
                "The paper provides a good overview of the experiments conducted to study the gradients at initialization for the Post-LN/Pre-LN Transformer in real scenarios. However, it would be beneficial to provide more specific details about these experiments. For example, the paper could elaborate on the tasks used in the experiments, such as the IWSLT14 German-English translation, WMT14 English-German translation, and BERT pre-training tasks. Additionally, the paper could provide more information about the model and training configuration followed for the IWSLT14 De-En task. The paper could also provide more insight into the findings from the experiments. For instance, it could explain why the norm of the hidden states satisfies the concentration property ((0.1,0.125)-bounded) and why the scale of the gradients are very small after the warm-up stage with Adam. Providing these additional details would help readers better understand the reasons that the Post-LN Transformer needs a careful learning rate scheduling in the beginning. It would also help readers understand why the Pre-LN Transformer does not require this warm-up stage and can be trained faster and with larger learning rates."
            ],
            "all": [
                "The paper acknowledges the sensitivity of the validation accuracy to random seeds and has taken steps to address this issue by repeating experiments with different random seeds. However, the paper does not provide specific details on the range of random seeds used or the statistical analysis of the results from these multiple runs. Providing these details would strengthen the paper by giving readers a better understanding of the robustness of the results. For example, the authors could report the range of random seeds used and the average and standard deviation of the results.",
                "The paper mentions a gap between the BLEU score and the validation loss when the model is not well-trained, which is attributed to the BLEU score being defined on the translation results generated by step-wise decoding. This is a significant issue that affects the interpretation of the results. However, the paper does not provide a clear explanation of why this gap occurs and how it affects the interpretation of the results. It would be helpful if the authors could provide more explanation on this issue. For example, they could discuss the impact of the learning rate warm-up stage on this gap, as it seems to have a significant effect on both the BLEU score and the validation loss.",
                "The paper provides a novel perspective on the Transformer architecture, focusing on the role of layer normalization and the learning rate warm-up stage. The authors' proposal to place layer normalization inside the residual blocks, creating the Pre-LN Transformer, is innovative and could have significant implications for the training of Transformer models. However, it would be helpful if the authors could provide more detail about the comparison between the Post-LN and Pre-LN Transformers, particularly in terms of their performance when stacking more layers. Additionally, the authors could clarify the explanation of the method and conduct additional experiments or analyses to further support their claims.",
                "While the authors provide a clear motivation for their work, the goals of the paper could be more explicitly defined. Specifically, the authors could provide more explicit hypotheses at the beginning of the paper, stating what they expect to find regarding the role of the learning rate warm-up stage and the impact of layer normalization positioning. Additionally, providing more background information on the Post-LN and Pre-LN Transformers, as well as a more detailed explanation of the warm-up stage and its significance in the training process, would help a broader audience understand the goals of the paper. Finally, a summary or overview of the proof of Lemma 3 in simpler terms, as well as a brief explanation of how the experimental settings and datasets contribute to the overall goals of the paper, would further clarify the goals of the paper.",
                "The paper presents its key findings effectively and supports them with empirical evidence. However, the paper could be strengthened by including detailed visualizations or quantitative analyses of the gradients at initialization and during training for both the original and modified Transformer models. These could provide more insight into the behavior of the models during training, helping readers understand why the modified architecture is easier and faster to train and why it allows the learning rate warm-up stage to be safely removed. For example, visualizations could show how the gradients evolve over time for different layers in the models, and quantitative analyses could compare the magnitude and variance of the gradients for the original and modified models.",
                "The authors reference important previous work, such as Vaswani et al., 2017 and Liu et al., 2019a, but could do a better job of explaining how their work expands on these studies. Specifically, the authors should clarify how their results differ from those of Vaswani et al., 2017 and Liu et al., 2019a, and why these differences are significant. This would help readers understand the novelty and significance of the authors' work.",
                "The authors' claim that their modified Transformer model does not require a learning rate warm-up stage is significant and well-justified by their experiments. However, the paper could benefit from a more detailed discussion of the conditions under which the experiments were conducted. Specifically, it would be helpful to know more about the batch sizes used in the experiments. Additionally, while the paper does include comparisons of the model with and without a warm-up stage, it would strengthen the authors' claim if they could provide more detailed comparisons under a wider variety of conditions. For example, the authors could conduct additional experiments using different optimizers, learning rates, and batch sizes, and discuss how these conditions affect the performance of the model with and without a warm-up stage.",
                "The authors have provided a thorough investigation into the importance of the learning rate warm-up stage in training the Transformer model using Adam and SGD optimizers. Given the specific focus of the study, the choice of optimizers seems appropriate. However, it would be interesting for future work to explore how the modified model performs with other popular optimizers, as this could provide additional insights into the generalizability of the findings and the robustness of the proposed architecture.",
                "While the authors propose a modified Transformer architecture that locates the layer normalization inside the residual blocks, which is easier and faster to train, the paper does not discuss any potential trade-offs or downsides of this approach. It would be beneficial for the authors to discuss potential trade-offs such as the impact on model performance when the learning rate warm-up stage is removed, or the potential for overfitting with a faster training time. Discussing these trade-offs would provide a more balanced view of the work and help readers understand the full implications of the proposed approach.",
                "The authors' plan to further investigate strategies of positioning the layer normalization in the Transformer architecture is promising. However, they should provide some preliminary insights or hypotheses to guide this research in future work. For example, they could discuss potential strategies for positioning the layer normalization in other architectures, or hypothesize about the impact of different positioning strategies on the learning rate warm-up stage. This would provide a clearer direction for future research and could potentially lead to more efficient training methods for Transformer models.",
                "The inclusion of mathematical proofs of several lemmas related to the Transformer model is a strength of the paper. However, it would be beneficial if the authors could provide a more explicit connection between these lemmas and the performance differences between the Post-LN and Pre-LN Transformer models. For instance, the authors could discuss how the lemmas support the observations about the scale of the gradients and hidden states in the models, and how these factors contribute to the need for a learning rate warm-up stage in the Post-LN Transformer and the faster training of the Pre-LN Transformer.",
                "While the authors' findings are significant and applicable to Transformer-based models, and they have used standard benchmarks for their experiments, it would be beneficial to see the modified model tested on a wider range of tasks and datasets. This would provide a more comprehensive assessment of its generalizability. The authors should also discuss the potential generalizability of their model in the paper, as this is a key aspect of its value.",
                "The paper provides some explanations of the Post-LN and Pre-LN Transformer architectures, but these could be expanded upon for clarity. Specifically, the paper could benefit from a more detailed comparison of the two architectures, highlighting the differences in their layer normalization and residual block structures. Additionally, the paper mentions that the Pre-LN Transformer is easier and faster to train, but does not provide sufficient evidence or explanation to support this claim. It would be helpful if the authors could elaborate on why the lack of a learning rate warm-up stage in the Pre-LN Transformer contributes to its ease and speed of training, and how this compares to the Post-LN Transformer's need for a learning rate warm-up stage.",
                "The paper provides some details about the removal of the learning rate warm-up stage in the Pre-LN Transformer, such as the motivation for its removal and the results of experiments showing that it can be safely removed. However, it would be helpful to provide more specific details about the alternative approach used in place of the warm-up stage. For example, the paper mentions that a linear learning rate decay starting from 3e \u22124 was used for the Pre-LN BERT, and specific learning rates and decay schedules were used for the IWSLT14 De-En task and the WMT14 En-De task. Providing more details about these approaches, such as why they were chosen and how they compare to the warm-up stage, would help readers understand the training process of the Pre-LN Transformer and why it does not require a learning rate warm-up stage.",
                "The paper lacks detailed information about the parameter initialization setting and theoretical findings related to the initialization of the Transformer. This information is crucial for understanding the learning rate warm-up stage in training the Post-LN Transformer and its relation to the position of the layer normalization. Specifically, the initialization of the parameters could significantly impact the optimization process and the necessity of the learning rate warm-up stage. Therefore, providing more detailed information about the parameter initialization setting and theoretical findings related to the initialization could help readers better understand why the learning rate warm-up stage is essential and how it is related to the position of the layer normalization.",
                "The paper provides a good overview of the experiments conducted to study the gradients at initialization for the Post-LN/Pre-LN Transformer in real scenarios. However, it would be beneficial to provide more specific details about these experiments. For example, the paper could elaborate on the tasks used in the experiments, such as the IWSLT14 German-English translation, WMT14 English-German translation, and BERT pre-training tasks. Additionally, the paper could provide more information about the model and training configuration followed for the IWSLT14 De-En task. The paper could also provide more insight into the findings from the experiments. For instance, it could explain why the norm of the hidden states satisfies the concentration property ((0.1,0.125)-bounded) and why the scale of the gradients are very small after the warm-up stage with Adam. Providing these additional details would help readers better understand the reasons that the Post-LN Transformer needs a careful learning rate scheduling in the beginning. It would also help readers understand why the Pre-LN Transformer does not require this warm-up stage and can be trained faster and with larger learning rates."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The authors should have performed multiple runs of their experiments and shown the standard deviations across runs to make their results more convincing.",
                    "predicted_comment": "The paper acknowledges the sensitivity of the validation accuracy to random seeds and has taken steps to address this issue by repeating experiments with different random seeds. However, the paper does not provide specific details on the range of random seeds used or the statistical analysis of the results from these multiple runs. Providing these details would strengthen the paper by giving readers a better understanding of the robustness of the results. For example, the authors could report the range of random seeds used and the average and standard deviation of the results."
                }
            ],
            "novelty_impact": [
                {
                    "reference_id": 1,
                    "predicted_id": 5,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The authors claim that using Pre-LN rather than Post-LN transformers leads to faster convergence, but the models converge to the same result. This work is incremental as Wang et al. have already shown the benefits of Pre-LN rather than Post-LN transformers for machine translation.",
                    "predicted_comment": "The authors reference important previous work, such as Vaswani et al., 2017 and Liu et al., 2019a, but could do a better job of explaining how their work expands on these studies. Specifically, the authors should clarify how their results differ from those of Vaswani et al., 2017 and Liu et al., 2019a, and why these differences are significant. This would help readers understand the novelty and significance of the authors' work."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 5,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The novelty of the paper is limited as Pre-LN placement has been used in prior work.",
                    "predicted_comment": "The authors reference important previous work, such as Vaswani et al., 2017 and Liu et al., 2019a, but could do a better job of explaining how their work expands on these studies. Specifically, the authors should clarify how their results differ from those of Vaswani et al., 2017 and Liu et al., 2019a, and why these differences are significant. This would help readers understand the novelty and significance of the authors' work."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 2,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The theoretical results do not clearly demonstrate that there is a problem with Post-LN and that this problem is fixed by switching to Pre-LN.",
                    "predicted_comment": "The paper provides a novel perspective on the Transformer architecture, focusing on the role of layer normalization and the learning rate warm-up stage. The authors' proposal to place layer normalization inside the residual blocks, creating the Pre-LN Transformer, is innovative and could have significant implications for the training of Transformer models. However, it would be helpful if the authors could provide more detail about the comparison between the Post-LN and Pre-LN Transformers, particularly in terms of their performance when stacking more layers. Additionally, the authors could clarify the explanation of the method and conduct additional experiments or analyses to further support their claims."
                },
                {
                    "reference_id": 3,
                    "predicted_id": 10,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper does not clearly explain the relationship between the theoretical results and the empirical study.",
                    "predicted_comment": "The inclusion of mathematical proofs of several lemmas related to the Transformer model is a strength of the paper. However, it would be beneficial if the authors could provide a more explicit connection between these lemmas and the performance differences between the Post-LN and Pre-LN Transformer models. For instance, the authors could discuss how the lemmas support the observations about the scale of the gradients and hidden states in the models, and how these factors contribute to the need for a learning rate warm-up stage in the Post-LN Transformer and the faster training of the Pre-LN Transformer."
                },
                {
                    "reference_id": 5,
                    "predicted_id": 10,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The connection between the expression that scales in the upper bound and the actual gradient norm is tenuous and there are multiple places where the argument could break down.",
                    "predicted_comment": "The inclusion of mathematical proofs of several lemmas related to the Transformer model is a strength of the paper. However, it would be beneficial if the authors could provide a more explicit connection between these lemmas and the performance differences between the Post-LN and Pre-LN Transformer models. For instance, the authors could discuss how the lemmas support the observations about the scale of the gradients and hidden states in the models, and how these factors contribute to the need for a learning rate warm-up stage in the Post-LN Transformer and the faster training of the Pre-LN Transformer."
                }
            ],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 1,
                    "predicted_id": 5,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The authors claim that using Pre-LN rather than Post-LN transformers leads to faster convergence, but the models converge to the same result. This work is incremental as Wang et al. have already shown the benefits of Pre-LN rather than Post-LN transformers for machine translation.",
                    "predicted_comment": "The authors reference important previous work, such as Vaswani et al., 2017 and Liu et al., 2019a, but could do a better job of explaining how their work expands on these studies. Specifically, the authors should clarify how their results differ from those of Vaswani et al., 2017 and Liu et al., 2019a, and why these differences are significant. This would help readers understand the novelty and significance of the authors' work."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The authors should have performed multiple runs of their experiments and shown the standard deviations across runs to make their results more convincing.",
                    "predicted_comment": "The paper acknowledges the sensitivity of the validation accuracy to random seeds and has taken steps to address this issue by repeating experiments with different random seeds. However, the paper does not provide specific details on the range of random seeds used or the statistical analysis of the results from these multiple runs. Providing these details would strengthen the paper by giving readers a better understanding of the robustness of the results. For example, the authors could report the range of random seeds used and the average and standard deviation of the results."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 5,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The novelty of the paper is limited as Pre-LN placement has been used in prior work.",
                    "predicted_comment": "The authors reference important previous work, such as Vaswani et al., 2017 and Liu et al., 2019a, but could do a better job of explaining how their work expands on these studies. Specifically, the authors should clarify how their results differ from those of Vaswani et al., 2017 and Liu et al., 2019a, and why these differences are significant. This would help readers understand the novelty and significance of the authors' work."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 2,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The theoretical results do not clearly demonstrate that there is a problem with Post-LN and that this problem is fixed by switching to Pre-LN.",
                    "predicted_comment": "The paper provides a novel perspective on the Transformer architecture, focusing on the role of layer normalization and the learning rate warm-up stage. The authors' proposal to place layer normalization inside the residual blocks, creating the Pre-LN Transformer, is innovative and could have significant implications for the training of Transformer models. However, it would be helpful if the authors could provide more detail about the comparison between the Post-LN and Pre-LN Transformers, particularly in terms of their performance when stacking more layers. Additionally, the authors could clarify the explanation of the method and conduct additional experiments or analyses to further support their claims."
                },
                {
                    "reference_id": 3,
                    "predicted_id": 10,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper does not clearly explain the relationship between the theoretical results and the empirical study.",
                    "predicted_comment": "The inclusion of mathematical proofs of several lemmas related to the Transformer model is a strength of the paper. However, it would be beneficial if the authors could provide a more explicit connection between these lemmas and the performance differences between the Post-LN and Pre-LN Transformer models. For instance, the authors could discuss how the lemmas support the observations about the scale of the gradients and hidden states in the models, and how these factors contribute to the need for a learning rate warm-up stage in the Post-LN Transformer and the faster training of the Pre-LN Transformer."
                },
                {
                    "reference_id": 5,
                    "predicted_id": 10,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The connection between the expression that scales in the upper bound and the actual gradient norm is tenuous and there are multiple places where the argument could break down.",
                    "predicted_comment": "The inclusion of mathematical proofs of several lemmas related to the Transformer model is a strength of the paper. However, it would be beneficial if the authors could provide a more explicit connection between these lemmas and the performance differences between the Post-LN and Pre-LN Transformer models. For instance, the authors could discuss how the lemmas support the observations about the scale of the gradients and hidden states in the models, and how these factors contribute to the need for a learning rate warm-up stage in the Post-LN Transformer and the faster training of the Pre-LN Transformer."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 16,
                "n_generated": 2,
                "n_total_aligns": 1,
                "recall": 0.0625,
                "precision": 0.25,
                "pseudo_jaccard": 0.05555555555555555,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 16,
                "n_generated": 10,
                "n_total_aligns": 5,
                "recall": 0.3125,
                "precision": 0.2,
                "pseudo_jaccard": 0.15010141987829614,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 16,
                "n_generated": 4,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 16,
                "n_generated": 16,
                "n_total_aligns": 6,
                "recall": 0.375,
                "precision": 0.15625,
                "pseudo_jaccard": 0.13082039911308205,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.125,
                    "precision": 0.5,
                    "pseudo_jaccard": 0.1111111111111111,
                    "n_real": 8
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.125,
                    "precision": 0.1,
                    "pseudo_jaccard": 0.058823529411764705,
                    "n_real": 8
                },
                {
                    "recall": 0.5,
                    "precision": 0.3,
                    "pseudo_jaccard": 0.2413793103448276,
                    "n_real": 8
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                }
            ],
            "all": [
                {
                    "recall": 0.25,
                    "precision": 0.125,
                    "pseudo_jaccard": 0.09090909090909091,
                    "n_real": 8
                },
                {
                    "recall": 0.5,
                    "precision": 0.1875,
                    "pseudo_jaccard": 0.17073170731707318,
                    "n_real": 8
                }
            ]
        }
    },
    {
        "doc_id": "o2UwRc8fbXI",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper does not clearly state the main claims. The proposed method does not demonstrate a speed advantage or a clear performance boost.",
                "The reason for coupling the proposed method with CapsGCN is unclear, especially if the RL module is intended to tune the depth and width of convolutional layers.",
                "The paper does not clearly differentiate the proposed method from Neural Architecture Search (NAS).",
                "The motivations and experimental results presented in the paper are weak."
            ],
            [
                "The use of Reinforcement Learning (RL) during the training process to search for and modify the neural structures may create stability issues. The paper should provide some theoretical justifications or 'proofs' of convergence, or at the minimum, provide some empirical analysis on the stability of the training process.",
                "The evaluation of the proposed Adaptive Graph Capsule Convolutional Networks (AdaGCCN) could be more thorough. The paper should include ablation studies to show how effective the RL is in adapting the network structure, and discuss the runtime complexity, among other things."
            ],
            [
                "The motivations for the paper are not well discussed.",
                "The experimental methodology used in the paper is not well detailed, making it difficult to reproduce the reported results.",
                "The authors' claims are not always supported by the experimental evidence.",
                "The authors do not provide information about how the models considered during the experiments were validated. The policy used to validate the hyperparameters of a model highly impacts the model's performance.",
                "The authors do not report the standard deviation of the accuracies in table 5. This is crucial to correctly assess the model performance.",
                "The results in Table 5 suggest that using the AM to optimize the D and W does not justify the increase of complexity due to the use of this external model.",
                "The results reported in Table 5 are outperformed by results published in the literature for Enzymes datasets and in NCI.",
                "The performance of the model in terms of memory consumption is not completely discussed. Using AM seems to significantly impact the memory consumption and/or the training time of the model.",
                "The authors state that adjusting the model structure through an RL-based searching process improves the explainability of the model. This point should be further discussed with empirical results."
            ],
            [
                "The capsule graph neural network section (2.2) is somewhat shallow and could benefit from a more detailed introduction to the topics discussed to make the paper more self-contained.",
                "The paper lacks ablation experiments on the epoch sliding window size and choice of search space for D and W. It's also unclear how the method would perform without the capsule components, i.e. is the assistant module strategy practically useful for other architectures.",
                "There are concerns about reporting test accuracy throughout training in figure 3, as models could be overfit to the test set, rendering the results potentially incomparable to previous works.",
                "No standard deviations are reported in the results in table 2, and generally only marginal improvements are observed despite the increase in runtime complexity of the proposed method.",
                "The paper lacks meaningful runtime comparisons between the proposed method and the various previous works without the assistant module. For example, a comparison to a parallel cross validation version of CapsGNN in figure 4 is suggested.",
                "In the discussion, the authors claim the proposed approach improves the explainability of the model, but it's not clear how that is the case. The authors should either elaborate on this or exclude it from the paper.",
                "The purpose of using dynamic routing in capsule networks is to adaptively modify the network connectivity based on the input. Therefore, it might be possible to reduce the search space by only considering depth D and not width W, since dynamic routing already takes care of 'pruning' W. The authors should address this point."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper does a commendable job of proposing the Adaptive Graph Capsule Convolutional Networks (AdaGCCN) model, which uses reinforcement learning to adaptively adjust its architecture during training. The model is validated through a comprehensive set of experiments on ten benchmark datasets, and its performance is compared with nine baseline models, providing a robust evaluation. However, the paper could benefit from a more detailed discussion on the limitations of the AdaGCCN model and potential future work. Additionally, the authors should address the issues with some datasets exceeding the GPU memory capacity or taking too much time in training.",
                "The paper acknowledges the computation cost introduced by the assistant module and the manual setting of the \u2206D list and the \u2206W list in the current AM as potential limitations. The authors propose deploying multiple workers to compute in parallel on a GPU to mitigate the computation cost and suggest that some hyper-parameters in AdaGCCN could be tuned for better model performance. However, it is not clear whether these solutions have been implemented or tested. It would be beneficial if the authors could provide more details about the implementation and testing of these solutions, or suggest future work to address these limitations. This would give readers a clearer understanding of the feasibility and effectiveness of the proposed solutions, and the potential for improvement in future work.",
                "The paper would be significantly strengthened by a more detailed explanation of the \u2206D list and the \u2206W list in the current AM of the AdaGCCN. Specifically, it would be beneficial to understand the process by which these lists are determined, as they represent crucial changes in the depth and width of the model structure. Understanding this process would provide valuable insight into how these changes can impact the model's ability to express the features of graphs.",
                "While the paper discusses the use of parallel processing to alleviate computation overhead and provides some details on the scheduling strategy used to assign an appropriate number of workers, it would be helpful if the authors could provide more specific details on how the strategy determines the optimal number of workers. Understanding this aspect could provide more insight into the efficiency of the model and its scalability with respect to the number of workers.",
                "While the paper provides a comparison of AdaGCCN with other state-of-the-art models, it would be beneficial if the authors could delve deeper into the specific reasons why AdaGCCN outperforms these models on almost all datasets, and why it falls short on the PROTEINS dataset. A more detailed discussion on the impact of the complexity of inner graph relationships and the manual setting of the \u2206D list and the \u2206W list in the current AM on the model's performance would provide a more balanced view of the model's strengths and potential areas for improvement."
            ],
            "novelty_impact": [
                "The authors have justified their choice of CapsGNN as the base model and have shown that AdaGCCN outperforms CapsGNN and other baseline methods on various datasets. However, it would strengthen the argument for the novelty and significance of AdaGCCN if the authors could provide a comparative analysis against other GNN models that also use vector-based node embeddings. This would help validate the effectiveness of AdaGCCN's adaptive model architecture and its use of Reinforcement Learning.",
                "The paper assumes that the RL process in the assistant module will always find the optimal solution for adjusting the model architecture during training. However, there are several potential limitations of this process that are not discussed. For instance, the computational cost of the RL process could be significant, and it's unclear how the process would handle situations where there are multiple actions with the same maximal reward. The choice of reward function or the choice of state representation could also significantly affect the RL process. Additionally, the manual setting of the \u2206D list and the \u2206W list in the current AM could restrict the extraction ability of AdaGCCN. Discussing these limitations and how they could be addressed would significantly improve the paper.",
                "The paper discusses the impacts of the model's depth and width on its performance and proposes an adaptive method for tuning these parameters. However, it lacks specific empirical evidence or theoretical analysis to support these claims. Providing empirical evidence, such as results from experiments that show the performance of the model with different depths and widths, would strengthen these claims. Additionally, a theoretical analysis that explains why the adaptive method works would also be beneficial. This would give readers a better understanding of the model and its performance.",
                "The paper proposes a parallel computation strategy that assumes the availability of a multi-core GPU with sufficient memory to handle multiple workers. This assumption could limit the applicability of the solution to systems with a single-core GPU, without a GPU, or with limited GPU memory or computational power. Discussing alternative strategies for these systems, or providing a more detailed analysis of the computational resources required for the proposed strategy, would improve the paper.",
                "The paper provides a good overview of the AdaGCCN method and its benefits over the CapsGNN model. However, it would be beneficial to provide a more detailed cost-benefit analysis to justify the assumption that the benefits of AdaGCCN outweigh its computational costs. Specifically, it would be helpful to include more quantitative data on the computational costs and how they are mitigated by the use of parallel computing on a GPU. This could include, for example, a comparison of the computational time of AdaGCCN with and without the use of parallel computing. This would provide a more comprehensive understanding of the trade-offs involved and strengthen the paper's significance.",
                "The authors provide a theoretical analysis of the proposed AdaGCCN model and discuss its use of Reinforcement Learning to adaptively adjust the model architecture during training. They also provide some empirical evidence showing that the model can extract more information in a highly efficient way when training graphs in different fields. However, it would be helpful if the authors could provide more empirical evidence, such as experimental results or comparative studies, to fully support their claims about the effectiveness of the proposed optimizations.",
                "The paper presents a novel solution, AdaGCCN, to address the limitations of GCNs. However, the assumptions underlying this solution could be better justified. For instance, the assumption that the assistant module in AdaGCCN evaluates the loss decrease speed and the accuracy on the validation fold, which helps to select the \u2206D and \u2206W with the biggest accumulated reward, lacks a detailed justification. Providing empirical evidence or a theoretical basis for this assumption would strengthen the paper.",
                "The proposed solution, AdaGCCN, is justified by its ability to adaptively adjust the model architecture and use vector-valued neurons. However, a more detailed comparison with other potential solutions would help to demonstrate its superiority. In particular, the paper could discuss potential limitations of AdaGCCN and alternative approaches.",
                "The trade-off analysis in the paper mainly focuses on the computational overhead of the assistant module in AdaGCCN. While the authors propose a parallel computing strategy to mitigate this, a more comprehensive analysis of other potential trade-offs would be beneficial. For example, the paper could discuss the impact on model interpretability or the feasibility of the parallel computing strategy in different computational environments. Additionally, the paper acknowledges that AdaGCCN's performance on the PROTEINS dataset is restricted due to the complexity of inner graph relationships and the manual setting of the \u2206D list and the \u2206W list in the current AM. A detailed analysis of this trade-off and potential ways to mitigate this limitation would improve the paper."
            ],
            "clarity_reproducibility": [
                "The authors propose AdaGCCN to address the issue of latent information loss in GCNs and discuss the limitations of static designs in capsule-based GCNs. However, the specific claim that scalar-valued neurons in GCNs can lead to latent information loss is not directly supported with specific theoretical or empirical evidence. It would be beneficial for the authors to provide more direct evidence or a more detailed explanation to support this claim. This could include theoretical justifications or empirical results demonstrating the limitations of scalar-valued neurons in GCNs, and how these limitations lead to latent information loss.",
                "The paper discusses the RL mechanism of AdaGCCN and mentions that the Q-value is updated based on the loss and validation accuracy and under certain conditions. However, a detailed formula for calculating the Q-value is not provided. This omission could make it difficult for readers to fully understand the model and reproduce the results. Specifically, the paper should provide the formula for calculating the Q-value, explain how it is updated under different conditions, and clarify how the update on Q(a t i) is determined whether a t i is chosen at epoch t \u2212 s (s is the length of the epoch sliding window). Providing these details would enhance the clarity and reproducibility of the paper.",
                "The paper mentions the ten benchmark datasets used in the experiments in paragraph 38, but it does not provide specific details such as the number of samples, the number of features, or the distribution of classes. These details are said to be described in Table 1, but this table is not included in the parts of the paper that we have. Providing these details is crucial for understanding the experimental setup and for reproducing the experiments. Please ensure that Table 1 is included in the final version of the paper and that it provides detailed information about each of the ten datasets.",
                "While the paper provides a general overview of AdaGCCN's applications and results in bioinformatics and social fields, it would be beneficial to include more detailed results and specific applications. This would not only allow readers to better understand the extent of AdaGCCN's performance and potential but also provide a more comprehensive evaluation of whether the paper meets its goals and supports its claims. The inclusion of such details would significantly enhance the paper's value by providing clearer practical implications and demonstrating the effectiveness of AdaGCCN.",
                "The paper lacks explicit information about the reproducibility of the experiments. While it is mentioned that the authors reproduced a model for comparison, there is no clear indication of the availability of the code, data, or other resources used in the experiments. Providing this information is crucial for the scientific community to validate and build upon the findings. Therefore, the authors should include detailed information about the resources used and the steps taken to reproduce the experiments.",
                "The paper discusses the computational cost and efficiency of AdaGCCN and other models like GCNs, but it does not provide a direct comparison of these models. A direct comparison, including both time and space complexity, would be valuable for understanding the relative efficiency of AdaGCCN. This could involve comparing the time taken by AdaGCCN and other models to process the same datasets, as well as the GPU memory consumed by these models. This comparison is particularly important given that the paper introduces AdaGCCN as a model that simplifies the search space and eases the computation burden, and it would help to substantiate these claims.",
                "The paper should provide a more explicit list of the limitations of AdaGCCN. Specifically, it would be beneficial to discuss the additional computation overhead introduced by the assistant module, the potential restrictions caused by the manual setting of the \u2206D list and the \u2206W list, and the challenges faced when dealing with datasets that either exceed the GPU memory capacity or require excessive training time. This information is crucial for understanding the current state of AdaGCCN and for guiding future research."
            ],
            "all": [
                "The paper does a commendable job of proposing the Adaptive Graph Capsule Convolutional Networks (AdaGCCN) model, which uses reinforcement learning to adaptively adjust its architecture during training. The model is validated through a comprehensive set of experiments on ten benchmark datasets, and its performance is compared with nine baseline models, providing a robust evaluation. However, the paper could benefit from a more detailed discussion on the limitations of the AdaGCCN model and potential future work. Additionally, the authors should address the issues with some datasets exceeding the GPU memory capacity or taking too much time in training.",
                "The paper acknowledges the computation cost introduced by the assistant module and the manual setting of the \u2206D list and the \u2206W list in the current AM as potential limitations. The authors propose deploying multiple workers to compute in parallel on a GPU to mitigate the computation cost and suggest that some hyper-parameters in AdaGCCN could be tuned for better model performance. However, it is not clear whether these solutions have been implemented or tested. It would be beneficial if the authors could provide more details about the implementation and testing of these solutions, or suggest future work to address these limitations. This would give readers a clearer understanding of the feasibility and effectiveness of the proposed solutions, and the potential for improvement in future work.",
                "The paper would be significantly strengthened by a more detailed explanation of the \u2206D list and the \u2206W list in the current AM of the AdaGCCN. Specifically, it would be beneficial to understand the process by which these lists are determined, as they represent crucial changes in the depth and width of the model structure. Understanding this process would provide valuable insight into how these changes can impact the model's ability to express the features of graphs.",
                "While the paper discusses the use of parallel processing to alleviate computation overhead and provides some details on the scheduling strategy used to assign an appropriate number of workers, it would be helpful if the authors could provide more specific details on how the strategy determines the optimal number of workers. Understanding this aspect could provide more insight into the efficiency of the model and its scalability with respect to the number of workers.",
                "While the paper provides a comparison of AdaGCCN with other state-of-the-art models, it would be beneficial if the authors could delve deeper into the specific reasons why AdaGCCN outperforms these models on almost all datasets, and why it falls short on the PROTEINS dataset. A more detailed discussion on the impact of the complexity of inner graph relationships and the manual setting of the \u2206D list and the \u2206W list in the current AM on the model's performance would provide a more balanced view of the model's strengths and potential areas for improvement.",
                "The authors have justified their choice of CapsGNN as the base model and have shown that AdaGCCN outperforms CapsGNN and other baseline methods on various datasets. However, it would strengthen the argument for the novelty and significance of AdaGCCN if the authors could provide a comparative analysis against other GNN models that also use vector-based node embeddings. This would help validate the effectiveness of AdaGCCN's adaptive model architecture and its use of Reinforcement Learning.",
                "The paper assumes that the RL process in the assistant module will always find the optimal solution for adjusting the model architecture during training. However, there are several potential limitations of this process that are not discussed. For instance, the computational cost of the RL process could be significant, and it's unclear how the process would handle situations where there are multiple actions with the same maximal reward. The choice of reward function or the choice of state representation could also significantly affect the RL process. Additionally, the manual setting of the \u2206D list and the \u2206W list in the current AM could restrict the extraction ability of AdaGCCN. Discussing these limitations and how they could be addressed would significantly improve the paper.",
                "The paper discusses the impacts of the model's depth and width on its performance and proposes an adaptive method for tuning these parameters. However, it lacks specific empirical evidence or theoretical analysis to support these claims. Providing empirical evidence, such as results from experiments that show the performance of the model with different depths and widths, would strengthen these claims. Additionally, a theoretical analysis that explains why the adaptive method works would also be beneficial. This would give readers a better understanding of the model and its performance.",
                "The paper proposes a parallel computation strategy that assumes the availability of a multi-core GPU with sufficient memory to handle multiple workers. This assumption could limit the applicability of the solution to systems with a single-core GPU, without a GPU, or with limited GPU memory or computational power. Discussing alternative strategies for these systems, or providing a more detailed analysis of the computational resources required for the proposed strategy, would improve the paper.",
                "The paper provides a good overview of the AdaGCCN method and its benefits over the CapsGNN model. However, it would be beneficial to provide a more detailed cost-benefit analysis to justify the assumption that the benefits of AdaGCCN outweigh its computational costs. Specifically, it would be helpful to include more quantitative data on the computational costs and how they are mitigated by the use of parallel computing on a GPU. This could include, for example, a comparison of the computational time of AdaGCCN with and without the use of parallel computing. This would provide a more comprehensive understanding of the trade-offs involved and strengthen the paper's significance.",
                "The authors provide a theoretical analysis of the proposed AdaGCCN model and discuss its use of Reinforcement Learning to adaptively adjust the model architecture during training. They also provide some empirical evidence showing that the model can extract more information in a highly efficient way when training graphs in different fields. However, it would be helpful if the authors could provide more empirical evidence, such as experimental results or comparative studies, to fully support their claims about the effectiveness of the proposed optimizations.",
                "The paper presents a novel solution, AdaGCCN, to address the limitations of GCNs. However, the assumptions underlying this solution could be better justified. For instance, the assumption that the assistant module in AdaGCCN evaluates the loss decrease speed and the accuracy on the validation fold, which helps to select the \u2206D and \u2206W with the biggest accumulated reward, lacks a detailed justification. Providing empirical evidence or a theoretical basis for this assumption would strengthen the paper.",
                "The proposed solution, AdaGCCN, is justified by its ability to adaptively adjust the model architecture and use vector-valued neurons. However, a more detailed comparison with other potential solutions would help to demonstrate its superiority. In particular, the paper could discuss potential limitations of AdaGCCN and alternative approaches.",
                "The trade-off analysis in the paper mainly focuses on the computational overhead of the assistant module in AdaGCCN. While the authors propose a parallel computing strategy to mitigate this, a more comprehensive analysis of other potential trade-offs would be beneficial. For example, the paper could discuss the impact on model interpretability or the feasibility of the parallel computing strategy in different computational environments. Additionally, the paper acknowledges that AdaGCCN's performance on the PROTEINS dataset is restricted due to the complexity of inner graph relationships and the manual setting of the \u2206D list and the \u2206W list in the current AM. A detailed analysis of this trade-off and potential ways to mitigate this limitation would improve the paper.",
                "The authors propose AdaGCCN to address the issue of latent information loss in GCNs and discuss the limitations of static designs in capsule-based GCNs. However, the specific claim that scalar-valued neurons in GCNs can lead to latent information loss is not directly supported with specific theoretical or empirical evidence. It would be beneficial for the authors to provide more direct evidence or a more detailed explanation to support this claim. This could include theoretical justifications or empirical results demonstrating the limitations of scalar-valued neurons in GCNs, and how these limitations lead to latent information loss.",
                "The paper discusses the RL mechanism of AdaGCCN and mentions that the Q-value is updated based on the loss and validation accuracy and under certain conditions. However, a detailed formula for calculating the Q-value is not provided. This omission could make it difficult for readers to fully understand the model and reproduce the results. Specifically, the paper should provide the formula for calculating the Q-value, explain how it is updated under different conditions, and clarify how the update on Q(a t i) is determined whether a t i is chosen at epoch t \u2212 s (s is the length of the epoch sliding window). Providing these details would enhance the clarity and reproducibility of the paper.",
                "The paper mentions the ten benchmark datasets used in the experiments in paragraph 38, but it does not provide specific details such as the number of samples, the number of features, or the distribution of classes. These details are said to be described in Table 1, but this table is not included in the parts of the paper that we have. Providing these details is crucial for understanding the experimental setup and for reproducing the experiments. Please ensure that Table 1 is included in the final version of the paper and that it provides detailed information about each of the ten datasets.",
                "While the paper provides a general overview of AdaGCCN's applications and results in bioinformatics and social fields, it would be beneficial to include more detailed results and specific applications. This would not only allow readers to better understand the extent of AdaGCCN's performance and potential but also provide a more comprehensive evaluation of whether the paper meets its goals and supports its claims. The inclusion of such details would significantly enhance the paper's value by providing clearer practical implications and demonstrating the effectiveness of AdaGCCN.",
                "The paper lacks explicit information about the reproducibility of the experiments. While it is mentioned that the authors reproduced a model for comparison, there is no clear indication of the availability of the code, data, or other resources used in the experiments. Providing this information is crucial for the scientific community to validate and build upon the findings. Therefore, the authors should include detailed information about the resources used and the steps taken to reproduce the experiments.",
                "The paper discusses the computational cost and efficiency of AdaGCCN and other models like GCNs, but it does not provide a direct comparison of these models. A direct comparison, including both time and space complexity, would be valuable for understanding the relative efficiency of AdaGCCN. This could involve comparing the time taken by AdaGCCN and other models to process the same datasets, as well as the GPU memory consumed by these models. This comparison is particularly important given that the paper introduces AdaGCCN as a model that simplifies the search space and eases the computation burden, and it would help to substantiate these claims.",
                "The paper should provide a more explicit list of the limitations of AdaGCCN. Specifically, it would be beneficial to discuss the additional computation overhead introduced by the assistant module, the potential restrictions caused by the manual setting of the \u2206D list and the \u2206W list, and the challenges faced when dealing with datasets that either exceed the GPU memory capacity or require excessive training time. This information is crucial for understanding the current state of AdaGCCN and for guiding future research."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [
                {
                    "reference_id": 2,
                    "predicted_id": 10,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The authors' claims are not always supported by the experimental evidence.",
                    "predicted_comment": "The authors provide a theoretical analysis of the proposed AdaGCCN model and discuss its use of Reinforcement Learning to adaptively adjust the model architecture during training. They also provide some empirical evidence showing that the model can extract more information in a highly efficient way when training graphs in different fields. However, it would be helpful if the authors could provide more empirical evidence, such as experimental results or comparative studies, to fully support their claims about the effectiveness of the proposed optimizations."
                }
            ],
            "clarity_reproducibility": [
                {
                    "reference_id": 1,
                    "predicted_id": 18,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The experimental methodology used in the paper is not well detailed, making it difficult to reproduce the reported results.",
                    "predicted_comment": "The paper lacks explicit information about the reproducibility of the experiments. While it is mentioned that the authors reproduced a model for comparison, there is no clear indication of the availability of the code, data, or other resources used in the experiments. Providing this information is crucial for the scientific community to validate and build upon the findings. Therefore, the authors should include detailed information about the resources used and the steps taken to reproduce the experiments."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 16,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The experimental methodology used in the paper is not well detailed, making it difficult to reproduce the reported results.",
                    "predicted_comment": "The paper mentions the ten benchmark datasets used in the experiments in paragraph 38, but it does not provide specific details such as the number of samples, the number of features, or the distribution of classes. These details are said to be described in Table 1, but this table is not included in the parts of the paper that we have. Providing these details is crucial for understanding the experimental setup and for reproducing the experiments. Please ensure that Table 1 is included in the final version of the paper and that it provides detailed information about each of the ten datasets."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 14,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The authors' claims are not always supported by the experimental evidence.",
                    "predicted_comment": "The authors propose AdaGCCN to address the issue of latent information loss in GCNs and discuss the limitations of static designs in capsule-based GCNs. However, the specific claim that scalar-valued neurons in GCNs can lead to latent information loss is not directly supported with specific theoretical or empirical evidence. It would be beneficial for the authors to provide more direct evidence or a more detailed explanation to support this claim. This could include theoretical justifications or empirical results demonstrating the limitations of scalar-valued neurons in GCNs, and how these limitations lead to latent information loss."
                },
                {
                    "reference_id": 7,
                    "predicted_id": 19,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The performance of the model in terms of memory consumption is not completely discussed. Using AM seems to significantly impact the memory consumption and/or the training time of the model.",
                    "predicted_comment": "The paper discusses the computational cost and efficiency of AdaGCCN and other models like GCNs, but it does not provide a direct comparison of these models. A direct comparison, including both time and space complexity, would be valuable for understanding the relative efficiency of AdaGCCN. This could involve comparing the time taken by AdaGCCN and other models to process the same datasets, as well as the GPU memory consumed by these models. This comparison is particularly important given that the paper introduces AdaGCCN as a model that simplifies the search space and eases the computation burden, and it would help to substantiate these claims."
                },
                {
                    "reference_id": 4,
                    "predicted_id": 19,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper lacks meaningful runtime comparisons between the proposed method and the various previous works without the assistant module. For example, a comparison to a parallel cross validation version of CapsGNN in figure 4 is suggested.",
                    "predicted_comment": "The paper discusses the computational cost and efficiency of AdaGCCN and other models like GCNs, but it does not provide a direct comparison of these models. A direct comparison, including both time and space complexity, would be valuable for understanding the relative efficiency of AdaGCCN. This could involve comparing the time taken by AdaGCCN and other models to process the same datasets, as well as the GPU memory consumed by these models. This comparison is particularly important given that the paper introduces AdaGCCN as a model that simplifies the search space and eases the computation burden, and it would help to substantiate these claims."
                }
            ],
            "all": [
                {
                    "reference_id": 2,
                    "predicted_id": 10,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The authors' claims are not always supported by the experimental evidence.",
                    "predicted_comment": "The authors provide a theoretical analysis of the proposed AdaGCCN model and discuss its use of Reinforcement Learning to adaptively adjust the model architecture during training. They also provide some empirical evidence showing that the model can extract more information in a highly efficient way when training graphs in different fields. However, it would be helpful if the authors could provide more empirical evidence, such as experimental results or comparative studies, to fully support their claims about the effectiveness of the proposed optimizations."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 18,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The experimental methodology used in the paper is not well detailed, making it difficult to reproduce the reported results.",
                    "predicted_comment": "The paper lacks explicit information about the reproducibility of the experiments. While it is mentioned that the authors reproduced a model for comparison, there is no clear indication of the availability of the code, data, or other resources used in the experiments. Providing this information is crucial for the scientific community to validate and build upon the findings. Therefore, the authors should include detailed information about the resources used and the steps taken to reproduce the experiments."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 16,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The experimental methodology used in the paper is not well detailed, making it difficult to reproduce the reported results.",
                    "predicted_comment": "The paper mentions the ten benchmark datasets used in the experiments in paragraph 38, but it does not provide specific details such as the number of samples, the number of features, or the distribution of classes. These details are said to be described in Table 1, but this table is not included in the parts of the paper that we have. Providing these details is crucial for understanding the experimental setup and for reproducing the experiments. Please ensure that Table 1 is included in the final version of the paper and that it provides detailed information about each of the ten datasets."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 14,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The authors' claims are not always supported by the experimental evidence.",
                    "predicted_comment": "The authors propose AdaGCCN to address the issue of latent information loss in GCNs and discuss the limitations of static designs in capsule-based GCNs. However, the specific claim that scalar-valued neurons in GCNs can lead to latent information loss is not directly supported with specific theoretical or empirical evidence. It would be beneficial for the authors to provide more direct evidence or a more detailed explanation to support this claim. This could include theoretical justifications or empirical results demonstrating the limitations of scalar-valued neurons in GCNs, and how these limitations lead to latent information loss."
                },
                {
                    "reference_id": 7,
                    "predicted_id": 19,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The performance of the model in terms of memory consumption is not completely discussed. Using AM seems to significantly impact the memory consumption and/or the training time of the model.",
                    "predicted_comment": "The paper discusses the computational cost and efficiency of AdaGCCN and other models like GCNs, but it does not provide a direct comparison of these models. A direct comparison, including both time and space complexity, would be valuable for understanding the relative efficiency of AdaGCCN. This could involve comparing the time taken by AdaGCCN and other models to process the same datasets, as well as the GPU memory consumed by these models. This comparison is particularly important given that the paper introduces AdaGCCN as a model that simplifies the search space and eases the computation burden, and it would help to substantiate these claims."
                },
                {
                    "reference_id": 4,
                    "predicted_id": 19,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper lacks meaningful runtime comparisons between the proposed method and the various previous works without the assistant module. For example, a comparison to a parallel cross validation version of CapsGNN in figure 4 is suggested.",
                    "predicted_comment": "The paper discusses the computational cost and efficiency of AdaGCCN and other models like GCNs, but it does not provide a direct comparison of these models. A direct comparison, including both time and space complexity, would be valuable for understanding the relative efficiency of AdaGCCN. This could involve comparing the time taken by AdaGCCN and other models to process the same datasets, as well as the GPU memory consumed by these models. This comparison is particularly important given that the paper introduces AdaGCCN as a model that simplifies the search space and eases the computation burden, and it would help to substantiate these claims."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 22,
                "n_generated": 5,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 22,
                "n_generated": 9,
                "n_total_aligns": 1,
                "recall": 0.027777777777777776,
                "precision": 0.027777777777777776,
                "pseudo_jaccard": 0.014705882352941176,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 22,
                "n_generated": 7,
                "n_total_aligns": 5,
                "recall": 0.11904761904761904,
                "precision": 0.17857142857142855,
                "pseudo_jaccard": 0.08923076923076924,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "all": {
                "n_reference": 22,
                "n_generated": 21,
                "n_total_aligns": 6,
                "recall": 0.11904761904761904,
                "precision": 0.07142857142857142,
                "pseudo_jaccard": 0.04772079772079772,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 9
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.1111111111111111,
                    "precision": 0.1111111111111111,
                    "pseudo_jaccard": 0.058823529411764705,
                    "n_real": 9
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.5714285714285714,
                    "pseudo_jaccard": 0.28,
                    "n_real": 9
                },
                {
                    "recall": 0.14285714285714285,
                    "precision": 0.14285714285714285,
                    "pseudo_jaccard": 0.07692307692307693,
                    "n_real": 7
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.23809523809523808,
                    "pseudo_jaccard": 0.15384615384615385,
                    "n_real": 9
                },
                {
                    "recall": 0.14285714285714285,
                    "precision": 0.047619047619047616,
                    "pseudo_jaccard": 0.037037037037037035,
                    "n_real": 7
                }
            ]
        }
    },
    {
        "doc_id": "tJCwZBHm-jW",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The novelty of the paper is limited as the idea of transforming a 2D image pre-train backbone into a 3D version has already been used in other 2D-3D domains. The paper needs to place extra emphasis on discussing why it works and conducting ablation studies.",
                "The discussion section provides some experiments, but the results seem quite random (figure 4). The explanation for why 'shape representation' are better transferred is not convincing.",
                "The paper does not discuss if there is a difference on which axis to inflate (x, y, or z) when designing inflation. This is different to video since 3 axis is symmetric.",
                "The paper does not compare the parameter size in section 4.1. This raises concerns about the fairness of the comparison with the baseline, as the performance gain could be solely due to the increase in parameters.",
                "In section 4.2, there is a possibility that the ResNet structure is not a good structure to train on point cloud, making it quite bad on scratch training. The training method for pointnet++ is also questionable."
            ],
            [
                "The paper's visualizations do not adequately explain why the transfer works or what information can be transferred.",
                "The conclusion on page 8 that 'shape representations are better transferred from image to point-cloud' cannot be inferred from overall dataset performances. The term 'shape representations' is also not clearly defined."
            ],
            [
                "The paper lacks comparisons between results from datasets of different sizes in Table 1. It is unclear whether a larger dataset would lead to better performance.",
                "The paper does not include results of state-of-the-art task specific methods for comparison. It would be beneficial to compare with different self-supervised learning methods, such as Foldingnet and Pointcontrast.",
                "The paper does not provide a theoretical analysis to explain why inflating 2D filters to 3D is reasonable, given the significant domain gap between 2D and 3D datasets."
            ],
            [
                "The idea of inflating model weight from 2D to 3D is not new, as shown in the work of Carreira and Zisserman.",
                "The effectiveness of pretrained weight would be more convincing if the author could do further ablation study to initialize new model with expanding subsets of layers and exploring the effect on model performance. This would indicate which part of the pretrained model is really useful in this transferred learning setting."
            ],
            [
                "The paper is missing detailed information, making it difficult to follow the process. For example, in Table 1, the performances of pointnet++ and the performances are listed, but it's unclear how the inflation in point net++ is performed.",
                "The authors only perform experiments on classification and segmentation tasks. The performance on detection tasks should also be evaluated.",
                "The paper does not clearly address why the proposed method of inflating 2D CNN of image for 3D pointcloud works."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper provides some details about the adaptation of pretrained-image models for point-cloud recognition and the process of 'inflating' a 2D pretrained ConvNet and minimal finetuning. However, the specifics of these processes, including any specific techniques or parameters used, are not detailed in all parts of the paper. It would be beneficial for the authors to provide a more detailed explanation of these processes, including the specific techniques or parameters used in the finetuning processes (FIP-IO, FIP-IO+BN, FIP-ALL), the specific techniques used to adapt the pretrained-image models for point-cloud recognition, and the specific techniques used to 'inflate' a 2D pretrained ConvNet. Providing these details would enhance the reader's understanding of the paper and improve the reproducibility of the research.",
                "The paper provides some comparison against other models such as ResNet series, PointNet++, ViT, and SimpleView, and mentions the use of top-1 accuracy and mIoU as metrics. However, the comparison could be more detailed and clear. The authors should consider providing a table summarizing the comparison results, including the specific models used for comparison, the metrics used, and the results of these comparisons. This would make it easier for readers to understand the performance of the pretrained-image models against a wide range of point-cloud models.",
                "The paper mentions the use of top-1 accuracy and mIoU as performance metrics, particularly in the context of the FIP-IO+BN model and FIP-ALL's performance on various tasks. However, it lacks a detailed explanation of how these metrics were calculated and why they were chosen. This makes it difficult for readers to evaluate the validity of the results. The authors should provide a more detailed explanation of these metrics, including the specific calculations used and the rationale behind choosing these metrics over others. This will enhance the transparency of the methodology and allow readers to better assess the performance improvements.",
                "The paper provides detailed information about how the datasets were prepared and used for the experiments, including specific training and evaluation splits, the models trained, and the specific parameters and techniques used during training. However, it does not provide explicit details about how these datasets were collected. The authors should provide more details about the data collection process, including any specific techniques or parameters used.",
                "The paper provides a comprehensive set of experiments and includes ablation studies, which are commendable. The method components and their contributions are well-explained, and the rationale behind the method is clear. However, the paper could be further improved by discussing potential areas for improvement in the method. This would provide valuable insights for future research and could also help in refining the method for better performance.",
                "The paper presents a variety of experiments testing the models on different datasets and tasks, which is commendable. However, the presentation of these experiments could be improved to make it clearer to the reader the breadth and depth of the testing. This could be achieved by providing a summary table of the experiments, datasets/tasks, and results, and by discussing the implications of the diversity of testing in the conclusion."
            ],
            "novelty_impact": [
                "The paper uses ResNet as the base architecture for the experiments but does not provide an explicit justification for this choice. While the use of other architectures such as PointNet++, ViT, and SimpleView as baselines is noted, a more detailed discussion on why these or other architectures might also be suitable for this task would be beneficial. This would provide more context for the choice of ResNet and could make the methodology more robust.",
                "While the paper provides a theoretical foundation for the use of the same pretrained weights for both 2D images and 3D point-clouds, it could be strengthened by providing more detailed reasoning or evidence to support this approach. Specifically, the paper could benefit from a more in-depth discussion on why the low-level representations of 3D point-clouds and 2D images, despite being drastically different, can represent the same underlying visual concept. Additionally, while the experimental results are promising, it would be helpful to see a comparison with other research efforts that have tried to directly transfer models from images to point-clouds or vice versa. Providing specific references or results from these prior efforts would give more context to the paper's findings and help validate the proposed method.",
                "While the authors have compared their proposed method with several models including ResNet series, PointNet++, ViT (Vision Transformers), SimpleView, 3D ShapeNet, DeepPano, PointNet, MVCNN, DGCNN, KDNet, ViT-B-16 and ViT-L-16, it would be beneficial to see a comparison with the latest state-of-the-art models in the field. This would help demonstrate the competitiveness of the proposed method and provide a more comprehensive understanding of its performance relative to the most recent advancements. If there are specific reasons for not including certain recent models in the comparison, it would be helpful to mention this in the paper.",
                "The paper presents a series of experiments to understand why image pretraining can be utilized for point-cloud understanding. However, the presentation of the experiments and their results could be improved. Specifically, it would be beneficial to clearly link the results to the specific experiments and their design. This would make it easier for readers to understand the purpose of each experiment and how the results support the paper's conclusions. Additionally, providing a summary of the key findings from all the experiments in one section could help readers get a comprehensive understanding of the experimental results.",
                "The paper provides comparative results on the ModelNet 3D Warehouse dataset, which is commendable. However, to provide a more comprehensive evaluation of the model's performance, it would be beneficial to include more comparative results on the S3DIS and SemanticKITTI datasets. These additional results would provide valuable insights into the model's versatility and robustness across different types of 3D point-cloud tasks and environments.",
                "The paper provides a detailed discussion on the improvements in data efficiency and training speed brought about by the proposed method. However, it lacks an explicit discussion on the additional complexity introduced by the method. Including this discussion would provide a more balanced view of the method, helping readers understand not only its benefits but also the potential costs or challenges associated with its implementation. This could be addressed by discussing the computational complexity, the difficulty in implementation, or any other factors that might make the method more complex than other existing methods."
            ],
            "clarity_reproducibility": [
                "The paper describes the process of inflating 2D convolutional filters to 3D by copying and repeating the 2D filter K times along a third dimension. However, the specific value of K is not provided. This detail is crucial for reproducing the method because the value of K determines the shape of the 3D convolutional filter. Without this information, it is difficult to understand the exact process of converting 2D filters to 3D and to reproduce the method accurately. I recommend that the authors specify the value of K used in their experiments and discuss the implications of different values of K for the inflation technique and the performance of the method.",
                "The paper mentions three settings for finetuning: FIP-IO, FIP-IO+BN, and FIP-ALL. However, it lacks specific details about the steps and hyperparameters used in the finetuning process for these settings. Providing these details is crucial for others to reproduce the results and understand the differences between these settings. Specifically, the authors should include information such as the learning rate, batch size, number of epochs, and any other hyperparameters used in the finetuning process. Additionally, the authors should clarify the specific steps taken during finetuning for each setting.",
                "The paper provides some details about the size of the few-shot learning task, including the number of shots (1, 5, and 10) and the number of trials conducted for each setting (3). However, to fully understand the experimental setup and the context of the reported results, it would be helpful to include additional details. Specifically, the paper should specify the number of classes involved, the distribution of instances across these classes, the size of the training and test sets, the complexity of the task, the specific selection process for the point-clouds used in training, and how the trials were conducted. These details are important because they provide a clearer picture of the experimental conditions under which the reported results were obtained, which is crucial for assessing the validity and generalizability of the findings.",
                "The paper provides some information about the preprocessing steps and parameters used for each dataset, but this information is not consistently detailed or easy to find. The preprocessing steps and parameters are crucial for understanding the results and for reproducing the study, as they can significantly impact the performance of the models. Therefore, it is recommended that the authors provide more detailed and consistent information about the preprocessing steps and parameters for each dataset. This will make it easier for others to accurately replicate the experiments and verify the results.",
                "The paper mentions the use of multiple GPUs for training the models on different datasets, but does not provide specific details on the setup for parallel processing or distributed training. It would be beneficial to include information such as the software and libraries used, the configuration of the GPUs, and how the data was distributed across the GPUs. This information is necessary for others to reproduce the results and understand the computational requirements of the method. Without these details, it is difficult to assess the scalability of the method and its applicability to different computational environments.",
                "The paper provides some details about the SSG version of PointNet++ pretraining on ImageNet1K, including how images are broken into pixels and treated as point-clouds, the coordinates used, the center sampling number and radius for the first and second stage, and the number of neighboring points queried for each center point. However, it would be helpful to provide more explicit details about the SSG version, such as the specific features or advantages that contribute to the effectiveness of image-pretrained models for point-cloud recognition. This information is crucial for understanding the basis of the results obtained and for replicating the study.",
                "The paper mentions the use of pretrained weights from Dosovitskiy et al. (2020) in the ViT models, but does not provide sufficient detail about the specific steps or modifications made during this process. For example, it would be helpful to include information about the specific parameters used during finetuning, any changes made to the model architecture, and the criteria used to evaluate the performance of the modified models. Additionally, more information about the linear embedding used to project the point-cloud patches into a sequence would improve clarity and reproducibility. Without these details, it may be difficult for others to reproduce the results or understand the modifications made to the original models."
            ],
            "all": [
                "The paper provides some details about the adaptation of pretrained-image models for point-cloud recognition and the process of 'inflating' a 2D pretrained ConvNet and minimal finetuning. However, the specifics of these processes, including any specific techniques or parameters used, are not detailed in all parts of the paper. It would be beneficial for the authors to provide a more detailed explanation of these processes, including the specific techniques or parameters used in the finetuning processes (FIP-IO, FIP-IO+BN, FIP-ALL), the specific techniques used to adapt the pretrained-image models for point-cloud recognition, and the specific techniques used to 'inflate' a 2D pretrained ConvNet. Providing these details would enhance the reader's understanding of the paper and improve the reproducibility of the research.",
                "The paper provides some comparison against other models such as ResNet series, PointNet++, ViT, and SimpleView, and mentions the use of top-1 accuracy and mIoU as metrics. However, the comparison could be more detailed and clear. The authors should consider providing a table summarizing the comparison results, including the specific models used for comparison, the metrics used, and the results of these comparisons. This would make it easier for readers to understand the performance of the pretrained-image models against a wide range of point-cloud models.",
                "The paper mentions the use of top-1 accuracy and mIoU as performance metrics, particularly in the context of the FIP-IO+BN model and FIP-ALL's performance on various tasks. However, it lacks a detailed explanation of how these metrics were calculated and why they were chosen. This makes it difficult for readers to evaluate the validity of the results. The authors should provide a more detailed explanation of these metrics, including the specific calculations used and the rationale behind choosing these metrics over others. This will enhance the transparency of the methodology and allow readers to better assess the performance improvements.",
                "The paper provides detailed information about how the datasets were prepared and used for the experiments, including specific training and evaluation splits, the models trained, and the specific parameters and techniques used during training. However, it does not provide explicit details about how these datasets were collected. The authors should provide more details about the data collection process, including any specific techniques or parameters used.",
                "The paper provides a comprehensive set of experiments and includes ablation studies, which are commendable. The method components and their contributions are well-explained, and the rationale behind the method is clear. However, the paper could be further improved by discussing potential areas for improvement in the method. This would provide valuable insights for future research and could also help in refining the method for better performance.",
                "The paper presents a variety of experiments testing the models on different datasets and tasks, which is commendable. However, the presentation of these experiments could be improved to make it clearer to the reader the breadth and depth of the testing. This could be achieved by providing a summary table of the experiments, datasets/tasks, and results, and by discussing the implications of the diversity of testing in the conclusion.",
                "The paper uses ResNet as the base architecture for the experiments but does not provide an explicit justification for this choice. While the use of other architectures such as PointNet++, ViT, and SimpleView as baselines is noted, a more detailed discussion on why these or other architectures might also be suitable for this task would be beneficial. This would provide more context for the choice of ResNet and could make the methodology more robust.",
                "While the paper provides a theoretical foundation for the use of the same pretrained weights for both 2D images and 3D point-clouds, it could be strengthened by providing more detailed reasoning or evidence to support this approach. Specifically, the paper could benefit from a more in-depth discussion on why the low-level representations of 3D point-clouds and 2D images, despite being drastically different, can represent the same underlying visual concept. Additionally, while the experimental results are promising, it would be helpful to see a comparison with other research efforts that have tried to directly transfer models from images to point-clouds or vice versa. Providing specific references or results from these prior efforts would give more context to the paper's findings and help validate the proposed method.",
                "While the authors have compared their proposed method with several models including ResNet series, PointNet++, ViT (Vision Transformers), SimpleView, 3D ShapeNet, DeepPano, PointNet, MVCNN, DGCNN, KDNet, ViT-B-16 and ViT-L-16, it would be beneficial to see a comparison with the latest state-of-the-art models in the field. This would help demonstrate the competitiveness of the proposed method and provide a more comprehensive understanding of its performance relative to the most recent advancements. If there are specific reasons for not including certain recent models in the comparison, it would be helpful to mention this in the paper.",
                "The paper presents a series of experiments to understand why image pretraining can be utilized for point-cloud understanding. However, the presentation of the experiments and their results could be improved. Specifically, it would be beneficial to clearly link the results to the specific experiments and their design. This would make it easier for readers to understand the purpose of each experiment and how the results support the paper's conclusions. Additionally, providing a summary of the key findings from all the experiments in one section could help readers get a comprehensive understanding of the experimental results.",
                "The paper provides comparative results on the ModelNet 3D Warehouse dataset, which is commendable. However, to provide a more comprehensive evaluation of the model's performance, it would be beneficial to include more comparative results on the S3DIS and SemanticKITTI datasets. These additional results would provide valuable insights into the model's versatility and robustness across different types of 3D point-cloud tasks and environments.",
                "The paper provides a detailed discussion on the improvements in data efficiency and training speed brought about by the proposed method. However, it lacks an explicit discussion on the additional complexity introduced by the method. Including this discussion would provide a more balanced view of the method, helping readers understand not only its benefits but also the potential costs or challenges associated with its implementation. This could be addressed by discussing the computational complexity, the difficulty in implementation, or any other factors that might make the method more complex than other existing methods.",
                "The paper describes the process of inflating 2D convolutional filters to 3D by copying and repeating the 2D filter K times along a third dimension. However, the specific value of K is not provided. This detail is crucial for reproducing the method because the value of K determines the shape of the 3D convolutional filter. Without this information, it is difficult to understand the exact process of converting 2D filters to 3D and to reproduce the method accurately. I recommend that the authors specify the value of K used in their experiments and discuss the implications of different values of K for the inflation technique and the performance of the method.",
                "The paper mentions three settings for finetuning: FIP-IO, FIP-IO+BN, and FIP-ALL. However, it lacks specific details about the steps and hyperparameters used in the finetuning process for these settings. Providing these details is crucial for others to reproduce the results and understand the differences between these settings. Specifically, the authors should include information such as the learning rate, batch size, number of epochs, and any other hyperparameters used in the finetuning process. Additionally, the authors should clarify the specific steps taken during finetuning for each setting.",
                "The paper provides some details about the size of the few-shot learning task, including the number of shots (1, 5, and 10) and the number of trials conducted for each setting (3). However, to fully understand the experimental setup and the context of the reported results, it would be helpful to include additional details. Specifically, the paper should specify the number of classes involved, the distribution of instances across these classes, the size of the training and test sets, the complexity of the task, the specific selection process for the point-clouds used in training, and how the trials were conducted. These details are important because they provide a clearer picture of the experimental conditions under which the reported results were obtained, which is crucial for assessing the validity and generalizability of the findings.",
                "The paper provides some information about the preprocessing steps and parameters used for each dataset, but this information is not consistently detailed or easy to find. The preprocessing steps and parameters are crucial for understanding the results and for reproducing the study, as they can significantly impact the performance of the models. Therefore, it is recommended that the authors provide more detailed and consistent information about the preprocessing steps and parameters for each dataset. This will make it easier for others to accurately replicate the experiments and verify the results.",
                "The paper mentions the use of multiple GPUs for training the models on different datasets, but does not provide specific details on the setup for parallel processing or distributed training. It would be beneficial to include information such as the software and libraries used, the configuration of the GPUs, and how the data was distributed across the GPUs. This information is necessary for others to reproduce the results and understand the computational requirements of the method. Without these details, it is difficult to assess the scalability of the method and its applicability to different computational environments.",
                "The paper provides some details about the SSG version of PointNet++ pretraining on ImageNet1K, including how images are broken into pixels and treated as point-clouds, the coordinates used, the center sampling number and radius for the first and second stage, and the number of neighboring points queried for each center point. However, it would be helpful to provide more explicit details about the SSG version, such as the specific features or advantages that contribute to the effectiveness of image-pretrained models for point-cloud recognition. This information is crucial for understanding the basis of the results obtained and for replicating the study.",
                "The paper mentions the use of pretrained weights from Dosovitskiy et al. (2020) in the ViT models, but does not provide sufficient detail about the specific steps or modifications made during this process. For example, it would be helpful to include information about the specific parameters used during finetuning, any changes made to the model architecture, and the criteria used to evaluate the performance of the modified models. Additionally, more information about the linear embedding used to project the point-cloud patches into a sequence would improve clarity and reproducibility. Without these details, it may be difficult for others to reproduce the results or understand the modifications made to the original models."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper is missing detailed information, making it difficult to follow the process. For example, in Table 1, the performances of pointnet++ and the performances are listed, but it's unclear how the inflation in point net++ is performed.",
                    "predicted_comment": "The paper provides some details about the adaptation of pretrained-image models for point-cloud recognition and the process of 'inflating' a 2D pretrained ConvNet and minimal finetuning. However, the specifics of these processes, including any specific techniques or parameters used, are not detailed in all parts of the paper. It would be beneficial for the authors to provide a more detailed explanation of these processes, including the specific techniques or parameters used in the finetuning processes (FIP-IO, FIP-IO+BN, FIP-ALL), the specific techniques used to adapt the pretrained-image models for point-cloud recognition, and the specific techniques used to 'inflate' a 2D pretrained ConvNet. Providing these details would enhance the reader's understanding of the paper and improve the reproducibility of the research."
                }
            ],
            "novelty_impact": [
                {
                    "reference_id": 0,
                    "predicted_id": 7,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The novelty of the paper is limited as the idea of transforming a 2D image pre-train backbone into a 3D version has already been used in other 2D-3D domains. The paper needs to place extra emphasis on discussing why it works and conducting ablation studies.",
                    "predicted_comment": "While the paper provides a theoretical foundation for the use of the same pretrained weights for both 2D images and 3D point-clouds, it could be strengthened by providing more detailed reasoning or evidence to support this approach. Specifically, the paper could benefit from a more in-depth discussion on why the low-level representations of 3D point-clouds and 2D images, despite being drastically different, can represent the same underlying visual concept. Additionally, while the experimental results are promising, it would be helpful to see a comparison with other research efforts that have tried to directly transfer models from images to point-clouds or vice versa. Providing specific references or results from these prior efforts would give more context to the paper's findings and help validate the proposed method."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 7,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper does not provide a theoretical analysis to explain why inflating 2D filters to 3D is reasonable, given the significant domain gap between 2D and 3D datasets.",
                    "predicted_comment": "While the paper provides a theoretical foundation for the use of the same pretrained weights for both 2D images and 3D point-clouds, it could be strengthened by providing more detailed reasoning or evidence to support this approach. Specifically, the paper could benefit from a more in-depth discussion on why the low-level representations of 3D point-clouds and 2D images, despite being drastically different, can represent the same underlying visual concept. Additionally, while the experimental results are promising, it would be helpful to see a comparison with other research efforts that have tried to directly transfer models from images to point-clouds or vice versa. Providing specific references or results from these prior efforts would give more context to the paper's findings and help validate the proposed method."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 7,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper does not clearly address why the proposed method of inflating 2D CNN of image for 3D pointcloud works.",
                    "predicted_comment": "While the paper provides a theoretical foundation for the use of the same pretrained weights for both 2D images and 3D point-clouds, it could be strengthened by providing more detailed reasoning or evidence to support this approach. Specifically, the paper could benefit from a more in-depth discussion on why the low-level representations of 3D point-clouds and 2D images, despite being drastically different, can represent the same underlying visual concept. Additionally, while the experimental results are promising, it would be helpful to see a comparison with other research efforts that have tried to directly transfer models from images to point-clouds or vice versa. Providing specific references or results from these prior efforts would give more context to the paper's findings and help validate the proposed method."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 10,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The authors only perform experiments on classification and segmentation tasks. The performance on detection tasks should also be evaluated.",
                    "predicted_comment": "The paper provides comparative results on the ModelNet 3D Warehouse dataset, which is commendable. However, to provide a more comprehensive evaluation of the model's performance, it would be beneficial to include more comparative results on the S3DIS and SemanticKITTI datasets. These additional results would provide valuable insights into the model's versatility and robustness across different types of 3D point-cloud tasks and environments."
                }
            ],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 0,
                    "predicted_id": 7,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The novelty of the paper is limited as the idea of transforming a 2D image pre-train backbone into a 3D version has already been used in other 2D-3D domains. The paper needs to place extra emphasis on discussing why it works and conducting ablation studies.",
                    "predicted_comment": "While the paper provides a theoretical foundation for the use of the same pretrained weights for both 2D images and 3D point-clouds, it could be strengthened by providing more detailed reasoning or evidence to support this approach. Specifically, the paper could benefit from a more in-depth discussion on why the low-level representations of 3D point-clouds and 2D images, despite being drastically different, can represent the same underlying visual concept. Additionally, while the experimental results are promising, it would be helpful to see a comparison with other research efforts that have tried to directly transfer models from images to point-clouds or vice versa. Providing specific references or results from these prior efforts would give more context to the paper's findings and help validate the proposed method."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 7,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper does not provide a theoretical analysis to explain why inflating 2D filters to 3D is reasonable, given the significant domain gap between 2D and 3D datasets.",
                    "predicted_comment": "While the paper provides a theoretical foundation for the use of the same pretrained weights for both 2D images and 3D point-clouds, it could be strengthened by providing more detailed reasoning or evidence to support this approach. Specifically, the paper could benefit from a more in-depth discussion on why the low-level representations of 3D point-clouds and 2D images, despite being drastically different, can represent the same underlying visual concept. Additionally, while the experimental results are promising, it would be helpful to see a comparison with other research efforts that have tried to directly transfer models from images to point-clouds or vice versa. Providing specific references or results from these prior efforts would give more context to the paper's findings and help validate the proposed method."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 7,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper does not clearly address why the proposed method of inflating 2D CNN of image for 3D pointcloud works.",
                    "predicted_comment": "While the paper provides a theoretical foundation for the use of the same pretrained weights for both 2D images and 3D point-clouds, it could be strengthened by providing more detailed reasoning or evidence to support this approach. Specifically, the paper could benefit from a more in-depth discussion on why the low-level representations of 3D point-clouds and 2D images, despite being drastically different, can represent the same underlying visual concept. Additionally, while the experimental results are promising, it would be helpful to see a comparison with other research efforts that have tried to directly transfer models from images to point-clouds or vice versa. Providing specific references or results from these prior efforts would give more context to the paper's findings and help validate the proposed method."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 10,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The authors only perform experiments on classification and segmentation tasks. The performance on detection tasks should also be evaluated.",
                    "predicted_comment": "The paper provides comparative results on the ModelNet 3D Warehouse dataset, which is commendable. However, to provide a more comprehensive evaluation of the model's performance, it would be beneficial to include more comparative results on the S3DIS and SemanticKITTI datasets. These additional results would provide valuable insights into the model's versatility and robustness across different types of 3D point-cloud tasks and environments."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper is missing detailed information, making it difficult to follow the process. For example, in Table 1, the performances of pointnet++ and the performances are listed, but it's unclear how the inflation in point net++ is performed.",
                    "predicted_comment": "The paper provides some details about the adaptation of pretrained-image models for point-cloud recognition and the process of 'inflating' a 2D pretrained ConvNet and minimal finetuning. However, the specifics of these processes, including any specific techniques or parameters used, are not detailed in all parts of the paper. It would be beneficial for the authors to provide a more detailed explanation of these processes, including the specific techniques or parameters used in the finetuning processes (FIP-IO, FIP-IO+BN, FIP-ALL), the specific techniques used to adapt the pretrained-image models for point-cloud recognition, and the specific techniques used to 'inflate' a 2D pretrained ConvNet. Providing these details would enhance the reader's understanding of the paper and improve the reproducibility of the research."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 15,
                "n_generated": 6,
                "n_total_aligns": 1,
                "recall": 0.06666666666666667,
                "precision": 0.03333333333333333,
                "pseudo_jaccard": 0.025,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 15,
                "n_generated": 6,
                "n_total_aligns": 4,
                "recall": 0.24,
                "precision": 0.13333333333333333,
                "pseudo_jaccard": 0.10214285714285713,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 15,
                "n_generated": 7,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 15,
                "n_generated": 19,
                "n_total_aligns": 5,
                "recall": 0.30666666666666664,
                "precision": 0.05263157894736842,
                "pseudo_jaccard": 0.04979840906614362,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.125,
                    "n_real": 3
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.2,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.1,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.125,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.6666666666666666,
                    "precision": 0.3333333333333333,
                    "pseudo_jaccard": 0.2857142857142857,
                    "n_real": 3
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "all": [
                {
                    "recall": 0.2,
                    "precision": 0.05263157894736842,
                    "pseudo_jaccard": 0.043478260869565216,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.05263157894736842,
                    "pseudo_jaccard": 0.047619047619047616,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 1.0,
                    "precision": 0.15789473684210525,
                    "pseudo_jaccard": 0.15789473684210525,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "H1enKkrFDB",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The role of the partitioning index in the proposed algorithm is unclear, as it does not appear to be used in the experiments. The authors should clarify its importance and usage.",
                "The empirical evaluation of the normalization technique is insufficient to demonstrate its practical usefulness. The authors should focus more on showing how their regularization can improve generalization in practice.",
                "The authors should provide experiments on at least one additional dataset, such as CIFAR10 or ImageNet, to strengthen their empirical results.",
                "The reported accuracies for ResNet and DenseNet on the CIFAR100 dataset are significantly lower than what is reported in the literature. This issue needs to be resolved.",
                "The current results are based on training with a fixed number of epochs. Instead, the authors should train with a stopping criterion based on the cross-entropy loss on the training set and use the same stopping criterion for all models. They should also add plots showing training and test errors based on the number of epochs."
            ],
            [
                "The paper should characterize the runtime difference between SRN and SN. It is presently unclear how computationally intensive the method is. What is the difference in time per training iteration? The authors should also indicate their hardware setup and overall training time.",
                "Table 1 is confusing as it lacks the test error. If there is a difference in testing error, then this table is misleading. If the test errors are approximately the same, then why should I care if the training error is higher? This would just be a way to decrease the stated \u201cgeneralization gap,\u201d which is not necessarily indicative of a better model.",
                "The paper does not explain what \u201cStable-50\u201d, \u201cSRN-50\u201d, and \u201cSRN-50%\u201d are. This should be explicitly stated so that the reader knows which model corresponds to which settings."
            ],
            [
                "The paper could mention some related older introductory approaches, such as representing linear layers as 'bottlenecks' to enforce low rank explicitly, or solving in the manifold of reduced-rank matrices directly.",
                "The authors target the same stable rank (srank) r=c*min(m,n) for all layers, even though only the sum of sranks is important. It would be interesting to see if there is any observable difference for CNNs with only a few linear layers by lightly deviating from this.",
                "The paper could explore whether the first linear layer typically contributes the majority to the sum of sranks.",
                "The paper could discuss how 'stable rank' concepts might be extended to the convolutional layers. As a starting point, spectral values of the block-circulant matrices corresponding to convolutions have been described in previous work."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper could provide a more detailed explanation of how the proposed Stable Rank Normalization (SRN) scheme works. This would help readers understand the process and the mathematics behind it.",
                "The paper could provide a more detailed explanation or intuitive understanding of the Lipschitz constant and its role in the performance of neural networks. This would help readers understand why the Lipschitz constant is important and how it affects the performance of neural networks.",
                "The paper could provide a more detailed explanation of how the value of c is determined and how it affects the stable rank. This would help readers understand how the stable rank is controlled and how it affects the performance of neural networks.",
                "The paper could provide a clear, layman's explanation of these concepts. This would make the paper more accessible to readers without a strong mathematical background.",
                "While the paper discusses how controlling these quantities can improve the generalization behaviour of neural networks, it could provide a clear, step-by-step method for doing so. This would make it easier for readers to understand how to apply these concepts in practice.",
                "While the paper's primary focus is on Stable Rank Normalization (SRN), the influence of learning rates and weight decay parameters on the effectiveness of SRN could be further explored. Although these parameters may not be central to the paper's main arguments, a discussion on their choice and impact could provide additional insights into the conditions under which SRN is most effective. This could enhance the comprehensiveness of the paper and provide readers with a more nuanced understanding of the factors that can affect the performance of SRN.",
                "The paper provides a good discussion on the local Lipschitz upper-bound for neural networks and its implications on the performance of neural networks. However, it could be beneficial to include more details on how the choice of norms affects the local Lipschitz upper-bound and its implications on the performance of neural networks. For example, the paper could discuss how different norms might lead to different upper bounds and how this might affect the generalization error and the realistic generalization behaviour. This would provide a more comprehensive understanding of the local Lipschitz upper-bound and its role in neural networks.",
                "While the paper presents empirical Lipschitz plots to analyze the empirical Lipschitzness of the models and compare the performance of different models and methods, it would be beneficial to provide a more detailed interpretation of these plots. Specifically, it would be helpful to explain how to read these plots and what specific features or trends in the plots indicate about the Lipschitz constant and the performance of the neural networks. This would make the paper more accessible to readers who are not familiar with Lipschitz plots and would allow them to better understand the significance of the Lipschitz constant in the context of neural networks.",
                "While the paper provides some details on the use of Inception and FID scores for the evaluation of the generated samples in the context of GANs, it lacks a detailed discussion on the choice of these metrics and their implications. Specifically, it would be beneficial to include a discussion on why these metrics were chosen over others, what they specifically measure in the context of GANs, and any potential limitations or drawbacks of these metrics. This would provide a more comprehensive understanding of the evaluation process and could potentially strengthen the paper's arguments.",
                "The paper provides a good introduction to SRN-GAN and shows that it provides improved Neural divergence score and Inception and FID scores in most experiments. However, the discussion on the implications of these results on the performance of GANs could be more detailed. Specifically, it would be beneficial to include: 1. A comparison of SRN-GAN with other normalization methods used in GANs, including specific performance metrics that quantify the differences. 2. A deeper analysis of the reasons behind the improved performance of SRN-GAN, including a discussion on the stability of the training process of SRN-GAN compared to the other GANs. 3. A discussion on the practical implications of these results, such as in what scenarios might SRN-GAN be preferred over other GANs. 4. A more detailed analysis of the effect of different values of the stable rank on the performance of SRN-GAN. These additions would provide a clearer understanding of the significance of the improved scores and their impact on the overall performance of GANs."
            ],
            "novelty_impact": [
                "The paper provides a detailed comparison of Stable Rank Normalization (SRN) with other normalization methods and highlights its uniqueness and advantages. However, it would be beneficial to provide a more structured comparison, perhaps in the form of a table or a chart, to make it easier for readers to understand the uniqueness and advantages of SRN. Additionally, providing more quantitative results to support the claims made about SRN would strengthen the paper.",
                "The paper provides a definition of stable rank and discusses its role in improving the generalization behavior of neural networks and the inception score of GANs. However, a more detailed explanation of the concept of stable rank, including its mathematical properties and its relationship with the rank operator, would be beneficial for readers who are not familiar with this concept. Additionally, while the paper mentions that minimizing stable rank can improve the generalization behavior of neural networks and the inception score of GANs, it would be helpful to provide a more detailed explanation of why this is the case. This could include a discussion of how stable rank affects the Lipschitz constant and the implications of this for the performance of neural networks and GANs.",
                "The paper provides a commendable analysis across a wide variety of neural networks using the Stable Rank Normalization (SRN) method. However, it lacks specific details on the magnitude of the improvements observed in each type of network. For instance, while the paper mentions that SRN improves the generalization behavior and classification accuracy of various NN architectures, it does not provide specific figures or percentages to quantify these improvements. Similarly, while the paper discusses the use of different learning rates, weight decay rates, and the number of epochs for training various network models, it does not provide specific metrics or magnitudes of improvements for these networks. Providing these details would make it easier for readers to understand the specific benefits of using SRN and would strengthen the paper's claims about the effectiveness of this method.",
                "The paper provides a good discussion on the impact of Stable Rank Normalization (SRN) when applied to the discriminator of Generative Adversarial Networks (GANs), including improvements in Inception, FID, and Neural divergence scores, and learning mappings with a low empirical Lipschitz constant. However, the paper could benefit from a more detailed explanation of why these improvements are significant in the context of GANs. For instance, it would be helpful to understand how these improvements contribute to the overall performance of GANs, and why SRN is more effective than other weight-normalization schemes in achieving these improvements. This additional context would help readers better understand the value of SRN in the context of GANs.",
                "The introduction of the Empirical Lipschitz Constant (eLhist) metric is a significant contribution. However, the paper could benefit from a more detailed explanation of the relationship between the eLhist metric and the stable rank of the discriminator in the GAN, and how this relationship affects the inception score. Additionally, more information about how the eLhist metric is used to evaluate the effectiveness of the Stable Rank Normalization (SRN) method would be helpful. These details would provide a clearer understanding of the importance of the eLhist metric in the context of this study.",
                "The paper provides comprehensive mathematical proofs and derivations, particularly in relation to the Stable Rank Normalization (SRN) method and its application in neural networks. However, these concepts are presented in a technical manner that may be difficult for readers unfamiliar with these concepts to understand. While the relevance of these concepts to the study is explained, it is also done so in a technical manner. Therefore, it would be beneficial for the paper to include a brief, non-technical overview of these mathematical concepts and their relevance to the study. This would make the paper more accessible to a wider audience and help readers understand the motivation behind the SRN method and its potential benefits for neural networks.",
                "The paper presents an interesting discussion on the effect of rank on the empirical Lipschitz constants. However, the paper would greatly benefit from a more detailed mathematical explanation or proof of why low rank mappings favor low empirical Lipschitz constants. This would provide a stronger theoretical foundation for the proposed method. Additionally, the details of the simple two-layer linear-NN example, which is currently in Appendix B.2, could be included in the main text for better accessibility and understanding.",
                "While the paper provides extensive details on the network architectures used in the experiments, including DenseNet, WideResNet, ResNet, Alexnet, and VGG, it lacks a clear explanation of why these specific architectures were chosen. Given the diverse characteristics and widespread use of these architectures, it can be inferred that they were selected to allow for a comprehensive evaluation of the proposed method. However, explicitly stating the rationale behind the selection of these architectures would strengthen the paper. This would help readers understand whether the choice was driven by the need to test the method on a variety of architectures, or if there were specific characteristics of these architectures that made them particularly suitable for the experiments.",
                "The paper provides a detailed account of the experiments conducted on generalization and GAN objective functions, including the use of various measures and comparisons with other methods. However, it would be beneficial to include more interpretation or discussion of these results. For example, the paper could discuss why certain measures were chosen, how the results compare to expectations or previous research, and what the implications of the results are for the field. This would help readers understand the significance of the results and how they contribute to the field.",
                "The comparison between SRN-GAN and other GANs such as SN-GAN, WGAN-GP, and Ortho-GAN is well conducted, with SRN-GAN showing improved Inception, FID, and Neural divergence scores, and more stable discriminator training than SN-GAN. However, it would be beneficial to have a more detailed discussion on the specific advantages of SRN-GAN. For instance, the paper could elaborate on how the unique optimal solution of SRN contributes to its performance, how its generalizability compares to other GANs, and how the trade-off between a higher Lipschitz constant and a better inception score works in practice. Additionally, the paper could discuss why the norm is the same for the points in the vicinity of the real data points and the generated data points for SRN-GAN and WGAN-GP, but varies for SN-GAN.",
                "The paper presents complex concepts and experimental results that could benefit from additional visual aids. Specifically, the sections on Stable Rank Normalization, the application of SRN in various scenarios, and the effect of rank on empirical Lipschitz constants could be enhanced with tables or graphs comparing the performance of SRN with other methods. Additionally, the sections on the training of Generative Adversarial Networks and the effect of Stable Rank on eLhist and Inception Score could use visual aids to illustrate the comparisons of SRN-GAN with other GANs. These visual aids would help readers better understand the impact of different parameters on the models and their performance.",
                "The paper presents a novel weight-normalization scheme, Stable Rank Normalization (SRN), and demonstrates its effectiveness in improving the generalization behavior of neural networks and the performance of GANs. However, the practical implications of these findings are not clearly explained. The authors should provide a clear explanation of how the improvements in classification accuracy, generalization, and reduction in memorization brought about by SRN could impact real-world applications of neural networks. Additionally, the authors should clarify how SRN reduces memorization, as this is a key aspect of its proposed benefits.",
                "While the mathematical equations and proofs related to Lipschitz constant, Stable Rank, Singular Value Decomposition (SVD), and Stable Rank Normalization (SRN) are comprehensive, they could be made more accessible to readers who may not have a strong mathematical background. Specifically:\n\n1. The concept of the Lipschitz constant and its mathematical representation could be explained intuitively as a measure of how much the output of a function can change for a small change in its input.\n\n2. The concept of Stable Rank could be explained as a measure of the 'complexity' or 'size' of a matrix that is less sensitive to small perturbations.\n\n3. The concept of Singular Value Decomposition (SVD) could be illustrated with a visual representation of transforming a matrix.\n\n4. The Stable Rank Normalization (SRN) problem statement could be broken down into simpler steps and components.\n\n5. The proofs related to these concepts could be explained in a step-by-step manner, with each step and mathematical operation explained in simple language.\n\n6. Real-world examples or analogies could be provided to help readers understand the practical applications of these mathematical concepts.\n\nBy making these improvements, the paper would be more accessible to a wider range of readers."
            ],
            "clarity_reproducibility": [
                "The paper introduces the Stable Rank Normalization (SRN) method and the role of the partitioning index 'k' in the problem formulation. However, it lacks explicit guidelines on how to choose 'k'. The choice of 'k' influences the optimal solution to the spectral norm problem and, consequently, the results and reproducibility of the method. For instance, when 'k' is 0, the problem becomes non-convex, and when 'k' is greater than or equal to 1, the problem is convex. The paper could be improved by providing more explicit guidelines or criteria for choosing 'k', including the implications of different 'k' values on the results and reproducibility of the method. This would help readers better understand the SRN method and its application.",
                "The paper introduces the concept of Stable Rank Normalization (SRN) and its application in neural networks (NNs) to normalize both the stable rank and the spectral norm of each linear layer of a NN simultaneously. However, the specific process of applying SRN in this manner is not detailed enough for the reader to fully understand the process. It would be beneficial to provide a more comprehensive and step-by-step explanation of how SRN is applied to each linear layer of a NN. This could include a detailed walkthrough of Algorithm 1 and 2, and how these algorithms are used in the context of different NN architectures. It would also be helpful to include a section that discusses the challenges and potential solutions when applying SRN to different types of NNs.",
                "The paper provides a good introduction to the Empirical Lipschitz Constant (eLhist) and its usage in the analysis. However, it lacks a clear and specific explanation of how eLhist is calculated. This is a crucial aspect as it directly impacts the understanding of the analysis and results. I suggest the authors provide a detailed explanation or a step-by-step process of how eLhist is calculated. This will greatly improve the clarity and completeness of the paper.",
                "While the paper discusses the effects of Stable Rank on eLhist and Inception Score, the specific methodology used to measure and analyze these effects could be more clearly detailed. For instance, it is mentioned that 2,000 pairs of samples are used to create histograms of the empirical Lipschitz constant (eLhist), and that lowering the value of c improves the inception score. However, it would be helpful to provide more explicit details about the process of creating these histograms and how the inception score is calculated. Additionally, the paper could benefit from a more thorough explanation of how the effects of extreme reduction in the stable rank on the histogram and inception score were analyzed.",
                "The paper lacks specific details on the experimental setup for SRN-GAN, which are crucial for the reproducibility of the experiments. Specifically, the paper does not provide information on the hyperparameters used, the training and testing splits, and any data augmentation techniques used. Providing these details would allow other researchers to replicate the experiments and verify the results. Furthermore, it would help readers understand the conditions under which the proposed method performs as reported.",
                "The authors provide detailed proofs for the optimal stable rank normalization theorem, which appear to be a novel contribution of this paper. However, it is not clearly stated whether these proofs are based on existing mathematical theorems or are novel contributions. To improve the clarity of the paper, the authors could explicitly state this in the main text. This would help readers understand the novelty and significance of these proofs. Additionally, the authors could provide a brief overview of the proofs in the main text, with a reference to the appendix for the full details. This would make the paper more accessible to readers who are not familiar with the mathematical details.",
                "The paper mentions the calculation of the local Lipschitz constant using the Jacobian in several contexts, such as the 2-matrix norm, the Jacobian norm (Jac-Norm), and a two-layer linear neural network. However, it does not provide a clear and comprehensive explanation of how this calculation is done across these different contexts. Providing a more detailed explanation, including the specific method of calculation and the results of this calculation, would greatly enhance the reader's understanding of the paper's methodology and its conclusions about the generalization behaviour of neural networks. This is particularly important as the local Lipschitz constant appears to be a key part of the paper's methodology and its understanding of the generalization behaviour of neural networks.",
                "The section discussing empirical Lipschitz plots could benefit from a more detailed explanation. While the paper mentions its use in the context of Shattering Experiments and Jac-Norm, it does not provide a clear definition or explanation of what empirical Lipschitz is and how it is calculated. Providing this information would help readers better understand the methodology and the results.",
                "The section discussing the use of conditional GANs could benefit from a more detailed explanation. While the paper mentions its use in the context of empirical Lipschitz constants and discusses the use of conditional GANs with projection discriminator, it does not provide a clear explanation of what conditional GANs are and how they are used in this context. Providing this information would help readers better understand the methodology and the results.",
                "The paper lacks specific details on the experimental setups for the additional experiments on generalization and GAN objective functions. For the reproducibility of the experiments, it is crucial to provide information such as the exact configuration of the models, the number of epochs for training, the batch size, the datasets used, and the exact procedures for calculating the sample complexity measures. Providing these details would allow other researchers to replicate the experiments and validate the results."
            ],
            "all": [
                "The paper could provide a more detailed explanation of how the proposed Stable Rank Normalization (SRN) scheme works. This would help readers understand the process and the mathematics behind it.",
                "The paper could provide a more detailed explanation or intuitive understanding of the Lipschitz constant and its role in the performance of neural networks. This would help readers understand why the Lipschitz constant is important and how it affects the performance of neural networks.",
                "The paper could provide a more detailed explanation of how the value of c is determined and how it affects the stable rank. This would help readers understand how the stable rank is controlled and how it affects the performance of neural networks.",
                "The paper could provide a clear, layman's explanation of these concepts. This would make the paper more accessible to readers without a strong mathematical background.",
                "While the paper discusses how controlling these quantities can improve the generalization behaviour of neural networks, it could provide a clear, step-by-step method for doing so. This would make it easier for readers to understand how to apply these concepts in practice.",
                "While the paper's primary focus is on Stable Rank Normalization (SRN), the influence of learning rates and weight decay parameters on the effectiveness of SRN could be further explored. Although these parameters may not be central to the paper's main arguments, a discussion on their choice and impact could provide additional insights into the conditions under which SRN is most effective. This could enhance the comprehensiveness of the paper and provide readers with a more nuanced understanding of the factors that can affect the performance of SRN.",
                "The paper provides a good discussion on the local Lipschitz upper-bound for neural networks and its implications on the performance of neural networks. However, it could be beneficial to include more details on how the choice of norms affects the local Lipschitz upper-bound and its implications on the performance of neural networks. For example, the paper could discuss how different norms might lead to different upper bounds and how this might affect the generalization error and the realistic generalization behaviour. This would provide a more comprehensive understanding of the local Lipschitz upper-bound and its role in neural networks.",
                "While the paper presents empirical Lipschitz plots to analyze the empirical Lipschitzness of the models and compare the performance of different models and methods, it would be beneficial to provide a more detailed interpretation of these plots. Specifically, it would be helpful to explain how to read these plots and what specific features or trends in the plots indicate about the Lipschitz constant and the performance of the neural networks. This would make the paper more accessible to readers who are not familiar with Lipschitz plots and would allow them to better understand the significance of the Lipschitz constant in the context of neural networks.",
                "While the paper provides some details on the use of Inception and FID scores for the evaluation of the generated samples in the context of GANs, it lacks a detailed discussion on the choice of these metrics and their implications. Specifically, it would be beneficial to include a discussion on why these metrics were chosen over others, what they specifically measure in the context of GANs, and any potential limitations or drawbacks of these metrics. This would provide a more comprehensive understanding of the evaluation process and could potentially strengthen the paper's arguments.",
                "The paper provides a good introduction to SRN-GAN and shows that it provides improved Neural divergence score and Inception and FID scores in most experiments. However, the discussion on the implications of these results on the performance of GANs could be more detailed. Specifically, it would be beneficial to include: 1. A comparison of SRN-GAN with other normalization methods used in GANs, including specific performance metrics that quantify the differences. 2. A deeper analysis of the reasons behind the improved performance of SRN-GAN, including a discussion on the stability of the training process of SRN-GAN compared to the other GANs. 3. A discussion on the practical implications of these results, such as in what scenarios might SRN-GAN be preferred over other GANs. 4. A more detailed analysis of the effect of different values of the stable rank on the performance of SRN-GAN. These additions would provide a clearer understanding of the significance of the improved scores and their impact on the overall performance of GANs.",
                "The paper provides a detailed comparison of Stable Rank Normalization (SRN) with other normalization methods and highlights its uniqueness and advantages. However, it would be beneficial to provide a more structured comparison, perhaps in the form of a table or a chart, to make it easier for readers to understand the uniqueness and advantages of SRN. Additionally, providing more quantitative results to support the claims made about SRN would strengthen the paper.",
                "The paper provides a definition of stable rank and discusses its role in improving the generalization behavior of neural networks and the inception score of GANs. However, a more detailed explanation of the concept of stable rank, including its mathematical properties and its relationship with the rank operator, would be beneficial for readers who are not familiar with this concept. Additionally, while the paper mentions that minimizing stable rank can improve the generalization behavior of neural networks and the inception score of GANs, it would be helpful to provide a more detailed explanation of why this is the case. This could include a discussion of how stable rank affects the Lipschitz constant and the implications of this for the performance of neural networks and GANs.",
                "The paper provides a commendable analysis across a wide variety of neural networks using the Stable Rank Normalization (SRN) method. However, it lacks specific details on the magnitude of the improvements observed in each type of network. For instance, while the paper mentions that SRN improves the generalization behavior and classification accuracy of various NN architectures, it does not provide specific figures or percentages to quantify these improvements. Similarly, while the paper discusses the use of different learning rates, weight decay rates, and the number of epochs for training various network models, it does not provide specific metrics or magnitudes of improvements for these networks. Providing these details would make it easier for readers to understand the specific benefits of using SRN and would strengthen the paper's claims about the effectiveness of this method.",
                "The paper provides a good discussion on the impact of Stable Rank Normalization (SRN) when applied to the discriminator of Generative Adversarial Networks (GANs), including improvements in Inception, FID, and Neural divergence scores, and learning mappings with a low empirical Lipschitz constant. However, the paper could benefit from a more detailed explanation of why these improvements are significant in the context of GANs. For instance, it would be helpful to understand how these improvements contribute to the overall performance of GANs, and why SRN is more effective than other weight-normalization schemes in achieving these improvements. This additional context would help readers better understand the value of SRN in the context of GANs.",
                "The introduction of the Empirical Lipschitz Constant (eLhist) metric is a significant contribution. However, the paper could benefit from a more detailed explanation of the relationship between the eLhist metric and the stable rank of the discriminator in the GAN, and how this relationship affects the inception score. Additionally, more information about how the eLhist metric is used to evaluate the effectiveness of the Stable Rank Normalization (SRN) method would be helpful. These details would provide a clearer understanding of the importance of the eLhist metric in the context of this study.",
                "The paper provides comprehensive mathematical proofs and derivations, particularly in relation to the Stable Rank Normalization (SRN) method and its application in neural networks. However, these concepts are presented in a technical manner that may be difficult for readers unfamiliar with these concepts to understand. While the relevance of these concepts to the study is explained, it is also done so in a technical manner. Therefore, it would be beneficial for the paper to include a brief, non-technical overview of these mathematical concepts and their relevance to the study. This would make the paper more accessible to a wider audience and help readers understand the motivation behind the SRN method and its potential benefits for neural networks.",
                "The paper presents an interesting discussion on the effect of rank on the empirical Lipschitz constants. However, the paper would greatly benefit from a more detailed mathematical explanation or proof of why low rank mappings favor low empirical Lipschitz constants. This would provide a stronger theoretical foundation for the proposed method. Additionally, the details of the simple two-layer linear-NN example, which is currently in Appendix B.2, could be included in the main text for better accessibility and understanding.",
                "While the paper provides extensive details on the network architectures used in the experiments, including DenseNet, WideResNet, ResNet, Alexnet, and VGG, it lacks a clear explanation of why these specific architectures were chosen. Given the diverse characteristics and widespread use of these architectures, it can be inferred that they were selected to allow for a comprehensive evaluation of the proposed method. However, explicitly stating the rationale behind the selection of these architectures would strengthen the paper. This would help readers understand whether the choice was driven by the need to test the method on a variety of architectures, or if there were specific characteristics of these architectures that made them particularly suitable for the experiments.",
                "The paper provides a detailed account of the experiments conducted on generalization and GAN objective functions, including the use of various measures and comparisons with other methods. However, it would be beneficial to include more interpretation or discussion of these results. For example, the paper could discuss why certain measures were chosen, how the results compare to expectations or previous research, and what the implications of the results are for the field. This would help readers understand the significance of the results and how they contribute to the field.",
                "The comparison between SRN-GAN and other GANs such as SN-GAN, WGAN-GP, and Ortho-GAN is well conducted, with SRN-GAN showing improved Inception, FID, and Neural divergence scores, and more stable discriminator training than SN-GAN. However, it would be beneficial to have a more detailed discussion on the specific advantages of SRN-GAN. For instance, the paper could elaborate on how the unique optimal solution of SRN contributes to its performance, how its generalizability compares to other GANs, and how the trade-off between a higher Lipschitz constant and a better inception score works in practice. Additionally, the paper could discuss why the norm is the same for the points in the vicinity of the real data points and the generated data points for SRN-GAN and WGAN-GP, but varies for SN-GAN.",
                "The paper presents complex concepts and experimental results that could benefit from additional visual aids. Specifically, the sections on Stable Rank Normalization, the application of SRN in various scenarios, and the effect of rank on empirical Lipschitz constants could be enhanced with tables or graphs comparing the performance of SRN with other methods. Additionally, the sections on the training of Generative Adversarial Networks and the effect of Stable Rank on eLhist and Inception Score could use visual aids to illustrate the comparisons of SRN-GAN with other GANs. These visual aids would help readers better understand the impact of different parameters on the models and their performance.",
                "The paper presents a novel weight-normalization scheme, Stable Rank Normalization (SRN), and demonstrates its effectiveness in improving the generalization behavior of neural networks and the performance of GANs. However, the practical implications of these findings are not clearly explained. The authors should provide a clear explanation of how the improvements in classification accuracy, generalization, and reduction in memorization brought about by SRN could impact real-world applications of neural networks. Additionally, the authors should clarify how SRN reduces memorization, as this is a key aspect of its proposed benefits.",
                "While the mathematical equations and proofs related to Lipschitz constant, Stable Rank, Singular Value Decomposition (SVD), and Stable Rank Normalization (SRN) are comprehensive, they could be made more accessible to readers who may not have a strong mathematical background. Specifically:\n\n1. The concept of the Lipschitz constant and its mathematical representation could be explained intuitively as a measure of how much the output of a function can change for a small change in its input.\n\n2. The concept of Stable Rank could be explained as a measure of the 'complexity' or 'size' of a matrix that is less sensitive to small perturbations.\n\n3. The concept of Singular Value Decomposition (SVD) could be illustrated with a visual representation of transforming a matrix.\n\n4. The Stable Rank Normalization (SRN) problem statement could be broken down into simpler steps and components.\n\n5. The proofs related to these concepts could be explained in a step-by-step manner, with each step and mathematical operation explained in simple language.\n\n6. Real-world examples or analogies could be provided to help readers understand the practical applications of these mathematical concepts.\n\nBy making these improvements, the paper would be more accessible to a wider range of readers.",
                "The paper introduces the Stable Rank Normalization (SRN) method and the role of the partitioning index 'k' in the problem formulation. However, it lacks explicit guidelines on how to choose 'k'. The choice of 'k' influences the optimal solution to the spectral norm problem and, consequently, the results and reproducibility of the method. For instance, when 'k' is 0, the problem becomes non-convex, and when 'k' is greater than or equal to 1, the problem is convex. The paper could be improved by providing more explicit guidelines or criteria for choosing 'k', including the implications of different 'k' values on the results and reproducibility of the method. This would help readers better understand the SRN method and its application.",
                "The paper introduces the concept of Stable Rank Normalization (SRN) and its application in neural networks (NNs) to normalize both the stable rank and the spectral norm of each linear layer of a NN simultaneously. However, the specific process of applying SRN in this manner is not detailed enough for the reader to fully understand the process. It would be beneficial to provide a more comprehensive and step-by-step explanation of how SRN is applied to each linear layer of a NN. This could include a detailed walkthrough of Algorithm 1 and 2, and how these algorithms are used in the context of different NN architectures. It would also be helpful to include a section that discusses the challenges and potential solutions when applying SRN to different types of NNs.",
                "The paper provides a good introduction to the Empirical Lipschitz Constant (eLhist) and its usage in the analysis. However, it lacks a clear and specific explanation of how eLhist is calculated. This is a crucial aspect as it directly impacts the understanding of the analysis and results. I suggest the authors provide a detailed explanation or a step-by-step process of how eLhist is calculated. This will greatly improve the clarity and completeness of the paper.",
                "While the paper discusses the effects of Stable Rank on eLhist and Inception Score, the specific methodology used to measure and analyze these effects could be more clearly detailed. For instance, it is mentioned that 2,000 pairs of samples are used to create histograms of the empirical Lipschitz constant (eLhist), and that lowering the value of c improves the inception score. However, it would be helpful to provide more explicit details about the process of creating these histograms and how the inception score is calculated. Additionally, the paper could benefit from a more thorough explanation of how the effects of extreme reduction in the stable rank on the histogram and inception score were analyzed.",
                "The paper lacks specific details on the experimental setup for SRN-GAN, which are crucial for the reproducibility of the experiments. Specifically, the paper does not provide information on the hyperparameters used, the training and testing splits, and any data augmentation techniques used. Providing these details would allow other researchers to replicate the experiments and verify the results. Furthermore, it would help readers understand the conditions under which the proposed method performs as reported.",
                "The authors provide detailed proofs for the optimal stable rank normalization theorem, which appear to be a novel contribution of this paper. However, it is not clearly stated whether these proofs are based on existing mathematical theorems or are novel contributions. To improve the clarity of the paper, the authors could explicitly state this in the main text. This would help readers understand the novelty and significance of these proofs. Additionally, the authors could provide a brief overview of the proofs in the main text, with a reference to the appendix for the full details. This would make the paper more accessible to readers who are not familiar with the mathematical details.",
                "The paper mentions the calculation of the local Lipschitz constant using the Jacobian in several contexts, such as the 2-matrix norm, the Jacobian norm (Jac-Norm), and a two-layer linear neural network. However, it does not provide a clear and comprehensive explanation of how this calculation is done across these different contexts. Providing a more detailed explanation, including the specific method of calculation and the results of this calculation, would greatly enhance the reader's understanding of the paper's methodology and its conclusions about the generalization behaviour of neural networks. This is particularly important as the local Lipschitz constant appears to be a key part of the paper's methodology and its understanding of the generalization behaviour of neural networks.",
                "The section discussing empirical Lipschitz plots could benefit from a more detailed explanation. While the paper mentions its use in the context of Shattering Experiments and Jac-Norm, it does not provide a clear definition or explanation of what empirical Lipschitz is and how it is calculated. Providing this information would help readers better understand the methodology and the results.",
                "The section discussing the use of conditional GANs could benefit from a more detailed explanation. While the paper mentions its use in the context of empirical Lipschitz constants and discusses the use of conditional GANs with projection discriminator, it does not provide a clear explanation of what conditional GANs are and how they are used in this context. Providing this information would help readers better understand the methodology and the results.",
                "The paper lacks specific details on the experimental setups for the additional experiments on generalization and GAN objective functions. For the reproducibility of the experiments, it is crucial to provide information such as the exact configuration of the models, the number of epochs for training, the batch size, the datasets used, and the exact procedures for calculating the sample complexity measures. Providing these details would allow other researchers to replicate the experiments and validate the results."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [
                {
                    "reference_id": 1,
                    "predicted_id": 21,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The empirical evaluation of the normalization technique is insufficient to demonstrate its practical usefulness. The authors should focus more on showing how their regularization can improve generalization in practice.",
                    "predicted_comment": "The paper presents a novel weight-normalization scheme, Stable Rank Normalization (SRN), and demonstrates its effectiveness in improving the generalization behavior of neural networks and the performance of GANs. However, the practical implications of these findings are not clearly explained. The authors should provide a clear explanation of how the improvements in classification accuracy, generalization, and reduction in memorization brought about by SRN could impact real-world applications of neural networks. Additionally, the authors should clarify how SRN reduces memorization, as this is a key aspect of its proposed benefits."
                }
            ],
            "clarity_reproducibility": [
                {
                    "reference_id": 0,
                    "predicted_id": 23,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The role of the partitioning index in the proposed algorithm is unclear, as it does not appear to be used in the experiments. The authors should clarify its importance and usage.",
                    "predicted_comment": "The paper introduces the Stable Rank Normalization (SRN) method and the role of the partitioning index 'k' in the problem formulation. However, it lacks explicit guidelines on how to choose 'k'. The choice of 'k' influences the optimal solution to the spectral norm problem and, consequently, the results and reproducibility of the method. For instance, when 'k' is 0, the problem becomes non-convex, and when 'k' is greater than or equal to 1, the problem is convex. The paper could be improved by providing more explicit guidelines or criteria for choosing 'k', including the implications of different 'k' values on the results and reproducibility of the method. This would help readers better understand the SRN method and its application."
                }
            ],
            "all": [
                {
                    "reference_id": 0,
                    "predicted_id": 23,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The role of the partitioning index in the proposed algorithm is unclear, as it does not appear to be used in the experiments. The authors should clarify its importance and usage.",
                    "predicted_comment": "The paper introduces the Stable Rank Normalization (SRN) method and the role of the partitioning index 'k' in the problem formulation. However, it lacks explicit guidelines on how to choose 'k'. The choice of 'k' influences the optimal solution to the spectral norm problem and, consequently, the results and reproducibility of the method. For instance, when 'k' is 0, the problem becomes non-convex, and when 'k' is greater than or equal to 1, the problem is convex. The paper could be improved by providing more explicit guidelines or criteria for choosing 'k', including the implications of different 'k' values on the results and reproducibility of the method. This would help readers better understand the SRN method and its application."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 21,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The empirical evaluation of the normalization technique is insufficient to demonstrate its practical usefulness. The authors should focus more on showing how their regularization can improve generalization in practice.",
                    "predicted_comment": "The paper presents a novel weight-normalization scheme, Stable Rank Normalization (SRN), and demonstrates its effectiveness in improving the generalization behavior of neural networks and the performance of GANs. However, the practical implications of these findings are not clearly explained. The authors should provide a clear explanation of how the improvements in classification accuracy, generalization, and reduction in memorization brought about by SRN could impact real-world applications of neural networks. Additionally, the authors should clarify how SRN reduces memorization, as this is a key aspect of its proposed benefits."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 12,
                "n_generated": 10,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 12,
                "n_generated": 13,
                "n_total_aligns": 1,
                "recall": 0.06666666666666667,
                "precision": 0.025641025641025644,
                "pseudo_jaccard": 0.0196078431372549,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 12,
                "n_generated": 10,
                "n_total_aligns": 1,
                "recall": 0.06666666666666667,
                "precision": 0.03333333333333333,
                "pseudo_jaccard": 0.023809523809523808,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "all": {
                "n_reference": 12,
                "n_generated": 33,
                "n_total_aligns": 2,
                "recall": 0.13333333333333333,
                "precision": 0.020202020202020204,
                "pseudo_jaccard": 0.018518518518518517,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.2,
                    "precision": 0.07692307692307693,
                    "pseudo_jaccard": 0.058823529411764705,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.2,
                    "precision": 0.1,
                    "pseudo_jaccard": 0.07142857142857142,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                }
            ],
            "all": [
                {
                    "recall": 0.4,
                    "precision": 0.06060606060606061,
                    "pseudo_jaccard": 0.05555555555555555,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                }
            ]
        }
    },
    {
        "doc_id": "Qm7R_SdqTpT",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper should provide insights on why switching actions at some fixed number of frames beats using the Gaussian variance for FVD.",
                "The paper should discuss the importance of the heuristic of changing states when using GP, and how the performance would be affected if a different threshold is used."
            ],
            [
                "The experimental section could be improved. More details about how the comparison to some baselines was made would be appreciated.",
                "The results for the VRNN model in Figure 4 and 5 do not follow the results in the literature, where it outperforms SVG and SAVP. It's unclear whether this is due to an architectural change, suboptimal hyperparameters, or a different reimplementation.",
                "The VRNN model is missing from some other comparisons such as Table 1.",
                "For SAVP, the results for Figure 4 seem much worse than those reported in the original paper."
            ],
            [
                "The application of Gaussian Processes (GP) in this work is straightforward and naive. Dynamical modeling with GP has been widely investigated since the introduction of Gaussian Process Dynamical Model in NIPs 2005.",
                "The modules of LSTM Frame Generation and GP Frame Generation in Equation (4) are not clearly explained or depicted in Figure 3. It is unclear if these modules correspond to 'D' in Stage 3.",
                "The use of GP to generate images is questionable as GP is more suitable for working in the latent space.",
                "The datasets used in the study are not representative due to their simplicity and experimental nature.",
                "The paper does not demonstrate the utility of the proposed method for high-level research topics such as large-scale action recognition, video captioning, etc."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "Diversity Metrics: The authors have used traditional metrics like SSIM and PSNR, and they have also proposed an alternative evaluation strategy to assess diversity, using action classifiers and a metric inspired by (Villegas et al., 2017b) that utilizes a video classifier. However, it was not immediately clear that these were intended as diversity metrics. To improve the paper, the authors could clarify this point by explicitly stating that these are diversity metrics and explaining why they are suitable for evaluating the diversity of the generated sequences.",
                "The authors have conducted ablation studies for different variants of recurrent modules in their temporal dynamics encoder networks. However, the paper does not mention conducting ablation studies for the Gaussian Process (GP) and GP variance, which are used to model the diversity of future states and indicate whether an action sequence is ongoing or finished, respectively. Conducting and reporting ablation studies for these components could provide valuable insight into their contribution to the model's performance. This would help readers better understand the importance of these components in the model and could potentially improve the model's interpretability.",
                "The authors have used Gaussian Processes (GP) and LSTM temporal dynamics encoder to model the temporal dynamics of their model, and they have compared the performance of different dynamics models (RNN, GRU, LSTM) on various datasets. However, it is unclear if the authors have conducted an explicit analysis of the temporal dynamics of their model. Given the importance of temporal dynamics in the model's operation, particularly in leveraging the changing GP distribution over time to estimate when an on-going action sequence completes, an explicit analysis of these dynamics could be beneficial. This analysis could include a discussion of how the temporal dynamics affect the model's performance and a comparison of the model's performance with and without the temporal dynamics.",
                "Robustness Tests: The paper lacks explicit discussion and execution of robustness tests to evaluate the model's performance under different conditions. While the authors have used a variety of metrics and compared the model's performance with other baseline models across different datasets, it would be beneficial to conduct specific robustness tests. These could include testing the model's performance under different noise levels, varying input data distributions, or other challenging conditions. Such tests would provide more insight into the model's generalizability and reliability, and help to ensure that the model's performance is not overly dependent on specific conditions or datasets.",
                "Qualitative Analysis: The authors have provided qualitative results and visual examples in the 'Qualitative Results' section. However, the descriptions of these examples could be more detailed to better demonstrate the diversity and quality of the generated sequences. A more detailed analysis of these visual examples would also be beneficial. For instance, the authors could discuss in more depth how their method is able to switch between action classes in the KTH dataset, how their method generates samples much closer to the ground-truth in the BAIR dataset, and how their best LPIPS sample matches the ground-truth person's pose closely in the Human3.6M dataset."
            ],
            "novelty_impact": [
                "The paper uses a Gaussian Process (GP) layer in the GP Temporal Dynamics Encoder, which inherently assumes a normal distribution for future states. While the paper discusses the use of GPs and their scalability, it does not provide a justification for the specific choice of a GP layer in the GP Temporal Dynamics Encoder or discuss the implications of its inherent assumption of a normal distribution. This is a significant assumption that could affect the performance of the Diverse Video Generator (DVG). The authors should provide a justification for this specific choice and discuss the implications of its inherent assumption of a normal distribution. This would strengthen the paper by making the method more transparent and its assumptions more explicit.",
                "The paper lacks a discussion on the robustness of the Diverse Video Generator (DVG) method to noise in the data. While the paper mentions the use of Gaussian Processes, which have inherent noise handling capabilities, it does not specifically discuss how noise in the data affects the DVG. This is a significant omission, as understanding how a method handles noise is crucial for assessing its robustness and applicability in real-world scenarios. The authors should consider adding a discussion on this topic, and possibly conducting experiments under noisy conditions to provide empirical evidence of the DVG's robustness to noise.",
                "The paper provides a detailed explanation of the Diverse Video Generator (DVG) method, including its use of a Gaussian Process to model the diversity of future states. However, it lacks a discussion on the limitations of the DVG method. For a balanced view of the method, it would be beneficial to include a section discussing potential limitations and challenges of the DVG method. Additionally, the paper does not address how the DVG method handles outliers or anomalies in the data. This is an important aspect, as the presence of outliers or anomalies can significantly impact the performance of the method. The authors should consider adding a discussion on this topic to provide a more comprehensive understanding of the DVG method.",
                "The paper could provide more detailed information about the training and validation process of the action classifiers used in the evaluation strategy. Specifically, it would be helpful to know more about the datasets used for training and validation, the selection of hyperparameters, and the evaluation metrics used. This information is crucial for understanding the robustness of the classifiers and for reproducing the results. Additionally, the paper mentions potential bias in the evaluation of the UCF101 dataset and a critique of traditional metrics used for video prediction. It would be beneficial to discuss how these biases could have affected the results and how they were mitigated. This would help ensure the validity of the evaluation and the reliability of the results.",
                "The paper mentions the use of pseudo inputs in the Sparse Variational Gaussian Process (SVGP) for scalability, implying a trade-off between computational efficiency and accuracy. However, it does not discuss the potential approximation errors that might be introduced by this method and how they might affect the performance of the Diverse Video Generator (DVG). Discussing these potential errors would provide a more comprehensive understanding of the limitations of your approach. This is important for readers to fully understand the trade-offs involved in your method and to assess its performance accurately. Please add a discussion on this topic in your paper.",
                "The paper provides some information about the hyperparameters used in the GP layer implementation and optimization process, such as the number of inducing points and the use of an RBF kernel and a gaussian likelihood. However, it would be beneficial to provide more specific details about these hyperparameters, such as their ranges or the rationale for their selection. Additionally, the paper does not discuss how sensitive the performance of the DVG is to these hyperparameters. Including such a discussion could help readers understand how robust the method is to changes in the hyperparameters and could aid in replication efforts."
            ],
            "clarity_reproducibility": [],
            "all": [
                "Diversity Metrics: The authors have used traditional metrics like SSIM and PSNR, and they have also proposed an alternative evaluation strategy to assess diversity, using action classifiers and a metric inspired by (Villegas et al., 2017b) that utilizes a video classifier. However, it was not immediately clear that these were intended as diversity metrics. To improve the paper, the authors could clarify this point by explicitly stating that these are diversity metrics and explaining why they are suitable for evaluating the diversity of the generated sequences.",
                "The authors have conducted ablation studies for different variants of recurrent modules in their temporal dynamics encoder networks. However, the paper does not mention conducting ablation studies for the Gaussian Process (GP) and GP variance, which are used to model the diversity of future states and indicate whether an action sequence is ongoing or finished, respectively. Conducting and reporting ablation studies for these components could provide valuable insight into their contribution to the model's performance. This would help readers better understand the importance of these components in the model and could potentially improve the model's interpretability.",
                "The authors have used Gaussian Processes (GP) and LSTM temporal dynamics encoder to model the temporal dynamics of their model, and they have compared the performance of different dynamics models (RNN, GRU, LSTM) on various datasets. However, it is unclear if the authors have conducted an explicit analysis of the temporal dynamics of their model. Given the importance of temporal dynamics in the model's operation, particularly in leveraging the changing GP distribution over time to estimate when an on-going action sequence completes, an explicit analysis of these dynamics could be beneficial. This analysis could include a discussion of how the temporal dynamics affect the model's performance and a comparison of the model's performance with and without the temporal dynamics.",
                "Robustness Tests: The paper lacks explicit discussion and execution of robustness tests to evaluate the model's performance under different conditions. While the authors have used a variety of metrics and compared the model's performance with other baseline models across different datasets, it would be beneficial to conduct specific robustness tests. These could include testing the model's performance under different noise levels, varying input data distributions, or other challenging conditions. Such tests would provide more insight into the model's generalizability and reliability, and help to ensure that the model's performance is not overly dependent on specific conditions or datasets.",
                "Qualitative Analysis: The authors have provided qualitative results and visual examples in the 'Qualitative Results' section. However, the descriptions of these examples could be more detailed to better demonstrate the diversity and quality of the generated sequences. A more detailed analysis of these visual examples would also be beneficial. For instance, the authors could discuss in more depth how their method is able to switch between action classes in the KTH dataset, how their method generates samples much closer to the ground-truth in the BAIR dataset, and how their best LPIPS sample matches the ground-truth person's pose closely in the Human3.6M dataset.",
                "The paper uses a Gaussian Process (GP) layer in the GP Temporal Dynamics Encoder, which inherently assumes a normal distribution for future states. While the paper discusses the use of GPs and their scalability, it does not provide a justification for the specific choice of a GP layer in the GP Temporal Dynamics Encoder or discuss the implications of its inherent assumption of a normal distribution. This is a significant assumption that could affect the performance of the Diverse Video Generator (DVG). The authors should provide a justification for this specific choice and discuss the implications of its inherent assumption of a normal distribution. This would strengthen the paper by making the method more transparent and its assumptions more explicit.",
                "The paper lacks a discussion on the robustness of the Diverse Video Generator (DVG) method to noise in the data. While the paper mentions the use of Gaussian Processes, which have inherent noise handling capabilities, it does not specifically discuss how noise in the data affects the DVG. This is a significant omission, as understanding how a method handles noise is crucial for assessing its robustness and applicability in real-world scenarios. The authors should consider adding a discussion on this topic, and possibly conducting experiments under noisy conditions to provide empirical evidence of the DVG's robustness to noise.",
                "The paper provides a detailed explanation of the Diverse Video Generator (DVG) method, including its use of a Gaussian Process to model the diversity of future states. However, it lacks a discussion on the limitations of the DVG method. For a balanced view of the method, it would be beneficial to include a section discussing potential limitations and challenges of the DVG method. Additionally, the paper does not address how the DVG method handles outliers or anomalies in the data. This is an important aspect, as the presence of outliers or anomalies can significantly impact the performance of the method. The authors should consider adding a discussion on this topic to provide a more comprehensive understanding of the DVG method.",
                "The paper could provide more detailed information about the training and validation process of the action classifiers used in the evaluation strategy. Specifically, it would be helpful to know more about the datasets used for training and validation, the selection of hyperparameters, and the evaluation metrics used. This information is crucial for understanding the robustness of the classifiers and for reproducing the results. Additionally, the paper mentions potential bias in the evaluation of the UCF101 dataset and a critique of traditional metrics used for video prediction. It would be beneficial to discuss how these biases could have affected the results and how they were mitigated. This would help ensure the validity of the evaluation and the reliability of the results.",
                "The paper mentions the use of pseudo inputs in the Sparse Variational Gaussian Process (SVGP) for scalability, implying a trade-off between computational efficiency and accuracy. However, it does not discuss the potential approximation errors that might be introduced by this method and how they might affect the performance of the Diverse Video Generator (DVG). Discussing these potential errors would provide a more comprehensive understanding of the limitations of your approach. This is important for readers to fully understand the trade-offs involved in your method and to assess its performance accurately. Please add a discussion on this topic in your paper.",
                "The paper provides some information about the hyperparameters used in the GP layer implementation and optimization process, such as the number of inducing points and the use of an RBF kernel and a gaussian likelihood. However, it would be beneficial to provide more specific details about these hyperparameters, such as their ranges or the rationale for their selection. Additionally, the paper does not discuss how sensitive the performance of the DVG is to these hyperparameters. Including such a discussion could help readers understand how robust the method is to changes in the hyperparameters and could aid in replication efforts."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 0,
                    "predicted_id": 2,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The application of Gaussian Processes (GP) in this work is straightforward and naive. Dynamical modeling with GP has been widely investigated since the introduction of Gaussian Process Dynamical Model in NIPs 2005.",
                    "predicted_comment": "The authors have used Gaussian Processes (GP) and LSTM temporal dynamics encoder to model the temporal dynamics of their model, and they have compared the performance of different dynamics models (RNN, GRU, LSTM) on various datasets. However, it is unclear if the authors have conducted an explicit analysis of the temporal dynamics of their model. Given the importance of temporal dynamics in the model's operation, particularly in leveraging the changing GP distribution over time to estimate when an on-going action sequence completes, an explicit analysis of these dynamics could be beneficial. This analysis could include a discussion of how the temporal dynamics affect the model's performance and a comparison of the model's performance with and without the temporal dynamics."
                }
            ],
            "novelty_impact": [
                {
                    "reference_id": 2,
                    "predicted_id": 5,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The use of GP to generate images is questionable as GP is more suitable for working in the latent space.",
                    "predicted_comment": "The paper uses a Gaussian Process (GP) layer in the GP Temporal Dynamics Encoder, which inherently assumes a normal distribution for future states. While the paper discusses the use of GPs and their scalability, it does not provide a justification for the specific choice of a GP layer in the GP Temporal Dynamics Encoder or discuss the implications of its inherent assumption of a normal distribution. This is a significant assumption that could affect the performance of the Diverse Video Generator (DVG). The authors should provide a justification for this specific choice and discuss the implications of its inherent assumption of a normal distribution. This would strengthen the paper by making the method more transparent and its assumptions more explicit."
                }
            ],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 2,
                    "predicted_id": 5,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The use of GP to generate images is questionable as GP is more suitable for working in the latent space.",
                    "predicted_comment": "The paper uses a Gaussian Process (GP) layer in the GP Temporal Dynamics Encoder, which inherently assumes a normal distribution for future states. While the paper discusses the use of GPs and their scalability, it does not provide a justification for the specific choice of a GP layer in the GP Temporal Dynamics Encoder or discuss the implications of its inherent assumption of a normal distribution. This is a significant assumption that could affect the performance of the Diverse Video Generator (DVG). The authors should provide a justification for this specific choice and discuss the implications of its inherent assumption of a normal distribution. This would strengthen the paper by making the method more transparent and its assumptions more explicit."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 2,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The application of Gaussian Processes (GP) in this work is straightforward and naive. Dynamical modeling with GP has been widely investigated since the introduction of Gaussian Process Dynamical Model in NIPs 2005.",
                    "predicted_comment": "The authors have used Gaussian Processes (GP) and LSTM temporal dynamics encoder to model the temporal dynamics of their model, and they have compared the performance of different dynamics models (RNN, GRU, LSTM) on various datasets. However, it is unclear if the authors have conducted an explicit analysis of the temporal dynamics of their model. Given the importance of temporal dynamics in the model's operation, particularly in leveraging the changing GP distribution over time to estimate when an on-going action sequence completes, an explicit analysis of these dynamics could be beneficial. This analysis could include a discussion of how the temporal dynamics affect the model's performance and a comparison of the model's performance with and without the temporal dynamics."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 11,
                "n_generated": 5,
                "n_total_aligns": 1,
                "recall": 0.06666666666666667,
                "precision": 0.06666666666666667,
                "pseudo_jaccard": 0.037037037037037035,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 11,
                "n_generated": 6,
                "n_total_aligns": 1,
                "recall": 0.06666666666666667,
                "precision": 0.05555555555555555,
                "pseudo_jaccard": 0.03333333333333333,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 11,
                "n_generated": 0,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 11,
                "n_generated": 11,
                "n_total_aligns": 2,
                "recall": 0.13333333333333333,
                "precision": 0.06060606060606061,
                "pseudo_jaccard": 0.047619047619047616,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.2,
                    "precision": 0.2,
                    "pseudo_jaccard": 0.1111111111111111,
                    "n_real": 5
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.2,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.1,
                    "n_real": 5
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.4,
                    "precision": 0.18181818181818182,
                    "pseudo_jaccard": 0.14285714285714285,
                    "n_real": 5
                }
            ]
        }
    },
    {
        "doc_id": "K5j7D81ABvt",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper does not clarify whether 'S_F = S_sTEX' refers to exact string equality or after white space normalization. The authors should specify the exact normalizations and provide the success rate before and after them.",
                "The paper does not discuss the potential impact of using larger GPT models.",
                "The paper does not explore the potential usefulness of unsupervised learning methods, which have recently seen significant improvements.",
                "The term 'fully disambiguated' is used in definition 4.1 without considering the types of variables. In systems with subtypes, the meaning and provability of a statement can change depending on the quantification over different types of numbers."
            ],
            [
                "The current benchmark lacks sufficient training and evaluation data. The lack of training corpora may be addressed by pretraining and building synthetic data.",
                "A larger and high-quality evaluation set is needed to validate any actual progress on this problem. The current evaluation set is too small and covers limited math topics.",
                "The evaluation protocol is unclear. The best evaluation protocol should be checking if S_F belongs to STEX(S_STEX), which is not used in this work. The authors should consider implementing this protocol.",
                "An ablation study on the corresponding contributions of pretraining and synthetic data is needed for the proposed approach."
            ],
            [
                "The paper does not compare the proposed method against other systems listed in section 4 that also aim to automate the formalization process.",
                "The choice of phrasing the task as a translation problem is questionable, as it seems to set up the NMT baselines to perform poorly due to the identity transformation required for much of the document.",
                "The evaluation methodology is confusing, particularly the use of automated procedures to generate some of the data. This raises questions about why these automated methods are not sufficient for the end task.",
                "The results section does not present the results of the baselines, making it difficult to assess their performance relative to the proposed method.",
                "The paper does not provide a clear understanding of the problem setting, what is being learned, and what is failing. This makes it difficult to assess the importance and difficulty of the targeted problem.",
                "The experimental section is short and lacks important comparisons to other work in this area.",
                "The model is used in a very off-the-shelf way and does not treat the problem setting any differently than a standard translation task, which raises questions about the novelty of the modeling contribution.",
                "The paper uses a lot of domain-specific terminology that may be difficult for a general ML audience to understand. For example, the terms 'formal' and 'informal' are used in a way that is not immediately clear.",
                "The paper needs major restructuring and refocusing, more experiments, and more analysis to be considered for acceptance."
            ],
            [
                "The problem definition was not clear. A concrete example demonstrating what exactly an algorithm for the problem gets as input and what is its output is missing.",
                "The description of the dataset was unclear. While there is a formal description, the lack of examples leaves the description at a very abstract level, making it difficult to understand what exactly should be expected in the dataset.",
                "The authors do not show awareness of work in semantic parsing and in style transfer. These works are very important both for the algorithmic approach and for understanding the challenges of evaluation. For example, there is lots of semantic parsing research on transferring text into SQL queries or on solving textual mathematical problems. The connection to relevant ideas and approaches should be part of the introduction of the task."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "Ablation Study: The paper presents a method with multiple components, but it lacks an ablation study to analyze the contribution of each component to the overall performance. An ablation study would provide valuable insights into the importance of each component and could potentially lead to further improvements. For example, it could help identify which components are critical for achieving the reported 96.9% success rate for the returned sentence being in L, and which components contribute to the 64.0% success rate for S L A T E X being in L A T E X(S F ). This would not only enhance the understanding of the method's workings but also guide future research in this area.",
                "Performance Metrics: While the paper does mention the performance metrics used to evaluate the proposed method, it does not provide clear definitions or justifications for these metrics. It would be helpful to explicitly define each metric and explain why it was chosen over traditional metrics such as loss during evaluation, perplexity, and BLEU. This would provide a clear measure of the method's success and allow for more meaningful comparisons with other methods.",
                "The paper presents a promising method for disambiguating symbolic expressions in informal STEM documents. However, it lacks a discussion on the generalization ability of the proposed method. It would be beneficial to include a section discussing how well the method is expected to perform on unseen data, especially given the small size of the dataset used for evaluation. This would provide readers with a better understanding of the method's potential applicability and robustness in different contexts.",
                "Scalability: The paper lacks a discussion on the scalability of the proposed method. This is a significant omission because scalability is a key factor in determining the method's applicability to larger datasets and its potential for use in practical applications. The authors should include a discussion on how the method might perform when applied to larger datasets, including any potential challenges and how they might be addressed. They should also discuss how the method's performance might change in practical applications, where data size and complexity can vary widely."
            ],
            "novelty_impact": [
                "The authors' approach of treating the task of disambiguating symbolic expressions as a neural machine translation task is novel and interesting. The authors have provided some references to support this approach, which is appreciated. However, it would be beneficial if the authors could provide more empirical results showing the effectiveness of this approach, such as comparisons with other methods or case studies showing successful application of this approach. Additionally, references to other studies that have successfully used similar approaches in related tasks would further strengthen the paper. These additions would provide more concrete evidence to support the novel approach and would help readers better understand its advantages and potential applications.",
                "The authors should provide more details on how they selected the sources from arxiv.org for pre-training the model. Specifically, it would be helpful to know how they ensured that these sources are representative of the broader scientific literature. This is crucial for the validity of the results obtained from the model, as the representativeness of the pre-training data could significantly impact the model's performance. Without these details, it is difficult to assess the robustness and generalizability of the results.",
                "The authors' choice of tools (sT E X package, the SMGloM library, and the MMT system) seems justified. However, the paper would be strengthened by a comparison with other potential tools that could be used for this task. This comparison could be based on a literature review to identify other tools commonly used in the field for the task of disambiguating symbolic expressions in informal STEM documents. This would provide readers with a broader context and allow them to better evaluate the advantages of the chosen tools.",
                "While the authors have discussed the challenges of semantically annotating informal documents, it would be beneficial to delve deeper into certain aspects. For instance, the authors could provide more detailed discussions on the limitations of the sTeX package in handling complex symbolic expressions and the challenges faced due to the small size of the training corpus. Additionally, the authors could elaborate on the real-world implications of their approach, such as its potential impact on the formal verification of results in STEM fields and the formalization of informal mathematical documents. This would provide a more comprehensive understanding of the approach and its applicability in real-world scenarios.",
                "The authors' future plans to combine the proposed symbolic disambiguation approach with an autoformalization framework is an interesting direction. However, the paper would be strengthened by providing some preliminary results or evidence to support this plan. Specifically, it would be helpful to see some initial experiments or theoretical analysis that suggest this combination would indeed yield better results. Without these details, it is hard to evaluate the potential impact of this future work.",
                "The authors' suggestion of integrating formal methods in a LaTeX development environment is indeed interesting and the discussion on the challenges related to this integration, such as the large discrepancy between the way new mathematical results are developed and the way they are formalized and implemented in formal systems, is appreciated. However, the paper could benefit from a more explicit discussion on the potential challenges or limitations of this integration. For instance, the complexity of formal methods and the difficulty of integrating them into a LaTeX environment that is primarily designed for typesetting and not for formal verification could be discussed. Additionally, the potential for errors or inconsistencies in the formalization of informal mathematical documents, which could lead to incorrect results or conclusions, could also be addressed.",
                "The authors mention the term 'specificity' in the conclusion section of the paper, but do not provide a clear definition or explanation of what they mean by this term in the context of their task. Providing a clear definition of 'specificity' and explaining its relevance to the task at hand would help readers understand the unique aspects of this task compared to other NMT problems. This could be achieved by adding a section in the paper that specifically addresses the concept of 'specificity' in this context, or by providing a more detailed explanation in the conclusion section where the term is currently used.",
                "While the authors have explained why traditional evaluation metrics are not suitable for their task, it would be helpful to provide more detailed justification for the specific evaluation techniques chosen. Specifically, how do the metrics S F \u2208 L, S L A T E X \u2208 L A T E X(S F ), S F \u2208 L sT E X, and S F = S sT E X contribute to the overall evaluation of the model? Understanding this would help readers appreciate the strengths and limitations of the evaluation approach.",
                "The authors have discussed the use of a transformer language model pre-trained on sources obtained from arxiv.org and a dedicated tokenizer trained on LATEX directly to address the issue of baseline models failing to yield syntactically valid LATEX before overfitting. However, it would be beneficial for the readers if the authors could provide a more detailed discussion on how this methodology improves upon the baseline models. Specifically, how does the pre-training on arxiv.org sources and the use of a dedicated tokenizer trained on LATEX directly contribute to the model's ability to yield syntactically valid LATEX without overfitting? This would help readers understand the improvements made by the proposed model and its advantages over the baseline models."
            ],
            "clarity_reproducibility": [],
            "all": [
                "Ablation Study: The paper presents a method with multiple components, but it lacks an ablation study to analyze the contribution of each component to the overall performance. An ablation study would provide valuable insights into the importance of each component and could potentially lead to further improvements. For example, it could help identify which components are critical for achieving the reported 96.9% success rate for the returned sentence being in L, and which components contribute to the 64.0% success rate for S L A T E X being in L A T E X(S F ). This would not only enhance the understanding of the method's workings but also guide future research in this area.",
                "Performance Metrics: While the paper does mention the performance metrics used to evaluate the proposed method, it does not provide clear definitions or justifications for these metrics. It would be helpful to explicitly define each metric and explain why it was chosen over traditional metrics such as loss during evaluation, perplexity, and BLEU. This would provide a clear measure of the method's success and allow for more meaningful comparisons with other methods.",
                "The paper presents a promising method for disambiguating symbolic expressions in informal STEM documents. However, it lacks a discussion on the generalization ability of the proposed method. It would be beneficial to include a section discussing how well the method is expected to perform on unseen data, especially given the small size of the dataset used for evaluation. This would provide readers with a better understanding of the method's potential applicability and robustness in different contexts.",
                "Scalability: The paper lacks a discussion on the scalability of the proposed method. This is a significant omission because scalability is a key factor in determining the method's applicability to larger datasets and its potential for use in practical applications. The authors should include a discussion on how the method might perform when applied to larger datasets, including any potential challenges and how they might be addressed. They should also discuss how the method's performance might change in practical applications, where data size and complexity can vary widely.",
                "The authors' approach of treating the task of disambiguating symbolic expressions as a neural machine translation task is novel and interesting. The authors have provided some references to support this approach, which is appreciated. However, it would be beneficial if the authors could provide more empirical results showing the effectiveness of this approach, such as comparisons with other methods or case studies showing successful application of this approach. Additionally, references to other studies that have successfully used similar approaches in related tasks would further strengthen the paper. These additions would provide more concrete evidence to support the novel approach and would help readers better understand its advantages and potential applications.",
                "The authors should provide more details on how they selected the sources from arxiv.org for pre-training the model. Specifically, it would be helpful to know how they ensured that these sources are representative of the broader scientific literature. This is crucial for the validity of the results obtained from the model, as the representativeness of the pre-training data could significantly impact the model's performance. Without these details, it is difficult to assess the robustness and generalizability of the results.",
                "The authors' choice of tools (sT E X package, the SMGloM library, and the MMT system) seems justified. However, the paper would be strengthened by a comparison with other potential tools that could be used for this task. This comparison could be based on a literature review to identify other tools commonly used in the field for the task of disambiguating symbolic expressions in informal STEM documents. This would provide readers with a broader context and allow them to better evaluate the advantages of the chosen tools.",
                "While the authors have discussed the challenges of semantically annotating informal documents, it would be beneficial to delve deeper into certain aspects. For instance, the authors could provide more detailed discussions on the limitations of the sTeX package in handling complex symbolic expressions and the challenges faced due to the small size of the training corpus. Additionally, the authors could elaborate on the real-world implications of their approach, such as its potential impact on the formal verification of results in STEM fields and the formalization of informal mathematical documents. This would provide a more comprehensive understanding of the approach and its applicability in real-world scenarios.",
                "The authors' future plans to combine the proposed symbolic disambiguation approach with an autoformalization framework is an interesting direction. However, the paper would be strengthened by providing some preliminary results or evidence to support this plan. Specifically, it would be helpful to see some initial experiments or theoretical analysis that suggest this combination would indeed yield better results. Without these details, it is hard to evaluate the potential impact of this future work.",
                "The authors' suggestion of integrating formal methods in a LaTeX development environment is indeed interesting and the discussion on the challenges related to this integration, such as the large discrepancy between the way new mathematical results are developed and the way they are formalized and implemented in formal systems, is appreciated. However, the paper could benefit from a more explicit discussion on the potential challenges or limitations of this integration. For instance, the complexity of formal methods and the difficulty of integrating them into a LaTeX environment that is primarily designed for typesetting and not for formal verification could be discussed. Additionally, the potential for errors or inconsistencies in the formalization of informal mathematical documents, which could lead to incorrect results or conclusions, could also be addressed.",
                "The authors mention the term 'specificity' in the conclusion section of the paper, but do not provide a clear definition or explanation of what they mean by this term in the context of their task. Providing a clear definition of 'specificity' and explaining its relevance to the task at hand would help readers understand the unique aspects of this task compared to other NMT problems. This could be achieved by adding a section in the paper that specifically addresses the concept of 'specificity' in this context, or by providing a more detailed explanation in the conclusion section where the term is currently used.",
                "While the authors have explained why traditional evaluation metrics are not suitable for their task, it would be helpful to provide more detailed justification for the specific evaluation techniques chosen. Specifically, how do the metrics S F \u2208 L, S L A T E X \u2208 L A T E X(S F ), S F \u2208 L sT E X, and S F = S sT E X contribute to the overall evaluation of the model? Understanding this would help readers appreciate the strengths and limitations of the evaluation approach.",
                "The authors have discussed the use of a transformer language model pre-trained on sources obtained from arxiv.org and a dedicated tokenizer trained on LATEX directly to address the issue of baseline models failing to yield syntactically valid LATEX before overfitting. However, it would be beneficial for the readers if the authors could provide a more detailed discussion on how this methodology improves upon the baseline models. Specifically, how does the pre-training on arxiv.org sources and the use of a dedicated tokenizer trained on LATEX directly contribute to the model's ability to yield syntactically valid LATEX without overfitting? This would help readers understand the improvements made by the proposed model and its advantages over the baseline models."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [],
            "clarity_reproducibility": [],
            "all": []
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 20,
                "n_generated": 4,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 20,
                "n_generated": 9,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "clarity_reproducibility": {
                "n_reference": 20,
                "n_generated": 0,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 20,
                "n_generated": 13,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 9
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 9
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 9
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 9
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "rkxZCJrtwS",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper should include an argument explaining that accurately estimating Q using Q_phi does not necessarily mean that the gradients of Q_phi will be good approximations of the true gradients of Q. The reviewer suggests referring to Fig 1 of arxiv.org/pdf/1705.07107.pdf for an informative example.",
                "The choice of Euclidean norm in line 10 needs justification. The reviewer suggests considering the importance of the direction of the gradient over the norm and proposes the use of cosine similarity instead.",
                "The paper argues that Deep Reinforcement Learning (DRL) is superior to Model Predictive Control (MPC) due to better exploration. The reviewer suggests exploring the possibility of using simulator gradients to further improve exploration in DRL."
            ],
            [
                "The paper's contribution is empirical in nature, and a promising future direction would be to add derivative information to and evaluate similar variants of newer actor-critic methods such as TD3 and SAC."
            ],
            [
                "The form of gradient given in equation (2) might contain an error. In DDPG, the action is given by a deterministic policy, thus we have $a_t = \\pi(s_t)$ for all $t\\geq 0$. For critic estimation, it seems you are basing on the Bellman equation $ Q(s,a) = r(s,a) + Q(s', \\pi(s'))$, where $s'$ is the next state following $(s,a)$. Then, it seems that Equation (2) is obtained by taking gradient with respect to $(s,a)$. However, I cannot understand what $\\nabla_{\\pi} Q$ stands for. If it is $\\nabla_a Q(s_{i+1}, a_{i+1}) \\cdot \\nabla_s \\pi(s_{i+1}) $, then that makes sense.",
                "Based on the experiments, the proposed method does not always outperform MPC or DDPG, even in a small-scale control problem Mountaincar. Moreover, it seems that the performance is similar to that of the DDPG.",
                "The model-based gradient in equation (2) is defined by only unroll one-step forward by going from $s_i, a_i$ to $s_{i+1}$. It would be interesting to see how the number of unroll steps affect the algorithm, which is a gradient version of TD($\\lambda$)."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "Ablation Study: The paper would significantly benefit from an ablation study to understand the contribution of each key component of the proposed hybrid algorithm. Specifically, it would be valuable to understand the individual and combined contributions of the following components: 1) the use of true gradients from a differentiable physical simulator, 2) the combination of model-based methods and DRL, and 3) the use of AE gradients to co-learn critic value and gradient estimation. Such a study would provide a clearer understanding of the contribution of each component to the overall performance of the algorithm, and help identify which parts are most critical for its performance. This would not only strengthen the current work but also provide valuable insights for future research in this area.",
                "A sensitivity analysis is recommended to understand the robustness of the algorithm to changes in its parameters. Specifically, the weights w1 and w2 in the critic loss function, the actor network, the critic network, and the parameter space noise applied for exploration should be included in the analysis. This would provide insights into how sensitive the algorithm's performance is to these parameter settings, which is important for practical applications. For example, the paper provides an example showing how different combinations of weights w1 and w2 can affect the convergence of the critic network, but a comprehensive sensitivity analysis of these parameters is not provided. Similarly, the paper mentions that actions are scaled by a constant such that they fall in the range [-1, 1] and gradients are computed with normalized Q, states, and actions, but does not provide specific details on how changes in these parameters might impact the algorithm's performance. A sensitivity analysis could help fill these gaps and provide a more complete understanding of the algorithm's behavior under different parameter settings.",
                "The paper compares the performance of four algorithms, but it is not clear if the observed differences in performance are statistically significant. To ensure the validity of the results, a repeated measures ANOVA could be conducted. This test is appropriate as it considers the within-subjects factor (the same task is performed multiple times by each algorithm) and can handle more than two groups (four algorithms in this case). If significant differences are found, post-hoc tests such as Tukey's HSD can be used to identify which specific pairs of algorithms have significantly different performances. This statistical analysis would provide a more robust comparison of the algorithms and strengthen the paper's conclusions.",
                "Comprehensive Computational Cost Analysis: While the paper does discuss the computational cost of the proposed algorithm, a more comprehensive computational cost analysis would be beneficial. This analysis should include a comparison with other methods in terms of efficiency and practicality, especially in the context of real-world applications where power and computational resource constraints are critical. This would provide a clearer picture of the algorithm's feasibility and practicality, and would strengthen the paper's claims."
            ],
            "novelty_impact": [
                "The paper presents a novel hybrid control algorithm that effectively combines the strengths of model-based control methods and Deep Reinforcement Learning (DRL). This is a significant contribution to the field, as it addresses the limitations of both methods and potentially offers a more efficient and robust solution for robot control tasks. Specifically, the algorithm is based on the Deep Deterministic Policy Gradients (DDPG) algorithm and uses true gradients from a differentiable physical simulator to increase the convergence rate of both the actor and the critic. This approach allows the algorithm to maintain the robustness of DRL to local minima while also benefiting from the high sample efficiency of model-based methods. The algorithm also leverages differentiable simulation to improve the efficacy of learned critic models. The use of Advantage Estimation (AE) gradients allows for co-learning of critic value and gradient estimation, which improves the convergence of both actor and critic. Furthermore, the algorithm is applicable to a wider range of reward structures, unlike MPC which requires a differentiable reward. This makes the hybrid algorithm more versatile. The hybrid algorithm is more robust than MPC as it can generalize even when noise is added or initial conditions and tasks change, while MPC would require expensive replanning in these scenarios. This makes the hybrid algorithm more suitable for deployment on physical hardware where power and computational resource constraints can render MPC inapplicable to real-time applications.",
                "The authors have done a commendable job in identifying the limitations of existing methods and proposing a hybrid algorithm to overcome these limitations. However, it would be beneficial if the authors could provide a more explicit and detailed discussion on how the proposed method builds and expands on existing work. This could include a clearer explanation of how each component of the hybrid algorithm is derived from previous methods and how it improves upon them. This would help readers better understand the novelty and significance of the proposed method.",
                "The paper presents a hybrid algorithm that combines the benefits of model-based methods and deep reinforcement learning (DRL), and provides empirical evidence to support its claims about the superiority of this algorithm over MPC in terms of applicability to a wider range of reward structures and robustness. However, it would be helpful if the authors could provide more detailed explanations of the experimental results, and perhaps include additional case studies or comparisons to further support these claims. This would make the evidence more convincing and easier to understand for readers.",
                "The paper discusses the robustness of the hybrid algorithm and the generalizable policies and robust state value estimations provided by DRL. However, it does not explicitly link this robustness to the handling of uncertainties or errors in the model. The authors should clearly state how the algorithm handles uncertainties or errors in the model and provide specific evidence supporting the claim that the use of DRL in the hybrid algorithm provides robustness to model inaccuracies. This will strengthen the paper by providing a clear understanding of the algorithm's robustness and its ability to handle uncertainties or errors.",
                "The paper would benefit from a more detailed discussion on the limitations of the algorithm, specifically under what conditions it might not perform as well. This information is crucial for readers to understand the algorithm's applicability and to anticipate potential challenges when implementing it. Additionally, while the paper presents results from robot control tasks using a differentiable rigid body simulator, it does not discuss the performance of the algorithm in a 3D environment or on more complex tasks. Including such discussions would provide a more comprehensive evaluation of the algorithm's performance and versatility.",
                "The paper lacks a discussion on how the proposed algorithm, which combines model-based control methods and deep reinforcement learning (DRL), handles real-world scenarios where collisions might not be perfectly modeled by the two simulation methods discussed, namely impulse-based and constraint-based collision responses. This discussion is crucial as it would provide insights into the algorithm's robustness and applicability in real-world scenarios, which often involve imperfect collision modeling. The authors could improve their paper by discussing specific real-world scenarios and explaining how the algorithm would handle imperfect collision modeling in these scenarios.",
                "The paper discusses the computational constraints of the algorithm and the use of true gradients from a differentiable physical simulator. However, it would be beneficial to provide specific evidence or data to support these claims. For example, the paper could include a comparison of the computational cost of the proposed hybrid algorithm and Model Predictive Control (MPC) under various conditions and tasks. This would help to clearly demonstrate the advantages of the hybrid algorithm in terms of computational efficiency.",
                "The paper presents a hybrid method that combines aspects of gradient-based methods and Deep Reinforcement Learning (DRL), using a differentiable physical simulator to increase the convergence rate of both the actor and the critic. However, it lacks specific evidence or examples demonstrating how the use of a differentiable physical simulator ensures the availability and accuracy of the gradients in situations where true gradients might not be readily available or accurately calculated. The authors should provide more details on this aspect, possibly by discussing how the simulator calculates gradient information manually in the simulation for classic control environments, and how this relates to the availability and accuracy of gradients. This would strengthen the paper by providing clear evidence to support the proposed method."
            ],
            "clarity_reproducibility": [
                "The paper mentions the use of parameter space noise for exploration, referencing the method proposed by Plappert et al., 2017. However, the specific details about how this method was implemented in this study are lacking. It would be beneficial for the readers to understand the specifics of the implementation, such as how the noise was generated, how it was applied to the actor network, and how it influenced the exploration process. Providing these details would strengthen the reproducibility of the study and allow readers to better understand the nuances of your approach.",
                "The paper lacks sufficient detail about the architecture of the critic network. While the function, input, and output of the network are explained, the specific structure and learning process of the network are not detailed. This information is crucial for understanding how the critic network contributes to the overall algorithm and for accurately reproducing the algorithm. The authors should provide more information about the critic network's architecture, including the number and type of layers, the activation functions used, and how the network is trained.",
                "The paper mentions DDPG, MPC with iLQR, and gradient descent as baselines for comparison but lacks specific implementation details for these methods. For instance, it would be helpful to know the specific configurations and parameters used for DDPG, MPC with iLQR, and gradient descent. Without these details, it is difficult to fully understand the comparative analysis and to reproduce the experiments. Providing these details would strengthen the paper by ensuring the reproducibility of the results and allowing readers to better understand the performance of the proposed method in relation to the baselines.",
                "The paper mentions the use of a differentiable rigid body simulator for the 2D control tasks, but it lacks specific details about the simulator's implementation and its contribution to the algorithm's performance. The authors should provide more information about the simulator's design and implementation, such as the specific settings related to the types of contact experienced and how these settings were chosen. Additionally, the authors should clarify how the simulator contributes to the performance of the algorithm, for example, how it interacts with the critic network and how it helps achieve task-irrelevant statistical stability. These details are crucial for understanding the simulation environment and for reproducing the tasks.",
                "The authors should provide more comprehensive details about the seven 2D robot control tasks and their implementation in the differentiable rigid body simulator. Specifically, the paper could benefit from the inclusion of specific details about the implementation process, the parameters used, and the exact methods of calculating rewards. This information is crucial for understanding the tasks in depth and for enabling other researchers to reproduce the results. The absence of these details currently limits the paper's clarity and accessibility, particularly for readers who are less familiar with the subject matter.",
                "The authors should provide more details about how the gradients were manually computed in the simulation for the classic control environments. This information is crucial for understanding the learning process in these specific environments and for reproducing the algorithm. Providing these details would allow readers to better understand the learning process in these specific environments and would make it easier for other researchers to reproduce the algorithm and verify the results."
            ],
            "all": [
                "Ablation Study: The paper would significantly benefit from an ablation study to understand the contribution of each key component of the proposed hybrid algorithm. Specifically, it would be valuable to understand the individual and combined contributions of the following components: 1) the use of true gradients from a differentiable physical simulator, 2) the combination of model-based methods and DRL, and 3) the use of AE gradients to co-learn critic value and gradient estimation. Such a study would provide a clearer understanding of the contribution of each component to the overall performance of the algorithm, and help identify which parts are most critical for its performance. This would not only strengthen the current work but also provide valuable insights for future research in this area.",
                "A sensitivity analysis is recommended to understand the robustness of the algorithm to changes in its parameters. Specifically, the weights w1 and w2 in the critic loss function, the actor network, the critic network, and the parameter space noise applied for exploration should be included in the analysis. This would provide insights into how sensitive the algorithm's performance is to these parameter settings, which is important for practical applications. For example, the paper provides an example showing how different combinations of weights w1 and w2 can affect the convergence of the critic network, but a comprehensive sensitivity analysis of these parameters is not provided. Similarly, the paper mentions that actions are scaled by a constant such that they fall in the range [-1, 1] and gradients are computed with normalized Q, states, and actions, but does not provide specific details on how changes in these parameters might impact the algorithm's performance. A sensitivity analysis could help fill these gaps and provide a more complete understanding of the algorithm's behavior under different parameter settings.",
                "The paper compares the performance of four algorithms, but it is not clear if the observed differences in performance are statistically significant. To ensure the validity of the results, a repeated measures ANOVA could be conducted. This test is appropriate as it considers the within-subjects factor (the same task is performed multiple times by each algorithm) and can handle more than two groups (four algorithms in this case). If significant differences are found, post-hoc tests such as Tukey's HSD can be used to identify which specific pairs of algorithms have significantly different performances. This statistical analysis would provide a more robust comparison of the algorithms and strengthen the paper's conclusions.",
                "Comprehensive Computational Cost Analysis: While the paper does discuss the computational cost of the proposed algorithm, a more comprehensive computational cost analysis would be beneficial. This analysis should include a comparison with other methods in terms of efficiency and practicality, especially in the context of real-world applications where power and computational resource constraints are critical. This would provide a clearer picture of the algorithm's feasibility and practicality, and would strengthen the paper's claims.",
                "The paper presents a novel hybrid control algorithm that effectively combines the strengths of model-based control methods and Deep Reinforcement Learning (DRL). This is a significant contribution to the field, as it addresses the limitations of both methods and potentially offers a more efficient and robust solution for robot control tasks. Specifically, the algorithm is based on the Deep Deterministic Policy Gradients (DDPG) algorithm and uses true gradients from a differentiable physical simulator to increase the convergence rate of both the actor and the critic. This approach allows the algorithm to maintain the robustness of DRL to local minima while also benefiting from the high sample efficiency of model-based methods. The algorithm also leverages differentiable simulation to improve the efficacy of learned critic models. The use of Advantage Estimation (AE) gradients allows for co-learning of critic value and gradient estimation, which improves the convergence of both actor and critic. Furthermore, the algorithm is applicable to a wider range of reward structures, unlike MPC which requires a differentiable reward. This makes the hybrid algorithm more versatile. The hybrid algorithm is more robust than MPC as it can generalize even when noise is added or initial conditions and tasks change, while MPC would require expensive replanning in these scenarios. This makes the hybrid algorithm more suitable for deployment on physical hardware where power and computational resource constraints can render MPC inapplicable to real-time applications.",
                "The authors have done a commendable job in identifying the limitations of existing methods and proposing a hybrid algorithm to overcome these limitations. However, it would be beneficial if the authors could provide a more explicit and detailed discussion on how the proposed method builds and expands on existing work. This could include a clearer explanation of how each component of the hybrid algorithm is derived from previous methods and how it improves upon them. This would help readers better understand the novelty and significance of the proposed method.",
                "The paper presents a hybrid algorithm that combines the benefits of model-based methods and deep reinforcement learning (DRL), and provides empirical evidence to support its claims about the superiority of this algorithm over MPC in terms of applicability to a wider range of reward structures and robustness. However, it would be helpful if the authors could provide more detailed explanations of the experimental results, and perhaps include additional case studies or comparisons to further support these claims. This would make the evidence more convincing and easier to understand for readers.",
                "The paper discusses the robustness of the hybrid algorithm and the generalizable policies and robust state value estimations provided by DRL. However, it does not explicitly link this robustness to the handling of uncertainties or errors in the model. The authors should clearly state how the algorithm handles uncertainties or errors in the model and provide specific evidence supporting the claim that the use of DRL in the hybrid algorithm provides robustness to model inaccuracies. This will strengthen the paper by providing a clear understanding of the algorithm's robustness and its ability to handle uncertainties or errors.",
                "The paper would benefit from a more detailed discussion on the limitations of the algorithm, specifically under what conditions it might not perform as well. This information is crucial for readers to understand the algorithm's applicability and to anticipate potential challenges when implementing it. Additionally, while the paper presents results from robot control tasks using a differentiable rigid body simulator, it does not discuss the performance of the algorithm in a 3D environment or on more complex tasks. Including such discussions would provide a more comprehensive evaluation of the algorithm's performance and versatility.",
                "The paper lacks a discussion on how the proposed algorithm, which combines model-based control methods and deep reinforcement learning (DRL), handles real-world scenarios where collisions might not be perfectly modeled by the two simulation methods discussed, namely impulse-based and constraint-based collision responses. This discussion is crucial as it would provide insights into the algorithm's robustness and applicability in real-world scenarios, which often involve imperfect collision modeling. The authors could improve their paper by discussing specific real-world scenarios and explaining how the algorithm would handle imperfect collision modeling in these scenarios.",
                "The paper discusses the computational constraints of the algorithm and the use of true gradients from a differentiable physical simulator. However, it would be beneficial to provide specific evidence or data to support these claims. For example, the paper could include a comparison of the computational cost of the proposed hybrid algorithm and Model Predictive Control (MPC) under various conditions and tasks. This would help to clearly demonstrate the advantages of the hybrid algorithm in terms of computational efficiency.",
                "The paper presents a hybrid method that combines aspects of gradient-based methods and Deep Reinforcement Learning (DRL), using a differentiable physical simulator to increase the convergence rate of both the actor and the critic. However, it lacks specific evidence or examples demonstrating how the use of a differentiable physical simulator ensures the availability and accuracy of the gradients in situations where true gradients might not be readily available or accurately calculated. The authors should provide more details on this aspect, possibly by discussing how the simulator calculates gradient information manually in the simulation for classic control environments, and how this relates to the availability and accuracy of gradients. This would strengthen the paper by providing clear evidence to support the proposed method.",
                "The paper mentions the use of parameter space noise for exploration, referencing the method proposed by Plappert et al., 2017. However, the specific details about how this method was implemented in this study are lacking. It would be beneficial for the readers to understand the specifics of the implementation, such as how the noise was generated, how it was applied to the actor network, and how it influenced the exploration process. Providing these details would strengthen the reproducibility of the study and allow readers to better understand the nuances of your approach.",
                "The paper lacks sufficient detail about the architecture of the critic network. While the function, input, and output of the network are explained, the specific structure and learning process of the network are not detailed. This information is crucial for understanding how the critic network contributes to the overall algorithm and for accurately reproducing the algorithm. The authors should provide more information about the critic network's architecture, including the number and type of layers, the activation functions used, and how the network is trained.",
                "The paper mentions DDPG, MPC with iLQR, and gradient descent as baselines for comparison but lacks specific implementation details for these methods. For instance, it would be helpful to know the specific configurations and parameters used for DDPG, MPC with iLQR, and gradient descent. Without these details, it is difficult to fully understand the comparative analysis and to reproduce the experiments. Providing these details would strengthen the paper by ensuring the reproducibility of the results and allowing readers to better understand the performance of the proposed method in relation to the baselines.",
                "The paper mentions the use of a differentiable rigid body simulator for the 2D control tasks, but it lacks specific details about the simulator's implementation and its contribution to the algorithm's performance. The authors should provide more information about the simulator's design and implementation, such as the specific settings related to the types of contact experienced and how these settings were chosen. Additionally, the authors should clarify how the simulator contributes to the performance of the algorithm, for example, how it interacts with the critic network and how it helps achieve task-irrelevant statistical stability. These details are crucial for understanding the simulation environment and for reproducing the tasks.",
                "The authors should provide more comprehensive details about the seven 2D robot control tasks and their implementation in the differentiable rigid body simulator. Specifically, the paper could benefit from the inclusion of specific details about the implementation process, the parameters used, and the exact methods of calculating rewards. This information is crucial for understanding the tasks in depth and for enabling other researchers to reproduce the results. The absence of these details currently limits the paper's clarity and accessibility, particularly for readers who are less familiar with the subject matter.",
                "The authors should provide more details about how the gradients were manually computed in the simulation for the classic control environments. This information is crucial for understanding the learning process in these specific environments and for reproducing the algorithm. Providing these details would allow readers to better understand the learning process in these specific environments and would make it easier for other researchers to reproduce the algorithm and verify the results."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [],
            "clarity_reproducibility": [],
            "all": []
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 7,
                "n_generated": 4,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 7,
                "n_generated": 8,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "clarity_reproducibility": {
                "n_reference": 7,
                "n_generated": 6,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 7,
                "n_generated": 18,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "xP37gkVKa_0",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper's proposed method, LBS, only supports one agent in a two-player game.",
                "The authors mention learning a centralized reward function using VDN, while other agent\u2019s q-value functions are unavailable. It's unclear how this is achieved.",
                "The method for using the total reward expectation to avoid having to unroll episodes until the end of the game is not clear."
            ],
            [
                "The paper needs to address how overestimation is handled when using argmax of $Q(a^i|\\tau^i)$ in n-step rollout, especially considering the large variance in rollout. The use of any variance reduction technique should be clarified.",
                "The experiments are only evaluated on Hanabi, raising concerns about the generalizability of the method. The paper should include at least one experiment evaluated on a different game, such as Leduc, to demonstrate its applicability to different imperfect information games.",
                "The paper needs to provide clear definitions and explanations for terms and concepts used, such as 'BP' on page 1, 'blueprint policies' in section 2.1, and the belief definition in section 3.",
                "Equations 2, 3, and 4 are confusing and need further explanation, as they are crucial to the paper."
            ],
            [
                "The motivation of why Learned Belief Search (LBS) can be challenging and useful for DEC-POMDP or the multi-agent game domains is not convincingly explained.",
                "The experiment settings, problem description, and discussions need more elaboration.",
                "The results in Table 1 and 3 are not discussed in the main text, making it hard to understand the reported numbers. The authors should elaborate on these in their response.",
                "A brief description of the Hanabi game should be included in the Appendix.",
                "Comparisons of different design/hyperparameter choices should be included to help judge the benefit of using learning for belief representation."
            ],
            [
                "The paper's proposed method, LBS, is highly dependent on the well-trained Blue Policy (BP). The importance of BP's contribution to the final performance is unclear.",
                "It is not clear how LBS would perform when based on a broken or flawed BP policy.",
                "The paper does not provide clarity on how LBS would perform when using an unconverging learned belief model or value model.",
                "The choice of the number of factorized private features in the belief model is not clear.",
                "The efficacy of LBS, especially in more complex environments like Contract Bridge, is not convincingly demonstrated despite the provided ablation studies."
            ],
            [
                "More detail is required to explain why multi-agent search is not theoretically sound. If the trained model is only accurate for single agent search, is it not possible to train a model that is compatible with multi-agent search under different settings for max range?",
                "The authors focus on 2-player Hanabi and claim it to be straightforward to extend to any number of players. However, SPARTA itself was computationally limited to single-agent search for these 3-player and up variants. Applying your approach in these experiments would make a stronger case for elevation of the state-of-the-art than the 6-card variant.",
                "The experimental setup requires significantly more details on the hardware used for training, testing and validating.",
                "Have you examined how this would look for more general partially observable settings? In particular, work on POSGs with public observations [Hor\u00e1k, K.; and Bosansk\u00fd, B. 2019. Solving Partially Observable Stochastic Games with Public Observations. In AAAI Conf. on Artificial Intelligence, 2029\u20132036. AAAI Press.]",
                "Are there any other DEC-POMDP settings that you considered? It may be insightful to compare the effects of different feature spaces."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper provides a robust application of the Learned Belief Search (LBS) method in the context of 2-player Hanabi, demonstrating its efficiency in a partially observable environment. However, to further substantiate the generalizability of the LBS method, it would be beneficial to test it in other environments that involve partially observable Markov decision processes (POMDPs). These could include other cooperative multi-agent settings where the state of the game is not fully observable by all players, such as other card games or cooperative multi-agent video games with limited player vision or information. This would provide a more comprehensive demonstration of the method's scalability and applicability to high-dimensional state spaces.",
                "The authors have identified a performance gap between the learned belief model and the exact belief, particularly noticeable in the later stages of the game. Given the computational efficiency and scalability of the Learned Belief Search (LBS) method, it would be beneficial to explore more powerful models to improve the belief learning. Specifically, the authors could consider using transformers, which are known for their ability to handle sequences and their efficiency in parallel computation. This could potentially enhance the performance of the belief model, especially in the later stages of the game, and further reduce the gap with the exact belief.",
                "The paper demonstrates the scalability of LBS using a modified version of Hanabi where each player holds 6 cards instead of 5. While this effectively shows the performance of LBS compared to SPARTA in this modified game, it raises questions about the generalizability of these results. It would strengthen the paper to include performance results of LBS in the standard form of Hanabi, which is more widely recognized and used. This would provide a more comprehensive understanding of LBS's performance and its potential applicability to other games or real-world scenarios.",
                "The paper could benefit from a more detailed comparison of the performance of Learned Belief Search (LBS), the blueprint policy, SPARTA, and the Rollout algorithm. Specifically, it would be helpful to see a comparison of the performance of these methods across a range of different environments, not just the six card version of Hanabi. This could help to demonstrate the generalizability of LBS. Additionally, the paper could include statistical tests, such as a t-test or ANOVA, to determine whether the differences in performance between these methods are statistically significant. This would provide stronger evidence for the superiority of LBS.",
                "While the paper provides a comparison of different versions of Learned Belief Search (LBS), it would be beneficial to include a traditional ablation study to understand the contribution of each component of LBS, specifically the blueprint policy (BP) and the belief model. This would provide a clearer understanding of the individual impact of each component on the overall performance of LBS. For instance, how does the performance change when the belief model is removed or replaced with a simpler model? Similarly, what is the impact of the blueprint policy on the performance? This additional analysis would strengthen the paper by providing deeper insights into the workings of LBS.",
                "The authors propose retraining the belief model with data generated from the Learned Belief Search (LBS) procedure as future work. Given the crucial role of the belief model in the LBS procedure and the potential of LBS data to bring the belief model to under-explored regions of the state space, it would be valuable to include some preliminary results of this experiment in the paper. This would provide initial insights into the potential benefits of this approach and could strengthen the paper's contributions."
            ],
            "novelty_impact": [
                "The paper presents a significant contribution to the field with the novel Learned Belief Search (LBS) method for POMDPs. However, the paper lacks specific details on how the LBS method determines inconsistency during the filtering of sampled hands based on their private observation. This is a crucial part of the LBS method as it directly impacts the efficiency of the search procedure. Providing a detailed explanation or a step-by-step process of how inconsistency is determined would greatly enhance the understanding of the method and its replicability. This could include the criteria used for determining inconsistency, the process of filtering out inconsistent hands, and how this process contributes to the overall efficiency of the LBS method.",
                "The paper reports a 35x reduction in compute requirements using the Learned Belief Search (LBS) method in the game of Hanabi. However, it is unclear if this reduction is consistent across different game scenarios. For instance, the paper mentions that the LBS method was tested in a modified version of Hanabi where each player holds 6 cards instead of 5, and in this scenario, the LBS method ran faster due to shorter games, delivering 76% of the improvement with 42\u00d7 less time compared to the SPARTA method. This suggests that the reduction in compute requirements can vary depending on the game scenario. Therefore, it would be helpful if the authors could provide more specific details about the variability of this reduction across the different game scenarios tested and discuss any factors that could affect this reduction.",
                "The paper presents the LBS method and its application in the game of Hanabi, but it does not provide evidence of the performance of LBS in other POMDPs. This raises questions about the generalizability of the LBS method. To strengthen the paper, the authors should provide evidence of the performance of LBS in other POMDPs or discuss the potential for its application in other POMDPs. This would help readers understand the broader applicability of the LBS method beyond the game of Hanabi.",
                "The paper does not clearly articulate whether the current Learned Belief Search (LBS) method involves any form of search during the training phase. This lack of clarity is inferred from the authors' suggestion of integrating the search process into the training process in future iterations of the method. To avoid any ambiguity, the authors should explicitly state whether the current LBS method involves any form of search during training. This clarification will help readers understand the current method better and appreciate the potential benefits of the proposed future work.",
                "While the paper provides a comparison of the LBS method with the exact search method and SPARTA, and also compares different variants of LBS, it would strengthen the paper to include a comparison with other existing methods as well. This would provide a broader context for the performance of LBS and a more comprehensive evaluation of its relative performance. However, this is not a major issue that significantly impacts the quality of the paper, but rather a suggestion for enhancement.",
                "The authors have discussed some of the potential limitations of the LBS method, including the accuracy of the learned belief model and the computational cost of training this model. However, these discussions could be expanded upon. Specifically, the authors could provide more detail on the computational cost of training the belief model, such as the resources required and the trade-offs involved. Additionally, while the authors mention that the LBS method is a general search method for POMDPs, they could discuss in more detail how this method could be generalized to other POMDPs, and what challenges might be encountered in doing so. This would give readers a better understanding of the potential applications and limitations of the LBS method."
            ],
            "clarity_reproducibility": [
                "The paper provides some details about the training process of the belief model in Learned Belief Search (LBS), but it could benefit from more specific information. In particular, it would be helpful to include more details about how the training data is split into training and validation sets, and how the model's performance is evaluated during training. For example, the paper could explain how the replay buffer is used to create training and validation sets, and what metrics are used to evaluate the model's performance during training. These details would make it easier for readers to understand the training process and evaluate the effectiveness of the model.",
                "The paper provides some details about the public-private model architecture, including what kind of information the public and private features contain and how they are represented. However, more details could be beneficial to fully understand the architecture and its implications. Specifically, it would be helpful to have more information about how the public and private features are separated and combined in the architecture, and how this affects the performance of the model. Additionally, more examples or case studies of how the architecture is used in different contexts, such as in Dec-POMDPs with a limited amount of hidden information or in games like Hanabi, could provide more insight into the versatility and applicability of the architecture.",
                "The blueprint training section of the paper lacks specific details about the reinforcement learning algorithm used, the reward function, and the definition of the state and action spaces. These details are crucial for understanding the methodology and replicating the results. Without these details, it is difficult for readers to fully understand the approach taken and to assess the validity of the results. It also poses challenges for other researchers who wish to replicate the study or build upon the work presented in the paper. The authors should provide a clear description of the reinforcement learning algorithm, including the reward function and the definition of the state and action spaces, to enhance the clarity and reproducibility of the paper.",
                "The paper describes the belief learning process and mentions that the Learned Belief Search (LBS) uses supervised learning (SL) to train an auto-regressive belief model. However, it lacks specific details about the supervised loss used for training and the type of loss function. Providing these details would allow readers to better understand the training process and potentially reproduce the method. Specifically, it would be beneficial to include information about how the supervised loss is computed and why the chosen loss function is suitable for this task. This would strengthen the paper by providing a more comprehensive description of the method.",
                "The paper provides a good overview of the search methods that Learned Belief Search (LBS) is compared to, including SPARTA, Rollout algorithm, Monte Carlo Tree Search, MuZero, and Other-Play. However, it would be beneficial to provide more detail about the strengths and weaknesses of these methods. This would help readers understand why LBS is an improvement over these methods and would provide a clearer context for the introduction of LBS. Specifically, the paper could delve deeper into the limitations of SPARTA's requirement for a sufficiently small belief space, the challenges of performing rollouts in the MDP induced by the belief states in the Rollout algorithm, and the approximation of beliefs in Monte Carlo Tree Search. This would strengthen the paper's argument for the necessity and advantages of LBS.",
                "The experimental setup used to evaluate the Learned Belief Search (LBS) method lacks crucial details that would allow for the replication of the study and the validation of the results. Specifically, the paper does not provide information about the hardware used for the experiments. This information is important as it could impact the performance and the efficiency of the LBS method. Additionally, while the paper mentions that the authors followed most practices from the open-source code of Other-Play (Hu et al., 2020) for training the BP with reinforcement learning and left their hyper-parameters unchanged, the exact values of these hyperparameters are not provided. Providing these details would allow other researchers to accurately replicate the training process and verify the results."
            ],
            "all": [
                "The paper provides a robust application of the Learned Belief Search (LBS) method in the context of 2-player Hanabi, demonstrating its efficiency in a partially observable environment. However, to further substantiate the generalizability of the LBS method, it would be beneficial to test it in other environments that involve partially observable Markov decision processes (POMDPs). These could include other cooperative multi-agent settings where the state of the game is not fully observable by all players, such as other card games or cooperative multi-agent video games with limited player vision or information. This would provide a more comprehensive demonstration of the method's scalability and applicability to high-dimensional state spaces.",
                "The authors have identified a performance gap between the learned belief model and the exact belief, particularly noticeable in the later stages of the game. Given the computational efficiency and scalability of the Learned Belief Search (LBS) method, it would be beneficial to explore more powerful models to improve the belief learning. Specifically, the authors could consider using transformers, which are known for their ability to handle sequences and their efficiency in parallel computation. This could potentially enhance the performance of the belief model, especially in the later stages of the game, and further reduce the gap with the exact belief.",
                "The paper demonstrates the scalability of LBS using a modified version of Hanabi where each player holds 6 cards instead of 5. While this effectively shows the performance of LBS compared to SPARTA in this modified game, it raises questions about the generalizability of these results. It would strengthen the paper to include performance results of LBS in the standard form of Hanabi, which is more widely recognized and used. This would provide a more comprehensive understanding of LBS's performance and its potential applicability to other games or real-world scenarios.",
                "The paper could benefit from a more detailed comparison of the performance of Learned Belief Search (LBS), the blueprint policy, SPARTA, and the Rollout algorithm. Specifically, it would be helpful to see a comparison of the performance of these methods across a range of different environments, not just the six card version of Hanabi. This could help to demonstrate the generalizability of LBS. Additionally, the paper could include statistical tests, such as a t-test or ANOVA, to determine whether the differences in performance between these methods are statistically significant. This would provide stronger evidence for the superiority of LBS.",
                "While the paper provides a comparison of different versions of Learned Belief Search (LBS), it would be beneficial to include a traditional ablation study to understand the contribution of each component of LBS, specifically the blueprint policy (BP) and the belief model. This would provide a clearer understanding of the individual impact of each component on the overall performance of LBS. For instance, how does the performance change when the belief model is removed or replaced with a simpler model? Similarly, what is the impact of the blueprint policy on the performance? This additional analysis would strengthen the paper by providing deeper insights into the workings of LBS.",
                "The authors propose retraining the belief model with data generated from the Learned Belief Search (LBS) procedure as future work. Given the crucial role of the belief model in the LBS procedure and the potential of LBS data to bring the belief model to under-explored regions of the state space, it would be valuable to include some preliminary results of this experiment in the paper. This would provide initial insights into the potential benefits of this approach and could strengthen the paper's contributions.",
                "The paper presents a significant contribution to the field with the novel Learned Belief Search (LBS) method for POMDPs. However, the paper lacks specific details on how the LBS method determines inconsistency during the filtering of sampled hands based on their private observation. This is a crucial part of the LBS method as it directly impacts the efficiency of the search procedure. Providing a detailed explanation or a step-by-step process of how inconsistency is determined would greatly enhance the understanding of the method and its replicability. This could include the criteria used for determining inconsistency, the process of filtering out inconsistent hands, and how this process contributes to the overall efficiency of the LBS method.",
                "The paper reports a 35x reduction in compute requirements using the Learned Belief Search (LBS) method in the game of Hanabi. However, it is unclear if this reduction is consistent across different game scenarios. For instance, the paper mentions that the LBS method was tested in a modified version of Hanabi where each player holds 6 cards instead of 5, and in this scenario, the LBS method ran faster due to shorter games, delivering 76% of the improvement with 42\u00d7 less time compared to the SPARTA method. This suggests that the reduction in compute requirements can vary depending on the game scenario. Therefore, it would be helpful if the authors could provide more specific details about the variability of this reduction across the different game scenarios tested and discuss any factors that could affect this reduction.",
                "The paper presents the LBS method and its application in the game of Hanabi, but it does not provide evidence of the performance of LBS in other POMDPs. This raises questions about the generalizability of the LBS method. To strengthen the paper, the authors should provide evidence of the performance of LBS in other POMDPs or discuss the potential for its application in other POMDPs. This would help readers understand the broader applicability of the LBS method beyond the game of Hanabi.",
                "The paper does not clearly articulate whether the current Learned Belief Search (LBS) method involves any form of search during the training phase. This lack of clarity is inferred from the authors' suggestion of integrating the search process into the training process in future iterations of the method. To avoid any ambiguity, the authors should explicitly state whether the current LBS method involves any form of search during training. This clarification will help readers understand the current method better and appreciate the potential benefits of the proposed future work.",
                "While the paper provides a comparison of the LBS method with the exact search method and SPARTA, and also compares different variants of LBS, it would strengthen the paper to include a comparison with other existing methods as well. This would provide a broader context for the performance of LBS and a more comprehensive evaluation of its relative performance. However, this is not a major issue that significantly impacts the quality of the paper, but rather a suggestion for enhancement.",
                "The authors have discussed some of the potential limitations of the LBS method, including the accuracy of the learned belief model and the computational cost of training this model. However, these discussions could be expanded upon. Specifically, the authors could provide more detail on the computational cost of training the belief model, such as the resources required and the trade-offs involved. Additionally, while the authors mention that the LBS method is a general search method for POMDPs, they could discuss in more detail how this method could be generalized to other POMDPs, and what challenges might be encountered in doing so. This would give readers a better understanding of the potential applications and limitations of the LBS method.",
                "The paper provides some details about the training process of the belief model in Learned Belief Search (LBS), but it could benefit from more specific information. In particular, it would be helpful to include more details about how the training data is split into training and validation sets, and how the model's performance is evaluated during training. For example, the paper could explain how the replay buffer is used to create training and validation sets, and what metrics are used to evaluate the model's performance during training. These details would make it easier for readers to understand the training process and evaluate the effectiveness of the model.",
                "The paper provides some details about the public-private model architecture, including what kind of information the public and private features contain and how they are represented. However, more details could be beneficial to fully understand the architecture and its implications. Specifically, it would be helpful to have more information about how the public and private features are separated and combined in the architecture, and how this affects the performance of the model. Additionally, more examples or case studies of how the architecture is used in different contexts, such as in Dec-POMDPs with a limited amount of hidden information or in games like Hanabi, could provide more insight into the versatility and applicability of the architecture.",
                "The blueprint training section of the paper lacks specific details about the reinforcement learning algorithm used, the reward function, and the definition of the state and action spaces. These details are crucial for understanding the methodology and replicating the results. Without these details, it is difficult for readers to fully understand the approach taken and to assess the validity of the results. It also poses challenges for other researchers who wish to replicate the study or build upon the work presented in the paper. The authors should provide a clear description of the reinforcement learning algorithm, including the reward function and the definition of the state and action spaces, to enhance the clarity and reproducibility of the paper.",
                "The paper describes the belief learning process and mentions that the Learned Belief Search (LBS) uses supervised learning (SL) to train an auto-regressive belief model. However, it lacks specific details about the supervised loss used for training and the type of loss function. Providing these details would allow readers to better understand the training process and potentially reproduce the method. Specifically, it would be beneficial to include information about how the supervised loss is computed and why the chosen loss function is suitable for this task. This would strengthen the paper by providing a more comprehensive description of the method.",
                "The paper provides a good overview of the search methods that Learned Belief Search (LBS) is compared to, including SPARTA, Rollout algorithm, Monte Carlo Tree Search, MuZero, and Other-Play. However, it would be beneficial to provide more detail about the strengths and weaknesses of these methods. This would help readers understand why LBS is an improvement over these methods and would provide a clearer context for the introduction of LBS. Specifically, the paper could delve deeper into the limitations of SPARTA's requirement for a sufficiently small belief space, the challenges of performing rollouts in the MDP induced by the belief states in the Rollout algorithm, and the approximation of beliefs in Monte Carlo Tree Search. This would strengthen the paper's argument for the necessity and advantages of LBS.",
                "The experimental setup used to evaluate the Learned Belief Search (LBS) method lacks crucial details that would allow for the replication of the study and the validation of the results. Specifically, the paper does not provide information about the hardware used for the experiments. This information is important as it could impact the performance and the efficiency of the LBS method. Additionally, while the paper mentions that the authors followed most practices from the open-source code of Other-Play (Hu et al., 2020) for training the BP with reinforcement learning and left their hyper-parameters unchanged, the exact values of these hyperparameters are not provided. Providing these details would allow other researchers to accurately replicate the training process and verify the results."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 1,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The experiments are only evaluated on Hanabi, raising concerns about the generalizability of the method. The paper should include at least one experiment evaluated on a different game, such as Leduc, to demonstrate its applicability to different imperfect information games.",
                    "predicted_comment": "The paper provides a robust application of the Learned Belief Search (LBS) method in the context of 2-player Hanabi, demonstrating its efficiency in a partially observable environment. However, to further substantiate the generalizability of the LBS method, it would be beneficial to test it in other environments that involve partially observable Markov decision processes (POMDPs). These could include other cooperative multi-agent settings where the state of the game is not fully observable by all players, such as other card games or cooperative multi-agent video games with limited player vision or information. This would provide a more comprehensive demonstration of the method's scalability and applicability to high-dimensional state spaces."
                },
                {
                    "reference_id": 4,
                    "predicted_id": 4,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The efficacy of LBS, especially in more complex environments like Contract Bridge, is not convincingly demonstrated despite the provided ablation studies.",
                    "predicted_comment": "While the paper provides a comparison of different versions of Learned Belief Search (LBS), it would be beneficial to include a traditional ablation study to understand the contribution of each component of LBS, specifically the blueprint policy (BP) and the belief model. This would provide a clearer understanding of the individual impact of each component on the overall performance of LBS. For instance, how does the performance change when the belief model is removed or replaced with a simpler model? Similarly, what is the impact of the blueprint policy on the performance? This additional analysis would strengthen the paper by providing deeper insights into the workings of LBS."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 4,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper's proposed method, LBS, is highly dependent on the well-trained Blue Policy (BP). The importance of BP's contribution to the final performance is unclear.",
                    "predicted_comment": "While the paper provides a comparison of different versions of Learned Belief Search (LBS), it would be beneficial to include a traditional ablation study to understand the contribution of each component of LBS, specifically the blueprint policy (BP) and the belief model. This would provide a clearer understanding of the individual impact of each component on the overall performance of LBS. For instance, how does the performance change when the belief model is removed or replaced with a simpler model? Similarly, what is the impact of the blueprint policy on the performance? This additional analysis would strengthen the paper by providing deeper insights into the workings of LBS."
                }
            ],
            "novelty_impact": [],
            "clarity_reproducibility": [
                {
                    "reference_id": 1,
                    "predicted_id": 17,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The experiment settings, problem description, and discussions need more elaboration.",
                    "predicted_comment": "The experimental setup used to evaluate the Learned Belief Search (LBS) method lacks crucial details that would allow for the replication of the study and the validation of the results. Specifically, the paper does not provide information about the hardware used for the experiments. This information is important as it could impact the performance and the efficiency of the LBS method. Additionally, while the paper mentions that the authors followed most practices from the open-source code of Other-Play (Hu et al., 2020) for training the BP with reinforcement learning and left their hyper-parameters unchanged, the exact values of these hyperparameters are not provided. Providing these details would allow other researchers to accurately replicate the training process and verify the results."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 17,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "The experimental setup requires significantly more details on the hardware used for training, testing and validating.",
                    "predicted_comment": "The experimental setup used to evaluate the Learned Belief Search (LBS) method lacks crucial details that would allow for the replication of the study and the validation of the results. Specifically, the paper does not provide information about the hardware used for the experiments. This information is important as it could impact the performance and the efficiency of the LBS method. Additionally, while the paper mentions that the authors followed most practices from the open-source code of Other-Play (Hu et al., 2020) for training the BP with reinforcement learning and left their hyper-parameters unchanged, the exact values of these hyperparameters are not provided. Providing these details would allow other researchers to accurately replicate the training process and verify the results."
                }
            ],
            "all": [
                {
                    "reference_id": 1,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The experiments are only evaluated on Hanabi, raising concerns about the generalizability of the method. The paper should include at least one experiment evaluated on a different game, such as Leduc, to demonstrate its applicability to different imperfect information games.",
                    "predicted_comment": "The paper provides a robust application of the Learned Belief Search (LBS) method in the context of 2-player Hanabi, demonstrating its efficiency in a partially observable environment. However, to further substantiate the generalizability of the LBS method, it would be beneficial to test it in other environments that involve partially observable Markov decision processes (POMDPs). These could include other cooperative multi-agent settings where the state of the game is not fully observable by all players, such as other card games or cooperative multi-agent video games with limited player vision or information. This would provide a more comprehensive demonstration of the method's scalability and applicability to high-dimensional state spaces."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 17,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The experiment settings, problem description, and discussions need more elaboration.",
                    "predicted_comment": "The experimental setup used to evaluate the Learned Belief Search (LBS) method lacks crucial details that would allow for the replication of the study and the validation of the results. Specifically, the paper does not provide information about the hardware used for the experiments. This information is important as it could impact the performance and the efficiency of the LBS method. Additionally, while the paper mentions that the authors followed most practices from the open-source code of Other-Play (Hu et al., 2020) for training the BP with reinforcement learning and left their hyper-parameters unchanged, the exact values of these hyperparameters are not provided. Providing these details would allow other researchers to accurately replicate the training process and verify the results."
                },
                {
                    "reference_id": 4,
                    "predicted_id": 4,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The efficacy of LBS, especially in more complex environments like Contract Bridge, is not convincingly demonstrated despite the provided ablation studies.",
                    "predicted_comment": "While the paper provides a comparison of different versions of Learned Belief Search (LBS), it would be beneficial to include a traditional ablation study to understand the contribution of each component of LBS, specifically the blueprint policy (BP) and the belief model. This would provide a clearer understanding of the individual impact of each component on the overall performance of LBS. For instance, how does the performance change when the belief model is removed or replaced with a simpler model? Similarly, what is the impact of the blueprint policy on the performance? This additional analysis would strengthen the paper by providing deeper insights into the workings of LBS."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 4,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper's proposed method, LBS, is highly dependent on the well-trained Blue Policy (BP). The importance of BP's contribution to the final performance is unclear.",
                    "predicted_comment": "While the paper provides a comparison of different versions of Learned Belief Search (LBS), it would be beneficial to include a traditional ablation study to understand the contribution of each component of LBS, specifically the blueprint policy (BP) and the belief model. This would provide a clearer understanding of the individual impact of each component on the overall performance of LBS. For instance, how does the performance change when the belief model is removed or replaced with a simpler model? Similarly, what is the impact of the blueprint policy on the performance? This additional analysis would strengthen the paper by providing deeper insights into the workings of LBS."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 17,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "The experimental setup requires significantly more details on the hardware used for training, testing and validating.",
                    "predicted_comment": "The experimental setup used to evaluate the Learned Belief Search (LBS) method lacks crucial details that would allow for the replication of the study and the validation of the results. Specifically, the paper does not provide information about the hardware used for the experiments. This information is important as it could impact the performance and the efficiency of the LBS method. Additionally, while the paper mentions that the authors followed most practices from the open-source code of Other-Play (Hu et al., 2020) for training the BP with reinforcement learning and left their hyper-parameters unchanged, the exact values of these hyperparameters are not provided. Providing these details would allow other researchers to accurately replicate the training process and verify the results."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 22,
                "n_generated": 6,
                "n_total_aligns": 3,
                "recall": 0.13,
                "precision": 0.06666666666666667,
                "pseudo_jaccard": 0.05380116959064327,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 22,
                "n_generated": 6,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "clarity_reproducibility": {
                "n_reference": 22,
                "n_generated": 6,
                "n_total_aligns": 2,
                "recall": 0.08,
                "precision": 0.06666666666666667,
                "pseudo_jaccard": 0.04,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "all": {
                "n_reference": 22,
                "n_generated": 18,
                "n_total_aligns": 5,
                "recall": 0.21000000000000002,
                "precision": 0.04444444444444444,
                "pseudo_jaccard": 0.04165911607772073,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.25,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.1111111111111111,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.4,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.15789473684210525,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.2,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.1,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.2,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.1,
                    "n_real": 5
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.25,
                    "precision": 0.05555555555555555,
                    "pseudo_jaccard": 0.047619047619047616,
                    "n_real": 4
                },
                {
                    "recall": 0.2,
                    "precision": 0.05555555555555555,
                    "pseudo_jaccard": 0.045454545454545456,
                    "n_real": 5
                },
                {
                    "recall": 0.4,
                    "precision": 0.05555555555555555,
                    "pseudo_jaccard": 0.06976744186046512,
                    "n_real": 5
                },
                {
                    "recall": 0.2,
                    "precision": 0.05555555555555555,
                    "pseudo_jaccard": 0.045454545454545456,
                    "n_real": 5
                }
            ]
        }
    },
    {
        "doc_id": "-qB7ZgRNRq",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The motivation for the new task proposed in the paper is not clear.",
                "The paper does not compare the proposed model, DDNet, to any other baselines that incorporate the audio information in a different way.",
                "The paper incorrectly claims that the method from Serdyuk et al. (2018) was 'concurrent', despite it being published in 2018.",
                "It is unclear why the Spoken-CoQA dataset includes text transcripts as well as the audio. It would make more sense for the text transcripts to be part of the model solving the dataset."
            ],
            [
                "The work is limited in its scope and is not yet ready to be published at ICLR due to lack of comparisons made to prior approaches, making it hard to assess the merits of the proposed approach.",
                "The authors do not mention any plans of releasing the new dataset described in this work, which is a concern.",
                "It would be interesting to show how F1 scores vary on the test instances as a function of the number of turns in the conversation, and how test F1 scores vary as a function of ASR accuracy of the spoken documents/questions.",
                "The error rate of the ASR system on the spoken documents and spoken questions should be provided to give the reader an idea of the accuracy of the transcriptions fed as input to the student model.",
                "In Table 4, the F1 scores using SDNet are higher for S-CoQA compared to CoQA which is unexpected. The authors should provide an explanation for this.",
                "The reliance of the proposed system on clean text is a significant issue that needs to be addressed."
            ],
            [
                "The paper's contributions and novelty are unclear, and it's uncertain if they warrant acceptance.",
                "The paper's methods, including the cross attention mechanism for speech and text embedding fusion and knowledge distillation for combatting ASR errors, are existing, well-studied methods.",
                "The Spoken-CoQA dataset, which is a significant contribution of the paper, is not made available.",
                "The paper should clarify whether the knowledge distillation and cross attention mechanisms can be combined for better performance.",
                "The paper should address whether the textual input is more useful than the audio input in the Spoken-CoQA dataset, especially considering the audio input is larger and more difficult to process."
            ],
            [
                "The biggest weakness of this paper is in its data construction. When the ASR output is noisy, the spans refer to ghost token positions based on the clean text. This makes the data difficult to learn from and may result in models predicting the wrong span.",
                "The paper falls short of delivering a usable dataset for speech-based conversational QA. The authors should consider returning the audio segments as answers instead of using text-based spans. This might require additional annotation but could make the dataset more meaningful and usable.",
                "The use of a knowledge distillation (KD) layer to address the issue of noisy ASR text is questionable. The KD layer shows the model the gold clean transcription (original passage), which could defeat the purpose of speech-based conversational QA as motivated in this paper."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper mentions that the DDNet is designed to handle noisy ASR transcriptions, which is a common issue in spoken dialogue systems. However, there is no explicit experiment or evaluation that tests the DDNet's robustness to artificially added noise in the ASR transcriptions. This is a significant omission, as it is crucial to evaluate the robustness of the DDNet to noisy ASR transcriptions to validate its effectiveness in real-world scenarios where noise is often present. I recommend adding an experiment where noise is artificially introduced to the ASR transcriptions, and the performance of the DDNet is evaluated under these conditions. This would provide a more comprehensive evaluation of the DDNet's capabilities and strengthen the paper's contributions.",
                "Unclear Evaluation of Knowledge Distillation: The authors mention the use of knowledge distillation in the training process of the models in the 'BASELINES' section. However, it is not explicitly stated which models were trained with and without knowledge distillation. This makes it difficult to evaluate the effectiveness of the knowledge distillation approach. The authors should consider clearly indicating which models were trained with and without knowledge distillation and discussing the impact of knowledge distillation on the performance of the models.",
                "While the authors have introduced the new Spoken-CoQA dataset and provided some evaluation, the evaluation could be more thorough and explicit. Specifically, it would be helpful to include more detailed results of the comparison with existing SQA datasets and the performance of the DDNet model trained on the Spoken-CoQA dataset. Additionally, an explicit comparison of the Spoken-CoQA dataset with other similar datasets could further demonstrate its value. Providing these details would make it easier for readers to assess the quality and value of the new dataset.",
                "The paper lacks a detailed ablation study for the proposed DDNet. An ablation study, which involves removing each component of the DDNet one by one and evaluating the performance of the resulting models, is crucial for understanding the contribution of each component to the overall performance. Without such a study, it's challenging to discern which components are essential and which are not. This information is not only valuable for readers seeking to replicate or build upon your work, but also beneficial for future research and for further improvements to the DDNet. Therefore, it is strongly recommended that the authors include a comprehensive ablation study in the paper.",
                "The paper introduces a novel fusion mechanism, Con Fusion, and provides a comparison of its performance with the Cross Attention mechanism. However, it would be beneficial to provide a more detailed analysis of the contribution of the Con Fusion mechanism to the overall performance of the DDNet. Specifically, an experiment comparing the performance of the DDNet with and without the Con Fusion mechanism could help to clarify its impact. This would allow readers to better understand the value of the Con Fusion mechanism and its role in the DDNet."
            ],
            "novelty_impact": [
                "The paper lacks detailed information about the diversity of the Spoken-CoQA dataset, specifically in terms of accents, languages, and topics. Understanding the diversity of the dataset is crucial for assessing the generalizability of the model. For instance, if the dataset is predominantly in one language or accent, or covers a limited range of topics, the model's performance may not generalize well to other languages, accents, or topics. Therefore, the authors should provide more comprehensive information about the diversity of the Spoken-CoQA dataset.",
                "The paper lacks a detailed explanation of how the DDNet handles noise in the audio data. Specifically, it would be beneficial to understand how the system distinguishes between noise and speech, and how it handles different types of noise. This information is crucial for evaluating the effectiveness of the system and its applicability in real-world scenarios where noise is often present.",
                "The paper should provide a more detailed comparison between the 'novel unified data distillation approach' and the 'multi-modality fusion mechanism' and existing methods such as FlowQA, SDNet, BERT-base, and ALBERT. Specifically, the paper should clearly highlight the novelty and advantages of these methods over the existing methods. This could be achieved by providing a detailed discussion of the results of the comparison, explaining why the novel methods outperform the existing methods, and discussing the implications of these results for the field of conversational question answering.",
                "The paper uses the term 'misalignment' to describe the discrepancy between automatic speech recognition (ASR) hypotheses and reference transcriptions, but it does not provide a specific definition or measurement for this term. A clear definition or measurement for 'misalignment' is crucial for understanding how the DDNet is evaluated and how it compares to other models. For example, the paper could define 'misalignment' as the number of words in the ASR transcript that do not match the manual transcript, and it could measure 'misalignment' as the percentage of words in the ASR transcript that are misaligned. This would allow readers to understand the scale of the 'misalignment' and how it affects the performance of the DDNet.",
                "The paper provides some reasons why the chosen models were used as baselines, namely their superior performance. However, it would be helpful to provide more specific details about the strengths and weaknesses of each baseline model. This would help readers understand the context of the paper's claims of superiority. For example, the paper could discuss how each model performs on single-turn tasks versus multi-turn conversational tasks, or how they handle spoken content and text documents simultaneously.",
                "The paper lacks a discussion on the choice of metrics used to evaluate the proposed method. Specifically, it does not justify why EM and F1 scores are the most appropriate metrics for this task. Other potentially relevant metrics such as precision, recall, and accuracy are not mentioned or discussed. A detailed discussion on the choice of metrics is crucial as it provides transparency about how the results were evaluated and allows for a fair comparison with other methods or models that use the same metrics. I recommend the authors to include a section discussing why EM and F1 scores were chosen, and why other metrics such as precision, recall, and accuracy were not used. This will help readers better understand the evaluation method and the performance of the proposed method."
            ],
            "clarity_reproducibility": [
                "While the paper provides an overview of the DDNet's data distillation process, it lacks specific details that are crucial for reproducibility and for other researchers to evaluate its effectiveness. Specifically, the paper does not provide the specific parameters, such as the learning rate, batch size, number of epochs, or any other hyperparameters used in training the models. Additionally, the paper does not describe the specific methods used for training the teacher and student models, the criteria used to evaluate the student model's learning, and any techniques used to ensure the student model effectively imbibes the teacher model's knowledge. Providing these details would greatly enhance the paper's value to the research community.",
                "The paper mentions the use of default attention layers from four baseline models but does not detail the specific types of attention layers used. Similarly, while the paper mentions that the Output Layer computes the probability distribution of the start and end index within the entire documents to predict an answer, it does not detail the method used to compute this probability distribution. These details are crucial for a clear understanding of the model's inner workings and for anyone attempting to replicate the study's results. Therefore, it is recommended that the authors provide these specific details to enhance the clarity and reproducibility of the paper.",
                "The paper discusses the use of a Knowledge Distillation method where a student model learns from a teacher model. However, it lacks specific details about the learning algorithm or method used for this process. This omission makes it difficult to fully understand the method and to reproduce it. To improve the paper, the authors should provide a clear and detailed description of the learning algorithm or method used in the Knowledge Distillation method. This could include, for example, the type of learning algorithm used (e.g., supervised, unsupervised, reinforcement), the specific algorithm or method (e.g., gradient descent, backpropagation), and any modifications made to the standard algorithm or method. Providing this information would greatly enhance the clarity and reproducibility of the paper.",
                "The paper provides some details about the training process and the hyperparameters used for the baseline models in the experimental setup, such as the use of BERT-base and ALBERT-base as starting points for training, the number of transformer encoders, the hidden size of each word vector, the tokenization methods, and the evaluation metrics. However, more specific details are needed for reproducibility and evaluation. For instance, the paper should include information about the training duration, the number of training iterations, the learning rate, and any regularization techniques used. These details are essential for understanding the full scope of the training process and for accurately reproducing the experiments and evaluating the models' performance.",
                "The paper should provide more specific details about the 'Con Fusion' mechanism, particularly about how the output embeddings from the speech-BERT and text-BERT models are concatenated, and about the encoding layer in the CMRC module where the concatenated output is passed. Without these details, it's difficult for other researchers to understand the mechanism's benefits or reproduce it."
            ],
            "all": [
                "The paper mentions that the DDNet is designed to handle noisy ASR transcriptions, which is a common issue in spoken dialogue systems. However, there is no explicit experiment or evaluation that tests the DDNet's robustness to artificially added noise in the ASR transcriptions. This is a significant omission, as it is crucial to evaluate the robustness of the DDNet to noisy ASR transcriptions to validate its effectiveness in real-world scenarios where noise is often present. I recommend adding an experiment where noise is artificially introduced to the ASR transcriptions, and the performance of the DDNet is evaluated under these conditions. This would provide a more comprehensive evaluation of the DDNet's capabilities and strengthen the paper's contributions.",
                "Unclear Evaluation of Knowledge Distillation: The authors mention the use of knowledge distillation in the training process of the models in the 'BASELINES' section. However, it is not explicitly stated which models were trained with and without knowledge distillation. This makes it difficult to evaluate the effectiveness of the knowledge distillation approach. The authors should consider clearly indicating which models were trained with and without knowledge distillation and discussing the impact of knowledge distillation on the performance of the models.",
                "While the authors have introduced the new Spoken-CoQA dataset and provided some evaluation, the evaluation could be more thorough and explicit. Specifically, it would be helpful to include more detailed results of the comparison with existing SQA datasets and the performance of the DDNet model trained on the Spoken-CoQA dataset. Additionally, an explicit comparison of the Spoken-CoQA dataset with other similar datasets could further demonstrate its value. Providing these details would make it easier for readers to assess the quality and value of the new dataset.",
                "The paper lacks a detailed ablation study for the proposed DDNet. An ablation study, which involves removing each component of the DDNet one by one and evaluating the performance of the resulting models, is crucial for understanding the contribution of each component to the overall performance. Without such a study, it's challenging to discern which components are essential and which are not. This information is not only valuable for readers seeking to replicate or build upon your work, but also beneficial for future research and for further improvements to the DDNet. Therefore, it is strongly recommended that the authors include a comprehensive ablation study in the paper.",
                "The paper introduces a novel fusion mechanism, Con Fusion, and provides a comparison of its performance with the Cross Attention mechanism. However, it would be beneficial to provide a more detailed analysis of the contribution of the Con Fusion mechanism to the overall performance of the DDNet. Specifically, an experiment comparing the performance of the DDNet with and without the Con Fusion mechanism could help to clarify its impact. This would allow readers to better understand the value of the Con Fusion mechanism and its role in the DDNet.",
                "The paper lacks detailed information about the diversity of the Spoken-CoQA dataset, specifically in terms of accents, languages, and topics. Understanding the diversity of the dataset is crucial for assessing the generalizability of the model. For instance, if the dataset is predominantly in one language or accent, or covers a limited range of topics, the model's performance may not generalize well to other languages, accents, or topics. Therefore, the authors should provide more comprehensive information about the diversity of the Spoken-CoQA dataset.",
                "The paper lacks a detailed explanation of how the DDNet handles noise in the audio data. Specifically, it would be beneficial to understand how the system distinguishes between noise and speech, and how it handles different types of noise. This information is crucial for evaluating the effectiveness of the system and its applicability in real-world scenarios where noise is often present.",
                "The paper should provide a more detailed comparison between the 'novel unified data distillation approach' and the 'multi-modality fusion mechanism' and existing methods such as FlowQA, SDNet, BERT-base, and ALBERT. Specifically, the paper should clearly highlight the novelty and advantages of these methods over the existing methods. This could be achieved by providing a detailed discussion of the results of the comparison, explaining why the novel methods outperform the existing methods, and discussing the implications of these results for the field of conversational question answering.",
                "The paper uses the term 'misalignment' to describe the discrepancy between automatic speech recognition (ASR) hypotheses and reference transcriptions, but it does not provide a specific definition or measurement for this term. A clear definition or measurement for 'misalignment' is crucial for understanding how the DDNet is evaluated and how it compares to other models. For example, the paper could define 'misalignment' as the number of words in the ASR transcript that do not match the manual transcript, and it could measure 'misalignment' as the percentage of words in the ASR transcript that are misaligned. This would allow readers to understand the scale of the 'misalignment' and how it affects the performance of the DDNet.",
                "The paper provides some reasons why the chosen models were used as baselines, namely their superior performance. However, it would be helpful to provide more specific details about the strengths and weaknesses of each baseline model. This would help readers understand the context of the paper's claims of superiority. For example, the paper could discuss how each model performs on single-turn tasks versus multi-turn conversational tasks, or how they handle spoken content and text documents simultaneously.",
                "The paper lacks a discussion on the choice of metrics used to evaluate the proposed method. Specifically, it does not justify why EM and F1 scores are the most appropriate metrics for this task. Other potentially relevant metrics such as precision, recall, and accuracy are not mentioned or discussed. A detailed discussion on the choice of metrics is crucial as it provides transparency about how the results were evaluated and allows for a fair comparison with other methods or models that use the same metrics. I recommend the authors to include a section discussing why EM and F1 scores were chosen, and why other metrics such as precision, recall, and accuracy were not used. This will help readers better understand the evaluation method and the performance of the proposed method.",
                "While the paper provides an overview of the DDNet's data distillation process, it lacks specific details that are crucial for reproducibility and for other researchers to evaluate its effectiveness. Specifically, the paper does not provide the specific parameters, such as the learning rate, batch size, number of epochs, or any other hyperparameters used in training the models. Additionally, the paper does not describe the specific methods used for training the teacher and student models, the criteria used to evaluate the student model's learning, and any techniques used to ensure the student model effectively imbibes the teacher model's knowledge. Providing these details would greatly enhance the paper's value to the research community.",
                "The paper mentions the use of default attention layers from four baseline models but does not detail the specific types of attention layers used. Similarly, while the paper mentions that the Output Layer computes the probability distribution of the start and end index within the entire documents to predict an answer, it does not detail the method used to compute this probability distribution. These details are crucial for a clear understanding of the model's inner workings and for anyone attempting to replicate the study's results. Therefore, it is recommended that the authors provide these specific details to enhance the clarity and reproducibility of the paper.",
                "The paper discusses the use of a Knowledge Distillation method where a student model learns from a teacher model. However, it lacks specific details about the learning algorithm or method used for this process. This omission makes it difficult to fully understand the method and to reproduce it. To improve the paper, the authors should provide a clear and detailed description of the learning algorithm or method used in the Knowledge Distillation method. This could include, for example, the type of learning algorithm used (e.g., supervised, unsupervised, reinforcement), the specific algorithm or method (e.g., gradient descent, backpropagation), and any modifications made to the standard algorithm or method. Providing this information would greatly enhance the clarity and reproducibility of the paper.",
                "The paper provides some details about the training process and the hyperparameters used for the baseline models in the experimental setup, such as the use of BERT-base and ALBERT-base as starting points for training, the number of transformer encoders, the hidden size of each word vector, the tokenization methods, and the evaluation metrics. However, more specific details are needed for reproducibility and evaluation. For instance, the paper should include information about the training duration, the number of training iterations, the learning rate, and any regularization techniques used. These details are essential for understanding the full scope of the training process and for accurately reproducing the experiments and evaluating the models' performance.",
                "The paper should provide more specific details about the 'Con Fusion' mechanism, particularly about how the output embeddings from the speech-BERT and text-BERT models are concatenated, and about the encoding layer in the CMRC module where the concatenated output is passed. Without these details, it's difficult for other researchers to understand the mechanism's benefits or reproduce it."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [
                {
                    "reference_id": 0,
                    "predicted_id": 7,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The work is limited in its scope and is not yet ready to be published at ICLR due to lack of comparisons made to prior approaches, making it hard to assess the merits of the proposed approach.",
                    "predicted_comment": "The paper should provide a more detailed comparison between the 'novel unified data distillation approach' and the 'multi-modality fusion mechanism' and existing methods such as FlowQA, SDNet, BERT-base, and ALBERT. Specifically, the paper should clearly highlight the novelty and advantages of these methods over the existing methods. This could be achieved by providing a detailed discussion of the results of the comparison, explaining why the novel methods outperform the existing methods, and discussing the implications of these results for the field of conversational question answering."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 7,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper's contributions and novelty are unclear, and it's uncertain if they warrant acceptance.",
                    "predicted_comment": "The paper should provide a more detailed comparison between the 'novel unified data distillation approach' and the 'multi-modality fusion mechanism' and existing methods such as FlowQA, SDNet, BERT-base, and ALBERT. Specifically, the paper should clearly highlight the novelty and advantages of these methods over the existing methods. This could be achieved by providing a detailed discussion of the results of the comparison, explaining why the novel methods outperform the existing methods, and discussing the implications of these results for the field of conversational question answering."
                }
            ],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 0,
                    "predicted_id": 7,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The work is limited in its scope and is not yet ready to be published at ICLR due to lack of comparisons made to prior approaches, making it hard to assess the merits of the proposed approach.",
                    "predicted_comment": "The paper should provide a more detailed comparison between the 'novel unified data distillation approach' and the 'multi-modality fusion mechanism' and existing methods such as FlowQA, SDNet, BERT-base, and ALBERT. Specifically, the paper should clearly highlight the novelty and advantages of these methods over the existing methods. This could be achieved by providing a detailed discussion of the results of the comparison, explaining why the novel methods outperform the existing methods, and discussing the implications of these results for the field of conversational question answering."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 7,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper's contributions and novelty are unclear, and it's uncertain if they warrant acceptance.",
                    "predicted_comment": "The paper should provide a more detailed comparison between the 'novel unified data distillation approach' and the 'multi-modality fusion mechanism' and existing methods such as FlowQA, SDNet, BERT-base, and ALBERT. Specifically, the paper should clearly highlight the novelty and advantages of these methods over the existing methods. This could be achieved by providing a detailed discussion of the results of the comparison, explaining why the novel methods outperform the existing methods, and discussing the implications of these results for the field of conversational question answering."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 18,
                "n_generated": 5,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 18,
                "n_generated": 6,
                "n_total_aligns": 2,
                "recall": 0.09166666666666667,
                "precision": 0.08333333333333333,
                "pseudo_jaccard": 0.04772727272727273,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 18,
                "n_generated": 5,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 18,
                "n_generated": 16,
                "n_total_aligns": 2,
                "recall": 0.09166666666666667,
                "precision": 0.03125,
                "pseudo_jaccard": 0.024404761904761905,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.16666666666666666,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.09090909090909091,
                    "n_real": 6
                },
                {
                    "recall": 0.2,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.1,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.16666666666666666,
                    "precision": 0.0625,
                    "pseudo_jaccard": 0.047619047619047616,
                    "n_real": 6
                },
                {
                    "recall": 0.2,
                    "precision": 0.0625,
                    "pseudo_jaccard": 0.05,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "0NQdxInFWT_",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The method is a small extension to the DPS method where the network that selects which rows to samples is conditioned on the existing measurements.",
                "The paper did not compare to any other active sampling strategies. The authors made no effort to replicate existing methods.",
                "The Markov chain example in section 4.1 was hard to follow and more distracting than informative.",
                "The results with active sampling were only marginally better than results with a fixed (learned) sampling strategy.",
                "The method is applicable only to true subsampling problems, not general sensing. That is, one isn't designing the rows of a measurement matrix on the fly but rather selecting which row from an existing matrix (identity in most of the examples) that one would like to sample from.",
                "Because of the nonlinearity in the forward model, equation (9) is not actually proximal gradient descent. There's a sign(F^HD\\circFX) term missing from the (sub) gradient.",
                "The expression A^H(Ax-Y\\circ sign(A(x))) is a subgradient of 1/2|| Y - |A(X)|||^2 but A^H(|Ax|-Y) is not. I would avoid calling (9) projected gradient descent as the 'gradient' isn't really a gradient."
            ],
            [
                "The proposed approach appears to be incremental: ADPS adds a simple sequential update structure (of a context vector) to DPS, which can be described by only two equations (6 and 7). The simplicity of the changes proposed (over DPS) is not a limitation, but it could be accompanied by an in-depth theoretical analysis, a convincing qualitative discussion or extensive experiments demonstrating the practical relevance of the proposed approach.",
                "The paper contributes by extending DPS into an active/sequential learning framework achieving significant performance gains over DPS, mainly on toy problems. However, there is room for more extensive experimental validation, perhaps beyond MRI."
            ],
            [
                "The paper lacks comparison to other active sub-sampling schemes, such as those proposed by Zhang et al., 2019 and Jin et al., 2019. It would be beneficial to see if the proposed method produces better results than these existing methods.",
                "The toy example provided is overly constructed and difficult to understand, which does not improve the quality of the paper.",
                "It is unclear what happens when the MNIST sampling ratio in Figure 3a is further increased. It would be helpful to know if A-DPS consistently outperforms DPS in low sampling ratio regimes."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper has demonstrated the versatility of the A-DPS network through tests on a toy example, the MNIST database, and the NYU fastMRI database. These tests cover a range of applications, from simple classification to complex image reconstruction. However, future work could potentially explore testing the A-DPS network on additional tasks and datasets. This could provide further insights into the network's performance and its ability to generalize to different types of data and tasks.",
                "The ablation study conducted on the LSTM component of the A-DPS network for the toy example and the MNIST classification task provides valuable insights into its role in the network. However, it would be beneficial to conduct further ablation studies on other components of the network, such as the fully connected layers in the task and sampling models, to fully understand their individual contributions to the network's performance. In particular, conducting these ablation studies across all tasks, including the MRI reconstruction task, would help to understand how each component contributes to the performance of A-DPS across different tasks. This would provide a more comprehensive understanding of the A-DPS network and could potentially lead to further improvements in its performance.",
                "The paper provides a useful discussion on the computational complexity of A-DPS and its comparison with DPS in terms of training time per epoch. However, it would be beneficial to include details about the memory usage of A-DPS to provide a more comprehensive understanding of the network's computational efficiency. Additionally, comparing the computational complexity of A-DPS not only with DPS but also with other state-of-the-art methods in the field would provide a broader context for the efficiency of A-DPS."
            ],
            "novelty_impact": [
                "The paper presents a novel method, A-DPS, which is an extension of the DPS method. The motivation behind this work is clear and relevant, aiming to improve the efficiency of data acquisition in various fields by reducing the number of sample acquisitions needed. However, while the paper mentions potential applications in various fields such as 3D and dynamic MRI, CT, ultrasound, radar, video, and MIMO systems, it lacks specific examples or theoretical discussions on how A-DPS could be adapted to these fields. This limits the generalizability of the method. To strengthen the paper, it would be beneficial to provide specific examples or theoretical discussions on how A-DPS could be adapted to these fields. This would not only demonstrate the versatility of the method but also provide readers with a clearer understanding of its potential applications.",
                "The paper presents the A-DPS method, an extension of the DPS framework, and demonstrates its capabilities through a series of experiments. However, the paper lacks a discussion on the sensitivity of the A-DPS method to changes in the data distribution. Given that the A-DPS method selects samples in an iterative fashion and the sampling distribution at each time step depends on the information acquired in previous time steps, it would be valuable to understand how changes in the data distribution could affect the performance of the method. Additionally, the paper does not provide strategies for dealing with such changes. Addressing these points in future work could strengthen the robustness of the method and its applicability in practice.",
                "The paper discusses the use of two sampling schemes: DPS and A-DPS. While the A-DPS scheme is more adaptive and can potentially yield better performance, especially on multimodal data, it has a higher computational complexity, which could limit its applicability in scenarios where computational resources are limited or where real-time processing is required. Furthermore, the paper does not provide a comprehensive comparison with other active sampling strategies, which could potentially limit the generalizability of the results. In future work, it would be beneficial to address these points, perhaps by exploring ways to reduce the computational complexity of A-DPS and by comparing A-DPS with other active sampling strategies.",
                "The method has been tested on the MNIST database and the NYU fastMRI database, which provide some evidence of its performance on image classification and MRI reconstruction tasks. However, to fully validate the effectiveness of the method in real-world scenarios and support the claim that it outperforms other sampling pattern selection methods on downstream task performance, it would be beneficial to test the method on more diverse and complex datasets. This could include datasets from different domains, datasets with different types of data (e.g., text, audio, video), and datasets with more complex structures and relationships. This would provide a more comprehensive understanding of the method's performance and its potential applications.",
                "The paper mentions the computational complexity and adaptation rate of the A-DPS method, but the details are scattered and not clearly linked. The computational complexity is discussed in Appendix A and in the conclusion section, while the adaptation rate is indirectly mentioned through the trade-off between computational complexity and adaptation rate. The paper also provides a comparison of training times per epoch for DPS and A-DPS, which could be seen as a practical implication of the trade-off. However, it would be helpful if the authors could consolidate this information and provide a clearer explanation of the trade-off. This would make it easier for readers to understand the practical implications of using the A-DPS method.",
                "The paper mentions that A-DPS improves over DPS and other methods in various contexts, such as MNIST classification at high subsampling rates and active acquisition MRI reconstruction. However, it lacks specific statistics or quantitative data to support these claims. For example, the paper could provide the specific scores based on the normalized mean square error (NMSE), the peak signal-to-noise ratio (PSNR), and the structural similarity index (SSIM) for A-DPS and the other methods. This would allow the reader to understand the extent of the improvement and to compare the performance of A-DPS with other methods. Without these statistics, it is difficult to assess the practical significance of the claimed improvements.",
                "The paper does not adequately address potential biases in the NYU fastMRI database, which is a significant limitation given that the database is used to test the Active Deep Probabilistic Subsampling (A-DPS) method. A more detailed discussion on the potential biases in the database and how these biases could affect the generalizability of the results would strengthen the paper. This is particularly important as the A-DPS method is proposed to be applicable to a wide variety of tasks. Therefore, understanding how potential biases could affect the generalizability of the results is crucial for future applications of the A-DPS method.",
                "The paper does not explicitly discuss the inherent assumptions and potential limitations of the LSTM model used in the A-DPS method. For instance, the LSTM model assumes the importance of long-term dependencies in the data and its ability to capture these dependencies. However, potential limitations such as susceptibility to vanishing and exploding gradients, or its computational complexity, are not addressed. These limitations could potentially affect the performance and efficiency of the A-DPS method, limiting its applicability to datasets with complex dependencies or noise. Future work could strengthen the robustness of the method by exploring alternative models that can capture long-term dependencies with less computational complexity, or implementing techniques to mitigate the issues of vanishing and exploding gradients."
            ],
            "clarity_reproducibility": [
                "The paper provides detailed information about the Active Deep Probabilistic Subsampling (A-DPS) method, including how it actively picks the next sample based on the information acquired so far, the context vector, and the output of the task model. However, these concepts might be complex for some readers to understand, and the information is distributed across different sections and might not be easy to follow. We suggest that the authors provide a more simplified explanation or a visual representation to help readers better understand these concepts, and consolidate and clarify this information in a dedicated section or subsection for better readability and understanding.",
                "The paper presents a 'challenging toy problem' where a model identifies informative elements from a data stream generated by a hidden Markov model. However, it lacks specific details about how the task model determines which elements are informative and how it observes one color channel out of every three. Providing these details would enhance the reader's understanding of the problem and the task model's strategy. For instance, the paper could explain the criteria or algorithm the task model uses to identify informative elements and the rationale behind observing only one color channel out of every three. The purpose of this suggestion is to improve the clarity and completeness of the paper. By providing specific details about the 'challenging toy problem', the authors can help readers better understand the problem and the task model's strategy. This could also strengthen the paper's argument by showing how the task model effectively handles this problem.",
                "The paper provides specific details about the training of the A-DPS network and the size of the training and validation datasets for the MNIST and MRI experiments. However, it would be helpful if the authors could provide similar details for all experiments, including the toy problem. Providing these details would enhance the reproducibility of the experiments and allow readers to better understand the performance of the A-DPS network.",
                "The paper could benefit from providing more specific details about the experiments conducted. For the MNIST database, the authors mention using different subsampling ratios, but the specific sampling rates are not provided. This information is crucial for understanding the experimental setup and for replicating the experiments. For the MRI data, the authors describe using a deep unfolded proximal gradient method for estimating the original image from the partial measurements. However, more specific details about this process, such as the number of iterations used (K), would be helpful for understanding the method and its implementation. Providing these details would strengthen the paper by allowing readers to fully understand the experimental setup and the methods used.",
                "Major Comment: The paper discusses the computational complexity of A-DPS, the unrolling of iterations, and the determination of the sampling ratio \"\u03c1\" in various sections. However, these details are not consolidated and clearly explained in a single section. It would be beneficial to provide a more detailed and consolidated explanation of these aspects in a dedicated section. This would make it easier for readers to understand these important aspects of the A-DPS algorithm. Specifically, the paper could explain why the computational complexity of A-DPS is higher than learned fixed sampling schemes due to the unrolling of iterations, and how the choice of \u03c1 between 1 and M constitutes a trade-off between computational complexity and adaptation rate. Understanding these aspects is crucial for interpreting the method's efficiency and applicability.",
                "The ablation study section could benefit from more specific details. While the paper mentions that replacing the LSTM with a fully connected layer resulted in unstable training, it does not provide a clear explanation or hypothesis for this outcome. Given the importance of the LSTM in the proposed framework, understanding why its absence leads to instability could provide valuable insights into the workings of the model. The authors are encouraged to elaborate on this point to enhance the clarity and depth of the paper.",
                "The paper mentions the use of a GeForce GTX 1080 Ti for the experiments, which is a good start for reproducibility. However, to fully ensure reproducibility and allow others to build upon this work, it would be beneficial to include more specific details about the software and versions used. This information is crucial as it can significantly affect the results and conclusions drawn from the experiments. Please consider adding this information in the methodology or experimental setup section.",
                "The paper provides a detailed description of the A-DPS method and its applications. However, it would be beneficial for the readers if the authors could also discuss any limitations or potential drawbacks of the method. For instance, the authors could discuss the computational cost of the method, its scalability, its performance compared to other methods, and any assumptions made in the method. This would provide a more balanced view of the method and help readers understand its potential limitations and areas for improvement."
            ],
            "all": [
                "The paper has demonstrated the versatility of the A-DPS network through tests on a toy example, the MNIST database, and the NYU fastMRI database. These tests cover a range of applications, from simple classification to complex image reconstruction. However, future work could potentially explore testing the A-DPS network on additional tasks and datasets. This could provide further insights into the network's performance and its ability to generalize to different types of data and tasks.",
                "The ablation study conducted on the LSTM component of the A-DPS network for the toy example and the MNIST classification task provides valuable insights into its role in the network. However, it would be beneficial to conduct further ablation studies on other components of the network, such as the fully connected layers in the task and sampling models, to fully understand their individual contributions to the network's performance. In particular, conducting these ablation studies across all tasks, including the MRI reconstruction task, would help to understand how each component contributes to the performance of A-DPS across different tasks. This would provide a more comprehensive understanding of the A-DPS network and could potentially lead to further improvements in its performance.",
                "The paper provides a useful discussion on the computational complexity of A-DPS and its comparison with DPS in terms of training time per epoch. However, it would be beneficial to include details about the memory usage of A-DPS to provide a more comprehensive understanding of the network's computational efficiency. Additionally, comparing the computational complexity of A-DPS not only with DPS but also with other state-of-the-art methods in the field would provide a broader context for the efficiency of A-DPS.",
                "The paper presents a novel method, A-DPS, which is an extension of the DPS method. The motivation behind this work is clear and relevant, aiming to improve the efficiency of data acquisition in various fields by reducing the number of sample acquisitions needed. However, while the paper mentions potential applications in various fields such as 3D and dynamic MRI, CT, ultrasound, radar, video, and MIMO systems, it lacks specific examples or theoretical discussions on how A-DPS could be adapted to these fields. This limits the generalizability of the method. To strengthen the paper, it would be beneficial to provide specific examples or theoretical discussions on how A-DPS could be adapted to these fields. This would not only demonstrate the versatility of the method but also provide readers with a clearer understanding of its potential applications.",
                "The paper presents the A-DPS method, an extension of the DPS framework, and demonstrates its capabilities through a series of experiments. However, the paper lacks a discussion on the sensitivity of the A-DPS method to changes in the data distribution. Given that the A-DPS method selects samples in an iterative fashion and the sampling distribution at each time step depends on the information acquired in previous time steps, it would be valuable to understand how changes in the data distribution could affect the performance of the method. Additionally, the paper does not provide strategies for dealing with such changes. Addressing these points in future work could strengthen the robustness of the method and its applicability in practice.",
                "The paper discusses the use of two sampling schemes: DPS and A-DPS. While the A-DPS scheme is more adaptive and can potentially yield better performance, especially on multimodal data, it has a higher computational complexity, which could limit its applicability in scenarios where computational resources are limited or where real-time processing is required. Furthermore, the paper does not provide a comprehensive comparison with other active sampling strategies, which could potentially limit the generalizability of the results. In future work, it would be beneficial to address these points, perhaps by exploring ways to reduce the computational complexity of A-DPS and by comparing A-DPS with other active sampling strategies.",
                "The method has been tested on the MNIST database and the NYU fastMRI database, which provide some evidence of its performance on image classification and MRI reconstruction tasks. However, to fully validate the effectiveness of the method in real-world scenarios and support the claim that it outperforms other sampling pattern selection methods on downstream task performance, it would be beneficial to test the method on more diverse and complex datasets. This could include datasets from different domains, datasets with different types of data (e.g., text, audio, video), and datasets with more complex structures and relationships. This would provide a more comprehensive understanding of the method's performance and its potential applications.",
                "The paper mentions the computational complexity and adaptation rate of the A-DPS method, but the details are scattered and not clearly linked. The computational complexity is discussed in Appendix A and in the conclusion section, while the adaptation rate is indirectly mentioned through the trade-off between computational complexity and adaptation rate. The paper also provides a comparison of training times per epoch for DPS and A-DPS, which could be seen as a practical implication of the trade-off. However, it would be helpful if the authors could consolidate this information and provide a clearer explanation of the trade-off. This would make it easier for readers to understand the practical implications of using the A-DPS method.",
                "The paper mentions that A-DPS improves over DPS and other methods in various contexts, such as MNIST classification at high subsampling rates and active acquisition MRI reconstruction. However, it lacks specific statistics or quantitative data to support these claims. For example, the paper could provide the specific scores based on the normalized mean square error (NMSE), the peak signal-to-noise ratio (PSNR), and the structural similarity index (SSIM) for A-DPS and the other methods. This would allow the reader to understand the extent of the improvement and to compare the performance of A-DPS with other methods. Without these statistics, it is difficult to assess the practical significance of the claimed improvements.",
                "The paper does not adequately address potential biases in the NYU fastMRI database, which is a significant limitation given that the database is used to test the Active Deep Probabilistic Subsampling (A-DPS) method. A more detailed discussion on the potential biases in the database and how these biases could affect the generalizability of the results would strengthen the paper. This is particularly important as the A-DPS method is proposed to be applicable to a wide variety of tasks. Therefore, understanding how potential biases could affect the generalizability of the results is crucial for future applications of the A-DPS method.",
                "The paper does not explicitly discuss the inherent assumptions and potential limitations of the LSTM model used in the A-DPS method. For instance, the LSTM model assumes the importance of long-term dependencies in the data and its ability to capture these dependencies. However, potential limitations such as susceptibility to vanishing and exploding gradients, or its computational complexity, are not addressed. These limitations could potentially affect the performance and efficiency of the A-DPS method, limiting its applicability to datasets with complex dependencies or noise. Future work could strengthen the robustness of the method by exploring alternative models that can capture long-term dependencies with less computational complexity, or implementing techniques to mitigate the issues of vanishing and exploding gradients.",
                "The paper provides detailed information about the Active Deep Probabilistic Subsampling (A-DPS) method, including how it actively picks the next sample based on the information acquired so far, the context vector, and the output of the task model. However, these concepts might be complex for some readers to understand, and the information is distributed across different sections and might not be easy to follow. We suggest that the authors provide a more simplified explanation or a visual representation to help readers better understand these concepts, and consolidate and clarify this information in a dedicated section or subsection for better readability and understanding.",
                "The paper presents a 'challenging toy problem' where a model identifies informative elements from a data stream generated by a hidden Markov model. However, it lacks specific details about how the task model determines which elements are informative and how it observes one color channel out of every three. Providing these details would enhance the reader's understanding of the problem and the task model's strategy. For instance, the paper could explain the criteria or algorithm the task model uses to identify informative elements and the rationale behind observing only one color channel out of every three. The purpose of this suggestion is to improve the clarity and completeness of the paper. By providing specific details about the 'challenging toy problem', the authors can help readers better understand the problem and the task model's strategy. This could also strengthen the paper's argument by showing how the task model effectively handles this problem.",
                "The paper provides specific details about the training of the A-DPS network and the size of the training and validation datasets for the MNIST and MRI experiments. However, it would be helpful if the authors could provide similar details for all experiments, including the toy problem. Providing these details would enhance the reproducibility of the experiments and allow readers to better understand the performance of the A-DPS network.",
                "The paper could benefit from providing more specific details about the experiments conducted. For the MNIST database, the authors mention using different subsampling ratios, but the specific sampling rates are not provided. This information is crucial for understanding the experimental setup and for replicating the experiments. For the MRI data, the authors describe using a deep unfolded proximal gradient method for estimating the original image from the partial measurements. However, more specific details about this process, such as the number of iterations used (K), would be helpful for understanding the method and its implementation. Providing these details would strengthen the paper by allowing readers to fully understand the experimental setup and the methods used.",
                "Major Comment: The paper discusses the computational complexity of A-DPS, the unrolling of iterations, and the determination of the sampling ratio \"\u03c1\" in various sections. However, these details are not consolidated and clearly explained in a single section. It would be beneficial to provide a more detailed and consolidated explanation of these aspects in a dedicated section. This would make it easier for readers to understand these important aspects of the A-DPS algorithm. Specifically, the paper could explain why the computational complexity of A-DPS is higher than learned fixed sampling schemes due to the unrolling of iterations, and how the choice of \u03c1 between 1 and M constitutes a trade-off between computational complexity and adaptation rate. Understanding these aspects is crucial for interpreting the method's efficiency and applicability.",
                "The ablation study section could benefit from more specific details. While the paper mentions that replacing the LSTM with a fully connected layer resulted in unstable training, it does not provide a clear explanation or hypothesis for this outcome. Given the importance of the LSTM in the proposed framework, understanding why its absence leads to instability could provide valuable insights into the workings of the model. The authors are encouraged to elaborate on this point to enhance the clarity and depth of the paper.",
                "The paper mentions the use of a GeForce GTX 1080 Ti for the experiments, which is a good start for reproducibility. However, to fully ensure reproducibility and allow others to build upon this work, it would be beneficial to include more specific details about the software and versions used. This information is crucial as it can significantly affect the results and conclusions drawn from the experiments. Please consider adding this information in the methodology or experimental setup section.",
                "The paper provides a detailed description of the A-DPS method and its applications. However, it would be beneficial for the readers if the authors could also discuss any limitations or potential drawbacks of the method. For instance, the authors could discuss the computational cost of the method, its scalability, its performance compared to other methods, and any assumptions made in the method. This would provide a more balanced view of the method and help readers understand its potential limitations and areas for improvement."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 1,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper contributes by extending DPS into an active/sequential learning framework achieving significant performance gains over DPS, mainly on toy problems. However, there is room for more extensive experimental validation, perhaps beyond MRI.",
                    "predicted_comment": "The paper has demonstrated the versatility of the A-DPS network through tests on a toy example, the MNIST database, and the NYU fastMRI database. These tests cover a range of applications, from simple classification to complex image reconstruction. However, future work could potentially explore testing the A-DPS network on additional tasks and datasets. This could provide further insights into the network's performance and its ability to generalize to different types of data and tasks."
                }
            ],
            "novelty_impact": [
                {
                    "reference_id": 1,
                    "predicted_id": 5,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper did not compare to any other active sampling strategies. The authors made no effort to replicate existing methods.",
                    "predicted_comment": "The paper discusses the use of two sampling schemes: DPS and A-DPS. While the A-DPS scheme is more adaptive and can potentially yield better performance, especially on multimodal data, it has a higher computational complexity, which could limit its applicability in scenarios where computational resources are limited or where real-time processing is required. Furthermore, the paper does not provide a comprehensive comparison with other active sampling strategies, which could potentially limit the generalizability of the results. In future work, it would be beneficial to address these points, perhaps by exploring ways to reduce the computational complexity of A-DPS and by comparing A-DPS with other active sampling strategies."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 6,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper contributes by extending DPS into an active/sequential learning framework achieving significant performance gains over DPS, mainly on toy problems. However, there is room for more extensive experimental validation, perhaps beyond MRI.",
                    "predicted_comment": "The method has been tested on the MNIST database and the NYU fastMRI database, which provide some evidence of its performance on image classification and MRI reconstruction tasks. However, to fully validate the effectiveness of the method in real-world scenarios and support the claim that it outperforms other sampling pattern selection methods on downstream task performance, it would be beneficial to test the method on more diverse and complex datasets. This could include datasets from different domains, datasets with different types of data (e.g., text, audio, video), and datasets with more complex structures and relationships. This would provide a more comprehensive understanding of the method's performance and its potential applications."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 5,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper lacks comparison to other active sub-sampling schemes, such as those proposed by Zhang et al., 2019 and Jin et al., 2019. It would be beneficial to see if the proposed method produces better results than these existing methods.",
                    "predicted_comment": "The paper discusses the use of two sampling schemes: DPS and A-DPS. While the A-DPS scheme is more adaptive and can potentially yield better performance, especially on multimodal data, it has a higher computational complexity, which could limit its applicability in scenarios where computational resources are limited or where real-time processing is required. Furthermore, the paper does not provide a comprehensive comparison with other active sampling strategies, which could potentially limit the generalizability of the results. In future work, it would be beneficial to address these points, perhaps by exploring ways to reduce the computational complexity of A-DPS and by comparing A-DPS with other active sampling strategies."
                }
            ],
            "clarity_reproducibility": [
                {
                    "reference_id": 1,
                    "predicted_id": 12,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The toy example provided is overly constructed and difficult to understand, which does not improve the quality of the paper.",
                    "predicted_comment": "The paper presents a 'challenging toy problem' where a model identifies informative elements from a data stream generated by a hidden Markov model. However, it lacks specific details about how the task model determines which elements are informative and how it observes one color channel out of every three. Providing these details would enhance the reader's understanding of the problem and the task model's strategy. For instance, the paper could explain the criteria or algorithm the task model uses to identify informative elements and the rationale behind observing only one color channel out of every three. The purpose of this suggestion is to improve the clarity and completeness of the paper. By providing specific details about the 'challenging toy problem', the authors can help readers better understand the problem and the task model's strategy. This could also strengthen the paper's argument by showing how the task model effectively handles this problem."
                }
            ],
            "all": [
                {
                    "reference_id": 1,
                    "predicted_id": 5,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper did not compare to any other active sampling strategies. The authors made no effort to replicate existing methods.",
                    "predicted_comment": "The paper discusses the use of two sampling schemes: DPS and A-DPS. While the A-DPS scheme is more adaptive and can potentially yield better performance, especially on multimodal data, it has a higher computational complexity, which could limit its applicability in scenarios where computational resources are limited or where real-time processing is required. Furthermore, the paper does not provide a comprehensive comparison with other active sampling strategies, which could potentially limit the generalizability of the results. In future work, it would be beneficial to address these points, perhaps by exploring ways to reduce the computational complexity of A-DPS and by comparing A-DPS with other active sampling strategies."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 6,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper contributes by extending DPS into an active/sequential learning framework achieving significant performance gains over DPS, mainly on toy problems. However, there is room for more extensive experimental validation, perhaps beyond MRI.",
                    "predicted_comment": "The method has been tested on the MNIST database and the NYU fastMRI database, which provide some evidence of its performance on image classification and MRI reconstruction tasks. However, to fully validate the effectiveness of the method in real-world scenarios and support the claim that it outperforms other sampling pattern selection methods on downstream task performance, it would be beneficial to test the method on more diverse and complex datasets. This could include datasets from different domains, datasets with different types of data (e.g., text, audio, video), and datasets with more complex structures and relationships. This would provide a more comprehensive understanding of the method's performance and its potential applications."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper contributes by extending DPS into an active/sequential learning framework achieving significant performance gains over DPS, mainly on toy problems. However, there is room for more extensive experimental validation, perhaps beyond MRI.",
                    "predicted_comment": "The paper has demonstrated the versatility of the A-DPS network through tests on a toy example, the MNIST database, and the NYU fastMRI database. These tests cover a range of applications, from simple classification to complex image reconstruction. However, future work could potentially explore testing the A-DPS network on additional tasks and datasets. This could provide further insights into the network's performance and its ability to generalize to different types of data and tasks."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 5,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper lacks comparison to other active sub-sampling schemes, such as those proposed by Zhang et al., 2019 and Jin et al., 2019. It would be beneficial to see if the proposed method produces better results than these existing methods.",
                    "predicted_comment": "The paper discusses the use of two sampling schemes: DPS and A-DPS. While the A-DPS scheme is more adaptive and can potentially yield better performance, especially on multimodal data, it has a higher computational complexity, which could limit its applicability in scenarios where computational resources are limited or where real-time processing is required. Furthermore, the paper does not provide a comprehensive comparison with other active sampling strategies, which could potentially limit the generalizability of the results. In future work, it would be beneficial to address these points, perhaps by exploring ways to reduce the computational complexity of A-DPS and by comparing A-DPS with other active sampling strategies."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 12,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The toy example provided is overly constructed and difficult to understand, which does not improve the quality of the paper.",
                    "predicted_comment": "The paper presents a 'challenging toy problem' where a model identifies informative elements from a data stream generated by a hidden Markov model. However, it lacks specific details about how the task model determines which elements are informative and how it observes one color channel out of every three. Providing these details would enhance the reader's understanding of the problem and the task model's strategy. For instance, the paper could explain the criteria or algorithm the task model uses to identify informative elements and the rationale behind observing only one color channel out of every three. The purpose of this suggestion is to improve the clarity and completeness of the paper. By providing specific details about the 'challenging toy problem', the authors can help readers better understand the problem and the task model's strategy. This could also strengthen the paper's argument by showing how the task model effectively handles this problem."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 12,
                "n_generated": 3,
                "n_total_aligns": 1,
                "recall": 0.16666666666666666,
                "precision": 0.1111111111111111,
                "pseudo_jaccard": 0.08333333333333333,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 12,
                "n_generated": 8,
                "n_total_aligns": 3,
                "recall": 0.32539682539682535,
                "precision": 0.125,
                "pseudo_jaccard": 0.09417989417989418,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 12,
                "n_generated": 8,
                "n_total_aligns": 1,
                "recall": 0.1111111111111111,
                "precision": 0.041666666666666664,
                "pseudo_jaccard": 0.03333333333333333,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "all": {
                "n_reference": 12,
                "n_generated": 19,
                "n_total_aligns": 5,
                "recall": 0.43650793650793646,
                "precision": 0.08771929824561403,
                "pseudo_jaccard": 0.07230769230769231,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.5,
                    "precision": 0.3333333333333333,
                    "pseudo_jaccard": 0.25,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.14285714285714285,
                    "precision": 0.125,
                    "pseudo_jaccard": 0.07142857142857142,
                    "n_real": 7
                },
                {
                    "recall": 0.5,
                    "precision": 0.125,
                    "pseudo_jaccard": 0.1111111111111111,
                    "n_real": 2
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.125,
                    "pseudo_jaccard": 0.1,
                    "n_real": 3
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.125,
                    "pseudo_jaccard": 0.1,
                    "n_real": 3
                }
            ],
            "all": [
                {
                    "recall": 0.14285714285714285,
                    "precision": 0.05263157894736842,
                    "pseudo_jaccard": 0.04,
                    "n_real": 7
                },
                {
                    "recall": 0.5,
                    "precision": 0.10526315789473684,
                    "pseudo_jaccard": 0.07692307692307693,
                    "n_real": 2
                },
                {
                    "recall": 0.6666666666666666,
                    "precision": 0.10526315789473684,
                    "pseudo_jaccard": 0.1,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "-spj8FZD4y2",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper lacks a complete theoretical analysis, such as the upper bound/lower bound on the regret of the algorithm.",
                "The algorithm should be explicitly written out with its pseudocode.",
                "The paper states that the communication is one-way from the decision-maker towards the controller, but it is strange not to consider the realization history observed by the decision-maker as communication. The goal of this work and how it differs from existing papers is unclear.",
                "The problem setup is unclear: it is not specified whether the state is i.i.d. sampled from the distribution $P_S$ or not.",
                "The relation between the constraint quantities $(\\rho,R)$ and the discussions in Section 3 is unclear.",
                "The constraints seem to be placed in the posterior probabilities involved in the TS algorithms. It is not clear what would happen if we consider UCB-class algorithms.",
                "The definitions of $D_\\alpha$ and reverse KL divergence $D_{KL}$ are missing. Usually, the original KL divergence is denoted by $D_{KL}$.",
                "In Section 3.3, it states ''If $S$ and $A$ are independent...'', which is not a common assumption in contextual bandits."
            ],
            [
                "There are inconsistencies and imprecisions in the writing, particularly in Section 2, which defines quantities, regrets, and gains that are not analyzed afterwards. Clarification is needed on the definitions and usage of G, \\rho, and R.",
                "The paper does not provide regret bounds. While it may be out of the scope of the paper, some commentary on this would be beneficial.",
                "The algorithm needs to be clearly outlined. While detailed explanations are provided, a pseudo-code would greatly improve readability and understanding.",
                "The paper could be better connected with existing literature on contextual bandit, such as contextual bandits with context uncertainties or Partially Observable MDPs."
            ],
            [
                "The results in the paper are largely adapted from known information-theoretic results, which limits the overall novelty of the work.",
                "The assumption that each agent's state is observed by the decision-maker, but not the agent itself, is unusual. The authors should clarify why they make this assumption.",
                "The practical coding scheme is about sending the state to the corresponding agent, which may not be necessary in practice.",
                "The controller 'decoder' function should have the joint states of all agents as an input to enhance the decoder design."
            ],
            [
                "The paper lacks theoretical performance guarantees for the clustering coding scheme. A characterization of how regret depends on the number of states, the rate constraint, number of actions, etc. would provide a more in depth understanding of the performances of the clustering scheme in various scenarios.",
                "The paper does not discuss how the proposed clustering schemes\u2019 performance decays as the total number of states grows. More states would induce larger clusters and hence larger distortion in the policy defined over the compressed states (compared to the target policy). This is particularly relevant as the paper uses personalized ad placements as a motivating example, where the content owner\u2019s valuation for a user view (i.e. impression) or conversion serves as the 'context' (or state), and may take a wide range of values."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "While the paper discusses the performance of the policy obtained through the asymptotic information theoretic formulation and the one obtained through the clustering scheme, it lacks a direct and explicit comparison between these two methods using specific metrics. This comparison is crucial to validate the effectiveness of the proposed methods and to understand their relative strengths and weaknesses. Therefore, it is suggested that the authors conduct an experiment where the performance of these methods is directly compared using regret and the rate of data transmission as metrics. This would provide a clearer understanding of the trade-off between the number of bits sent per agent and the acquired average reward, and the relation between the asymptotic rate bound and the learning phase of agents.",
                "While the paper does discuss the relation between the asymptotic rate bound and the learning phase of agents, and even conducts an experiment analyzing this relationship, the analysis could be clearer and more detailed. Understanding this relation is crucial to understand how to save communication resources when training a multi-agent system. It would be helpful if the authors could provide more details about the experiment conducted and the results obtained, specifically in relation to the regret and the rate of data transmission. A graphical representation of the results would also be beneficial for readers to better understand the relationship and its implications. Furthermore, a more explicit discussion on how these findings could be used to save communication resources when training a multi-agent system would be valuable.",
                "While the paper does compare experimental results with theoretical predictions in some sections, it would be beneficial to consistently apply this approach throughout the paper. Specifically, the performance of the proposed solution in terms of regret and communication rate, as discussed in the theoretical sections, could be compared with experimental results. This would provide a more comprehensive validation of the theoretical analysis and the effectiveness of the proposed methods."
            ],
            "novelty_impact": [
                "While the paper provides a clear definition of the Contextual Multi-Armed Bandit (CMAB) problem and introduces a novel Rate-Constrained CMAB (RC-CMAB) problem, it lacks a direct comparison between the two. Specifically, it would be beneficial to highlight the unique challenges and constraints introduced by the communication rate in the RC-CMAB problem, and how these differentiate it from the standard CMAB problem.",
                "The paper presents a novel coding scheme based on state reduction and the Lloyd algorithm. However, it lacks a direct comparison with other existing coding schemes, such as JPEG, BPG, MPEG, H.264, and deep learning-based compression algorithms. Including a comparison with these specific schemes would strengthen the paper by providing a clearer context for the proposed scheme's performance and advantages. This comparison could highlight the novelty or superiority of the proposed scheme, helping readers better understand its value.",
                "The paper provides a comprehensive analysis of the rate-constrained contextual multi-armed bandit (RC-CMAB) problem, including the impact of the communication rate constraint on the decision-making process and the performance of the agents. However, it does not discuss the reliability of the communication channel or the impact of errors or losses in communication. In real-world scenarios, communication errors are common and can significantly affect the performance of the system. Therefore, it would be beneficial to include a discussion on how such errors or losses could impact the proposed method and its performance. This could increase the applicability of the results in real-world scenarios and provide a more robust analysis of the system under different conditions.",
                "The paper makes several strong assumptions that are crucial for the formulation of the problem and the proposed methods. These include a limited communication link between the decision-maker and the controller, a uniform state distribution, and the best action response not being a one-to-one mapping with the state. However, the paper does not discuss potential extensions or modifications of the proposed methods for scenarios where these assumptions do not hold. For instance, the paper could explore how additional communication resources could be utilized to improve the performance of the proposed methods if the communication link is not limited. It could also discuss different strategies for optimizing the decision-maker's policy in scenarios where the state distribution is not uniform. Furthermore, the paper could explore different strategies for mapping the history and states of the agents to a message index and mapping the received message to a set of actions for the agents if the best action response is a one-to-one mapping with the state. Discussing these potential extensions or modifications would help readers understand how the proposed methods perform under different conditions and increase the applicability of the results. It would also provide valuable insights into the flexibility and adaptability of the proposed methods.",
                "The paper provides a clear discussion of the use of the KL-divergence in a rate-distortion optimization problem and the application of the rate-distortion function in three different problems. However, it would be beneficial to provide more explicit details on how the KL-divergence is defined and used in this context. For instance, in the optimization objective of Eq. (3), it would be helpful to explain why KL-divergence is used as a distortion function and how it contributes to the double minimization problem. In the practical coding scheme, it would be useful to elaborate on how KL-divergence is used to minimize the divergence between the representative \u00b5 j * and the original policy, and why this minimization is important. Similarly, in the formulation of the RC-CMAB problem, it would be beneficial to explain how KL-divergence measures the divergence between the sampling distribution Q and the target posterior probability \u03c0, and why this measurement is significant. Providing these details would make the paper more accessible to readers who are not familiar with KL-divergence.",
                "While the paper compares the performance of different agents (Perfect, Comm, Cluster, and Marginal) in the RC-CMAB problem based on their regret, it would be beneficial to provide more details on the specific benchmarks used for this comparison. The regret is a standard measure in multi-armed bandit problems, but the use of additional benchmarks could provide a more comprehensive evaluation of the agents' performance. This would help readers understand the relative strengths and weaknesses of the agents and the practical implications of the results.",
                "The paper provides a clear discussion of the relation between the asymptotic rate bound and the learning phase of agents, supported by both theoretical proof and empirical evidence. However, it would be beneficial to further strengthen this claim by providing more detailed empirical evidence. Specifically, additional experiments that demonstrate the relationship in different scenarios or under varying conditions could help to further validate the claim. Furthermore, a more in-depth discussion of the theoretical proof, particularly how it applies to the finite agent scenario, could help to clarify and solidify the claim.",
                "The paper presents a comparison between the policy obtained through the asymptotic information theoretic formulation and the one obtained through the clustering scheme, supported by graphical representations. However, the paper could be significantly improved by providing more quantitative results or statistical tests. These could provide a measure of the variability or uncertainty associated with the results, allowing for a more rigorous comparison between the different policies. This would not only strengthen the paper's conclusions but also make them more convincing to the reader.",
                "The paper presents a novel approach to the CMAB problem and provides a clear discussion of the proposed methods. The problem statement is well-articulated, and the novelty of the approach is evident. The method description is thorough, and the conclusions have significant implications. However, the paper could be further improved by providing a more detailed description of the experimental design, including the specific scenarios tested, the parameters used, and the rationale behind the chosen design. Additionally, a more thorough interpretation of the results, including a discussion on the implications of the results and how they support the conclusions, would enhance the paper's impact. These improvements would provide readers with a better understanding of the experiments conducted and the significance of the results, thereby increasing the paper's impact."
            ],
            "clarity_reproducibility": [],
            "all": [
                "While the paper discusses the performance of the policy obtained through the asymptotic information theoretic formulation and the one obtained through the clustering scheme, it lacks a direct and explicit comparison between these two methods using specific metrics. This comparison is crucial to validate the effectiveness of the proposed methods and to understand their relative strengths and weaknesses. Therefore, it is suggested that the authors conduct an experiment where the performance of these methods is directly compared using regret and the rate of data transmission as metrics. This would provide a clearer understanding of the trade-off between the number of bits sent per agent and the acquired average reward, and the relation between the asymptotic rate bound and the learning phase of agents.",
                "While the paper does discuss the relation between the asymptotic rate bound and the learning phase of agents, and even conducts an experiment analyzing this relationship, the analysis could be clearer and more detailed. Understanding this relation is crucial to understand how to save communication resources when training a multi-agent system. It would be helpful if the authors could provide more details about the experiment conducted and the results obtained, specifically in relation to the regret and the rate of data transmission. A graphical representation of the results would also be beneficial for readers to better understand the relationship and its implications. Furthermore, a more explicit discussion on how these findings could be used to save communication resources when training a multi-agent system would be valuable.",
                "While the paper does compare experimental results with theoretical predictions in some sections, it would be beneficial to consistently apply this approach throughout the paper. Specifically, the performance of the proposed solution in terms of regret and communication rate, as discussed in the theoretical sections, could be compared with experimental results. This would provide a more comprehensive validation of the theoretical analysis and the effectiveness of the proposed methods.",
                "While the paper provides a clear definition of the Contextual Multi-Armed Bandit (CMAB) problem and introduces a novel Rate-Constrained CMAB (RC-CMAB) problem, it lacks a direct comparison between the two. Specifically, it would be beneficial to highlight the unique challenges and constraints introduced by the communication rate in the RC-CMAB problem, and how these differentiate it from the standard CMAB problem.",
                "The paper presents a novel coding scheme based on state reduction and the Lloyd algorithm. However, it lacks a direct comparison with other existing coding schemes, such as JPEG, BPG, MPEG, H.264, and deep learning-based compression algorithms. Including a comparison with these specific schemes would strengthen the paper by providing a clearer context for the proposed scheme's performance and advantages. This comparison could highlight the novelty or superiority of the proposed scheme, helping readers better understand its value.",
                "The paper provides a comprehensive analysis of the rate-constrained contextual multi-armed bandit (RC-CMAB) problem, including the impact of the communication rate constraint on the decision-making process and the performance of the agents. However, it does not discuss the reliability of the communication channel or the impact of errors or losses in communication. In real-world scenarios, communication errors are common and can significantly affect the performance of the system. Therefore, it would be beneficial to include a discussion on how such errors or losses could impact the proposed method and its performance. This could increase the applicability of the results in real-world scenarios and provide a more robust analysis of the system under different conditions.",
                "The paper makes several strong assumptions that are crucial for the formulation of the problem and the proposed methods. These include a limited communication link between the decision-maker and the controller, a uniform state distribution, and the best action response not being a one-to-one mapping with the state. However, the paper does not discuss potential extensions or modifications of the proposed methods for scenarios where these assumptions do not hold. For instance, the paper could explore how additional communication resources could be utilized to improve the performance of the proposed methods if the communication link is not limited. It could also discuss different strategies for optimizing the decision-maker's policy in scenarios where the state distribution is not uniform. Furthermore, the paper could explore different strategies for mapping the history and states of the agents to a message index and mapping the received message to a set of actions for the agents if the best action response is a one-to-one mapping with the state. Discussing these potential extensions or modifications would help readers understand how the proposed methods perform under different conditions and increase the applicability of the results. It would also provide valuable insights into the flexibility and adaptability of the proposed methods.",
                "The paper provides a clear discussion of the use of the KL-divergence in a rate-distortion optimization problem and the application of the rate-distortion function in three different problems. However, it would be beneficial to provide more explicit details on how the KL-divergence is defined and used in this context. For instance, in the optimization objective of Eq. (3), it would be helpful to explain why KL-divergence is used as a distortion function and how it contributes to the double minimization problem. In the practical coding scheme, it would be useful to elaborate on how KL-divergence is used to minimize the divergence between the representative \u00b5 j * and the original policy, and why this minimization is important. Similarly, in the formulation of the RC-CMAB problem, it would be beneficial to explain how KL-divergence measures the divergence between the sampling distribution Q and the target posterior probability \u03c0, and why this measurement is significant. Providing these details would make the paper more accessible to readers who are not familiar with KL-divergence.",
                "While the paper compares the performance of different agents (Perfect, Comm, Cluster, and Marginal) in the RC-CMAB problem based on their regret, it would be beneficial to provide more details on the specific benchmarks used for this comparison. The regret is a standard measure in multi-armed bandit problems, but the use of additional benchmarks could provide a more comprehensive evaluation of the agents' performance. This would help readers understand the relative strengths and weaknesses of the agents and the practical implications of the results.",
                "The paper provides a clear discussion of the relation between the asymptotic rate bound and the learning phase of agents, supported by both theoretical proof and empirical evidence. However, it would be beneficial to further strengthen this claim by providing more detailed empirical evidence. Specifically, additional experiments that demonstrate the relationship in different scenarios or under varying conditions could help to further validate the claim. Furthermore, a more in-depth discussion of the theoretical proof, particularly how it applies to the finite agent scenario, could help to clarify and solidify the claim.",
                "The paper presents a comparison between the policy obtained through the asymptotic information theoretic formulation and the one obtained through the clustering scheme, supported by graphical representations. However, the paper could be significantly improved by providing more quantitative results or statistical tests. These could provide a measure of the variability or uncertainty associated with the results, allowing for a more rigorous comparison between the different policies. This would not only strengthen the paper's conclusions but also make them more convincing to the reader.",
                "The paper presents a novel approach to the CMAB problem and provides a clear discussion of the proposed methods. The problem statement is well-articulated, and the novelty of the approach is evident. The method description is thorough, and the conclusions have significant implications. However, the paper could be further improved by providing a more detailed description of the experimental design, including the specific scenarios tested, the parameters used, and the rationale behind the chosen design. Additionally, a more thorough interpretation of the results, including a discussion on the implications of the results and how they support the conclusions, would enhance the paper's impact. These improvements would provide readers with a better understanding of the experiments conducted and the significance of the results, thereby increasing the paper's impact."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [],
            "clarity_reproducibility": [],
            "all": []
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 18,
                "n_generated": 3,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 18,
                "n_generated": 9,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "clarity_reproducibility": {
                "n_reference": 18,
                "n_generated": 0,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 18,
                "n_generated": 12,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                }
            ]
        }
    },
    {
        "doc_id": "7udZAsEzd60",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The implication of Theorem 4.2 is not fully understood. The size(rho, kappa) critically depends on r, the rank of the weight matrix W. But if we decompose W=UV, where U is d*r and V=r*k, it seems like the VC dimension of HANN is still higher than the number of effective parameters, dr+rk. In this case, what benefit do HANN have over ReLU and sigmoid networks?",
                "The role of the binary activation in HANN is unclear. The critical assumption of Theorem 4.2 is that the weight matrix is low-rank. In this case, can't the layer with low-rank weight be reformulated as two layers with a small bottleneck hidden layer of dimensionality r, which has much smaller number of parameters than the equivalent network? If this is the case, the result seems to be only relevant to the low-rank assumption, not the HANN itself."
            ],
            [
                "The title and introduction of the paper suggest that a more general class of Quantized Neural Networks (QNNs) have small VC dimension, but the model studied is essentially a two-layer NN. This raises doubts about whether any non-toy quantized models will have small VC dimension. An experiment similar to Zhang et al 2017 could easily verify this.",
                "The paper's focus on studying the VC dimension of Neural Networks (NNs) is questionable. The authors should clarify what we are aiming to learn from this, as in practice VC dimensions often provide non-meaningful bounds.",
                "The title and introduction of the paper may overstate the significance of the results, and the motivation for the study may not be strong enough."
            ],
            [
                "The bound on the VC dimension for hyperplane augmented neural networks (HANNs) seems quite large: O(k^r). This seems to be a major limitation in the application of the main result of the paper.",
                "The paper could significantly benefit from a discussion on the large bound on the VC dimension, either explaining more cases where the results are useful or clarifying this limitation.",
                "Using the numbers given in the experiments, with a dataset of size 77904 and k=100, and r=3 seems to result in vacuous bounds on generalization again, which questions the effectiveness of the main result."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "While the paper provides a detailed discussion on the performance of HANNs and uses unseen data in the form of a held-out test set, it lacks explicit generalization testing. Generalization testing is a key aspect of machine learning that demonstrates how well a model can perform on unseen data. It would be beneficial to include experiments that specifically test the generalization of the neural networks on unseen data. This could involve using a separate dataset not used in the training or validation process, or applying techniques such as cross-validation. Including such tests would not only strengthen the paper's claims about the performance of HANNs but also provide a more comprehensive evaluation of the model's performance.",
                "While the paper provides strong theoretical proofs about the VC dimension and expressiveness of the HANNs, it would be beneficial to include more detailed empirical evidence. Specifically, it would be helpful to see more detailed results from the experiments conducted using HANNs for classifying synthetic and real datasets. This could include a breakdown of the performance on different types of datasets, a comparison with other methods, and an analysis of the impact of different hyperparameters. This additional empirical evidence would provide further validation for the theoretical proofs and make the paper more convincing.",
                "Empirical Validation of Theoretical Claims: The paper provides mathematical proofs for many of its claims and includes empirical tests that benchmark the performance of HANNs on a panel of 121 UCI datasets. However, additional empirical tests could strengthen the paper. Specifically, applying HANNs to a wider variety of datasets, including those with different types of data (e.g., images, text, etc.) and different levels of complexity, could provide further evidence of the versatility and effectiveness of HANNs. Varying the parameters d, r, and k and observing the impact on performance could provide practical insights into the performance of HAC(d, r, k) and its applicability to real-world problems. These additional tests would provide empirical evidence to support the theoretical claims and demonstrate that the theoretical models accurately predict real-world outcomes.",
                "While the paper does provide comparisons of the performance of the HANNs with the self-normalizing neural network and the dendritic neural network, it would be beneficial to include comparisons with other relevant state-of-the-art models as well. This would provide a more comprehensive benchmarking of the new method. Additionally, while accuracy is a useful metric, considering other metrics could provide a more complete picture of the method's performance. For example, precision, recall, and F1 score could be considered for classification tasks, and mean squared error or mean absolute error could be considered for regression tasks.",
                "While the paper does discuss the testing of the SwishSign heuristic and the impact of the hyperparameter 'k', it would be beneficial to see a more comprehensive exploration of these aspects. For instance, testing a wider range of values for 'k' and comparing the performance of different training heuristics could provide a more robust understanding of the model's performance. This would also allow readers to better understand the sensitivity of the model to changes in these parameters.",
                "The paper could benefit from a more detailed explanation of the mathematical formulas and their implications. This would help readers who are not familiar with the subject matter to better understand the paper's findings."
            ],
            "novelty_impact": [
                "The paper presents a novel approach to understanding the generalization error of overparametrized neural networks by introducing a new class of neural networks called hyperplane arrangement neural networks (HANNs) and using a sample compression scheme to provide an upper bound on the VC dimension of HANNs. This approach challenges the conventional wisdom that the VC dimension of a neural network is proportional to its number of weights, providing a new perspective on a complex problem. This is a significant contribution to the field as it provides a new theoretical framework for understanding the generalization capabilities of overparametrized neural networks, a central problem in modern machine learning.",
                "The motivations and goals of the paper are clearly explained, and the authors' proposal of HANNs as a solution to the limitations of VC theory is well-presented. However, there are several areas where the authors could provide more detail or clarity to strengthen their argument and make the paper more accessible to readers.",
                "The authors could provide more clarity on the practical implications of their findings. Specifically, how does the use of HANNs impact the performance of neural networks in real-world applications?",
                "The methodology used to benchmark the performance of HANNs could be explained in more detail.",
                "In the section on sample compression schemes, the authors could provide more clarity on the steps of the proof for Theorem 4.2. They could also provide more detail on why the characteristics of the reconstruction function \u03c1 and the sample compression scheme \u03c1, \u03ba are significant and how they affect the results.",
                "In the section on empirical results, the authors could provide a detailed explanation of why HANNs perform better than other neural networks. They could also provide a clear explanation of how the hyperparameter k affects the performance of HANNs.",
                "The authors could provide more clarity on the mathematical derivations and assumptions in the 'Thus, x *' section. Providing more detailed explanations or intuitive interpretations could improve the paper. Additionally, in the 'B TRAINING DETAILS' section, the authors could provide more information about the choice of hyperparameters and the rationale behind them.",
                "The paper effectively presents its key findings. The authors provide a clear and logical explanation of how Hyperplane Arrangement Neural Networks (HANNs) can have a VC dimension significantly smaller than the number of weights, while still being highly expressive. This is supported by a sample compression analysis and empirical evidence showing that overparametrized HANNs can match the performance of state-of-the-art full-precision models on a panel of 121 UCI datasets. The paper also convincingly demonstrates that empirical risk minimization over HANNs in the overparametrized regime achieves the minimax rate for classification with Lipschitz posterior class probability. This is supported by Theorem 5.2 and a proof sketch. These findings are central to the paper's argument and contribute to its overall narrative. The authors have done a commendable job in presenting these findings in a manner that is both clear and logically connected to the rest of the paper.",
                "The paper makes a significant contribution to the field by addressing the 'generalization puzzle' of overparametrized neural networks, a topic of interest in the machine learning community. The introduction of Hyperplane Arrangement Neural Networks (HANNs) is well-executed, with a thorough evaluation on a panel of 121 UCI datasets and a detailed discussion of the impact of the number of hyperplanes used on the accuracy of HANNs. The paper also effectively builds on VC theory by presenting an upper bound on the VC dimension of HAC(d, r, k) and constructing a sample compression scheme for HAC(d, r, k). However, it would be beneficial to provide more explanation of how the VC theorem is used to prove minimaxoptimality of ERM over HANNs in an overparametrized setting with Lipschitz posterior. This would help readers better understand the significance of this contribution.",
                "While the expressiveness of HANNs is discussed in terms of their VC dimension and their ability to achieve minimax-optimality in an overparametrized setting, a specific measure or definition of expressiveness would strengthen the argument. This would provide a more concrete basis for comparing HANNs with other neural networks.",
                "The paper mentions that SwishSign leads to higher validation accuracy, but it would be beneficial to provide a hypothesis or reasoning that supports this finding. This would help readers understand why SwishSign outperforms SteSign.",
                "The paper claims that HANN models achieved comparable accuracy to other neural networks with fewer weights on 121 UCI datasets. However, a thorough statistical analysis supporting this claim would strengthen the argument. This could include details such as the specific statistical tests used, the p-values, and the effect sizes.",
                "The paper uses the VC theorem to provide an upper bound on the VC dimension of HAC(d, r, k) and discusses the limitations of VC theory in explaining the generalization puzzle of overparametrized neural networks. However, a discussion on potential limitations or biases associated with the use of the VC theorem itself would ensure a balanced and comprehensive analysis.",
                "The paper mentions that the HANN15 model is trained with a hyperparameter grid of size 3 where only the dropout rate is tuned. However, providing more information about the specific features or characteristics of the HANN15 model would help readers understand its relevance and significance in the study.",
                "The paper discusses some limitations of the study, including the non-permutation-invariance of the reconstruction function \u03c1 in the sample compression scheme and the instability of the overall sample compression scheme \u03c1, \u03ba. It also mentions that without distributional assumptions, no classifier can be minimax optimal in light of the No-Free-Lunch Theorem. However, a more comprehensive discussion of the limitations of the study would provide a balanced view of the work. This could include potential limitations of the methods used, assumptions made, and the implications of these limitations for the findings of the study."
            ],
            "clarity_reproducibility": [
                "The paper mentions the Vapnik-Chervonenkis (VC) theory and its limitations in several sections (Abstract, Introduction, Related Work, Activations, Discussion), but the explanation might not be detailed enough for readers who are not familiar with VC theory. It would be helpful to provide a more comprehensive explanation of VC theory and its limitations, particularly in relation to the small generalization error of overparametrized neural networks. For example, the paper could explain in more detail why existing bounds on the VC dimensions of neural networks are on the order of the number of weights, and why this is a limitation. The paper could also explain more about the Fundamental Theorem of Statistical Learning and how it relates to VC theory. This would help readers better understand the context and significance of the paper's findings.",
                "The proof of Theorem 4.2 involving the construction of a convex polyhedron in a (d + 1)k-dimensional space could be made more intuitive. The current explanation, particularly the section on the construction of the sample compression scheme, is quite technical and may be difficult for readers to follow. To improve the clarity and accessibility of this proof, consider the following: 1. Simplify the language: Replace some of the mathematical notations with plain English explanations where possible. For example, instead of saying '\u03c3 sgn (W x i + b) = \u03c3 sgn (V x i + c) = s i by construction', you could say 'the sign of the sum of the weights and bias in our model matches the sign of the sum of the weights and bias in the original model, as per our construction'. 2. Add diagrams: Include visual representations of the convex polyhedron P and the concept of a minimum norm element. A diagram showing the dimensions of the polyhedron and how the minimum norm element is determined could make these concepts more understandable. 3. Break down complex steps: Break down the process of constructing the sample compression scheme into smaller, more manageable steps. Each step could be explained in detail, with clear explanations of why each step is necessary and how it contributes to the overall proof. These changes would make the proof more accessible to readers and enhance their understanding of your work.",
                "The paper provides some details about the heuristics for training networks with threshold activation, specifically the straight-through-estimator (SteSign) and the SwishSign. However, it would be beneficial for the readers if the authors could provide a more comprehensive explanation of why SwishSign was chosen for the experiments, beyond the fact that it reliably leads to higher validation accuracy. This would help readers understand the rationale behind the choice of SwishSign and its advantages over other heuristics.",
                "The paper uses the probability function R(fn) to denote the risk of a learning algorithm or a binary classifier in various contexts such as minimax optimality for learning Lipschitz class, bounding terms in a proof, and demonstrating bounds on its value. However, the specific nature of R(fn) and its relationship with other variables such as n, d, k, C, and L are not clearly explained in all sections. Providing a more detailed explanation of R(fn) and its relationship with these variables would enhance the reader's understanding of the mathematical part of the paper.",
                "The paper could provide more specific details about how the dropout rate is tuned in the hyperparameter grid of size 3. Specifically, it would be helpful to know the range of dropout rates tested, the criteria used to select the optimal dropout rate, any considerations or constraints that influenced the selection of the dropout rate, and the method used to search the hyperparameter grid. This would significantly improve the reproducibility of the experiments."
            ],
            "all": [
                "While the paper provides a detailed discussion on the performance of HANNs and uses unseen data in the form of a held-out test set, it lacks explicit generalization testing. Generalization testing is a key aspect of machine learning that demonstrates how well a model can perform on unseen data. It would be beneficial to include experiments that specifically test the generalization of the neural networks on unseen data. This could involve using a separate dataset not used in the training or validation process, or applying techniques such as cross-validation. Including such tests would not only strengthen the paper's claims about the performance of HANNs but also provide a more comprehensive evaluation of the model's performance.",
                "While the paper provides strong theoretical proofs about the VC dimension and expressiveness of the HANNs, it would be beneficial to include more detailed empirical evidence. Specifically, it would be helpful to see more detailed results from the experiments conducted using HANNs for classifying synthetic and real datasets. This could include a breakdown of the performance on different types of datasets, a comparison with other methods, and an analysis of the impact of different hyperparameters. This additional empirical evidence would provide further validation for the theoretical proofs and make the paper more convincing.",
                "Empirical Validation of Theoretical Claims: The paper provides mathematical proofs for many of its claims and includes empirical tests that benchmark the performance of HANNs on a panel of 121 UCI datasets. However, additional empirical tests could strengthen the paper. Specifically, applying HANNs to a wider variety of datasets, including those with different types of data (e.g., images, text, etc.) and different levels of complexity, could provide further evidence of the versatility and effectiveness of HANNs. Varying the parameters d, r, and k and observing the impact on performance could provide practical insights into the performance of HAC(d, r, k) and its applicability to real-world problems. These additional tests would provide empirical evidence to support the theoretical claims and demonstrate that the theoretical models accurately predict real-world outcomes.",
                "While the paper does provide comparisons of the performance of the HANNs with the self-normalizing neural network and the dendritic neural network, it would be beneficial to include comparisons with other relevant state-of-the-art models as well. This would provide a more comprehensive benchmarking of the new method. Additionally, while accuracy is a useful metric, considering other metrics could provide a more complete picture of the method's performance. For example, precision, recall, and F1 score could be considered for classification tasks, and mean squared error or mean absolute error could be considered for regression tasks.",
                "While the paper does discuss the testing of the SwishSign heuristic and the impact of the hyperparameter 'k', it would be beneficial to see a more comprehensive exploration of these aspects. For instance, testing a wider range of values for 'k' and comparing the performance of different training heuristics could provide a more robust understanding of the model's performance. This would also allow readers to better understand the sensitivity of the model to changes in these parameters.",
                "The paper could benefit from a more detailed explanation of the mathematical formulas and their implications. This would help readers who are not familiar with the subject matter to better understand the paper's findings.",
                "The paper presents a novel approach to understanding the generalization error of overparametrized neural networks by introducing a new class of neural networks called hyperplane arrangement neural networks (HANNs) and using a sample compression scheme to provide an upper bound on the VC dimension of HANNs. This approach challenges the conventional wisdom that the VC dimension of a neural network is proportional to its number of weights, providing a new perspective on a complex problem. This is a significant contribution to the field as it provides a new theoretical framework for understanding the generalization capabilities of overparametrized neural networks, a central problem in modern machine learning.",
                "The motivations and goals of the paper are clearly explained, and the authors' proposal of HANNs as a solution to the limitations of VC theory is well-presented. However, there are several areas where the authors could provide more detail or clarity to strengthen their argument and make the paper more accessible to readers.",
                "The authors could provide more clarity on the practical implications of their findings. Specifically, how does the use of HANNs impact the performance of neural networks in real-world applications?",
                "The methodology used to benchmark the performance of HANNs could be explained in more detail.",
                "In the section on sample compression schemes, the authors could provide more clarity on the steps of the proof for Theorem 4.2. They could also provide more detail on why the characteristics of the reconstruction function \u03c1 and the sample compression scheme \u03c1, \u03ba are significant and how they affect the results.",
                "In the section on empirical results, the authors could provide a detailed explanation of why HANNs perform better than other neural networks. They could also provide a clear explanation of how the hyperparameter k affects the performance of HANNs.",
                "The authors could provide more clarity on the mathematical derivations and assumptions in the 'Thus, x *' section. Providing more detailed explanations or intuitive interpretations could improve the paper. Additionally, in the 'B TRAINING DETAILS' section, the authors could provide more information about the choice of hyperparameters and the rationale behind them.",
                "The paper effectively presents its key findings. The authors provide a clear and logical explanation of how Hyperplane Arrangement Neural Networks (HANNs) can have a VC dimension significantly smaller than the number of weights, while still being highly expressive. This is supported by a sample compression analysis and empirical evidence showing that overparametrized HANNs can match the performance of state-of-the-art full-precision models on a panel of 121 UCI datasets. The paper also convincingly demonstrates that empirical risk minimization over HANNs in the overparametrized regime achieves the minimax rate for classification with Lipschitz posterior class probability. This is supported by Theorem 5.2 and a proof sketch. These findings are central to the paper's argument and contribute to its overall narrative. The authors have done a commendable job in presenting these findings in a manner that is both clear and logically connected to the rest of the paper.",
                "The paper makes a significant contribution to the field by addressing the 'generalization puzzle' of overparametrized neural networks, a topic of interest in the machine learning community. The introduction of Hyperplane Arrangement Neural Networks (HANNs) is well-executed, with a thorough evaluation on a panel of 121 UCI datasets and a detailed discussion of the impact of the number of hyperplanes used on the accuracy of HANNs. The paper also effectively builds on VC theory by presenting an upper bound on the VC dimension of HAC(d, r, k) and constructing a sample compression scheme for HAC(d, r, k). However, it would be beneficial to provide more explanation of how the VC theorem is used to prove minimaxoptimality of ERM over HANNs in an overparametrized setting with Lipschitz posterior. This would help readers better understand the significance of this contribution.",
                "While the expressiveness of HANNs is discussed in terms of their VC dimension and their ability to achieve minimax-optimality in an overparametrized setting, a specific measure or definition of expressiveness would strengthen the argument. This would provide a more concrete basis for comparing HANNs with other neural networks.",
                "The paper mentions that SwishSign leads to higher validation accuracy, but it would be beneficial to provide a hypothesis or reasoning that supports this finding. This would help readers understand why SwishSign outperforms SteSign.",
                "The paper claims that HANN models achieved comparable accuracy to other neural networks with fewer weights on 121 UCI datasets. However, a thorough statistical analysis supporting this claim would strengthen the argument. This could include details such as the specific statistical tests used, the p-values, and the effect sizes.",
                "The paper uses the VC theorem to provide an upper bound on the VC dimension of HAC(d, r, k) and discusses the limitations of VC theory in explaining the generalization puzzle of overparametrized neural networks. However, a discussion on potential limitations or biases associated with the use of the VC theorem itself would ensure a balanced and comprehensive analysis.",
                "The paper mentions that the HANN15 model is trained with a hyperparameter grid of size 3 where only the dropout rate is tuned. However, providing more information about the specific features or characteristics of the HANN15 model would help readers understand its relevance and significance in the study.",
                "The paper discusses some limitations of the study, including the non-permutation-invariance of the reconstruction function \u03c1 in the sample compression scheme and the instability of the overall sample compression scheme \u03c1, \u03ba. It also mentions that without distributional assumptions, no classifier can be minimax optimal in light of the No-Free-Lunch Theorem. However, a more comprehensive discussion of the limitations of the study would provide a balanced view of the work. This could include potential limitations of the methods used, assumptions made, and the implications of these limitations for the findings of the study.",
                "The paper mentions the Vapnik-Chervonenkis (VC) theory and its limitations in several sections (Abstract, Introduction, Related Work, Activations, Discussion), but the explanation might not be detailed enough for readers who are not familiar with VC theory. It would be helpful to provide a more comprehensive explanation of VC theory and its limitations, particularly in relation to the small generalization error of overparametrized neural networks. For example, the paper could explain in more detail why existing bounds on the VC dimensions of neural networks are on the order of the number of weights, and why this is a limitation. The paper could also explain more about the Fundamental Theorem of Statistical Learning and how it relates to VC theory. This would help readers better understand the context and significance of the paper's findings.",
                "The proof of Theorem 4.2 involving the construction of a convex polyhedron in a (d + 1)k-dimensional space could be made more intuitive. The current explanation, particularly the section on the construction of the sample compression scheme, is quite technical and may be difficult for readers to follow. To improve the clarity and accessibility of this proof, consider the following: 1. Simplify the language: Replace some of the mathematical notations with plain English explanations where possible. For example, instead of saying '\u03c3 sgn (W x i + b) = \u03c3 sgn (V x i + c) = s i by construction', you could say 'the sign of the sum of the weights and bias in our model matches the sign of the sum of the weights and bias in the original model, as per our construction'. 2. Add diagrams: Include visual representations of the convex polyhedron P and the concept of a minimum norm element. A diagram showing the dimensions of the polyhedron and how the minimum norm element is determined could make these concepts more understandable. 3. Break down complex steps: Break down the process of constructing the sample compression scheme into smaller, more manageable steps. Each step could be explained in detail, with clear explanations of why each step is necessary and how it contributes to the overall proof. These changes would make the proof more accessible to readers and enhance their understanding of your work.",
                "The paper provides some details about the heuristics for training networks with threshold activation, specifically the straight-through-estimator (SteSign) and the SwishSign. However, it would be beneficial for the readers if the authors could provide a more comprehensive explanation of why SwishSign was chosen for the experiments, beyond the fact that it reliably leads to higher validation accuracy. This would help readers understand the rationale behind the choice of SwishSign and its advantages over other heuristics.",
                "The paper uses the probability function R(fn) to denote the risk of a learning algorithm or a binary classifier in various contexts such as minimax optimality for learning Lipschitz class, bounding terms in a proof, and demonstrating bounds on its value. However, the specific nature of R(fn) and its relationship with other variables such as n, d, k, C, and L are not clearly explained in all sections. Providing a more detailed explanation of R(fn) and its relationship with these variables would enhance the reader's understanding of the mathematical part of the paper.",
                "The paper could provide more specific details about how the dropout rate is tuned in the hyperparameter grid of size 3. Specifically, it would be helpful to know the range of dropout rates tested, the criteria used to select the optimal dropout rate, any considerations or constraints that influenced the selection of the dropout rate, and the method used to search the hyperparameter grid. This would significantly improve the reproducibility of the experiments."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [
                {
                    "reference_id": 0,
                    "predicted_id": 18,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The bound on the VC dimension for hyperplane augmented neural networks (HANNs) seems quite large: O(k^r). This seems to be a major limitation in the application of the main result of the paper.",
                    "predicted_comment": "The paper uses the VC theorem to provide an upper bound on the VC dimension of HAC(d, r, k) and discusses the limitations of VC theory in explaining the generalization puzzle of overparametrized neural networks. However, a discussion on potential limitations or biases associated with the use of the VC theorem itself would ensure a balanced and comprehensive analysis."
                }
            ],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 0,
                    "predicted_id": 18,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The bound on the VC dimension for hyperplane augmented neural networks (HANNs) seems quite large: O(k^r). This seems to be a major limitation in the application of the main result of the paper.",
                    "predicted_comment": "The paper uses the VC theorem to provide an upper bound on the VC dimension of HAC(d, r, k) and discusses the limitations of VC theory in explaining the generalization puzzle of overparametrized neural networks. However, a discussion on potential limitations or biases associated with the use of the VC theorem itself would ensure a balanced and comprehensive analysis."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 8,
                "n_generated": 6,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 8,
                "n_generated": 15,
                "n_total_aligns": 1,
                "recall": 0.1111111111111111,
                "precision": 0.022222222222222223,
                "pseudo_jaccard": 0.0196078431372549,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 8,
                "n_generated": 5,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 8,
                "n_generated": 26,
                "n_total_aligns": 1,
                "recall": 0.1111111111111111,
                "precision": 0.012820512820512822,
                "pseudo_jaccard": 0.011904761904761904,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.06666666666666667,
                    "pseudo_jaccard": 0.058823529411764705,
                    "n_real": 3
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.038461538461538464,
                    "pseudo_jaccard": 0.03571428571428571,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "QmKblFEgQJ",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The empirical performance gains provided by the DIGRAC method are difficult to understand. In many of the ARI graphs on synthetic data, the performance of various methods seems quite close. In the real data, methods overlap in their error bars.",
                "More explanation is needed about when DIGRAC's model for clustering is appropriate for a dataset and when it is not, and how adaptive the model can be in terms of performance despite those considerations."
            ],
            [
                "The practicality of the problem and the method studied in this paper is questionable. The authors mention that 'real data sets with ground-truth flow imbalances are not available to date', so they use normalized imbalance scores to evaluate clustering performance. This approach seems to go against the typical order of how machine learning models are developed.",
                "The experiments in Section 4 do not demonstrate the practical importance of clustering with respect to flow imbalances. As this is an empirical paper, it is necessary to provide solid empirical evidence that showcases the need for clustering with respect to flow imbalances.",
                "The authors use 80% of all nodes for training in their empirical evaluations. This percentage seems too high and it is suggested to consider using a smaller percentage, such as 5%, of all nodes for training."
            ],
            [
                "The theoretical properties of the loss function defined in Eq. (2) and (3) are not clear. The paper should provide a theoretical characterization for this loss and discuss cases where it could lead to degenerate solutions.",
                "The constraints of the optimization problem need to be clearly presented.",
                "The paper should examine how the loss behaves in terms of convergence.",
                "The novelty of the DIMPA algorithm is unclear. The paper should clarify what are the novel parts of this model with respect to previous works.",
                "The baseline models used for comparison could be enhanced. The paper should consider adding models that capture information flow, including algorithms that rely on stochastic blockmodels and a model that is based on directed modularity.",
                "The paper lacks scalability experiments with the running time of the algorithm and its comparison to baseline models."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper introduces a novel probabilistic imbalance loss and a directed mixed path aggregation scheme (DIMPA) as part of the proposed method. However, there are no ablation studies to evaluate their individual contributions. It would be beneficial to conduct experiments where the probabilistic imbalance loss and DIMPA are removed or modified one at a time, and the impact on the results is observed. This would help to understand the importance of each component and how they contribute to the overall performance. For example, an ablation study could involve comparing the performance of the method with and without the probabilistic imbalance loss, or with different variants of this loss function. Similarly, the impact of the DIMPA scheme could be evaluated by comparing the performance with other path aggregation schemes.",
                "Incorporation of Label Information: The paper presents a method, DIGRAC, that can naturally incorporate node features and does not require label supervision. The method has been tested on both synthetic and real-world datasets and has shown promising results. However, the paper does not explicitly discuss the incorporation of label information or the performance of the method in a semi-supervised learning setting. It would be beneficial to conduct additional experiments where a small amount of labeled data is used in addition to the unlabeled data. This would allow for a comparison of the performance of the semi-supervised learning approach with the unsupervised learning approach. This additional experiment could provide valuable insights into the versatility and adaptability of the method when additional label information is available.",
                "The authors discuss several potential improvements to the DIGRAC model in the conclusion and future work sections of the paper, including conducting additional experiments in the semi-supervised setting, extending the framework to detect the number of clusters automatically, and building a more powerful framework that can automatically detect the value \u03b2 used in the current model. They also mention addressing the performance in the sparse regime and adapting the pipeline for directed clustering in extremely large networks. However, the authors do not conduct experiments to evaluate these improvements. Conducting such experiments would provide more concrete evidence of the method's potential and areas for future work. It would be particularly beneficial to see experiments evaluating the proposed semi-supervised setting and automatic detection of the number of clusters and the value \u03b2, as these improvements could significantly enhance the model's usability and performance."
            ],
            "novelty_impact": [
                "The imbalance score is a significant contribution of this paper, forming the basis of the self-supervised loss function used in the DIGRAC method. It is used to reveal clusters in the network even when there is no density difference between clusters, which is a novel approach in the field. However, the paper does not explicitly compare this new measure with other existing measures in the field. While the authors mention that DIGRAC outperforms other methods in terms of the Adjusted Rand Index (ARI) for synthetic data and using imbalance scores as outcome success measures for real-world data, it would be beneficial to see a direct comparison of the imbalance score with other measures. This would help to validate its effectiveness and provide a more comprehensive evaluation of DIGRAC's performance. For instance, the authors could consider comparing the imbalance score with other measures used in graph neural network (GNN) frameworks or other measures used to evaluate clustering performance.",
                "While the paper mentions that DIGRAC is an efficient GNN-based method and discusses its potential for large scale industrial applications, it does not provide a direct comparison of computational efficiency between DIGRAC and existing methods. Such a comparison is crucial for understanding the potential of DIGRAC for large scale industrial applications. The authors should consider providing this comparison, possibly by comparing DIGRAC's speed and space complexity with those of other methods such as DGCN. This would give a more complete picture of DIGRAC's performance and help readers understand its advantages and limitations.",
                "The paper describes the creation of 'ambient nodes' in the synthetic data for experiments, which are nodes not part of a meaningful cluster. However, it is not clear how the DIGRAC algorithm handles these ambient nodes during the clustering process. Providing this information would help readers understand the full capabilities of the DIGRAC algorithm and its potential limitations. Could the authors clarify this in the method description?",
                "The authors have provided some details on how DIGRAC can detect alternative patterns in the data, such as meta-graph structures, and how it can include exogenous information. However, these details could be explained more thoroughly. For instance, it would be helpful to provide more information on how the direction-based flow imbalance allows DIGRAC to reveal structures in directed graphs that may not be detectable by existing methods. Additionally, the authors could elaborate on how DIGRAC incorporates node-level features or labels into its analysis and how this ability allows it to overcome the limitations of existing spectral methods. The authors could also provide more clarity on how DIGRAC uses a directed mixed path aggregation scheme to obtain the probability assignment matrix P and feed it to the loss function.",
                "The paper would benefit from a more explicit discussion on the limitations or assumptions of the self-supervised loss function and the directed mixed path aggregation scheme. For instance, the self-supervised loss function is inspired by Cucuringu et al. (2020a), but it's unclear what assumptions this brings with it and how these might limit the function's applicability. Similarly, while the directed mixed path aggregation scheme is described as simple and effective, it would be helpful to understand any potential limitations of this simplicity. This discussion would help to identify potential shortcomings and areas for future improvement.",
                "The paper compares DIGRAC with other methods in terms of their ability to handle directionality, edge density, and other factors, which is commendable. However, it does not provide a comparison with existing methods that incorporate semi-supervised settings and automatic detection of the number of clusters. Given that the paper mentions future work in these areas, such a comparison could be beneficial for validating the effectiveness of DIGRAC and for identifying areas for future improvement. The authors should consider providing a comparison with these methods in the current paper or in future work."
            ],
            "clarity_reproducibility": [
                "The paper introduces a novel probabilistic imbalance loss, which is crucial to the proposed method. However, it lacks sufficient details on how the CI vol_sum, a key component of this loss, is calculated. Specifically, the paper does not provide the equation or the method to calculate CI vol_sum. This information is essential for understanding the proposed method and for replicating the results. Therefore, I recommend that the authors include a detailed explanation and the specific equation for calculating CI vol_sum in the paper.",
                "The paper provides an overview of the Directed Mixed Path Aggregation (DIMPA) scheme, but it lacks specific details and equations about how the weighted average of information from neighbors is calculated and what factors determine the weight assigned to each neighbor. These details are crucial for understanding the DIMPA scheme and for other researchers to replicate the study. We recommend the authors to include these details in the paper to enhance its clarity and reproducibility.",
                "The paper mentions the use of the Adjusted Rand Index (ARI) and accuracy for node classification and link direction prediction as performance measures. However, it lacks specific details on how these calculations are performed. Providing these details would allow readers to fully understand the evaluation process and replicate the results. It would also be helpful to explain why these particular metrics were chosen and how they contribute to the evaluation of the proposed method.",
                "The method section and the experimental results section discuss the use of the largest K eigenvectors of a Hermitian matrix to construct an input feature matrix. However, the paper does not provide specific guidance on how to choose the value of K. This lack of guidance could hinder the reproducibility of the method and make it difficult for readers to understand the method fully. It would be beneficial if the authors could provide more detailed guidance or a rationale for choosing the value of K. This could include, for example, a discussion on the impact of different values of K on the results, or a method for determining the optimal value of K based on the data or problem context.",
                "The paper mentions the use of seed nodes in a semi-supervised setting and the application of a supervised loss function to these nodes. However, it lacks specific guidance on how to select these seed nodes. This lack of detail could make it difficult for readers to understand or implement the proposed method. It would be beneficial to provide more explicit instructions or criteria for selecting seed nodes, which could include factors to consider, potential strategies, or examples from the experiments conducted.",
                "While the paper introduces DIGRAC as a novel method for obtaining node embeddings for clustering directed networks and mentions its potential use in a semi-supervised setting, it lacks sufficient details about how this would work in practice. Specifically, it would be beneficial to provide more information about how DIGRAC can be enhanced with seed nodes that have known cluster labels or when additional information is available in the form of must-link and cannot-link constraints. This would help readers understand how to apply DIGRAC in a semi-supervised setting and could potentially broaden the applicability of the method. Furthermore, including additional experiments or case studies demonstrating the use of DIGRAC in a semi-supervised setting could provide valuable evidence of its effectiveness in such scenarios.",
                "The paper provides some details about the hyperparameters used in the experiments, which is appreciated. However, it lacks specific information about the equipment and material specifications used in the experiments. This information is crucial for other researchers to accurately reproduce the experiments and validate the results. Please provide more details about the equipment and material specifications, such as the type and model of the equipment, the software and hardware specifications, and any other materials used."
            ],
            "all": [
                "The paper introduces a novel probabilistic imbalance loss and a directed mixed path aggregation scheme (DIMPA) as part of the proposed method. However, there are no ablation studies to evaluate their individual contributions. It would be beneficial to conduct experiments where the probabilistic imbalance loss and DIMPA are removed or modified one at a time, and the impact on the results is observed. This would help to understand the importance of each component and how they contribute to the overall performance. For example, an ablation study could involve comparing the performance of the method with and without the probabilistic imbalance loss, or with different variants of this loss function. Similarly, the impact of the DIMPA scheme could be evaluated by comparing the performance with other path aggregation schemes.",
                "Incorporation of Label Information: The paper presents a method, DIGRAC, that can naturally incorporate node features and does not require label supervision. The method has been tested on both synthetic and real-world datasets and has shown promising results. However, the paper does not explicitly discuss the incorporation of label information or the performance of the method in a semi-supervised learning setting. It would be beneficial to conduct additional experiments where a small amount of labeled data is used in addition to the unlabeled data. This would allow for a comparison of the performance of the semi-supervised learning approach with the unsupervised learning approach. This additional experiment could provide valuable insights into the versatility and adaptability of the method when additional label information is available.",
                "The authors discuss several potential improvements to the DIGRAC model in the conclusion and future work sections of the paper, including conducting additional experiments in the semi-supervised setting, extending the framework to detect the number of clusters automatically, and building a more powerful framework that can automatically detect the value \u03b2 used in the current model. They also mention addressing the performance in the sparse regime and adapting the pipeline for directed clustering in extremely large networks. However, the authors do not conduct experiments to evaluate these improvements. Conducting such experiments would provide more concrete evidence of the method's potential and areas for future work. It would be particularly beneficial to see experiments evaluating the proposed semi-supervised setting and automatic detection of the number of clusters and the value \u03b2, as these improvements could significantly enhance the model's usability and performance.",
                "The imbalance score is a significant contribution of this paper, forming the basis of the self-supervised loss function used in the DIGRAC method. It is used to reveal clusters in the network even when there is no density difference between clusters, which is a novel approach in the field. However, the paper does not explicitly compare this new measure with other existing measures in the field. While the authors mention that DIGRAC outperforms other methods in terms of the Adjusted Rand Index (ARI) for synthetic data and using imbalance scores as outcome success measures for real-world data, it would be beneficial to see a direct comparison of the imbalance score with other measures. This would help to validate its effectiveness and provide a more comprehensive evaluation of DIGRAC's performance. For instance, the authors could consider comparing the imbalance score with other measures used in graph neural network (GNN) frameworks or other measures used to evaluate clustering performance.",
                "While the paper mentions that DIGRAC is an efficient GNN-based method and discusses its potential for large scale industrial applications, it does not provide a direct comparison of computational efficiency between DIGRAC and existing methods. Such a comparison is crucial for understanding the potential of DIGRAC for large scale industrial applications. The authors should consider providing this comparison, possibly by comparing DIGRAC's speed and space complexity with those of other methods such as DGCN. This would give a more complete picture of DIGRAC's performance and help readers understand its advantages and limitations.",
                "The paper describes the creation of 'ambient nodes' in the synthetic data for experiments, which are nodes not part of a meaningful cluster. However, it is not clear how the DIGRAC algorithm handles these ambient nodes during the clustering process. Providing this information would help readers understand the full capabilities of the DIGRAC algorithm and its potential limitations. Could the authors clarify this in the method description?",
                "The authors have provided some details on how DIGRAC can detect alternative patterns in the data, such as meta-graph structures, and how it can include exogenous information. However, these details could be explained more thoroughly. For instance, it would be helpful to provide more information on how the direction-based flow imbalance allows DIGRAC to reveal structures in directed graphs that may not be detectable by existing methods. Additionally, the authors could elaborate on how DIGRAC incorporates node-level features or labels into its analysis and how this ability allows it to overcome the limitations of existing spectral methods. The authors could also provide more clarity on how DIGRAC uses a directed mixed path aggregation scheme to obtain the probability assignment matrix P and feed it to the loss function.",
                "The paper would benefit from a more explicit discussion on the limitations or assumptions of the self-supervised loss function and the directed mixed path aggregation scheme. For instance, the self-supervised loss function is inspired by Cucuringu et al. (2020a), but it's unclear what assumptions this brings with it and how these might limit the function's applicability. Similarly, while the directed mixed path aggregation scheme is described as simple and effective, it would be helpful to understand any potential limitations of this simplicity. This discussion would help to identify potential shortcomings and areas for future improvement.",
                "The paper compares DIGRAC with other methods in terms of their ability to handle directionality, edge density, and other factors, which is commendable. However, it does not provide a comparison with existing methods that incorporate semi-supervised settings and automatic detection of the number of clusters. Given that the paper mentions future work in these areas, such a comparison could be beneficial for validating the effectiveness of DIGRAC and for identifying areas for future improvement. The authors should consider providing a comparison with these methods in the current paper or in future work.",
                "The paper introduces a novel probabilistic imbalance loss, which is crucial to the proposed method. However, it lacks sufficient details on how the CI vol_sum, a key component of this loss, is calculated. Specifically, the paper does not provide the equation or the method to calculate CI vol_sum. This information is essential for understanding the proposed method and for replicating the results. Therefore, I recommend that the authors include a detailed explanation and the specific equation for calculating CI vol_sum in the paper.",
                "The paper provides an overview of the Directed Mixed Path Aggregation (DIMPA) scheme, but it lacks specific details and equations about how the weighted average of information from neighbors is calculated and what factors determine the weight assigned to each neighbor. These details are crucial for understanding the DIMPA scheme and for other researchers to replicate the study. We recommend the authors to include these details in the paper to enhance its clarity and reproducibility.",
                "The paper mentions the use of the Adjusted Rand Index (ARI) and accuracy for node classification and link direction prediction as performance measures. However, it lacks specific details on how these calculations are performed. Providing these details would allow readers to fully understand the evaluation process and replicate the results. It would also be helpful to explain why these particular metrics were chosen and how they contribute to the evaluation of the proposed method.",
                "The method section and the experimental results section discuss the use of the largest K eigenvectors of a Hermitian matrix to construct an input feature matrix. However, the paper does not provide specific guidance on how to choose the value of K. This lack of guidance could hinder the reproducibility of the method and make it difficult for readers to understand the method fully. It would be beneficial if the authors could provide more detailed guidance or a rationale for choosing the value of K. This could include, for example, a discussion on the impact of different values of K on the results, or a method for determining the optimal value of K based on the data or problem context.",
                "The paper mentions the use of seed nodes in a semi-supervised setting and the application of a supervised loss function to these nodes. However, it lacks specific guidance on how to select these seed nodes. This lack of detail could make it difficult for readers to understand or implement the proposed method. It would be beneficial to provide more explicit instructions or criteria for selecting seed nodes, which could include factors to consider, potential strategies, or examples from the experiments conducted.",
                "While the paper introduces DIGRAC as a novel method for obtaining node embeddings for clustering directed networks and mentions its potential use in a semi-supervised setting, it lacks sufficient details about how this would work in practice. Specifically, it would be beneficial to provide more information about how DIGRAC can be enhanced with seed nodes that have known cluster labels or when additional information is available in the form of must-link and cannot-link constraints. This would help readers understand how to apply DIGRAC in a semi-supervised setting and could potentially broaden the applicability of the method. Furthermore, including additional experiments or case studies demonstrating the use of DIGRAC in a semi-supervised setting could provide valuable evidence of its effectiveness in such scenarios.",
                "The paper provides some details about the hyperparameters used in the experiments, which is appreciated. However, it lacks specific information about the equipment and material specifications used in the experiments. This information is crucial for other researchers to accurately reproduce the experiments and validate the results. Please provide more details about the equipment and material specifications, such as the type and model of the equipment, the software and hardware specifications, and any other materials used."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [
                {
                    "reference_id": 5,
                    "predicted_id": 4,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper lacks scalability experiments with the running time of the algorithm and its comparison to baseline models.",
                    "predicted_comment": "While the paper mentions that DIGRAC is an efficient GNN-based method and discusses its potential for large scale industrial applications, it does not provide a direct comparison of computational efficiency between DIGRAC and existing methods. Such a comparison is crucial for understanding the potential of DIGRAC for large scale industrial applications. The authors should consider providing this comparison, possibly by comparing DIGRAC's speed and space complexity with those of other methods such as DGCN. This would give a more complete picture of DIGRAC's performance and help readers understand its advantages and limitations."
                }
            ],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 5,
                    "predicted_id": 4,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper lacks scalability experiments with the running time of the algorithm and its comparison to baseline models.",
                    "predicted_comment": "While the paper mentions that DIGRAC is an efficient GNN-based method and discusses its potential for large scale industrial applications, it does not provide a direct comparison of computational efficiency between DIGRAC and existing methods. Such a comparison is crucial for understanding the potential of DIGRAC for large scale industrial applications. The authors should consider providing this comparison, possibly by comparing DIGRAC's speed and space complexity with those of other methods such as DGCN. This would give a more complete picture of DIGRAC's performance and help readers understand its advantages and limitations."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 11,
                "n_generated": 3,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 11,
                "n_generated": 6,
                "n_total_aligns": 1,
                "recall": 0.05555555555555555,
                "precision": 0.05555555555555555,
                "pseudo_jaccard": 0.030303030303030304,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 11,
                "n_generated": 7,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 11,
                "n_generated": 16,
                "n_total_aligns": 1,
                "recall": 0.05555555555555555,
                "precision": 0.020833333333333332,
                "pseudo_jaccard": 0.015873015873015872,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.16666666666666666,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.09090909090909091,
                    "n_real": 6
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.16666666666666666,
                    "precision": 0.0625,
                    "pseudo_jaccard": 0.047619047619047616,
                    "n_real": 6
                }
            ]
        }
    },
    {
        "doc_id": "b-ny3x071E5",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The name 'matching function' makes the presentation of the method a little bit harder to digest. Since the function is a pseudometric (i.e., the larger it is, the larger the distance from the target), it should really be called with a name that reminds the reader of this nature (e.g., 'mismatch function').",
                "The theoretical results only deal with targets of specific forms and, especially, with $L=1$ only. Ideally, theoretical result with a dependency on $L$ would shed some light on the benefits and limitations of longer bootstrapping horizons.",
                "The connection between the way the bootstrapping target is formed in the method and traditional temporal difference learning needs to be elaborated on. In particular, the grounding role of that subtracted gradient 'nudging the trajectory in a descent direction' is the same as the one of the reward in temporal difference learning; but, while the reward is at the beginning of the trajectory, the grounding is here at the end of the optimization subtrajectory. Is there any mathematical connection beyond the general shared motivation?",
                "When the underlying function is highly nonlinear, there is the risk that the bootstrapping mechanism can lead the optimization process in worse areas of the landscape. For instance, if the function in Figure 1 had a bump/plateaux where $\tilde w$ is, the bootstrapping mechanism would cause more troubles than standard meta-gradients. Why is this not happening in practice?"
            ],
            [
                "The algorithm description in section 3 is very abstract. The paper would benefit from a running example and a dedicated section and pseudo-code describing the algorithm and how it can be instantiated in different experimental settings.",
                "The theoretical analysis is limited to noiseless 1-step target updates.",
                "The experimental evaluation in the multi-task meta-learning setting is limited to only compare with MAML on computer vision applications.",
                "Some engineering / handcrafting is still required by the machine learning practitioner to select what 'target' the meta-learner is going to optimize, as well as the proper 'metric' for the meta-learner to optimize for. Could the authors comment a bit about what heuristics they used when making these decisions? and whether the automation for this process is possible or not?",
                "What would it take to extend the analysis beyond 1-step noiseless target updates?",
                "How does the performance of BMG compare to alternative meta-learning algorithms like R2D2, Meta-OPT-net and prototypical networks? Have the authors experimented with other meta-learning benchmarks beyond image classification?"
            ],
            [
                "Given that you say BMG is compatible with any update function (so long as it is differentiable in the meta-parameters), it would be nice to have some experiments on learned sequence model update rules (e.g. RNN). All current experiments use update rules with a fixed functional form.",
                "I am not putting much weight on section 4 ('Performance Guarantees') given the gap between its assumptions and results vs. what is actually implemented, and the restriction to local optimization."
            ],
            [
                "The paper could benefit from more detailed discussions in certain areas to improve clarity and understanding.",
                "The methodologies of the experiments are difficult to follow, making it hard to link the results back to intuition or interpretation. More detailed descriptions or an algorithm box in the main text could help address this.",
                "The implementation and implications of the experiment on multi-task few-shot learning (Sec 6) are unclear. The authors should clarify the intuition of why a 'hot' expert transforms more information than a 'cold' expert, and why BMG is able to use this to improve performance.",
                "The training procedure for BMG in the multi-task few-shot learning experiment should be formally described in the main text.",
                "The analysis presented in Sec 4 ('Performance Guarantees') is not especially insightful. The conclusion drawn from this section is evident from the experiments themselves."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper provides a good amount of detail about the experimental setup and parameters used in various sections. However, to ensure reproducibility, it would be beneficial to include the specific versions of the software or libraries used in the experiments. This information is crucial for others to accurately reproduce your results, as different versions of software or libraries can lead to different outcomes. Including this information would greatly enhance the reproducibility and reliability of your work.",
                "The paper provides several performance comparisons between the proposed Bootstrapped Meta-Gradient (BMG) method and the baselines. However, it does not explicitly mention the statistical tests used to verify the significance of these results. To help readers understand whether the observed differences in performance are statistically significant and not due to random chance, please provide more information about the statistical tests used. This should include the type of test, the variables involved, and the results. For example, if a t-test was used, please provide the t-value and the degrees of freedom. If a chi-square test was used, please provide the chi-square value and the p-value. This additional information will help readers evaluate the robustness of your findings and the validity of your conclusions.",
                "Although the paper mentions conducting experiments with multiple runs under different seeds, it does not provide specific information about the average performance over these runs or measures of variability such as the standard deviation or confidence intervals. This information is crucial for understanding the robustness and reliability of the results. Please provide detailed results for each experiment, including the average performance over the runs and measures of variability.",
                "Ensure Fair Comparison: The paper provides scattered information about the resources given to each method during the comparison and how each method was optimized. It would be helpful to consolidate this information in a dedicated section to ensure a fair comparison. Specifically, the paper should clearly state the resources allocated to each method, the optimization process for each method, and the rationale behind these decisions. This will make it easier for readers to understand the comparison and the results.",
                "Conduct a Comprehensive Ablation Study: The paper would benefit from a more detailed ablation study that covers all major components of the proposed Bootstrapped Meta-Gradient (BMG) method. Specifically, it would be helpful to understand the individual and combined contributions of the Target Bootstrap, Matching Function, Meta-Learning Horizon, Actor-Critic Experiments, Entropy Regularization, Q-Learning Experiments, Policy-Matching, Value-Matching, and the use of SGD and RMSProp in the target update rule. This will provide readers with a clearer understanding of how each component contributes to the overall performance of the BMG method."
            ],
            "novelty_impact": [
                "The paper presents a novel approach to meta-learning with the introduction of a bootstrapping method. The theoretical basis for this method is explained in detail across different sections of the paper, including the application of the concepts of meta-learning and temporal difference algorithms. However, it might be beneficial to consolidate this information into a dedicated section that provides a comprehensive explanation of the theoretical basis of the proposed method. This would make it easier for the reader to understand how these concepts are applied in the proposed method and how they contribute to its performance.",
                "The authors provide various definitions and measurements of 'performance' and 'efficiency' in the context of the BMG algorithm and multi-task meta-learning, and explain why these metrics are important in this research context. However, these definitions and measurements are scattered throughout the paper, which may make it difficult for readers to find and understand them. It would be helpful if the authors could consolidate this information in a dedicated section or subsection, and provide a more detailed and systematic explanation of how these metrics are defined, measured, and why they are important in this research context.",
                "The paper discusses the performance guarantees of MG and BMG in online optimization and multi-task settings. However, the exact nature of these guarantees and how they are derived could be better explained. Specifically, the paper does not provide the exact forms of Lemma 1, Theorem 1, and Corollary 2, which makes it difficult to fully understand the mathematical basis of these claims. Additionally, a clear comparison of the performance guarantees of MG and BMG in online optimization and multi-task settings is missing. Providing more detail about these guarantees and a clear comparison would strengthen the paper's arguments and make it more accessible to readers who are not familiar with these concepts.",
                "The paper provides a substantial amount of detail about the concept of a target bootstrap (TB) and its role in performance improvement. However, this information appears to be scattered throughout the paper, which may make it difficult for readers to fully understand the concept. It would be beneficial to consolidate this information into a dedicated section or subsection to provide a comprehensive overview of the TB concept, its role in the proposed method, and how it contributes to performance improvement. This would enhance the readability of the paper and make it easier for readers to grasp the significance of the TB concept.",
                "The authors claim that BMG outperforms MG in a reinforcement learning setup and in the Atari Arcade Learning Environment, but the specific metrics used to evaluate performance in these settings are not clearly defined. For instance, the paper mentions the use of Human Normalized Score (HNS) in the Atari Arcade Learning Environment, but does not provide a detailed definition of this metric. Similarly, the paper mentions the use of cumulative reward curves, entropy regularization weight range, and learned \u03b5-schedules for different meta-learning horizons in the reinforcement learning setup, but the definitions and explanations of these metrics are not clearly provided. Providing more detail about these metrics, including their definitions and how they were calculated, would help readers understand the results and their significance. This is particularly important for readers who are not familiar with these metrics.",
                "The paper presents results from applying BMG in the context of Atari games and Multi-task Few-Shot Learning. However, the paper lacks explicit discussion on how these results can be generalized to other domains. The authors mention that BMG can be applied to other optimization routines and can enable new forms of meta-learning, but without specific examples or results from other domains, it is difficult to assess the generalizability of the results. Providing concrete examples of how BMG can be applied in other domains, along with results from these applications, would strengthen the paper's arguments and make it more relevant to a wider audience.",
                "The paper provides a definition for 'short-horizon bias', but it does not explain how this concept is measured. Providing a method for measuring 'short-horizon bias' would help readers understand how it affects the performance of the proposed methods in the experiments.",
                "The paper does not provide a definition for 'exploration schedule' or explain how this concept is measured. Providing a definition and a method for measuring 'exploration schedule' would help readers understand how it is used in the proposed methods and the experiments.",
                "The authors have conducted ablation studies to understand the role of meta-regularization and the importance of taking a final step under a different update rule. However, the details about these studies and their findings could be more explicit. For instance, the paper could clarify the impact of using the meta-learned update rule for all target update steps and the resulting positive feedback loop that leads to maximal entropy regularization. Additionally, the paper could provide more information about the experiments comparing BMG with and without entropy regularization in the Lth target update step. Providing more explicit details about these studies would help readers understand the proposed method and its benefits.",
                "The authors introduce the concept of changing the target update rule in the context of their BMG method and discuss the significance of controlling for curvature. However, it would be beneficial if the authors could explicitly state the change from SGD to RMSProp across all sections of the paper for clarity. Additionally, the quantification of the gain from this change is not consistently provided. Providing specific numerical values or percentages to quantify this gain would help readers understand the impact of this change. Furthermore, a more detailed explanation of the concept of controlling for curvature, including what is meant by 'correcting for curvature', would enhance the reader's understanding of the proposed method and its benefits.",
                "In the Few-Shot MiniImagenet section, the authors compare BMG and MG in terms of data-efficiency and computational efficiency. While the paper does provide definitions and measurements for these efficiencies, they are scattered throughout the paper and not explicitly labeled as such. It would be helpful if the authors could consolidate this information and provide explicit definitions and measurements for data-efficiency and computational efficiency in the Few-Shot MiniImagenet section. This would make it easier for readers to understand the experiments and their results.",
                "The paper mentions 'ill-conditioning' and 'meta-gradient variance' but does not provide explicit definitions for these terms. While some readers with a background in machine learning might infer that 'ill-conditioning' refers to a situation where the Hessian matrix of second derivatives is not well-behaved and 'meta-gradient variance' refers to the variability in the gradients computed during meta-learning, these interpretations might not be obvious to all readers. Providing explicit definitions for these terms would help ensure that all readers understand these concepts. Additionally, the paper could provide more detail on how 'ill-conditioning' is estimated through cosine similarity between consecutive meta-gradients and how 'meta-gradient variance' is reported on a per meta-batch basis.",
                "The authors compare the throughput of BMG and MAML in the context of meta-training steps per second, with BMG being 50% slower. However, the paper does not provide a specific definition of 'throughput' or detail how it is measured. To improve the clarity of the paper, the authors should provide a specific definition of 'throughput' in the context of meta-training steps per second and detail the method used to measure it. This will help readers understand the comparison of BMG and MAML in terms of throughput."
            ],
            "clarity_reproducibility": [
                "The paper provides a comprehensive overview of the Bootstrapped Meta-Gradient (BMG) algorithm, its applications, and its performance. However, the specific steps of the algorithm are not detailed in any section of the paper. Providing a step-by-step description of the BMG algorithm would make it easier for others to understand and implement the algorithm. This could include details such as how the matching function and Target Bootstrap are used in the algorithm, how the algorithm is implemented in different contexts (e.g., reinforcement learning, actor-critic experiments, Q-learning experiments, Atari games), and how the algorithm's performance varies with different parameters (e.g., the meta-learning horizon). This level of detail would not only enhance the clarity of the paper but also its utility for other researchers in the field.",
                "The authors provide some detail about how the metrics (KL-divergence and squared Euclidean distance) are used in the context of the BMG algorithm. However, it would be beneficial to provide more detail about why these specific metrics were chosen, how they affect the performance of the BMG algorithm, and how they compare to other potential metrics. This additional detail would help readers better understand the design choices made in the BMG algorithm and their implications.",
                "The paper discusses the performance guarantees for the MG and BMG updates, but does not provide specific details about these guarantees. The authors should provide more detail about these performance guarantees, including the full definitions and explanations of the terms and symbols used in the equations, the mathematical basis for the claims made about the performance of the BMG update, and the complete context of the experiments and results. These details are important because they would provide a comprehensive understanding of how these methods work and their potential advantages and disadvantages. Without these details, it is difficult to fully understand or replicate the MG and BMG updates, and to validate the effectiveness of the proposed method.",
                "The paper presents several mathematical lemmas, theorems, and corollaries, such as Lemma 1 (MG Descent), Theorem 1 (BMG Descent), and Corollary 2. However, their proofs are deferred to Appendix A. While this is a common practice, it might be helpful for the reader if the paper could provide the proofs in the same section where these mathematical elements are first mentioned, or at least provide a brief summary or intuitive explanation of the proofs. This would make it easier for the reader to follow the logical flow of the paper and understand the significance of these mathematical elements in the context of the paper's argument.",
                "The paper provides an explanation of how a typical reinforcement learning problem is modeled as a Markov Decision Process (MDP) in paragraph 47. However, it lacks a detailed explanation of the terms used in this model, such as the policy, the transition, and the action-value. These terms are crucial for understanding the model and how it works. Additionally, while the paper discusses meta-learning and introduces a new algorithm for meta-optimisation, it does not delve into the specifics of modeling these as MDPs. Providing more detail on these aspects would strengthen the paper by making the modeling process clearer to the reader.",
                "The paper contains detailed information about the experiments conducted in the Atari Arcade Learning Environment (ALE), including the specific settings and how the results were measured. However, this information is scattered throughout the paper, making it difficult for readers to find and understand. I suggest consolidating this information into a dedicated 'Experiments' section for easier reference. This would enhance the readability of the paper and allow readers to better understand the experimental setup and results.",
                "The paper provides a detailed application of BMG in the context of Atari games, including the adaptation of BMG to the Self-Tuning Actor-Critic (STACX) to meta-learn on the 57 environments in the Atari Arcade Learning Environment (ALE), the comparison of BMG with STACX, and the use of policy matching and value matching in BMG. However, the paper could benefit from more specific details about the implementation of BMG in the Atari games, such as the specific algorithms used, the exact parameters used in the experiments, and the specific results of the experiments.",
                "In the context of Multi-Task Few-Shot Learning, the paper discusses how BMG is applied by computing task-specific bootstrap targets, with the meta-gradient being the expectation over task-specific matching losses. It also discusses the goal of an experiment to study how the BMG objective behaves in the multi-task setting. However, the paper does not provide specific details about the results of the experiment or how the BMG objective behaved in the multi-task setting. The authors should provide more detail about these results and their implications.",
                "While the paper does provide some details about the experiments conducted to study how the BMG objective behaves in the multi-task setting, these details are scattered across different sections of the paper. It would be helpful for the reader if the authors could consolidate this information and provide a more organized and coherent presentation of the experiments, including the specific settings and how the results were measured. This would make it easier for the reader to understand the experiments and their results, and it would provide the necessary context for evaluating the validity of the paper's claims. Therefore, this is a major comment that should be addressed to improve the quality of the paper.",
                "The paper provides detailed information about the experiment setup using the MiniImagenet benchmark, including the specific settings and how the results were measured. However, this information is spread across different sections of the paper, which may make it difficult for readers to find and understand. The authors could improve the paper by consolidating this information into one section.",
                "The paper provides a detailed description of the settings for the experiment conducted in the two-colors domain, which includes the agent's interaction with the environment and the steps involved in updating its parameters. However, it does not provide specific details on how the results of this experiment were measured. This information is crucial for readers to understand the validity and reliability of the results. The authors should include a clear explanation of the measurement methods used, including any metrics or statistical tests applied, and how these were chosen.",
                "The paper uses several terms and concepts that are crucial to understanding its findings and methodology, but these are not clearly defined or explained. Specifically, terms such as 'BMG', 'MG', 'meta-updates', 'target bootstrap', 'meta-overfitting', 'meta-learning horizon', 'meta-gradient variance', 'KL-divergence', and 'temperature in its target' are used without sufficient explanation. Clear definitions and explanations of these terms and concepts are necessary for readers to fully understand the paper's methodology, findings, and their significance. The authors should provide these definitions and explanations to improve the clarity and comprehensibility of the paper.",
                "The 'Introduction' section lacks implementation and testing details of the proposed algorithm. Providing these details would allow others to reproduce your work more accurately.",
                "In the 'Related Work' section, the experimental settings in which BMG was tested are not detailed. Please provide these details to enhance the reproducibility of your work.",
                "The 'Performance Guarantees' section does not detail the experimental settings in which the performance guarantees were tested. Providing these details would strengthen your claims.",
                "In the 'Reinforcement Learning' section, the details about the algorithms used or how the steps are implemented are missing. Please provide these details to enhance the clarity of your work.",
                "The 'Atari' section does not detail the specific parameters used, the number of trials conducted, and the statistical methods used to analyze the results. Providing these details would enhance the reproducibility of your work.",
                "In the 'Multi-task Few-shot Learning' section, the specific tasks used, the number of trials conducted, and the statistical methods used to analyze the results are not detailed. Please provide these details to enhance the reproducibility of your work.",
                "The 'Conclusion' section does not detail the methods used to derive the results or the experimental setup. Providing these details would enhance the clarity of your work.",
                "In the 'B.2 Actor-Critic Experiments' section, the specific tasks used, the number of trials conducted, and the statistical methods used to analyze the results are not detailed. Please provide these details to enhance the reproducibility of your work.",
                "The 'Main experiment: Detailed results' section lacks implementation details of entropy regularization and the concept of a meta-objective. Providing these details would enhance the clarity of your work.",
                "In the 'B.3 Q-Learning Experiments' section, the specific details about the MLP or the EMA, such as their structure, parameters, or how they are implemented, are not provided. Please provide these details to enhance the clarity of your work.",
                "The 'C Atari' section does not detail the settings and parameters used in the experiments. Providing these details would enhance the reproducibility of your work.",
                "In the 'D Multi-task Meta-learning D.1 Problem Formulation' section, the details about the tasks used in these experiments, how the tasks were selected and distributed, or how the performance on these tasks was evaluated are not provided. Please provide these details to enhance the clarity of your work.",
                "The 'D.2 Few-shot MiniImagenet' section does not provide specific details about the tasks used in these experiments, how the tasks were selected and distributed, or how the performance on these tasks was evaluated. Providing these details would enhance the reproducibility of your work."
            ],
            "all": [
                "The paper provides a good amount of detail about the experimental setup and parameters used in various sections. However, to ensure reproducibility, it would be beneficial to include the specific versions of the software or libraries used in the experiments. This information is crucial for others to accurately reproduce your results, as different versions of software or libraries can lead to different outcomes. Including this information would greatly enhance the reproducibility and reliability of your work.",
                "The paper provides several performance comparisons between the proposed Bootstrapped Meta-Gradient (BMG) method and the baselines. However, it does not explicitly mention the statistical tests used to verify the significance of these results. To help readers understand whether the observed differences in performance are statistically significant and not due to random chance, please provide more information about the statistical tests used. This should include the type of test, the variables involved, and the results. For example, if a t-test was used, please provide the t-value and the degrees of freedom. If a chi-square test was used, please provide the chi-square value and the p-value. This additional information will help readers evaluate the robustness of your findings and the validity of your conclusions.",
                "Although the paper mentions conducting experiments with multiple runs under different seeds, it does not provide specific information about the average performance over these runs or measures of variability such as the standard deviation or confidence intervals. This information is crucial for understanding the robustness and reliability of the results. Please provide detailed results for each experiment, including the average performance over the runs and measures of variability.",
                "Ensure Fair Comparison: The paper provides scattered information about the resources given to each method during the comparison and how each method was optimized. It would be helpful to consolidate this information in a dedicated section to ensure a fair comparison. Specifically, the paper should clearly state the resources allocated to each method, the optimization process for each method, and the rationale behind these decisions. This will make it easier for readers to understand the comparison and the results.",
                "Conduct a Comprehensive Ablation Study: The paper would benefit from a more detailed ablation study that covers all major components of the proposed Bootstrapped Meta-Gradient (BMG) method. Specifically, it would be helpful to understand the individual and combined contributions of the Target Bootstrap, Matching Function, Meta-Learning Horizon, Actor-Critic Experiments, Entropy Regularization, Q-Learning Experiments, Policy-Matching, Value-Matching, and the use of SGD and RMSProp in the target update rule. This will provide readers with a clearer understanding of how each component contributes to the overall performance of the BMG method.",
                "The paper presents a novel approach to meta-learning with the introduction of a bootstrapping method. The theoretical basis for this method is explained in detail across different sections of the paper, including the application of the concepts of meta-learning and temporal difference algorithms. However, it might be beneficial to consolidate this information into a dedicated section that provides a comprehensive explanation of the theoretical basis of the proposed method. This would make it easier for the reader to understand how these concepts are applied in the proposed method and how they contribute to its performance.",
                "The authors provide various definitions and measurements of 'performance' and 'efficiency' in the context of the BMG algorithm and multi-task meta-learning, and explain why these metrics are important in this research context. However, these definitions and measurements are scattered throughout the paper, which may make it difficult for readers to find and understand them. It would be helpful if the authors could consolidate this information in a dedicated section or subsection, and provide a more detailed and systematic explanation of how these metrics are defined, measured, and why they are important in this research context.",
                "The paper discusses the performance guarantees of MG and BMG in online optimization and multi-task settings. However, the exact nature of these guarantees and how they are derived could be better explained. Specifically, the paper does not provide the exact forms of Lemma 1, Theorem 1, and Corollary 2, which makes it difficult to fully understand the mathematical basis of these claims. Additionally, a clear comparison of the performance guarantees of MG and BMG in online optimization and multi-task settings is missing. Providing more detail about these guarantees and a clear comparison would strengthen the paper's arguments and make it more accessible to readers who are not familiar with these concepts.",
                "The paper provides a substantial amount of detail about the concept of a target bootstrap (TB) and its role in performance improvement. However, this information appears to be scattered throughout the paper, which may make it difficult for readers to fully understand the concept. It would be beneficial to consolidate this information into a dedicated section or subsection to provide a comprehensive overview of the TB concept, its role in the proposed method, and how it contributes to performance improvement. This would enhance the readability of the paper and make it easier for readers to grasp the significance of the TB concept.",
                "The authors claim that BMG outperforms MG in a reinforcement learning setup and in the Atari Arcade Learning Environment, but the specific metrics used to evaluate performance in these settings are not clearly defined. For instance, the paper mentions the use of Human Normalized Score (HNS) in the Atari Arcade Learning Environment, but does not provide a detailed definition of this metric. Similarly, the paper mentions the use of cumulative reward curves, entropy regularization weight range, and learned \u03b5-schedules for different meta-learning horizons in the reinforcement learning setup, but the definitions and explanations of these metrics are not clearly provided. Providing more detail about these metrics, including their definitions and how they were calculated, would help readers understand the results and their significance. This is particularly important for readers who are not familiar with these metrics.",
                "The paper presents results from applying BMG in the context of Atari games and Multi-task Few-Shot Learning. However, the paper lacks explicit discussion on how these results can be generalized to other domains. The authors mention that BMG can be applied to other optimization routines and can enable new forms of meta-learning, but without specific examples or results from other domains, it is difficult to assess the generalizability of the results. Providing concrete examples of how BMG can be applied in other domains, along with results from these applications, would strengthen the paper's arguments and make it more relevant to a wider audience.",
                "The paper provides a definition for 'short-horizon bias', but it does not explain how this concept is measured. Providing a method for measuring 'short-horizon bias' would help readers understand how it affects the performance of the proposed methods in the experiments.",
                "The paper does not provide a definition for 'exploration schedule' or explain how this concept is measured. Providing a definition and a method for measuring 'exploration schedule' would help readers understand how it is used in the proposed methods and the experiments.",
                "The authors have conducted ablation studies to understand the role of meta-regularization and the importance of taking a final step under a different update rule. However, the details about these studies and their findings could be more explicit. For instance, the paper could clarify the impact of using the meta-learned update rule for all target update steps and the resulting positive feedback loop that leads to maximal entropy regularization. Additionally, the paper could provide more information about the experiments comparing BMG with and without entropy regularization in the Lth target update step. Providing more explicit details about these studies would help readers understand the proposed method and its benefits.",
                "The authors introduce the concept of changing the target update rule in the context of their BMG method and discuss the significance of controlling for curvature. However, it would be beneficial if the authors could explicitly state the change from SGD to RMSProp across all sections of the paper for clarity. Additionally, the quantification of the gain from this change is not consistently provided. Providing specific numerical values or percentages to quantify this gain would help readers understand the impact of this change. Furthermore, a more detailed explanation of the concept of controlling for curvature, including what is meant by 'correcting for curvature', would enhance the reader's understanding of the proposed method and its benefits.",
                "In the Few-Shot MiniImagenet section, the authors compare BMG and MG in terms of data-efficiency and computational efficiency. While the paper does provide definitions and measurements for these efficiencies, they are scattered throughout the paper and not explicitly labeled as such. It would be helpful if the authors could consolidate this information and provide explicit definitions and measurements for data-efficiency and computational efficiency in the Few-Shot MiniImagenet section. This would make it easier for readers to understand the experiments and their results.",
                "The paper mentions 'ill-conditioning' and 'meta-gradient variance' but does not provide explicit definitions for these terms. While some readers with a background in machine learning might infer that 'ill-conditioning' refers to a situation where the Hessian matrix of second derivatives is not well-behaved and 'meta-gradient variance' refers to the variability in the gradients computed during meta-learning, these interpretations might not be obvious to all readers. Providing explicit definitions for these terms would help ensure that all readers understand these concepts. Additionally, the paper could provide more detail on how 'ill-conditioning' is estimated through cosine similarity between consecutive meta-gradients and how 'meta-gradient variance' is reported on a per meta-batch basis.",
                "The authors compare the throughput of BMG and MAML in the context of meta-training steps per second, with BMG being 50% slower. However, the paper does not provide a specific definition of 'throughput' or detail how it is measured. To improve the clarity of the paper, the authors should provide a specific definition of 'throughput' in the context of meta-training steps per second and detail the method used to measure it. This will help readers understand the comparison of BMG and MAML in terms of throughput.",
                "The paper provides a comprehensive overview of the Bootstrapped Meta-Gradient (BMG) algorithm, its applications, and its performance. However, the specific steps of the algorithm are not detailed in any section of the paper. Providing a step-by-step description of the BMG algorithm would make it easier for others to understand and implement the algorithm. This could include details such as how the matching function and Target Bootstrap are used in the algorithm, how the algorithm is implemented in different contexts (e.g., reinforcement learning, actor-critic experiments, Q-learning experiments, Atari games), and how the algorithm's performance varies with different parameters (e.g., the meta-learning horizon). This level of detail would not only enhance the clarity of the paper but also its utility for other researchers in the field.",
                "The authors provide some detail about how the metrics (KL-divergence and squared Euclidean distance) are used in the context of the BMG algorithm. However, it would be beneficial to provide more detail about why these specific metrics were chosen, how they affect the performance of the BMG algorithm, and how they compare to other potential metrics. This additional detail would help readers better understand the design choices made in the BMG algorithm and their implications.",
                "The paper discusses the performance guarantees for the MG and BMG updates, but does not provide specific details about these guarantees. The authors should provide more detail about these performance guarantees, including the full definitions and explanations of the terms and symbols used in the equations, the mathematical basis for the claims made about the performance of the BMG update, and the complete context of the experiments and results. These details are important because they would provide a comprehensive understanding of how these methods work and their potential advantages and disadvantages. Without these details, it is difficult to fully understand or replicate the MG and BMG updates, and to validate the effectiveness of the proposed method.",
                "The paper presents several mathematical lemmas, theorems, and corollaries, such as Lemma 1 (MG Descent), Theorem 1 (BMG Descent), and Corollary 2. However, their proofs are deferred to Appendix A. While this is a common practice, it might be helpful for the reader if the paper could provide the proofs in the same section where these mathematical elements are first mentioned, or at least provide a brief summary or intuitive explanation of the proofs. This would make it easier for the reader to follow the logical flow of the paper and understand the significance of these mathematical elements in the context of the paper's argument.",
                "The paper provides an explanation of how a typical reinforcement learning problem is modeled as a Markov Decision Process (MDP) in paragraph 47. However, it lacks a detailed explanation of the terms used in this model, such as the policy, the transition, and the action-value. These terms are crucial for understanding the model and how it works. Additionally, while the paper discusses meta-learning and introduces a new algorithm for meta-optimisation, it does not delve into the specifics of modeling these as MDPs. Providing more detail on these aspects would strengthen the paper by making the modeling process clearer to the reader.",
                "The paper contains detailed information about the experiments conducted in the Atari Arcade Learning Environment (ALE), including the specific settings and how the results were measured. However, this information is scattered throughout the paper, making it difficult for readers to find and understand. I suggest consolidating this information into a dedicated 'Experiments' section for easier reference. This would enhance the readability of the paper and allow readers to better understand the experimental setup and results.",
                "The paper provides a detailed application of BMG in the context of Atari games, including the adaptation of BMG to the Self-Tuning Actor-Critic (STACX) to meta-learn on the 57 environments in the Atari Arcade Learning Environment (ALE), the comparison of BMG with STACX, and the use of policy matching and value matching in BMG. However, the paper could benefit from more specific details about the implementation of BMG in the Atari games, such as the specific algorithms used, the exact parameters used in the experiments, and the specific results of the experiments.",
                "In the context of Multi-Task Few-Shot Learning, the paper discusses how BMG is applied by computing task-specific bootstrap targets, with the meta-gradient being the expectation over task-specific matching losses. It also discusses the goal of an experiment to study how the BMG objective behaves in the multi-task setting. However, the paper does not provide specific details about the results of the experiment or how the BMG objective behaved in the multi-task setting. The authors should provide more detail about these results and their implications.",
                "While the paper does provide some details about the experiments conducted to study how the BMG objective behaves in the multi-task setting, these details are scattered across different sections of the paper. It would be helpful for the reader if the authors could consolidate this information and provide a more organized and coherent presentation of the experiments, including the specific settings and how the results were measured. This would make it easier for the reader to understand the experiments and their results, and it would provide the necessary context for evaluating the validity of the paper's claims. Therefore, this is a major comment that should be addressed to improve the quality of the paper.",
                "The paper provides detailed information about the experiment setup using the MiniImagenet benchmark, including the specific settings and how the results were measured. However, this information is spread across different sections of the paper, which may make it difficult for readers to find and understand. The authors could improve the paper by consolidating this information into one section.",
                "The paper provides a detailed description of the settings for the experiment conducted in the two-colors domain, which includes the agent's interaction with the environment and the steps involved in updating its parameters. However, it does not provide specific details on how the results of this experiment were measured. This information is crucial for readers to understand the validity and reliability of the results. The authors should include a clear explanation of the measurement methods used, including any metrics or statistical tests applied, and how these were chosen.",
                "The paper uses several terms and concepts that are crucial to understanding its findings and methodology, but these are not clearly defined or explained. Specifically, terms such as 'BMG', 'MG', 'meta-updates', 'target bootstrap', 'meta-overfitting', 'meta-learning horizon', 'meta-gradient variance', 'KL-divergence', and 'temperature in its target' are used without sufficient explanation. Clear definitions and explanations of these terms and concepts are necessary for readers to fully understand the paper's methodology, findings, and their significance. The authors should provide these definitions and explanations to improve the clarity and comprehensibility of the paper.",
                "The 'Introduction' section lacks implementation and testing details of the proposed algorithm. Providing these details would allow others to reproduce your work more accurately.",
                "In the 'Related Work' section, the experimental settings in which BMG was tested are not detailed. Please provide these details to enhance the reproducibility of your work.",
                "The 'Performance Guarantees' section does not detail the experimental settings in which the performance guarantees were tested. Providing these details would strengthen your claims.",
                "In the 'Reinforcement Learning' section, the details about the algorithms used or how the steps are implemented are missing. Please provide these details to enhance the clarity of your work.",
                "The 'Atari' section does not detail the specific parameters used, the number of trials conducted, and the statistical methods used to analyze the results. Providing these details would enhance the reproducibility of your work.",
                "In the 'Multi-task Few-shot Learning' section, the specific tasks used, the number of trials conducted, and the statistical methods used to analyze the results are not detailed. Please provide these details to enhance the reproducibility of your work.",
                "The 'Conclusion' section does not detail the methods used to derive the results or the experimental setup. Providing these details would enhance the clarity of your work.",
                "In the 'B.2 Actor-Critic Experiments' section, the specific tasks used, the number of trials conducted, and the statistical methods used to analyze the results are not detailed. Please provide these details to enhance the reproducibility of your work.",
                "The 'Main experiment: Detailed results' section lacks implementation details of entropy regularization and the concept of a meta-objective. Providing these details would enhance the clarity of your work.",
                "In the 'B.3 Q-Learning Experiments' section, the specific details about the MLP or the EMA, such as their structure, parameters, or how they are implemented, are not provided. Please provide these details to enhance the clarity of your work.",
                "The 'C Atari' section does not detail the settings and parameters used in the experiments. Providing these details would enhance the reproducibility of your work.",
                "In the 'D Multi-task Meta-learning D.1 Problem Formulation' section, the details about the tasks used in these experiments, how the tasks were selected and distributed, or how the performance on these tasks was evaluated are not provided. Please provide these details to enhance the clarity of your work.",
                "The 'D.2 Few-shot MiniImagenet' section does not provide specific details about the tasks used in these experiments, how the tasks were selected and distributed, or how the performance on these tasks was evaluated. Providing these details would enhance the reproducibility of your work."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [],
            "clarity_reproducibility": [
                {
                    "reference_id": 0,
                    "predicted_id": 18,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The algorithm description in section 3 is very abstract. The paper would benefit from a running example and a dedicated section and pseudo-code describing the algorithm and how it can be instantiated in different experimental settings.",
                    "predicted_comment": "The paper provides a comprehensive overview of the Bootstrapped Meta-Gradient (BMG) algorithm, its applications, and its performance. However, the specific steps of the algorithm are not detailed in any section of the paper. Providing a step-by-step description of the BMG algorithm would make it easier for others to understand and implement the algorithm. This could include details such as how the matching function and Target Bootstrap are used in the algorithm, how the algorithm is implemented in different contexts (e.g., reinforcement learning, actor-critic experiments, Q-learning experiments, Atari games), and how the algorithm's performance varies with different parameters (e.g., the meta-learning horizon). This level of detail would not only enhance the clarity of the paper but also its utility for other researchers in the field."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 25,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The implementation and implications of the experiment on multi-task few-shot learning (Sec 6) are unclear. The authors should clarify the intuition of why a 'hot' expert transforms more information than a 'cold' expert, and why BMG is able to use this to improve performance.",
                    "predicted_comment": "In the context of Multi-Task Few-Shot Learning, the paper discusses how BMG is applied by computing task-specific bootstrap targets, with the meta-gradient being the expectation over task-specific matching losses. It also discusses the goal of an experiment to study how the BMG objective behaves in the multi-task setting. However, the paper does not provide specific details about the results of the experiment or how the BMG objective behaved in the multi-task setting. The authors should provide more detail about these results and their implications."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 18,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The methodologies of the experiments are difficult to follow, making it hard to link the results back to intuition or interpretation. More detailed descriptions or an algorithm box in the main text could help address this.",
                    "predicted_comment": "The paper provides a comprehensive overview of the Bootstrapped Meta-Gradient (BMG) algorithm, its applications, and its performance. However, the specific steps of the algorithm are not detailed in any section of the paper. Providing a step-by-step description of the BMG algorithm would make it easier for others to understand and implement the algorithm. This could include details such as how the matching function and Target Bootstrap are used in the algorithm, how the algorithm is implemented in different contexts (e.g., reinforcement learning, actor-critic experiments, Q-learning experiments, Atari games), and how the algorithm's performance varies with different parameters (e.g., the meta-learning horizon). This level of detail would not only enhance the clarity of the paper but also its utility for other researchers in the field."
                }
            ],
            "all": [
                {
                    "reference_id": 0,
                    "predicted_id": 18,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The algorithm description in section 3 is very abstract. The paper would benefit from a running example and a dedicated section and pseudo-code describing the algorithm and how it can be instantiated in different experimental settings.",
                    "predicted_comment": "The paper provides a comprehensive overview of the Bootstrapped Meta-Gradient (BMG) algorithm, its applications, and its performance. However, the specific steps of the algorithm are not detailed in any section of the paper. Providing a step-by-step description of the BMG algorithm would make it easier for others to understand and implement the algorithm. This could include details such as how the matching function and Target Bootstrap are used in the algorithm, how the algorithm is implemented in different contexts (e.g., reinforcement learning, actor-critic experiments, Q-learning experiments, Atari games), and how the algorithm's performance varies with different parameters (e.g., the meta-learning horizon). This level of detail would not only enhance the clarity of the paper but also its utility for other researchers in the field."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 25,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The implementation and implications of the experiment on multi-task few-shot learning (Sec 6) are unclear. The authors should clarify the intuition of why a 'hot' expert transforms more information than a 'cold' expert, and why BMG is able to use this to improve performance.",
                    "predicted_comment": "In the context of Multi-Task Few-Shot Learning, the paper discusses how BMG is applied by computing task-specific bootstrap targets, with the meta-gradient being the expectation over task-specific matching losses. It also discusses the goal of an experiment to study how the BMG objective behaves in the multi-task setting. However, the paper does not provide specific details about the results of the experiment or how the BMG objective behaved in the multi-task setting. The authors should provide more detail about these results and their implications."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 18,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The methodologies of the experiments are difficult to follow, making it hard to link the results back to intuition or interpretation. More detailed descriptions or an algorithm box in the main text could help address this.",
                    "predicted_comment": "The paper provides a comprehensive overview of the Bootstrapped Meta-Gradient (BMG) algorithm, its applications, and its performance. However, the specific steps of the algorithm are not detailed in any section of the paper. Providing a step-by-step description of the BMG algorithm would make it easier for others to understand and implement the algorithm. This could include details such as how the matching function and Target Bootstrap are used in the algorithm, how the algorithm is implemented in different contexts (e.g., reinforcement learning, actor-critic experiments, Q-learning experiments, Atari games), and how the algorithm's performance varies with different parameters (e.g., the meta-learning horizon). This level of detail would not only enhance the clarity of the paper but also its utility for other researchers in the field."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 17,
                "n_generated": 5,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 17,
                "n_generated": 13,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "clarity_reproducibility": {
                "n_reference": 17,
                "n_generated": 25,
                "n_total_aligns": 3,
                "recall": 0.14166666666666666,
                "precision": 0.03,
                "pseudo_jaccard": 0.026190476190476188,
                "specificity_more": 0.6666666666666666,
                "specificity_less": 0.0,
                "specificity_same": 0.3333333333333333
            },
            "all": {
                "n_reference": 17,
                "n_generated": 43,
                "n_total_aligns": 3,
                "recall": 0.14166666666666666,
                "precision": 0.01744186046511628,
                "pseudo_jaccard": 0.016077898550724636,
                "specificity_more": 0.6666666666666666,
                "specificity_less": 0.0,
                "specificity_same": 0.3333333333333333
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.16666666666666666,
                    "precision": 0.04,
                    "pseudo_jaccard": 0.03333333333333333,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.4,
                    "precision": 0.08,
                    "pseudo_jaccard": 0.07142857142857142,
                    "n_real": 5
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.16666666666666666,
                    "precision": 0.023255813953488372,
                    "pseudo_jaccard": 0.020833333333333332,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.4,
                    "precision": 0.046511627906976744,
                    "pseudo_jaccard": 0.043478260869565216,
                    "n_real": 5
                }
            ]
        }
    },
    {
        "doc_id": "uB12zutkXJR",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper does not convincingly demonstrate model novelty or significant improvement to the state-of-the-art on the provided baseline.",
                "The model architecture seems very closely based on the HOPPITY baseline, with the primary change being to the multi-head graph encoder. This does not introduce a fundamentally new idea.",
                "Quantitative evaluations show only marginal improvement at best in comparison to the large pre-trained models.",
                "The evaluation omits a comparison to the closely related Yao 2021 model that is cited throughout the paper.",
                "The pre-training regime, which is presented as a contribution of this paper, does not seem to produce markedly stronger results. It might be valid to apply the same pre-training regime to some of the other models for a fair comparison.",
                "The qualitative evaluation does not yield clear conclusions. It would be beneficial to provide evidence that 'GRAPHIX demonstrates strong inductive biases in learning complex bug-fix patterns' or some conclusion on the kinds of bugs that GRAPHIX tends to fix.",
                "The pre-training regime is not particularly novel. The authors suggest deleting elements of subtrees from existing code and reconstructing them, which is not sufficiently novel."
            ],
            [
                "The paper does not clearly explain the relevance of the presented work despite not beating the state of the art. A discussion on this topic is missing.",
                "The anecdotal examples are not compared to the results other approaches give, making it unclear what to make of them.",
                "The claim that the model is more than 10x smaller than 'current large scale sequence models' is misleading as only one of the evaluated baselines, 'BART', is that much larger. The presented approach is even outperformed by CodeT5-small, which is less than 2x as large.",
                "The paper lacks a discussion on the accuracy of detecting bug fixes by filtering for 'fix', 'bug', 'error', and 'exception'.",
                "The reasoning behind having exactly 600 nodes is not explained."
            ],
            [
                "The pre-training task, while claimed to be novel, is not particularly innovative. It involves 'masking sub-trees' on AST, which essentially asks the model to do 'missing code prediction', covering only a small subset of general program repair samples. The pre-training task could be more innovative if more thought was put into generating the pre-training dataset, such as how to generate data for pre-training other edits like copy, remove, and update.",
                "Pre-training is costly and does not provide a significant performance gain (<2%). This issue is more pronounced with medium-sized data. The reviewer suggests two questions for consideration: 1) Why is pre-training only done on 'sub-trees between 2 and 6 descendants'? Could the size be increased to cover longer edit sequences during pre-training? Is it possible to add such an experiment? 2) Could the lack of performance gain be due to the pre-training task only asking the model to predict a sequence of addition operations? Could more diverse tasks be added to allow the model to predict a variety of edits?",
                "The multi-head graph encoder does not seem particularly novel.",
                "The paper does not mention related works such as 'TFix: Learning to Fix Coding Errors with a Text-to-Text Transformer' and 'Learning semantic program embeddings with graph interval neural network'.",
                "The last paragraph in Section 4.1 mentions a single-head 'base' model, which is not mentioned in any experiment in the main paper. The reviewer suggests moving this description into the Appendix or including the 'base' model and its performance in Table 1/2.",
                "In Listing 2 Top, the code is not valid if the highlighted green part is removed. The reviewer asks for clarification on the before and after of that code snippet.",
                "The paper claims 'these observations suggest that the code abstraction may not be necessary'. The reviewer finds this argument weak and disagrees with the initial premise that 'code abstraction is necessary'."
            ],
            [
                "The paper lacks a comprehensive comparison with prior work, making the contributions over previous work hard to assess.",
                "The efficacy of the proposed multihead Graph Neural Network (GNN) scheme is not adequately ablated. The paper needs to include ablations with models of the same size.",
                "There are concerns about the scalability of the model given the need to encode the entire tree at each intermediate step.",
                "The paper needs to provide equal comparisons of Hoppity and one or more deeper non-multihead Graphix models in the 32M parameter domain.",
                "The contribution of pretraining seems quite slim; performance only increases a little on the Small dataset and actually decreases on the Medium data. This needs more analysis.",
                "The paper needs to analyze and discuss the potential scalability problems of reinstantiating and encoding the program graph on every edit, especially on longer chains of edits.",
                "The paper needs to clarify whether the ensemble-of-graphs model is meant to be a contribution in general or just for source code.",
                "The paper needs to provide a more detailed analysis of the ASDL-guided decoder, especially in comparison with a small instantiation of Hoppity."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper provides a comparison of GRAPHIX with other models and uses the standard top-1 exact match accuracy (EM) metric for evaluation. However, it would be beneficial to include more detailed comparison results, such as the exact scores of GRAPHIX and other models on the same benchmark and using the same metrics. This would provide a clearer picture of how GRAPHIX stands in comparison to these models and validate the claim of competitive performance with fewer parameters. Additionally, it would be helpful to include a discussion on why GRAPHIX underperforms PL-BART and CodeT5-small on the medium when pre-trained on CodeSearchNet Java, and how this could be improved.",
                "Analysis of Pre-training Strategy: The paper introduces a novel pre-training strategy, deleted sub-tree reconstruction, and provides some comparison of the performance of GRAPHIX with and without this strategy. However, the analysis could be more detailed. Specifically, it would be beneficial to include a more thorough investigation of why the pre-training strategy improves performance on the small subsets but does not have the same impact on the medium sets. This could involve exploring whether the synthetic edits used for pre-training are more aligned with the small dataset than the medium, as the authors hypothesize, and discussing potential strategies to address this issue. Such an analysis would provide valuable insights into the effectiveness of the pre-training strategy and could suggest ways to further improve the performance of GRAPHIX."
            ],
            "novelty_impact": [
                "The paper provides some details about the computational resources used, such as the type of GPUs and the batch size. However, it would be helpful to have more specific information about the computational resources required for the multi-head graph encoder. For example, how much memory does the encoder require? How long does it take to train the encoder with the specified batch size on the mentioned hardware? This information is important for understanding the practicality of the model and for researchers who may want to reproduce or build upon this work.",
                "The authors' model, GRAPHIX, is designed to work with Abstract Syntax Trees (ASTs) with an underlying syntax language. This design choice allows the model to leverage the hierarchical and structural information inherent in ASTs, which is beneficial for the task of automated program repair. However, this also means that the model may not be directly applicable to arbitrary graph structures that do not have an underlying syntax language. The authors do not discuss this limitation in detail, nor do they provide evidence or examples to demonstrate how their model could be adapted to work with such graph structures. It would be beneficial for the authors to address this limitation in more detail and provide evidence or examples to support their claim that the model could be extended to other programming languages or code structures.",
                "The paper does not address the potential biases that could be introduced by the use of code change data in the model. Code changes could be influenced by individual programmer's styles or preferences, which could introduce biases that affect the model's performance. This is a significant issue because it could limit the generalizability of the model's performance across different programming styles. The authors should discuss this potential issue, provide evidence to show whether or not it affects the model's performance, and if it does, discuss how they plan to mitigate this issue.",
                "The authors propose incorporating more program dependencies into the graph representation as a direction for future work. However, they do not provide specific details on how this could be done or what the potential benefits might be. Furthermore, they do not discuss the potential increase in complexity and interpretability issues this could cause. For example, their model, GRAPHIX, is a sequential decision process that iteratively performs tree edits on the partial tree at each step, and they employ a grammar-based tree decoder to ensure the grammatical correctness of each edit action. This could potentially increase the complexity of the model, but the authors do not discuss this in detail. The authors should discuss these issues and the trade-off between incorporating more program dependencies and the increase in complexity and interpretability issues in more detail to provide a more complete picture of the potential benefits and challenges of their proposal.",
                "The paper frequently uses the terms 'meaningful' and 'interpretable' in the context of the GRAPHIX model's ability to learn code patterns and bug fixes, but it does not provide explicit definitions or criteria for these terms. This lack of clarity could affect the reproducibility of the study and the ability of other researchers to build upon this work. The authors should provide clear definitions of these terms in the section where they first introduce the GRAPHIX model. This would help readers understand exactly what the authors consider to be 'meaningful' code patterns and 'interpretable' bug fixes, and would allow other researchers to apply the same criteria in their own work.",
                "The paper provides a detailed presentation of GRAPHIX, an automated program repair model, but it lacks a discussion on the potential ethical implications of such technology. Specifically, the authors could consider discussing the potential impact on employment in the software development industry. This discussion is important as it provides a holistic view of the technology and its potential societal implications, which is crucial for readers to fully understand the broader context and potential consequences of the work.",
                "While the paper does discuss some unsuccessful cases of bug fixes suggested by the model, it would be beneficial to provide a more detailed analysis of these cases. Specifically, identifying common patterns among these failures could help readers understand the limitations of the model more clearly. Additionally, discussing potential improvements to the model based on these failures could provide valuable insights for future work. This would not only enhance the completeness of the paper but also provide a clear direction for future research.",
                "The paper would benefit from a more detailed discussion on the limitations and potential issues of the multi-head graph encoder. Specifically, the authors should address the potential complexity issues arising from the graph encoder being the component with the most parameters, and how this might impact the training of the model. Additionally, the authors should provide more clarity on the optimal configuration of the graph encoder, as the paper mentions that two architectures were experimented with. Providing this information would help readers better understand the challenges and trade-offs involved in using a multi-head graph encoder.",
                "The paper provides some details about the implementation and training process of GRAPHIX, as well as some limitations and potential issues. However, it would be helpful to provide more details about the specific limitations and potential issues that were encountered during the implementation and training process. For example, the paper mentions that more sampling strategies may be needed to bridge the gap between the small and medium datasets during pre-training. Could the authors elaborate on what these strategies might be and how they could improve the training process? Additionally, the paper mentions that a small number of samples were discarded during data processing because they could not be parsed. Could the authors discuss whether this issue could be mitigated in future work, and what impact it might have on the generalizability of the results?",
                "The paper discusses the high precision regime for GRAPHIX, demonstrating how the prediction scores can be calibrated to trade-off recall for increased precision. However, it does not provide specific details about the limitations and potential issues with this approach. Providing this information would help readers to fully understand the trade-offs involved in using GRAPHIX, and could be particularly valuable for those considering using it in their own work. Therefore, I recommend that the authors add a section discussing the limitations and potential issues with the high precision regime for GRAPHIX.",
                "The paper provides a detailed description of the Patches in The Wild Java benchmark and uses it for evaluation. However, it does not discuss the limitations and potential issues of using this benchmark. It would be helpful to include a discussion on the potential biases in the benchmark, how it might affect the evaluation results, and any limitations in the benchmark that might limit the generalizability of the results. This would provide a more balanced view of the evaluation and help readers understand the robustness of the results.",
                "The paper provides a detailed discussion of the GRAPHIX model and proposes several future directions. However, it would be beneficial for the authors to explicitly link the discussed limitations of the GRAPHIX model to these future directions. For instance, how might the tendency of the model to suggest incomplete and/or incorrect bug fixes impact the proposed future direction of pre-training based on multiple sub-tree additions and replacements? Additionally, the authors could discuss potential issues with the proposed future directions. For example, what challenges might arise when incorporating more program dependencies into the graph representation or when leveraging Transformer and GNNs in a unified architecture for the global code context in the sequence? Providing this information would give readers a more comprehensive understanding of the potential challenges and opportunities associated with the proposed future directions.",
                "The paper's discussion of the ablation studies comparing the performance of GRAPHIX with other models lacks specific details that are crucial for understanding the extent of the improvements achieved by GRAPHIX and validating the paper's claims. Specifically, the paper should provide more information on the performance metrics used for comparison, the exact improvements achieved by GRAPHIX, and the detailed configuration of the models used for comparison. Providing these details would enhance the clarity and depth of the paper's discussion of the ablation studies and strengthen its claims.",
                "While the paper discusses the use of code change data to address the lack of large human-labeled datasets, it does not provide a detailed discussion on the limitations or potential issues of using such data. It would be beneficial for the authors to elaborate on this aspect. For instance, they could discuss potential biases in the code change data, the risk of overfitting to the specific characteristics of the code changes, or the generalizability of the findings to other types of data. This would provide a more balanced view of the proposed method and help readers understand its potential drawbacks."
            ],
            "clarity_reproducibility": [
                "The paper provides some details about how the parameters for the multi-head graph encoder were chosen, but it lacks a detailed explanation or rationale for these specific choices. Understanding the rationale behind these choices is crucial for reproducing the results and for adapting the model for other tasks or datasets. We recommend that the authors provide more specific details about the rationale behind the chosen parameters, including any optimization or experimentation process involved. This will enhance the reproducibility of the work and provide readers with a better understanding of the decisions made in the study.",
                "The paper mentions the role of the Abstract Syntax Description Language (ASDL) in guiding the decoder in the GRAPHIX model, but it does not provide detailed information about how ASDL works or how it guides the decoder. Understanding these aspects is crucial for understanding the functioning of the decoder and the performance of the GRAPHIX model. The authors should provide more details about how ASDL guides the decoder in maintaining the syntax of the code during the editing process, and why this guidance is important for learning generic fixing and refactoring patterns from code changes.",
                "The paper mentions that during pre-training, sub-trees between 2 and 6 descendants are selected for deletion in the deleted sub-tree reconstruction pre-training strategy. However, it does not detail the specific method or distribution used for this selection. Providing this information would enhance the understanding of the pre-training strategy and ensure the reproducibility of the work. Please add these details.",
                "The paper mentions the use of a beam search of size 5 during inference for GRAPHIX and e-HOPPITY, but it does not provide details on how the size of the beam search was chosen. This information is crucial for understanding the inference process and for ensuring the reproducibility of the work. Specifically, the choice of beam size could significantly impact the results of the inference process. Therefore, the authors should provide a justification for the choice of a beam search of size 5, including any experiments or comparisons that led to this choice.",
                "The authors claim that GRAPHIX is insensitive to the naming of variables, types, and APIs, based on the observation of a negligible drop in accuracy from abstract to concrete code. However, the paper lacks specific experiments or analyses to substantiate this claim. It would be beneficial for the authors to provide more concrete evidence to support this claim. This could include conducting additional experiments that specifically test the sensitivity of GRAPHIX to naming, or providing a more detailed analysis of the existing results that demonstrates this insensitivity. This information is crucial for understanding the robustness of GRAPHIX and its ability to learn meaningful structural and semantic code patterns.",
                "The paper provides some details about how a sub-tree was added to HOPPITY, which is appreciated. However, it lacks details about how HOPPITY was modified to handle Java source code. This information is crucial for understanding the comparison between HOPPITY and the proposed model, GRAPHIX, and for ensuring the reproducibility of the work. Please provide more details about how HOPPITY was modified to handle Java source code.",
                "The paper lacks specific details about the learning rates and number of epochs used in the implementation and experimental setup. While the hardware used for training, the batch size, and the optimizer are mentioned, the absence of information about the learning rates and number of epochs hinders the reproducibility of the work and makes it difficult for readers to fully understand the implementation and experimental setup. Providing these details would greatly enhance the paper."
            ],
            "all": [
                "The paper provides a comparison of GRAPHIX with other models and uses the standard top-1 exact match accuracy (EM) metric for evaluation. However, it would be beneficial to include more detailed comparison results, such as the exact scores of GRAPHIX and other models on the same benchmark and using the same metrics. This would provide a clearer picture of how GRAPHIX stands in comparison to these models and validate the claim of competitive performance with fewer parameters. Additionally, it would be helpful to include a discussion on why GRAPHIX underperforms PL-BART and CodeT5-small on the medium when pre-trained on CodeSearchNet Java, and how this could be improved.",
                "Analysis of Pre-training Strategy: The paper introduces a novel pre-training strategy, deleted sub-tree reconstruction, and provides some comparison of the performance of GRAPHIX with and without this strategy. However, the analysis could be more detailed. Specifically, it would be beneficial to include a more thorough investigation of why the pre-training strategy improves performance on the small subsets but does not have the same impact on the medium sets. This could involve exploring whether the synthetic edits used for pre-training are more aligned with the small dataset than the medium, as the authors hypothesize, and discussing potential strategies to address this issue. Such an analysis would provide valuable insights into the effectiveness of the pre-training strategy and could suggest ways to further improve the performance of GRAPHIX.",
                "The paper provides some details about the computational resources used, such as the type of GPUs and the batch size. However, it would be helpful to have more specific information about the computational resources required for the multi-head graph encoder. For example, how much memory does the encoder require? How long does it take to train the encoder with the specified batch size on the mentioned hardware? This information is important for understanding the practicality of the model and for researchers who may want to reproduce or build upon this work.",
                "The authors' model, GRAPHIX, is designed to work with Abstract Syntax Trees (ASTs) with an underlying syntax language. This design choice allows the model to leverage the hierarchical and structural information inherent in ASTs, which is beneficial for the task of automated program repair. However, this also means that the model may not be directly applicable to arbitrary graph structures that do not have an underlying syntax language. The authors do not discuss this limitation in detail, nor do they provide evidence or examples to demonstrate how their model could be adapted to work with such graph structures. It would be beneficial for the authors to address this limitation in more detail and provide evidence or examples to support their claim that the model could be extended to other programming languages or code structures.",
                "The paper does not address the potential biases that could be introduced by the use of code change data in the model. Code changes could be influenced by individual programmer's styles or preferences, which could introduce biases that affect the model's performance. This is a significant issue because it could limit the generalizability of the model's performance across different programming styles. The authors should discuss this potential issue, provide evidence to show whether or not it affects the model's performance, and if it does, discuss how they plan to mitigate this issue.",
                "The authors propose incorporating more program dependencies into the graph representation as a direction for future work. However, they do not provide specific details on how this could be done or what the potential benefits might be. Furthermore, they do not discuss the potential increase in complexity and interpretability issues this could cause. For example, their model, GRAPHIX, is a sequential decision process that iteratively performs tree edits on the partial tree at each step, and they employ a grammar-based tree decoder to ensure the grammatical correctness of each edit action. This could potentially increase the complexity of the model, but the authors do not discuss this in detail. The authors should discuss these issues and the trade-off between incorporating more program dependencies and the increase in complexity and interpretability issues in more detail to provide a more complete picture of the potential benefits and challenges of their proposal.",
                "The paper frequently uses the terms 'meaningful' and 'interpretable' in the context of the GRAPHIX model's ability to learn code patterns and bug fixes, but it does not provide explicit definitions or criteria for these terms. This lack of clarity could affect the reproducibility of the study and the ability of other researchers to build upon this work. The authors should provide clear definitions of these terms in the section where they first introduce the GRAPHIX model. This would help readers understand exactly what the authors consider to be 'meaningful' code patterns and 'interpretable' bug fixes, and would allow other researchers to apply the same criteria in their own work.",
                "The paper provides a detailed presentation of GRAPHIX, an automated program repair model, but it lacks a discussion on the potential ethical implications of such technology. Specifically, the authors could consider discussing the potential impact on employment in the software development industry. This discussion is important as it provides a holistic view of the technology and its potential societal implications, which is crucial for readers to fully understand the broader context and potential consequences of the work.",
                "While the paper does discuss some unsuccessful cases of bug fixes suggested by the model, it would be beneficial to provide a more detailed analysis of these cases. Specifically, identifying common patterns among these failures could help readers understand the limitations of the model more clearly. Additionally, discussing potential improvements to the model based on these failures could provide valuable insights for future work. This would not only enhance the completeness of the paper but also provide a clear direction for future research.",
                "The paper would benefit from a more detailed discussion on the limitations and potential issues of the multi-head graph encoder. Specifically, the authors should address the potential complexity issues arising from the graph encoder being the component with the most parameters, and how this might impact the training of the model. Additionally, the authors should provide more clarity on the optimal configuration of the graph encoder, as the paper mentions that two architectures were experimented with. Providing this information would help readers better understand the challenges and trade-offs involved in using a multi-head graph encoder.",
                "The paper provides some details about the implementation and training process of GRAPHIX, as well as some limitations and potential issues. However, it would be helpful to provide more details about the specific limitations and potential issues that were encountered during the implementation and training process. For example, the paper mentions that more sampling strategies may be needed to bridge the gap between the small and medium datasets during pre-training. Could the authors elaborate on what these strategies might be and how they could improve the training process? Additionally, the paper mentions that a small number of samples were discarded during data processing because they could not be parsed. Could the authors discuss whether this issue could be mitigated in future work, and what impact it might have on the generalizability of the results?",
                "The paper discusses the high precision regime for GRAPHIX, demonstrating how the prediction scores can be calibrated to trade-off recall for increased precision. However, it does not provide specific details about the limitations and potential issues with this approach. Providing this information would help readers to fully understand the trade-offs involved in using GRAPHIX, and could be particularly valuable for those considering using it in their own work. Therefore, I recommend that the authors add a section discussing the limitations and potential issues with the high precision regime for GRAPHIX.",
                "The paper provides a detailed description of the Patches in The Wild Java benchmark and uses it for evaluation. However, it does not discuss the limitations and potential issues of using this benchmark. It would be helpful to include a discussion on the potential biases in the benchmark, how it might affect the evaluation results, and any limitations in the benchmark that might limit the generalizability of the results. This would provide a more balanced view of the evaluation and help readers understand the robustness of the results.",
                "The paper provides a detailed discussion of the GRAPHIX model and proposes several future directions. However, it would be beneficial for the authors to explicitly link the discussed limitations of the GRAPHIX model to these future directions. For instance, how might the tendency of the model to suggest incomplete and/or incorrect bug fixes impact the proposed future direction of pre-training based on multiple sub-tree additions and replacements? Additionally, the authors could discuss potential issues with the proposed future directions. For example, what challenges might arise when incorporating more program dependencies into the graph representation or when leveraging Transformer and GNNs in a unified architecture for the global code context in the sequence? Providing this information would give readers a more comprehensive understanding of the potential challenges and opportunities associated with the proposed future directions.",
                "The paper's discussion of the ablation studies comparing the performance of GRAPHIX with other models lacks specific details that are crucial for understanding the extent of the improvements achieved by GRAPHIX and validating the paper's claims. Specifically, the paper should provide more information on the performance metrics used for comparison, the exact improvements achieved by GRAPHIX, and the detailed configuration of the models used for comparison. Providing these details would enhance the clarity and depth of the paper's discussion of the ablation studies and strengthen its claims.",
                "While the paper discusses the use of code change data to address the lack of large human-labeled datasets, it does not provide a detailed discussion on the limitations or potential issues of using such data. It would be beneficial for the authors to elaborate on this aspect. For instance, they could discuss potential biases in the code change data, the risk of overfitting to the specific characteristics of the code changes, or the generalizability of the findings to other types of data. This would provide a more balanced view of the proposed method and help readers understand its potential drawbacks.",
                "The paper provides some details about how the parameters for the multi-head graph encoder were chosen, but it lacks a detailed explanation or rationale for these specific choices. Understanding the rationale behind these choices is crucial for reproducing the results and for adapting the model for other tasks or datasets. We recommend that the authors provide more specific details about the rationale behind the chosen parameters, including any optimization or experimentation process involved. This will enhance the reproducibility of the work and provide readers with a better understanding of the decisions made in the study.",
                "The paper mentions the role of the Abstract Syntax Description Language (ASDL) in guiding the decoder in the GRAPHIX model, but it does not provide detailed information about how ASDL works or how it guides the decoder. Understanding these aspects is crucial for understanding the functioning of the decoder and the performance of the GRAPHIX model. The authors should provide more details about how ASDL guides the decoder in maintaining the syntax of the code during the editing process, and why this guidance is important for learning generic fixing and refactoring patterns from code changes.",
                "The paper mentions that during pre-training, sub-trees between 2 and 6 descendants are selected for deletion in the deleted sub-tree reconstruction pre-training strategy. However, it does not detail the specific method or distribution used for this selection. Providing this information would enhance the understanding of the pre-training strategy and ensure the reproducibility of the work. Please add these details.",
                "The paper mentions the use of a beam search of size 5 during inference for GRAPHIX and e-HOPPITY, but it does not provide details on how the size of the beam search was chosen. This information is crucial for understanding the inference process and for ensuring the reproducibility of the work. Specifically, the choice of beam size could significantly impact the results of the inference process. Therefore, the authors should provide a justification for the choice of a beam search of size 5, including any experiments or comparisons that led to this choice.",
                "The authors claim that GRAPHIX is insensitive to the naming of variables, types, and APIs, based on the observation of a negligible drop in accuracy from abstract to concrete code. However, the paper lacks specific experiments or analyses to substantiate this claim. It would be beneficial for the authors to provide more concrete evidence to support this claim. This could include conducting additional experiments that specifically test the sensitivity of GRAPHIX to naming, or providing a more detailed analysis of the existing results that demonstrates this insensitivity. This information is crucial for understanding the robustness of GRAPHIX and its ability to learn meaningful structural and semantic code patterns.",
                "The paper provides some details about how a sub-tree was added to HOPPITY, which is appreciated. However, it lacks details about how HOPPITY was modified to handle Java source code. This information is crucial for understanding the comparison between HOPPITY and the proposed model, GRAPHIX, and for ensuring the reproducibility of the work. Please provide more details about how HOPPITY was modified to handle Java source code.",
                "The paper lacks specific details about the learning rates and number of epochs used in the implementation and experimental setup. While the hardware used for training, the batch size, and the optimizer are mentioned, the absence of information about the learning rates and number of epochs hinders the reproducibility of the work and makes it difficult for readers to fully understand the implementation and experimental setup. Providing these details would greatly enhance the paper."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 4,
                    "predicted_id": 1,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The contribution of pretraining seems quite slim; performance only increases a little on the Small dataset and actually decreases on the Medium data. This needs more analysis.",
                    "predicted_comment": "Analysis of Pre-training Strategy: The paper introduces a novel pre-training strategy, deleted sub-tree reconstruction, and provides some comparison of the performance of GRAPHIX with and without this strategy. However, the analysis could be more detailed. Specifically, it would be beneficial to include a more thorough investigation of why the pre-training strategy improves performance on the small subsets but does not have the same impact on the medium sets. This could involve exploring whether the synthetic edits used for pre-training are more aligned with the small dataset than the medium, as the authors hypothesize, and discussing potential strategies to address this issue. Such an analysis would provide valuable insights into the effectiveness of the pre-training strategy and could suggest ways to further improve the performance of GRAPHIX."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper lacks a comprehensive comparison with prior work, making the contributions over previous work hard to assess.",
                    "predicted_comment": "The paper provides a comparison of GRAPHIX with other models and uses the standard top-1 exact match accuracy (EM) metric for evaluation. However, it would be beneficial to include more detailed comparison results, such as the exact scores of GRAPHIX and other models on the same benchmark and using the same metrics. This would provide a clearer picture of how GRAPHIX stands in comparison to these models and validate the claim of competitive performance with fewer parameters. Additionally, it would be helpful to include a discussion on why GRAPHIX underperforms PL-BART and CodeT5-small on the medium when pre-trained on CodeSearchNet Java, and how this could be improved."
                }
            ],
            "novelty_impact": [
                {
                    "reference_id": 5,
                    "predicted_id": 8,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The qualitative evaluation does not yield clear conclusions. It would be beneficial to provide evidence that 'GRAPHIX demonstrates strong inductive biases in learning complex bug-fix patterns' or some conclusion on the kinds of bugs that GRAPHIX tends to fix.",
                    "predicted_comment": "While the paper does discuss some unsuccessful cases of bug fixes suggested by the model, it would be beneficial to provide a more detailed analysis of these cases. Specifically, identifying common patterns among these failures could help readers understand the limitations of the model more clearly. Additionally, discussing potential improvements to the model based on these failures could provide valuable insights for future work. This would not only enhance the completeness of the paper but also provide a clear direction for future research."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 14,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper does not convincingly demonstrate model novelty or significant improvement to the state-of-the-art on the provided baseline.",
                    "predicted_comment": "The paper's discussion of the ablation studies comparing the performance of GRAPHIX with other models lacks specific details that are crucial for understanding the extent of the improvements achieved by GRAPHIX and validating the paper's claims. Specifically, the paper should provide more information on the performance metrics used for comparison, the exact improvements achieved by GRAPHIX, and the detailed configuration of the models used for comparison. Providing these details would enhance the clarity and depth of the paper's discussion of the ablation studies and strengthen its claims."
                },
                {
                    "reference_id": 3,
                    "predicted_id": 14,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper needs to provide equal comparisons of Hoppity and one or more deeper non-multihead Graphix models in the 32M parameter domain.",
                    "predicted_comment": "The paper's discussion of the ablation studies comparing the performance of GRAPHIX with other models lacks specific details that are crucial for understanding the extent of the improvements achieved by GRAPHIX and validating the paper's claims. Specifically, the paper should provide more information on the performance metrics used for comparison, the exact improvements achieved by GRAPHIX, and the detailed configuration of the models used for comparison. Providing these details would enhance the clarity and depth of the paper's discussion of the ablation studies and strengthen its claims."
                },
                {
                    "reference_id": 6,
                    "predicted_id": 3,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper needs to clarify whether the ensemble-of-graphs model is meant to be a contribution in general or just for source code.",
                    "predicted_comment": "The authors' model, GRAPHIX, is designed to work with Abstract Syntax Trees (ASTs) with an underlying syntax language. This design choice allows the model to leverage the hierarchical and structural information inherent in ASTs, which is beneficial for the task of automated program repair. However, this also means that the model may not be directly applicable to arbitrary graph structures that do not have an underlying syntax language. The authors do not discuss this limitation in detail, nor do they provide evidence or examples to demonstrate how their model could be adapted to work with such graph structures. It would be beneficial for the authors to address this limitation in more detail and provide evidence or examples to support their claim that the model could be extended to other programming languages or code structures."
                }
            ],
            "clarity_reproducibility": [
                {
                    "reference_id": 7,
                    "predicted_id": 17,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "The paper needs to provide a more detailed analysis of the ASDL-guided decoder, especially in comparison with a small instantiation of Hoppity.",
                    "predicted_comment": "The paper mentions the role of the Abstract Syntax Description Language (ASDL) in guiding the decoder in the GRAPHIX model, but it does not provide detailed information about how ASDL works or how it guides the decoder. Understanding these aspects is crucial for understanding the functioning of the decoder and the performance of the GRAPHIX model. The authors should provide more details about how ASDL guides the decoder in maintaining the syntax of the code during the editing process, and why this guidance is important for learning generic fixing and refactoring patterns from code changes."
                }
            ],
            "all": [
                {
                    "reference_id": 5,
                    "predicted_id": 8,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The qualitative evaluation does not yield clear conclusions. It would be beneficial to provide evidence that 'GRAPHIX demonstrates strong inductive biases in learning complex bug-fix patterns' or some conclusion on the kinds of bugs that GRAPHIX tends to fix.",
                    "predicted_comment": "While the paper does discuss some unsuccessful cases of bug fixes suggested by the model, it would be beneficial to provide a more detailed analysis of these cases. Specifically, identifying common patterns among these failures could help readers understand the limitations of the model more clearly. Additionally, discussing potential improvements to the model based on these failures could provide valuable insights for future work. This would not only enhance the completeness of the paper but also provide a clear direction for future research."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 14,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper does not convincingly demonstrate model novelty or significant improvement to the state-of-the-art on the provided baseline.",
                    "predicted_comment": "The paper's discussion of the ablation studies comparing the performance of GRAPHIX with other models lacks specific details that are crucial for understanding the extent of the improvements achieved by GRAPHIX and validating the paper's claims. Specifically, the paper should provide more information on the performance metrics used for comparison, the exact improvements achieved by GRAPHIX, and the detailed configuration of the models used for comparison. Providing these details would enhance the clarity and depth of the paper's discussion of the ablation studies and strengthen its claims."
                },
                {
                    "reference_id": 7,
                    "predicted_id": 17,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "The paper needs to provide a more detailed analysis of the ASDL-guided decoder, especially in comparison with a small instantiation of Hoppity.",
                    "predicted_comment": "The paper mentions the role of the Abstract Syntax Description Language (ASDL) in guiding the decoder in the GRAPHIX model, but it does not provide detailed information about how ASDL works or how it guides the decoder. Understanding these aspects is crucial for understanding the functioning of the decoder and the performance of the GRAPHIX model. The authors should provide more details about how ASDL guides the decoder in maintaining the syntax of the code during the editing process, and why this guidance is important for learning generic fixing and refactoring patterns from code changes."
                },
                {
                    "reference_id": 4,
                    "predicted_id": 1,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The contribution of pretraining seems quite slim; performance only increases a little on the Small dataset and actually decreases on the Medium data. This needs more analysis.",
                    "predicted_comment": "Analysis of Pre-training Strategy: The paper introduces a novel pre-training strategy, deleted sub-tree reconstruction, and provides some comparison of the performance of GRAPHIX with and without this strategy. However, the analysis could be more detailed. Specifically, it would be beneficial to include a more thorough investigation of why the pre-training strategy improves performance on the small subsets but does not have the same impact on the medium sets. This could involve exploring whether the synthetic edits used for pre-training are more aligned with the small dataset than the medium, as the authors hypothesize, and discussing potential strategies to address this issue. Such an analysis would provide valuable insights into the effectiveness of the pre-training strategy and could suggest ways to further improve the performance of GRAPHIX."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper lacks a comprehensive comparison with prior work, making the contributions over previous work hard to assess.",
                    "predicted_comment": "The paper provides a comparison of GRAPHIX with other models and uses the standard top-1 exact match accuracy (EM) metric for evaluation. However, it would be beneficial to include more detailed comparison results, such as the exact scores of GRAPHIX and other models on the same benchmark and using the same metrics. This would provide a clearer picture of how GRAPHIX stands in comparison to these models and validate the claim of competitive performance with fewer parameters. Additionally, it would be helpful to include a discussion on why GRAPHIX underperforms PL-BART and CodeT5-small on the medium when pre-trained on CodeSearchNet Java, and how this could be improved."
                },
                {
                    "reference_id": 3,
                    "predicted_id": 14,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper needs to provide equal comparisons of Hoppity and one or more deeper non-multihead Graphix models in the 32M parameter domain.",
                    "predicted_comment": "The paper's discussion of the ablation studies comparing the performance of GRAPHIX with other models lacks specific details that are crucial for understanding the extent of the improvements achieved by GRAPHIX and validating the paper's claims. Specifically, the paper should provide more information on the performance metrics used for comparison, the exact improvements achieved by GRAPHIX, and the detailed configuration of the models used for comparison. Providing these details would enhance the clarity and depth of the paper's discussion of the ablation studies and strengthen its claims."
                },
                {
                    "reference_id": 6,
                    "predicted_id": 3,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper needs to clarify whether the ensemble-of-graphs model is meant to be a contribution in general or just for source code.",
                    "predicted_comment": "The authors' model, GRAPHIX, is designed to work with Abstract Syntax Trees (ASTs) with an underlying syntax language. This design choice allows the model to leverage the hierarchical and structural information inherent in ASTs, which is beneficial for the task of automated program repair. However, this also means that the model may not be directly applicable to arbitrary graph structures that do not have an underlying syntax language. The authors do not discuss this limitation in detail, nor do they provide evidence or examples to demonstrate how their model could be adapted to work with such graph structures. It would be beneficial for the authors to address this limitation in more detail and provide evidence or examples to support their claim that the model could be extended to other programming languages or code structures."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 27,
                "n_generated": 2,
                "n_total_aligns": 2,
                "recall": 0.0625,
                "precision": 0.25,
                "pseudo_jaccard": 0.0625,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 27,
                "n_generated": 14,
                "n_total_aligns": 4,
                "recall": 0.13392857142857142,
                "precision": 0.07142857142857142,
                "pseudo_jaccard": 0.05131578947368421,
                "specificity_more": 0.75,
                "specificity_less": 0.0,
                "specificity_same": 0.25
            },
            "clarity_reproducibility": {
                "n_reference": 27,
                "n_generated": 7,
                "n_total_aligns": 1,
                "recall": 0.03125,
                "precision": 0.03571428571428571,
                "pseudo_jaccard": 0.017857142857142856,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "all": {
                "n_reference": 27,
                "n_generated": 23,
                "n_total_aligns": 7,
                "recall": 0.22767857142857142,
                "precision": 0.07608695652173914,
                "pseudo_jaccard": 0.06593406593406594,
                "specificity_more": 0.8571428571428571,
                "specificity_less": 0.0,
                "specificity_same": 0.14285714285714285
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.25,
                    "precision": 1.0,
                    "pseudo_jaccard": 0.25,
                    "n_real": 8
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.2857142857142857,
                    "precision": 0.14285714285714285,
                    "pseudo_jaccard": 0.10526315789473684,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.25,
                    "precision": 0.14285714285714285,
                    "pseudo_jaccard": 0.1,
                    "n_real": 8
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.125,
                    "precision": 0.14285714285714285,
                    "pseudo_jaccard": 0.07142857142857142,
                    "n_real": 8
                }
            ],
            "all": [
                {
                    "recall": 0.2857142857142857,
                    "precision": 0.08695652173913043,
                    "pseudo_jaccard": 0.07142857142857142,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.625,
                    "precision": 0.21739130434782608,
                    "pseudo_jaccard": 0.19230769230769232,
                    "n_real": 8
                }
            ]
        }
    },
    {
        "doc_id": "nLb60uXd6Np",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The quantitative results are good, but not revolutionary. The model does well on the crystal classification task, but this setting is described as 'not difficult'. It performs slightly worse than GemNet on molecular force regression. On the backmapping task, it is only compared against a naive transformer, as opposed to another rotation invariant model. Another instance of the proposed model outperforming a strong baseline on a difficult task would significantly improve the case the paper is making.",
                "While one of the advantages of the proposed method is scaling to arbitrary rank, the experiments are all limited to pairwise attention (not counting the current key element). It would be interesting to demonstrate the effect of scaling up the rank, especially as this is noted as a potential advantage of the GemNet baseline.",
                "Group representation-based approaches have not been compared against."
            ],
            [
                "The details of the four functions that make up the attention mechanism are vague. More elaboration on the technical details, such as what are rotation-invariant geometric quantities for tuples, is needed to improve readability.",
                "The paper lacks a mathematical proof of rotational equivariance.",
                "The paper does not provide information on the efficiency of the proposed method, especially when compared with other works.",
                "The paper does not include experiments testing rotation equivariance. It is recommended to have an experimental test for the rotation equivariance by comparing with other works, such as rotation equivariance achieved via data augmentation.",
                "The paper does not clarify whether the proposed method can be extended to larger point clouds. It is also unclear what the definition of 'small point clouds' is. An experiment on 3D point cloud classification, such as the popularly used Modelnet 40, is recommended."
            ],
            [
                "In Table 1, the proposed model performs worse than the baseline GemNet-Q. The authors should consider adjusting their method to incorporate properties such as the use of quadruplets atoms, the incorporation of energy, or a better architecture, in order to perform a fair comparison with this baseline."
            ],
            [
                "The authors do not clearly describe how the geometric product helps to achieve the rotation-invariant attributes. This should be explained in the main manuscript since this is one of the main contributions.",
                "Geometric products do not appear in both model architectures and appendix B. The authors need to explicitly describe how it is implemented.",
                "The interpretation of the linear combination in geometric algebra is not trivial. The paper lacks the interpretation, justification, and discussion about the attention mechanism in geometric algebra.",
                "The proposed architecture is limited to 'small point clouds' without clear explanation. If the statement about the computational complexity is the reason, the connection to it should be clarified.",
                "The authors defined four functions, V, M, J, and S. However, another function R is defined later. Why do the authors distinguish R from the others? Are the four functions cover everything sufficiently? Justification is required.",
                "The authors stated that GemNets can operate on quadruplets of atoms but the proposed network worked only in pairs of bonds from the central atom. If this is the limitation of the proposed network, clear statement and discussion are required. Moreover, a discussion is appreciated whether the proposed network has technical difficulties to include the force label."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper mentions using the freud python library to determine the 'nearest neighbors' but does not provide specific details about the method used within this library. The lack of a clearly stated method could potentially introduce bias into the experiment, as different methods may yield different results. This is particularly important in the context of backmapping of coarse-graining operators and the attention mechanism described in the paper. Therefore, it is recommended to use a standard method for determining nearest neighbors in point clouds, such as k-nearest neighbors (k-NN) or a radius-based method, and to clearly state this method in the paper to ensure the reproducibility and reliability of the results.",
                "The authors mention that the models are conservative, permutation-invariant, and rotation-equivariant by construction, and that they calculate the gradient of the per-molecule energy with respect to the input coordinates to ensure that a conservative force field is learned. However, the paper lacks specific details on how the conservativeness of the force field was ensured or verified. This is crucial for the validity of the results. The authors should provide a more detailed explanation of the process used to ensure the conservativeness of the force field. Additionally, conducting an experiment to verify the conservativeness of the force field, such as calculating the work done by the force field around a closed loop and verifying that it is zero, would strengthen the paper.",
                "The method used to incorporate local geometry information into the coarse-grained bead representations is described in the paper using geometric algebra and an attention mechanism. However, the description could be made clearer by providing more details about how these techniques are applied in the context of protein coarse-grain backmapping. For example, it would be helpful to explain how the geometric algebra provides a mathematical structure to deal with geometric objects and how the attention mechanism operates on each tuple of points. Additionally, while the paper mentions the use of geometric algebra attention networks, a type of geometric deep learning method, it would be beneficial to provide more information about how this method explicitly takes into account local geometry."
            ],
            "novelty_impact": [
                "The paper presents a novel approach to geometric deep learning, but it could benefit from a more detailed explanation of how the rotation-and permutation-equivariant architectures were specifically applied in the tasks of crystal structure identification, molecular force regression, and backmapping of coarse-graining operators. For instance, in the crystal structure identification task, it would be helpful to elaborate on how the architecture was used to identify local environments extracted from ordered structures using a rotation-invariant classifier built on the attention mechanism. In the molecular force regression task, it would be beneficial to provide more details on how the architecture was used to predict atomic forces calculated using ab initio molecular dynamics and density functional theory. For the backmapping of coarse-graining operators task, it would be useful to explain how the architecture was used to predict the coordinates of the heavy atoms that form an amino acid from the centers of mass of the nearest-neighbor amino acids in protein entries found within the Protein Data Bank. Additionally, providing specific examples or case studies to illustrate the application of your approach in these tasks would help readers better understand the practical implications and potential benefits of your work.",
                "The paper uses Protein Data Bank (PDB) entries in the task of backmapping of coarse-graining operators, which involves predicting the coordinates of the heavy atoms that form an amino acid from the centers of mass of the nearest-neighbor amino acids. While the use of PDB entries is clear, the paper could benefit from a more explicit explanation of why these specific entries were chosen. For instance, the paper could explain why entries with high-resolution structural refinements and recent publication dates were selected. This would provide readers with a better understanding of the selection criteria and the quality of the data used in the study.",
                "The paper provides a detailed explanation of the integration of geometric algebra and attention mechanisms in the development of the deep learning architectures. However, it would be beneficial to include more examples or case studies to illustrate this integration process and its impact on the performance of the architectures. This would help readers better understand the practical implications of this integration and its potential benefits.",
                "The paper provides a comparison of the proposed model with several existing models, using various metrics and datasets. However, it would be beneficial to provide more context on these comparisons. Specifically, it would be helpful to include the specific metrics and datasets used for comparison with the models mentioned in the 'Related Work' section. Additionally, the exact metrics used for comparison in the protein coarse-grain backmapping task should be specified. Providing this context would help readers understand the significance of the comparisons and the value of the proposed model.",
                "The paper suggests potential methods to avoid polynomial scaling, such as reducing the set of products according to the edges of a specified graph, the Voronoi diagram of the point cloud, or by randomly sampling tuples of points. However, it lacks a detailed discussion on the implications of these methods and why they were not explored in the current study. Elaborating on these aspects could provide readers with a better understanding of the potential improvements and future directions of this research. Therefore, it is recommended that the authors include this information in the paper.",
                "While the paper does mention some limitations and potential areas for future research, these are scattered throughout the text and not consolidated in a dedicated section. It would be beneficial to have a more detailed and focused discussion on the limitations of the study, such as the need to avoid polynomial scaling, the simplification of the architecture for systems at nonzero temperature, and the use of the training set error to characterize model performance due to the resolution of the structural refinement algorithms. Additionally, potential areas for future research, such as the reduction of the set of products according to the edges of a specified graph, the Voronoi diagram of the point cloud, or by randomly sampling tuples of points, and the exploration of the attention mechanism that accounts for both geometric and node-level signals, could be elaborated on."
            ],
            "clarity_reproducibility": [
                "The paper mentions a learning rate adjustment strategy in the 'MODEL TRAINING' section, but it does not provide specific details on the initial learning rate, the batch size, or any regularization techniques used in the training process. These details are crucial for the reproducibility of the experiments, as they directly impact the training process and the results obtained. Without these details, it would be difficult for other researchers to replicate the experiments and verify the results. Therefore, the authors should include these details in the paper to enhance its reproducibility and reliability.",
                "The paper would benefit from a more explicit explanation of the selection criteria for the 19 protein structures used in the coarse-graining task. While it is mentioned that these structures have high-resolution structural refinements and were published between 2015 and 2020, additional details would enhance the reproducibility of the study and strengthen the validity of the results. For example, were the proteins selected based on their size, complexity, or relevance to a particular biological process? Were there any specific characteristics of the proteins that made them suitable for the coarse-graining task? This information would help readers understand the scope and applicability of your method.",
                "The paper mentions potential extensions for reducing the set of products according to the edges of a specified graph, the Voronoi diagram of the point cloud, or by randomly sampling tuples of points. However, it lacks sufficient details about these methods. Providing more information about why these methods were chosen, how they could be implemented, and what impact they could have on the results would enhance the understanding of the paper's methods and its potential for future work.",
                "The paper provides a detailed discussion of 'invariant' and 'covariant' attributes in the context of geometric deep learning, including the use of rotation-and permutation-equivariant architectures and geometric algebra. However, it would be helpful to include specific examples of these attributes in the proposed architecture or the experiments. This would provide readers with a concrete understanding of these concepts and how they are applied in the model. For instance, the authors could illustrate how the model is invariant to rotations and permutations in the context of a specific task, or how the rotation-covariant layers in the model work with a specific input data."
            ],
            "all": [
                "The paper mentions using the freud python library to determine the 'nearest neighbors' but does not provide specific details about the method used within this library. The lack of a clearly stated method could potentially introduce bias into the experiment, as different methods may yield different results. This is particularly important in the context of backmapping of coarse-graining operators and the attention mechanism described in the paper. Therefore, it is recommended to use a standard method for determining nearest neighbors in point clouds, such as k-nearest neighbors (k-NN) or a radius-based method, and to clearly state this method in the paper to ensure the reproducibility and reliability of the results.",
                "The authors mention that the models are conservative, permutation-invariant, and rotation-equivariant by construction, and that they calculate the gradient of the per-molecule energy with respect to the input coordinates to ensure that a conservative force field is learned. However, the paper lacks specific details on how the conservativeness of the force field was ensured or verified. This is crucial for the validity of the results. The authors should provide a more detailed explanation of the process used to ensure the conservativeness of the force field. Additionally, conducting an experiment to verify the conservativeness of the force field, such as calculating the work done by the force field around a closed loop and verifying that it is zero, would strengthen the paper.",
                "The method used to incorporate local geometry information into the coarse-grained bead representations is described in the paper using geometric algebra and an attention mechanism. However, the description could be made clearer by providing more details about how these techniques are applied in the context of protein coarse-grain backmapping. For example, it would be helpful to explain how the geometric algebra provides a mathematical structure to deal with geometric objects and how the attention mechanism operates on each tuple of points. Additionally, while the paper mentions the use of geometric algebra attention networks, a type of geometric deep learning method, it would be beneficial to provide more information about how this method explicitly takes into account local geometry.",
                "The paper presents a novel approach to geometric deep learning, but it could benefit from a more detailed explanation of how the rotation-and permutation-equivariant architectures were specifically applied in the tasks of crystal structure identification, molecular force regression, and backmapping of coarse-graining operators. For instance, in the crystal structure identification task, it would be helpful to elaborate on how the architecture was used to identify local environments extracted from ordered structures using a rotation-invariant classifier built on the attention mechanism. In the molecular force regression task, it would be beneficial to provide more details on how the architecture was used to predict atomic forces calculated using ab initio molecular dynamics and density functional theory. For the backmapping of coarse-graining operators task, it would be useful to explain how the architecture was used to predict the coordinates of the heavy atoms that form an amino acid from the centers of mass of the nearest-neighbor amino acids in protein entries found within the Protein Data Bank. Additionally, providing specific examples or case studies to illustrate the application of your approach in these tasks would help readers better understand the practical implications and potential benefits of your work.",
                "The paper uses Protein Data Bank (PDB) entries in the task of backmapping of coarse-graining operators, which involves predicting the coordinates of the heavy atoms that form an amino acid from the centers of mass of the nearest-neighbor amino acids. While the use of PDB entries is clear, the paper could benefit from a more explicit explanation of why these specific entries were chosen. For instance, the paper could explain why entries with high-resolution structural refinements and recent publication dates were selected. This would provide readers with a better understanding of the selection criteria and the quality of the data used in the study.",
                "The paper provides a detailed explanation of the integration of geometric algebra and attention mechanisms in the development of the deep learning architectures. However, it would be beneficial to include more examples or case studies to illustrate this integration process and its impact on the performance of the architectures. This would help readers better understand the practical implications of this integration and its potential benefits.",
                "The paper provides a comparison of the proposed model with several existing models, using various metrics and datasets. However, it would be beneficial to provide more context on these comparisons. Specifically, it would be helpful to include the specific metrics and datasets used for comparison with the models mentioned in the 'Related Work' section. Additionally, the exact metrics used for comparison in the protein coarse-grain backmapping task should be specified. Providing this context would help readers understand the significance of the comparisons and the value of the proposed model.",
                "The paper suggests potential methods to avoid polynomial scaling, such as reducing the set of products according to the edges of a specified graph, the Voronoi diagram of the point cloud, or by randomly sampling tuples of points. However, it lacks a detailed discussion on the implications of these methods and why they were not explored in the current study. Elaborating on these aspects could provide readers with a better understanding of the potential improvements and future directions of this research. Therefore, it is recommended that the authors include this information in the paper.",
                "While the paper does mention some limitations and potential areas for future research, these are scattered throughout the text and not consolidated in a dedicated section. It would be beneficial to have a more detailed and focused discussion on the limitations of the study, such as the need to avoid polynomial scaling, the simplification of the architecture for systems at nonzero temperature, and the use of the training set error to characterize model performance due to the resolution of the structural refinement algorithms. Additionally, potential areas for future research, such as the reduction of the set of products according to the edges of a specified graph, the Voronoi diagram of the point cloud, or by randomly sampling tuples of points, and the exploration of the attention mechanism that accounts for both geometric and node-level signals, could be elaborated on.",
                "The paper mentions a learning rate adjustment strategy in the 'MODEL TRAINING' section, but it does not provide specific details on the initial learning rate, the batch size, or any regularization techniques used in the training process. These details are crucial for the reproducibility of the experiments, as they directly impact the training process and the results obtained. Without these details, it would be difficult for other researchers to replicate the experiments and verify the results. Therefore, the authors should include these details in the paper to enhance its reproducibility and reliability.",
                "The paper would benefit from a more explicit explanation of the selection criteria for the 19 protein structures used in the coarse-graining task. While it is mentioned that these structures have high-resolution structural refinements and were published between 2015 and 2020, additional details would enhance the reproducibility of the study and strengthen the validity of the results. For example, were the proteins selected based on their size, complexity, or relevance to a particular biological process? Were there any specific characteristics of the proteins that made them suitable for the coarse-graining task? This information would help readers understand the scope and applicability of your method.",
                "The paper mentions potential extensions for reducing the set of products according to the edges of a specified graph, the Voronoi diagram of the point cloud, or by randomly sampling tuples of points. However, it lacks sufficient details about these methods. Providing more information about why these methods were chosen, how they could be implemented, and what impact they could have on the results would enhance the understanding of the paper's methods and its potential for future work.",
                "The paper provides a detailed discussion of 'invariant' and 'covariant' attributes in the context of geometric deep learning, including the use of rotation-and permutation-equivariant architectures and geometric algebra. However, it would be helpful to include specific examples of these attributes in the proposed architecture or the experiments. This would provide readers with a concrete understanding of these concepts and how they are applied in the model. For instance, the authors could illustrate how the model is invariant to rotations and permutations in the context of a specific task, or how the rotation-covariant layers in the model work with a specific input data."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 0,
                    "predicted_id": 2,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The details of the four functions that make up the attention mechanism are vague. More elaboration on the technical details, such as what are rotation-invariant geometric quantities for tuples, is needed to improve readability.",
                    "predicted_comment": "The method used to incorporate local geometry information into the coarse-grained bead representations is described in the paper using geometric algebra and an attention mechanism. However, the description could be made clearer by providing more details about how these techniques are applied in the context of protein coarse-grain backmapping. For example, it would be helpful to explain how the geometric algebra provides a mathematical structure to deal with geometric objects and how the attention mechanism operates on each tuple of points. Additionally, while the paper mentions the use of geometric algebra attention networks, a type of geometric deep learning method, it would be beneficial to provide more information about how this method explicitly takes into account local geometry."
                }
            ],
            "novelty_impact": [
                {
                    "reference_id": 2,
                    "predicted_id": 6,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper does not provide information on the efficiency of the proposed method, especially when compared with other works.",
                    "predicted_comment": "The paper provides a comparison of the proposed model with several existing models, using various metrics and datasets. However, it would be beneficial to provide more context on these comparisons. Specifically, it would be helpful to include the specific metrics and datasets used for comparison with the models mentioned in the 'Related Work' section. Additionally, the exact metrics used for comparison in the protein coarse-grain backmapping task should be specified. Providing this context would help readers understand the significance of the comparisons and the value of the proposed model."
                }
            ],
            "clarity_reproducibility": [
                {
                    "reference_id": 0,
                    "predicted_id": 12,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The authors do not clearly describe how the geometric product helps to achieve the rotation-invariant attributes. This should be explained in the main manuscript since this is one of the main contributions.",
                    "predicted_comment": "The paper provides a detailed discussion of 'invariant' and 'covariant' attributes in the context of geometric deep learning, including the use of rotation-and permutation-equivariant architectures and geometric algebra. However, it would be helpful to include specific examples of these attributes in the proposed architecture or the experiments. This would provide readers with a concrete understanding of these concepts and how they are applied in the model. For instance, the authors could illustrate how the model is invariant to rotations and permutations in the context of a specific task, or how the rotation-covariant layers in the model work with a specific input data."
                }
            ],
            "all": [
                {
                    "reference_id": 2,
                    "predicted_id": 6,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper does not provide information on the efficiency of the proposed method, especially when compared with other works.",
                    "predicted_comment": "The paper provides a comparison of the proposed model with several existing models, using various metrics and datasets. However, it would be beneficial to provide more context on these comparisons. Specifically, it would be helpful to include the specific metrics and datasets used for comparison with the models mentioned in the 'Related Work' section. Additionally, the exact metrics used for comparison in the protein coarse-grain backmapping task should be specified. Providing this context would help readers understand the significance of the comparisons and the value of the proposed model."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 2,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The details of the four functions that make up the attention mechanism are vague. More elaboration on the technical details, such as what are rotation-invariant geometric quantities for tuples, is needed to improve readability.",
                    "predicted_comment": "The method used to incorporate local geometry information into the coarse-grained bead representations is described in the paper using geometric algebra and an attention mechanism. However, the description could be made clearer by providing more details about how these techniques are applied in the context of protein coarse-grain backmapping. For example, it would be helpful to explain how the geometric algebra provides a mathematical structure to deal with geometric objects and how the attention mechanism operates on each tuple of points. Additionally, while the paper mentions the use of geometric algebra attention networks, a type of geometric deep learning method, it would be beneficial to provide more information about how this method explicitly takes into account local geometry."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 12,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The authors do not clearly describe how the geometric product helps to achieve the rotation-invariant attributes. This should be explained in the main manuscript since this is one of the main contributions.",
                    "predicted_comment": "The paper provides a detailed discussion of 'invariant' and 'covariant' attributes in the context of geometric deep learning, including the use of rotation-and permutation-equivariant architectures and geometric algebra. However, it would be helpful to include specific examples of these attributes in the proposed architecture or the experiments. This would provide readers with a concrete understanding of these concepts and how they are applied in the model. For instance, the authors could illustrate how the model is invariant to rotations and permutations in the context of a specific task, or how the rotation-covariant layers in the model work with a specific input data."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 15,
                "n_generated": 3,
                "n_total_aligns": 1,
                "recall": 0.05,
                "precision": 0.08333333333333333,
                "pseudo_jaccard": 0.03571428571428571,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 15,
                "n_generated": 6,
                "n_total_aligns": 1,
                "recall": 0.05,
                "precision": 0.041666666666666664,
                "pseudo_jaccard": 0.025,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 15,
                "n_generated": 4,
                "n_total_aligns": 1,
                "recall": 0.041666666666666664,
                "precision": 0.0625,
                "pseudo_jaccard": 0.027777777777777776,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "all": {
                "n_reference": 15,
                "n_generated": 13,
                "n_total_aligns": 3,
                "recall": 0.14166666666666666,
                "precision": 0.057692307692307696,
                "pseudo_jaccard": 0.04513888888888889,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.2,
                    "precision": 0.3333333333333333,
                    "pseudo_jaccard": 0.14285714285714285,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.2,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.1,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.16666666666666666,
                    "precision": 0.25,
                    "pseudo_jaccard": 0.1111111111111111,
                    "n_real": 6
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.4,
                    "precision": 0.15384615384615385,
                    "pseudo_jaccard": 0.125,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.16666666666666666,
                    "precision": 0.07692307692307693,
                    "pseudo_jaccard": 0.05555555555555555,
                    "n_real": 6
                }
            ]
        }
    },
    {
        "doc_id": "cVak2hs06z",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The motivation for CnC is centered on the class-conditional alignment loss. There is even a bound on the worst-group loss in terms of the average-group and class-conditional alignment loss. So why not replace step 2, and instead fine-tune the model using L_avg_group + L_alignment? Or even just train models from scratch with this loss. It would be good to compare to these. If CnC is simply more empirically successful than this alternative, then it would be good to see this.",
                "In a related vein to the previous question, how much is CnC buying us as compared to the usual supervised contrastive training? It would be good to see SupCon as a baseline in Table 1. This seems an important baseline, since the main idea of CnC is to pull items from the same class together in feature space, which is also done using SupCon.  The main (even only?) difference is the hard positive/negative sampling approach of CnC."
            ],
            [
                "The observation of aligning representation for improving model robustness is not novel. There are a lot of work with similar ideas in domain generalization/adaptation literature, e.g., [1], [2]. There\u2019s also a recent work [3] that applies contrastive learning for doing so. A more comprehensive discussion for these related work needs to be included.",
                "The assumption of Theorem 3.1 is not well explained and motivated. In particular, the assumption that \u201cthe loss function l(x; y) is 1-Lipschitz in x and bounded from above by one.\u201d seems to be necessary and simplify the proof a lot, but does not hold for typical losses like cross-entropy for classification and MSE for regression.",
                "Using ERM prediction as the group label is not convincing enough, and it is not clear how it would affect the contrastive part. It could be interesting to more extensively analyze how the label prediction affects the improvement given by the contrastive method, probably using a scientific setup where the label prediction is controlled.",
                "For the contrastive part, the current empirical comparison obfuscates the advantage on its own. To decouple it from the effect of wrong group prediction, it is important to compare in the setting where group labels are available, i.e., GDRO vs GDRO + contrastive. Also, there could be a lot of choices of negative selections but only one is used without sufficient explanations, it would be great to include more explanation or compare with some other possible choices as an ablation study.",
                "Though the proposed contrastive method leads to improved worst-group performance, it seems to decrease the average-case performance compared to baselines."
            ],
            [
                "The proof provided in the paper may not be intuitively useful, particularly due to the bound B on the weight matrix. The weight matrix and the representations are learned jointly, and equivalent predictions could be achieved by scaling the weight matrix down and the representations up. Additionally, the Lipschitz and boundedness constraints on the loss functions do not seem to apply in any of the settings explored experimentally.",
                "Section 5.2 is unclear, particularly regarding the role of ERM 'predicting the sensitive attribute'. It is not clear how ERM's predictions of the sensitive attribute play into the CNC algorithm.",
                "Some important training details, such as the clustering-based prediction from the first step ERM model, are buried in the appendix. These details seem crucial to the functioning of the method and should be discussed in the main body, possibly with an ablation study."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "Comparison with Existing Methods: The paper compares the proposed method with several state-of-the-art methods, which is commendable. However, it would be beneficial to provide explicit details about the conditions under which these methods were implemented. This would ensure that the comparison is fair and that the results are reproducible. For instance, details about the model architectures, training hyperparameters, and any specific implementation details unique to each method would be useful.",
                "Your ablation study provides valuable insights into the robustness of the Correct-N-Contrast (CNC) method to the quality of the Stage 1 ERM model's predictions. However, to better understand the individual and combined contributions of each stage, consider testing all possible combinations of the two stages. For example, you could test the performance of the method when only the first stage is used, when only the second stage is used, and when both stages are used together. This would provide a clearer picture of how each stage contributes to the overall performance of the method and could help identify areas for further improvement.",
                "The paper provides details on how the representations were visualized or quantified and discusses how the choice of method could influence the results. However, it would be helpful to provide more details on the specific visualization or quantification methods used, such as the UMAP dimensionality reduction, clustering with K-means or GMM, and GradCAM visualizations. Specifically, it would be beneficial to discuss how these methods were chosen, their limitations, and how they could potentially influence the results. This additional detail would provide a more comprehensive understanding of the methods used and their potential impact on the results.",
                "The theoretical analysis section of the paper presents a complex set of results, including Theorem 3.1 and various methods for dealing with spurious correlations. However, the proof of Theorem 3.1 is deferred to another section, which could potentially make it difficult for readers to follow the logic and verify the results. Therefore, it would be beneficial to have these results independently verified. Independent verification would not only strengthen the validity of these results but also make it easier for readers to understand and trust the results. Consider providing more details about the process of independent verification, such as who would perform it and how it would be done, to further enhance the credibility of your theoretical analysis.",
                "Robustness Test: The paper provides some information about how spurious correlations were introduced into the datasets used for the robustness test. However, it would be beneficial to provide more specific details about the methods used to introduce these correlations. For example, in the Colored MNIST dataset, it is mentioned that a certain percentage of each class's datapoints are colored with an associated color, and the rest are colored randomly. However, the specific method used to determine which datapoints are colored randomly is not clear. Similarly, for the other datasets, more specific details about the methods used to introduce the spurious correlations would be helpful. The method of introduction could potentially influence the results, and providing these details would allow other researchers to replicate your experiments more accurately."
            ],
            "novelty_impact": [
                "The paper does an excellent job of identifying the problem of spurious correlations in machine learning models and proposing a novel method, Correct-N-Contrast (CNC), to address this issue. However, the explanation of the CNC method could be more detailed. Specifically, it would be beneficial to elaborate on the two-stage procedure of the CNC method. In the first stage, an Empirical Risk Minimization (ERM) model is trained, which helps infer group information, i.e., spurious attributes. In the second stage, the aim is to improve representation alignment by 'pulling together' same-class datapoints and 'pushing apart' different-class datapoints, regardless of their individual groups or spurious features. This is achieved via supervised contrastive learning, using the heuristic that samples with the same ERM predictions exhibit similar spurious features (and vice versa). With a randomly sampled anchor, samples with the same class but different ERM predictions are selected as 'positives' to be pulled together, and samples from different classes but the same ERM prediction as hard 'negatives' to be pushed apart. Training a second model with this sampling scheme and supervised contrastive learning encourages this model to ignore spurious correlations that the initial ERM model learned, and improves representation alignment between same-class data points. Providing these details in the paper would make the CNC method clearer to the reader.",
                "The authors have provided some details about the CNC method and the ERM model and how they are used to differentiate between groups. However, it would be beneficial if the authors could provide a more detailed explanation of how the ERM model's predictions are used to infer group information and how the CNC method uses these predictions to improve representation alignment. Specifically, it would be helpful to understand how the CNC method 'pulls together' same-class datapoints and 'pushes apart' different-class datapoints, regardless of their individual groups or spurious features. This would provide a clearer understanding of the CNC method and its use in the paper.",
                "The paper introduces the use of a form of two-sided contrastive sampling and additional intrinsic hard positive/negative mining techniques in the CNC method. However, these techniques are not explicitly labeled or explained in detail. For instance, the paper could provide more clarity on how the roles of the anchor and positive datapoints are switched in the two-sided contrastive sampling and how this contributes to the contrastive learning signal. Additionally, the paper could elaborate on how the hard positive/negative mining is implemented by sampling data points with the same class label but different ERM predictions as anchor and positive views, and how this approach allows the CNC method to exploit both 'hard' positive and negative criteria for the downstream classification task. Providing a more thorough explanation of these techniques and how they contribute to the effectiveness of the CNC method would strengthen the paper.",
                "The authors have provided some information about the benchmarks used for comparison and the performance of the CNC method. However, the paper could benefit from more detailed comparison figures or performance metrics. For example, it would be helpful to include specific performance metrics for each benchmark and a direct comparison of these metrics for the CNC method and other methods such as ERM, DRO, IRM, and GDRO. This would give readers a more comprehensive understanding of the performance of the CNC method and its advantages over existing methods.",
                "The authors have discussed some limitations of their theoretical analysis, such as the requirement of knowing the group labels to compute the alignment loss and the lack of analysis for the joint cross-entropy and contrastive optimization and the reason for choosing the cluster-based prediction approach. However, these limitations are not clearly outlined and could be elaborated on further. In terms of future work, the authors propose to provide a better theoretical understanding of the alignment induced by CNC in the context of spurious correlations and to further analyze the joint cross-entropy and contrastive optimization and the reason for choosing the cluster-based prediction approach. However, the paper does not provide specific details about what this future work might entail or how it would address the limitations of the current study. Providing more details on these points would be beneficial. For example, the authors could explain how knowing the group labels impacts the computation of the alignment loss and how this limitation could be addressed. They could also provide more details about the joint cross-entropy and contrastive optimization and why the cluster-based prediction approach was chosen. For the future work, the authors could outline potential directions for improving the theoretical understanding of the alignment induced by CNC in the context of spurious correlations and for further analyzing the joint cross-entropy and contrastive optimization and the reason for choosing the cluster-based prediction approach. They could also explain how this future work would address the limitations of the current study.",
                "The paper provides a comparison of the proposed CNC method with several other methods including ERM, GDRO, CVaR DRO, GEORGE, LfF, EIIL, CIM, and JTT. However, the comparison lacks depth in terms of explaining how each method works and why CNC outperforms them. For instance, while the paper mentions that CNC is more robust to noisy ERM predictions than JTT, it does not provide a detailed explanation of why this is the case. Similarly, the paper states that CNC does not require perfectly inferred groups to perform well, but it does not elaborate on how this is achieved. Providing a more detailed comparison, including the strengths and weaknesses of each method and the reasons behind CNC's superior performance, would greatly enhance the paper's value to readers who are not already familiar with these methods.",
                "The authors discuss the similarity of their work to Invariant Risk Minimization (IRM) and other related works in domain-invariant learning in section C.3 titled 'LEARNING INVARIANT REPRESENTATIONS'. However, the specific similarities and differences are not clearly outlined in all sections of the paper. It would be beneficial for the authors to provide a more detailed comparison of these methods in all relevant sections. This could involve discussing how their method, Correct-N-Contrast (CNC), improves upon existing methods, its unique contributions, and how it handles environment labels differently from traditional methods like IRM. This comparison is crucial for understanding the authors' work and its significance in the field of domain-invariant learning.",
                "While the paper provides a detailed discussion of the experimental results and the specific improvements achieved by the CNC method over prior state-of-the-art methods, it would be beneficial to include a more explicit discussion on the choice of benchmarks. Specifically, the authors should explain why these particular benchmarks were chosen and how they provide a comprehensive evaluation of the CNC method's performance and robustness against different types of spurious correlations."
            ],
            "clarity_reproducibility": [
                "The paper provides some details about the hyperparameters for the Empirical Risk Minimization (ERM) model, but there are some key details missing that would enhance the completeness and reproducibility of the paper. Specifically, the paper does not provide the specific values for the hyperparameters such as batch size, learning rate, momentum, optimizer, default weight decay, and number of epochs. It also does not explain what each hyperparameter does or why specific values were chosen, and it does not discuss how these hyperparameters might be adjusted for different datasets or tasks. While this is a minor issue that could be easily addressed in a revision, providing these details would enhance the completeness and reproducibility of the paper.",
                "The paper provides some information about how the datasets were split into training, validation, and test sets, but it lacks specific details about how the datasets were preprocessed. This information is essential for reproducing the experiments and comparing results. Please provide specific details about the preprocessing steps applied to each dataset used in the experiments.",
                "While the paper does compare the CNC method with standard supervised contrastive learning in the context of an ablation study, a more direct and comprehensive comparison would be beneficial. Specifically, it would be helpful to see a comparison that focuses on the unique design choices and properties of the CNC method, such as its use of a two-stage procedure, its avoidance of a projection network, and its use of two-sided contrastive sampling. This would provide a clearer understanding of the unique contributions of the CNC method and its advantages over standard supervised contrastive learning.",
                "The paper includes a reproducibility statement and provides details about the contrastive algorithm design used in the CNC method. However, it would be helpful if the authors could further clarify the reproducibility statement by providing more practical examples or step-by-step instructions.",
                "While the contrastive algorithm design is explained, it might be beneficial to include a more detailed discussion about the choice of design elements and their impact on the results."
            ],
            "all": [
                "Comparison with Existing Methods: The paper compares the proposed method with several state-of-the-art methods, which is commendable. However, it would be beneficial to provide explicit details about the conditions under which these methods were implemented. This would ensure that the comparison is fair and that the results are reproducible. For instance, details about the model architectures, training hyperparameters, and any specific implementation details unique to each method would be useful.",
                "Your ablation study provides valuable insights into the robustness of the Correct-N-Contrast (CNC) method to the quality of the Stage 1 ERM model's predictions. However, to better understand the individual and combined contributions of each stage, consider testing all possible combinations of the two stages. For example, you could test the performance of the method when only the first stage is used, when only the second stage is used, and when both stages are used together. This would provide a clearer picture of how each stage contributes to the overall performance of the method and could help identify areas for further improvement.",
                "The paper provides details on how the representations were visualized or quantified and discusses how the choice of method could influence the results. However, it would be helpful to provide more details on the specific visualization or quantification methods used, such as the UMAP dimensionality reduction, clustering with K-means or GMM, and GradCAM visualizations. Specifically, it would be beneficial to discuss how these methods were chosen, their limitations, and how they could potentially influence the results. This additional detail would provide a more comprehensive understanding of the methods used and their potential impact on the results.",
                "The theoretical analysis section of the paper presents a complex set of results, including Theorem 3.1 and various methods for dealing with spurious correlations. However, the proof of Theorem 3.1 is deferred to another section, which could potentially make it difficult for readers to follow the logic and verify the results. Therefore, it would be beneficial to have these results independently verified. Independent verification would not only strengthen the validity of these results but also make it easier for readers to understand and trust the results. Consider providing more details about the process of independent verification, such as who would perform it and how it would be done, to further enhance the credibility of your theoretical analysis.",
                "Robustness Test: The paper provides some information about how spurious correlations were introduced into the datasets used for the robustness test. However, it would be beneficial to provide more specific details about the methods used to introduce these correlations. For example, in the Colored MNIST dataset, it is mentioned that a certain percentage of each class's datapoints are colored with an associated color, and the rest are colored randomly. However, the specific method used to determine which datapoints are colored randomly is not clear. Similarly, for the other datasets, more specific details about the methods used to introduce the spurious correlations would be helpful. The method of introduction could potentially influence the results, and providing these details would allow other researchers to replicate your experiments more accurately.",
                "The paper does an excellent job of identifying the problem of spurious correlations in machine learning models and proposing a novel method, Correct-N-Contrast (CNC), to address this issue. However, the explanation of the CNC method could be more detailed. Specifically, it would be beneficial to elaborate on the two-stage procedure of the CNC method. In the first stage, an Empirical Risk Minimization (ERM) model is trained, which helps infer group information, i.e., spurious attributes. In the second stage, the aim is to improve representation alignment by 'pulling together' same-class datapoints and 'pushing apart' different-class datapoints, regardless of their individual groups or spurious features. This is achieved via supervised contrastive learning, using the heuristic that samples with the same ERM predictions exhibit similar spurious features (and vice versa). With a randomly sampled anchor, samples with the same class but different ERM predictions are selected as 'positives' to be pulled together, and samples from different classes but the same ERM prediction as hard 'negatives' to be pushed apart. Training a second model with this sampling scheme and supervised contrastive learning encourages this model to ignore spurious correlations that the initial ERM model learned, and improves representation alignment between same-class data points. Providing these details in the paper would make the CNC method clearer to the reader.",
                "The authors have provided some details about the CNC method and the ERM model and how they are used to differentiate between groups. However, it would be beneficial if the authors could provide a more detailed explanation of how the ERM model's predictions are used to infer group information and how the CNC method uses these predictions to improve representation alignment. Specifically, it would be helpful to understand how the CNC method 'pulls together' same-class datapoints and 'pushes apart' different-class datapoints, regardless of their individual groups or spurious features. This would provide a clearer understanding of the CNC method and its use in the paper.",
                "The paper introduces the use of a form of two-sided contrastive sampling and additional intrinsic hard positive/negative mining techniques in the CNC method. However, these techniques are not explicitly labeled or explained in detail. For instance, the paper could provide more clarity on how the roles of the anchor and positive datapoints are switched in the two-sided contrastive sampling and how this contributes to the contrastive learning signal. Additionally, the paper could elaborate on how the hard positive/negative mining is implemented by sampling data points with the same class label but different ERM predictions as anchor and positive views, and how this approach allows the CNC method to exploit both 'hard' positive and negative criteria for the downstream classification task. Providing a more thorough explanation of these techniques and how they contribute to the effectiveness of the CNC method would strengthen the paper.",
                "The authors have provided some information about the benchmarks used for comparison and the performance of the CNC method. However, the paper could benefit from more detailed comparison figures or performance metrics. For example, it would be helpful to include specific performance metrics for each benchmark and a direct comparison of these metrics for the CNC method and other methods such as ERM, DRO, IRM, and GDRO. This would give readers a more comprehensive understanding of the performance of the CNC method and its advantages over existing methods.",
                "The authors have discussed some limitations of their theoretical analysis, such as the requirement of knowing the group labels to compute the alignment loss and the lack of analysis for the joint cross-entropy and contrastive optimization and the reason for choosing the cluster-based prediction approach. However, these limitations are not clearly outlined and could be elaborated on further. In terms of future work, the authors propose to provide a better theoretical understanding of the alignment induced by CNC in the context of spurious correlations and to further analyze the joint cross-entropy and contrastive optimization and the reason for choosing the cluster-based prediction approach. However, the paper does not provide specific details about what this future work might entail or how it would address the limitations of the current study. Providing more details on these points would be beneficial. For example, the authors could explain how knowing the group labels impacts the computation of the alignment loss and how this limitation could be addressed. They could also provide more details about the joint cross-entropy and contrastive optimization and why the cluster-based prediction approach was chosen. For the future work, the authors could outline potential directions for improving the theoretical understanding of the alignment induced by CNC in the context of spurious correlations and for further analyzing the joint cross-entropy and contrastive optimization and the reason for choosing the cluster-based prediction approach. They could also explain how this future work would address the limitations of the current study.",
                "The paper provides a comparison of the proposed CNC method with several other methods including ERM, GDRO, CVaR DRO, GEORGE, LfF, EIIL, CIM, and JTT. However, the comparison lacks depth in terms of explaining how each method works and why CNC outperforms them. For instance, while the paper mentions that CNC is more robust to noisy ERM predictions than JTT, it does not provide a detailed explanation of why this is the case. Similarly, the paper states that CNC does not require perfectly inferred groups to perform well, but it does not elaborate on how this is achieved. Providing a more detailed comparison, including the strengths and weaknesses of each method and the reasons behind CNC's superior performance, would greatly enhance the paper's value to readers who are not already familiar with these methods.",
                "The authors discuss the similarity of their work to Invariant Risk Minimization (IRM) and other related works in domain-invariant learning in section C.3 titled 'LEARNING INVARIANT REPRESENTATIONS'. However, the specific similarities and differences are not clearly outlined in all sections of the paper. It would be beneficial for the authors to provide a more detailed comparison of these methods in all relevant sections. This could involve discussing how their method, Correct-N-Contrast (CNC), improves upon existing methods, its unique contributions, and how it handles environment labels differently from traditional methods like IRM. This comparison is crucial for understanding the authors' work and its significance in the field of domain-invariant learning.",
                "While the paper provides a detailed discussion of the experimental results and the specific improvements achieved by the CNC method over prior state-of-the-art methods, it would be beneficial to include a more explicit discussion on the choice of benchmarks. Specifically, the authors should explain why these particular benchmarks were chosen and how they provide a comprehensive evaluation of the CNC method's performance and robustness against different types of spurious correlations.",
                "The paper provides some details about the hyperparameters for the Empirical Risk Minimization (ERM) model, but there are some key details missing that would enhance the completeness and reproducibility of the paper. Specifically, the paper does not provide the specific values for the hyperparameters such as batch size, learning rate, momentum, optimizer, default weight decay, and number of epochs. It also does not explain what each hyperparameter does or why specific values were chosen, and it does not discuss how these hyperparameters might be adjusted for different datasets or tasks. While this is a minor issue that could be easily addressed in a revision, providing these details would enhance the completeness and reproducibility of the paper.",
                "The paper provides some information about how the datasets were split into training, validation, and test sets, but it lacks specific details about how the datasets were preprocessed. This information is essential for reproducing the experiments and comparing results. Please provide specific details about the preprocessing steps applied to each dataset used in the experiments.",
                "While the paper does compare the CNC method with standard supervised contrastive learning in the context of an ablation study, a more direct and comprehensive comparison would be beneficial. Specifically, it would be helpful to see a comparison that focuses on the unique design choices and properties of the CNC method, such as its use of a two-stage procedure, its avoidance of a projection network, and its use of two-sided contrastive sampling. This would provide a clearer understanding of the unique contributions of the CNC method and its advantages over standard supervised contrastive learning.",
                "The paper includes a reproducibility statement and provides details about the contrastive algorithm design used in the CNC method. However, it would be helpful if the authors could further clarify the reproducibility statement by providing more practical examples or step-by-step instructions.",
                "While the contrastive algorithm design is explained, it might be beneficial to include a more detailed discussion about the choice of design elements and their impact on the results."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [
                {
                    "reference_id": 0,
                    "predicted_id": 11,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The observation of aligning representation for improving model robustness is not novel. There are a lot of work with similar ideas in domain generalization/adaptation literature, e.g., [1], [2]. There\u2019s also a recent work [3] that applies contrastive learning for doing so. A more comprehensive discussion for these related work needs to be included.",
                    "predicted_comment": "The authors discuss the similarity of their work to Invariant Risk Minimization (IRM) and other related works in domain-invariant learning in section C.3 titled 'LEARNING INVARIANT REPRESENTATIONS'. However, the specific similarities and differences are not clearly outlined in all sections of the paper. It would be beneficial for the authors to provide a more detailed comparison of these methods in all relevant sections. This could involve discussing how their method, Correct-N-Contrast (CNC), improves upon existing methods, its unique contributions, and how it handles environment labels differently from traditional methods like IRM. This comparison is crucial for understanding the authors' work and its significance in the field of domain-invariant learning."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 5,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "Section 5.2 is unclear, particularly regarding the role of ERM 'predicting the sensitive attribute'. It is not clear how ERM's predictions of the sensitive attribute play into the CNC algorithm.",
                    "predicted_comment": "The paper does an excellent job of identifying the problem of spurious correlations in machine learning models and proposing a novel method, Correct-N-Contrast (CNC), to address this issue. However, the explanation of the CNC method could be more detailed. Specifically, it would be beneficial to elaborate on the two-stage procedure of the CNC method. In the first stage, an Empirical Risk Minimization (ERM) model is trained, which helps infer group information, i.e., spurious attributes. In the second stage, the aim is to improve representation alignment by 'pulling together' same-class datapoints and 'pushing apart' different-class datapoints, regardless of their individual groups or spurious features. This is achieved via supervised contrastive learning, using the heuristic that samples with the same ERM predictions exhibit similar spurious features (and vice versa). With a randomly sampled anchor, samples with the same class but different ERM predictions are selected as 'positives' to be pulled together, and samples from different classes but the same ERM prediction as hard 'negatives' to be pushed apart. Training a second model with this sampling scheme and supervised contrastive learning encourages this model to ignore spurious correlations that the initial ERM model learned, and improves representation alignment between same-class data points. Providing these details in the paper would make the CNC method clearer to the reader."
                }
            ],
            "clarity_reproducibility": [
                {
                    "reference_id": 1,
                    "predicted_id": 15,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "In a related vein to the previous question, how much is CnC buying us as compared to the usual supervised contrastive training? It would be good to see SupCon as a baseline in Table 1. This seems an important baseline, since the main idea of CnC is to pull items from the same class together in feature space, which is also done using SupCon.  The main (even only?) difference is the hard positive/negative sampling approach of CnC.",
                    "predicted_comment": "While the paper does compare the CNC method with standard supervised contrastive learning in the context of an ablation study, a more direct and comprehensive comparison would be beneficial. Specifically, it would be helpful to see a comparison that focuses on the unique design choices and properties of the CNC method, such as its use of a two-stage procedure, its avoidance of a projection network, and its use of two-sided contrastive sampling. This would provide a clearer understanding of the unique contributions of the CNC method and its advantages over standard supervised contrastive learning."
                }
            ],
            "all": [
                {
                    "reference_id": 1,
                    "predicted_id": 15,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "In a related vein to the previous question, how much is CnC buying us as compared to the usual supervised contrastive training? It would be good to see SupCon as a baseline in Table 1. This seems an important baseline, since the main idea of CnC is to pull items from the same class together in feature space, which is also done using SupCon.  The main (even only?) difference is the hard positive/negative sampling approach of CnC.",
                    "predicted_comment": "While the paper does compare the CNC method with standard supervised contrastive learning in the context of an ablation study, a more direct and comprehensive comparison would be beneficial. Specifically, it would be helpful to see a comparison that focuses on the unique design choices and properties of the CNC method, such as its use of a two-stage procedure, its avoidance of a projection network, and its use of two-sided contrastive sampling. This would provide a clearer understanding of the unique contributions of the CNC method and its advantages over standard supervised contrastive learning."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 11,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The observation of aligning representation for improving model robustness is not novel. There are a lot of work with similar ideas in domain generalization/adaptation literature, e.g., [1], [2]. There\u2019s also a recent work [3] that applies contrastive learning for doing so. A more comprehensive discussion for these related work needs to be included.",
                    "predicted_comment": "The authors discuss the similarity of their work to Invariant Risk Minimization (IRM) and other related works in domain-invariant learning in section C.3 titled 'LEARNING INVARIANT REPRESENTATIONS'. However, the specific similarities and differences are not clearly outlined in all sections of the paper. It would be beneficial for the authors to provide a more detailed comparison of these methods in all relevant sections. This could involve discussing how their method, Correct-N-Contrast (CNC), improves upon existing methods, its unique contributions, and how it handles environment labels differently from traditional methods like IRM. This comparison is crucial for understanding the authors' work and its significance in the field of domain-invariant learning."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 5,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "Section 5.2 is unclear, particularly regarding the role of ERM 'predicting the sensitive attribute'. It is not clear how ERM's predictions of the sensitive attribute play into the CNC algorithm.",
                    "predicted_comment": "The paper does an excellent job of identifying the problem of spurious correlations in machine learning models and proposing a novel method, Correct-N-Contrast (CNC), to address this issue. However, the explanation of the CNC method could be more detailed. Specifically, it would be beneficial to elaborate on the two-stage procedure of the CNC method. In the first stage, an Empirical Risk Minimization (ERM) model is trained, which helps infer group information, i.e., spurious attributes. In the second stage, the aim is to improve representation alignment by 'pulling together' same-class datapoints and 'pushing apart' different-class datapoints, regardless of their individual groups or spurious features. This is achieved via supervised contrastive learning, using the heuristic that samples with the same ERM predictions exhibit similar spurious features (and vice versa). With a randomly sampled anchor, samples with the same class but different ERM predictions are selected as 'positives' to be pulled together, and samples from different classes but the same ERM prediction as hard 'negatives' to be pushed apart. Training a second model with this sampling scheme and supervised contrastive learning encourages this model to ignore spurious correlations that the initial ERM model learned, and improves representation alignment between same-class data points. Providing these details in the paper would make the CNC method clearer to the reader."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 10,
                "n_generated": 5,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 10,
                "n_generated": 8,
                "n_total_aligns": 2,
                "recall": 0.17777777777777778,
                "precision": 0.08333333333333333,
                "pseudo_jaccard": 0.061111111111111116,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 10,
                "n_generated": 5,
                "n_total_aligns": 1,
                "recall": 0.16666666666666666,
                "precision": 0.06666666666666667,
                "pseudo_jaccard": 0.05555555555555555,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "all": {
                "n_reference": 10,
                "n_generated": 18,
                "n_total_aligns": 3,
                "recall": 0.3444444444444444,
                "precision": 0.05555555555555555,
                "pseudo_jaccard": 0.04936204146730463,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.2,
                    "precision": 0.125,
                    "pseudo_jaccard": 0.08333333333333333,
                    "n_real": 5
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.125,
                    "pseudo_jaccard": 0.1,
                    "n_real": 3
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.5,
                    "precision": 0.2,
                    "pseudo_jaccard": 0.16666666666666666,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "all": [
                {
                    "recall": 0.5,
                    "precision": 0.05555555555555555,
                    "pseudo_jaccard": 0.05263157894736842,
                    "n_real": 2
                },
                {
                    "recall": 0.2,
                    "precision": 0.05555555555555555,
                    "pseudo_jaccard": 0.045454545454545456,
                    "n_real": 5
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.05555555555555555,
                    "pseudo_jaccard": 0.05,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "2234Pp-9ikZ",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper lacks conceptual innovation. While the results are solid, there is little insight about the behavior of the method. No ablations are performed beyond the comparison with NAGO, making it hard to assess the importance of the individual components.",
                "The paper does not include results on ImageNet.",
                "The results are compared with models that use other teacher architectures, making it hard to determine if the improvements are due to an improved teacher or a better knowledge distillation method.",
                "The claim that a standard neural architecture search would produce architectures sampled from the same distributions and therefore not be efficient in exploring the space of students is insufficiently explored.",
                "The paper does not provide comparisons with other architecture search algorithms for knowledge distillation or surrogate functions, therefore the introduction of a new framework may not be warranted under the claims of the authors."
            ],
            [
                "The application of knowledge distillation (KD) into network architecture generator optimization (NAGO) appears to be naively done, suggesting that the proposed method is incremental and the contribution is limited.",
                "The differences between the proposed method and existing methods are not clearly highlighted.",
                "The comparison of results in Table 1 is not fair. The KD used in this paper used better teachers following the convention, but the competitor KD-LSR and SKD in the table are self-distillation methods, making the comparison meaningless.",
                "On the CIFAR100 dataset, the paper inaccurately reports that CRD used ShuffleNetV1, which has more parameters, while the original paper used WRN-40-2 as a teacher and trained the student of WRN-16-2.",
                "On the MIT67 dataset, AutoKD used the fine-tuned teacher, which is much beneficial to KD in terms of performance, while VID used an ImageNet-pretrained model for transfer learning.",
                "On the CIFAR10 dataset, the compared models have fewer parameters than that of NAGO for AutoKD, making it hard to say that AutoKD outperforms them.",
                "The experimental results are unconvincing. For example, as weight is zero in Figure 2, the accuracies in the table should show consistent performance but are deviated with respect to temperature.",
                "The accuracies of NAGO in Figure 4 appear low compared to the other results in the paper, which needs clarification.",
                "The use of KD on NAS introduces additional computational cost, but this is not clearly compared quantitatively."
            ],
            [
                "The paper's main difference from existing KD-NAS approaches is the search for a family of architectures, which mainly takes advantage of the generator in NAGO. This reduces the novelty and contribution of the work.",
                "The purpose of some experiments is unclear. It would be more beneficial to compare AutoKD with earlier KD-NAS approaches instead of NAGO to show the impact of KD on NAS.",
                "The paper does not clearly articulate how the family of student architectures can benefit knowledge distillation. It is suggested that the performance gains may result from the ensemble of networks.",
                "The comparisons shown in Figure 7 do not seem to make much sense.",
                "The logic and results shown in the visualization of Figure 6 are not clear."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper would be significantly strengthened by the inclusion of ablation studies. Specifically, testing the performance of AutoKD without the NAS component would help to isolate the impact of this component on the overall performance. Additionally, comparing the performance of AutoKD with different KD methods, beyond the traditional KD loss function currently used, would provide valuable insights into the robustness of AutoKD and its performance relative to other methods. These ablation studies would provide a clearer understanding of the individual contributions of different components of the proposed method, and could help to substantiate the paper's claims.",
                "Statistical Analysis: The paper compares the performance of the proposed method, AutoKD, with other methods in terms of correlation, memory usage, parameter count, and accuracy. However, it lacks statistical tests or p-values to quantify the significance of the observed differences. Including a statistical analysis would provide a more rigorous evaluation of the results and would give readers more confidence in the results. Specifically, consider performing a t-test or ANOVA to determine if the differences in performance are statistically significant.",
                "The paper discusses the importance of hyperparameters in the AutoKD framework and includes knowledge distillation hyperparameters, temperature and weight, in the search space. However, a detailed sensitivity analysis of how changes in these hyperparameters affect the performance of AutoKD is missing. Conducting such an analysis would provide valuable insights into the robustness of the method to changes in its parameters. Specifically, it would be beneficial to see how varying the temperature from 1 to 10 and the weight from 0 to 1 impacts the performance of AutoKD. This could strengthen the paper's claims by providing empirical evidence to support the importance of these hyperparameters."
            ],
            "novelty_impact": [
                "The paper provides some information about the computational cost of AutoKD, such as its sample efficiency and the number of models trained during the search procedure. However, it lacks specific details about memory usage, power consumption, and hardware requirements. These details are crucial for understanding the practicality of AutoKD, especially for deployment on small devices. Including this information would provide a clearer picture of the resources required to implement and run AutoKD, thereby enhancing the paper's value to readers interested in practical applications.",
                "The paper presents a strong focus on vision tasks, which leaves a gap in understanding how AutoKD performs with other types of data. To strengthen the claims about the effectiveness and versatility of AutoKD, it would be beneficial to see experiments conducted on a variety of datasets, including text and audio. This would provide evidence of the applicability of AutoKD across different types of data, not just vision tasks. However, the absence of such experiments does not necessarily invalidate the paper's current findings and conclusions, which are based on the datasets they have used.",
                "The paper mentions the use of traditional KD loss in AutoKD and how it outperforms more advanced KD variants using hand-designed students. However, the paper lacks an explicit comparison of AutoKD's performance when using different types of loss functions. This comparison could provide valuable insights into the effectiveness of different loss functions in the context of AutoKD and could strengthen the paper's argument about the superiority of traditional KD loss. Therefore, it is recommended that the authors include a comparison of AutoKD's performance when using different types of loss functions.",
                "The paper presents AutoKD, an innovative approach to knowledge distillation that significantly reduces model size while maintaining performance. This is a valuable contribution, particularly for deployment on devices with hardware limitations. However, the paper does not explicitly discuss the impact of this reduction in model size on the interpretability of the models. Given that interpretability is a key factor in the practical application and user trust of machine learning models, it would be beneficial for the authors to address this aspect. Specifically, how does the reduction in model size, achieved through focusing on the macro-structure of the network, impact the interpretability of the models? This discussion could further strengthen the paper by addressing potential limitations and providing a more comprehensive understanding of AutoKD's practical implications.",
                "The paper mentions the use of a multi-fidelity evaluation scheme and provides correlation values between different epochs. However, it would be beneficial to have more specific details on how this multi-fidelity correlation is measured. For instance, what metrics or methods are used to calculate this correlation? Additionally, while the paper states that AutoKD enables better multi-fidelity correlation, it would be helpful to understand how this improved correlation directly impacts the overall performance of AutoKD. Does it lead to more accurate model selection, faster training times, or some other benefits?",
                "While the AutoKD method's ability to reach the performance of the teacher model with significantly fewer parameters is impressive, the paper could benefit from a more detailed discussion on how this reduction in parameters impacts the model's ability to capture complex patterns in the data. Specifically, it would be helpful to understand how the AutoKD method ensures that the richness of the model's representations is not compromised despite the reduction in parameters. This could include a discussion on the role of Knowledge Distillation (KD) and Neural Architecture Search (NAS) in maintaining the model's ability to capture complex patterns.",
                "The paper does not provide sufficient information on how the AutoKD algorithm prevents overfitting. Overfitting is a common issue in machine learning, and it's crucial for readers to understand how this is addressed in AutoKD. Specifically, it would be helpful to include details on any regularization techniques used, how the model complexity is controlled, or how the training data is split to validate the model's performance.",
                "While the paper provides a definition for an 'optimal' student architecture, it may be beneficial to provide a more concrete definition or set of criteria that can be consistently applied across different applications or datasets. This could include specific performance metrics or constraints that must be met for a student architecture to be considered 'optimal'. Providing a clearer definition would help readers better understand the methodology and results of the paper.",
                "The paper provides a comparison of AutoKD with other KD methods, which is appreciated. However, the comparison with other automated approaches lacks detail. Providing a more detailed comparison with other automated approaches would offer a clearer understanding of AutoKD's strengths and weaknesses relative to these methods. This would also help validate the claim that AutoKD is more sample efficient than other NAS-based KD solutions, and provide more context for the performance of AutoKD. Therefore, I recommend expanding on the comparison of AutoKD with other automated approaches in the field of automated knowledge distillation."
            ],
            "clarity_reproducibility": [
                "The paper should provide more specific details about how the surrogate model in the Bayesian Optimization and Neural Architecture Search methods used in AutoKD is updated based on the student performances. Specifically, it would be helpful to know what specific metrics or data are used to evaluate student performances and how this data is used to update the surrogate model. This information is crucial for readers to fully understand the methodology and reproduce the work.",
                "The paper could improve by explicitly labeling the definition of 'sample efficiency' and the explanation of how it is measured, perhaps in a dedicated subsection for clarity.",
                "Additionally, the paper claims that AutoKD is up to 20x more sample efficient than alternative NAS-based KD solutions, but it does not provide explicit details on how this comparison was made. It would be helpful to include a direct comparison or calculation to support this claim, which would provide more context and clarity to the readers.",
                "The paper lacks specific details about the hardware and software used in the experimental setup, such as the model and specifications of the hardware, and the version and configuration of the software. These details are crucial for other researchers to replicate the experiments under the same conditions and validate the results. Providing this information would significantly enhance the reproducibility of the study.",
                "The paper would benefit from a more detailed explanation of how the multi-fidelity correlation is measured and how the correlation coefficient is calculated. Specifically, it would be helpful to understand how the correlation between 30 and 120 epochs improves from 0.49 to 0.82 by using KD. Providing these details would enhance the clarity of the evaluation metrics and enable readers to better understand the results.",
                "The paper discusses the performance of the models in terms of their accuracy on various datasets, the number of parameters they use, the amount of memory they require, and their ability to emulate the performance of larger models. However, it lacks specific details on how these measurements are calculated or what they specifically entail. Providing these details would allow readers to better understand the benchmarks used in the paper and the significance of the results. For example, it would be helpful to know how the efficiency of the AutoKD method is measured and calculated, as this is a key claim of the paper.",
                "The paper provides a detailed description of the structure of the NAGO search space and how it is generated. However, it lacks explicit information on how the size of the networks in this search space is determined. It is also unclear if there is a way to adjust the size of these networks for different tasks or datasets. Providing this information would greatly enhance the understanding of the flexibility and adaptability of the proposed method. For instance, it would be helpful to know if the size of the networks is determined by the random graph generators and if these generators can be adjusted to produce networks of different sizes. It would also be beneficial to know if the method can handle tasks or datasets that require networks of a size different from the default 4M parameters.",
                "The paper would benefit from a more explicit discussion of the potential limitations or drawbacks of AutoKD. For instance, the paper suggests that future work should explore how to fully exploit the distribution of the optimal student family, implying that this is an area that could be improved. However, this is not clearly framed as a limitation of the current method. Additionally, the paper could discuss potential limitations related to the comparison with other methods, the efficiency of the search for the optimal student architecture, and the emulation of the performance of large models with smaller students. This would provide a more balanced view of the proposed method and help readers understand its potential challenges and areas for future improvement."
            ],
            "all": [
                "The paper would be significantly strengthened by the inclusion of ablation studies. Specifically, testing the performance of AutoKD without the NAS component would help to isolate the impact of this component on the overall performance. Additionally, comparing the performance of AutoKD with different KD methods, beyond the traditional KD loss function currently used, would provide valuable insights into the robustness of AutoKD and its performance relative to other methods. These ablation studies would provide a clearer understanding of the individual contributions of different components of the proposed method, and could help to substantiate the paper's claims.",
                "Statistical Analysis: The paper compares the performance of the proposed method, AutoKD, with other methods in terms of correlation, memory usage, parameter count, and accuracy. However, it lacks statistical tests or p-values to quantify the significance of the observed differences. Including a statistical analysis would provide a more rigorous evaluation of the results and would give readers more confidence in the results. Specifically, consider performing a t-test or ANOVA to determine if the differences in performance are statistically significant.",
                "The paper discusses the importance of hyperparameters in the AutoKD framework and includes knowledge distillation hyperparameters, temperature and weight, in the search space. However, a detailed sensitivity analysis of how changes in these hyperparameters affect the performance of AutoKD is missing. Conducting such an analysis would provide valuable insights into the robustness of the method to changes in its parameters. Specifically, it would be beneficial to see how varying the temperature from 1 to 10 and the weight from 0 to 1 impacts the performance of AutoKD. This could strengthen the paper's claims by providing empirical evidence to support the importance of these hyperparameters.",
                "The paper provides some information about the computational cost of AutoKD, such as its sample efficiency and the number of models trained during the search procedure. However, it lacks specific details about memory usage, power consumption, and hardware requirements. These details are crucial for understanding the practicality of AutoKD, especially for deployment on small devices. Including this information would provide a clearer picture of the resources required to implement and run AutoKD, thereby enhancing the paper's value to readers interested in practical applications.",
                "The paper presents a strong focus on vision tasks, which leaves a gap in understanding how AutoKD performs with other types of data. To strengthen the claims about the effectiveness and versatility of AutoKD, it would be beneficial to see experiments conducted on a variety of datasets, including text and audio. This would provide evidence of the applicability of AutoKD across different types of data, not just vision tasks. However, the absence of such experiments does not necessarily invalidate the paper's current findings and conclusions, which are based on the datasets they have used.",
                "The paper mentions the use of traditional KD loss in AutoKD and how it outperforms more advanced KD variants using hand-designed students. However, the paper lacks an explicit comparison of AutoKD's performance when using different types of loss functions. This comparison could provide valuable insights into the effectiveness of different loss functions in the context of AutoKD and could strengthen the paper's argument about the superiority of traditional KD loss. Therefore, it is recommended that the authors include a comparison of AutoKD's performance when using different types of loss functions.",
                "The paper presents AutoKD, an innovative approach to knowledge distillation that significantly reduces model size while maintaining performance. This is a valuable contribution, particularly for deployment on devices with hardware limitations. However, the paper does not explicitly discuss the impact of this reduction in model size on the interpretability of the models. Given that interpretability is a key factor in the practical application and user trust of machine learning models, it would be beneficial for the authors to address this aspect. Specifically, how does the reduction in model size, achieved through focusing on the macro-structure of the network, impact the interpretability of the models? This discussion could further strengthen the paper by addressing potential limitations and providing a more comprehensive understanding of AutoKD's practical implications.",
                "The paper mentions the use of a multi-fidelity evaluation scheme and provides correlation values between different epochs. However, it would be beneficial to have more specific details on how this multi-fidelity correlation is measured. For instance, what metrics or methods are used to calculate this correlation? Additionally, while the paper states that AutoKD enables better multi-fidelity correlation, it would be helpful to understand how this improved correlation directly impacts the overall performance of AutoKD. Does it lead to more accurate model selection, faster training times, or some other benefits?",
                "While the AutoKD method's ability to reach the performance of the teacher model with significantly fewer parameters is impressive, the paper could benefit from a more detailed discussion on how this reduction in parameters impacts the model's ability to capture complex patterns in the data. Specifically, it would be helpful to understand how the AutoKD method ensures that the richness of the model's representations is not compromised despite the reduction in parameters. This could include a discussion on the role of Knowledge Distillation (KD) and Neural Architecture Search (NAS) in maintaining the model's ability to capture complex patterns.",
                "The paper does not provide sufficient information on how the AutoKD algorithm prevents overfitting. Overfitting is a common issue in machine learning, and it's crucial for readers to understand how this is addressed in AutoKD. Specifically, it would be helpful to include details on any regularization techniques used, how the model complexity is controlled, or how the training data is split to validate the model's performance.",
                "While the paper provides a definition for an 'optimal' student architecture, it may be beneficial to provide a more concrete definition or set of criteria that can be consistently applied across different applications or datasets. This could include specific performance metrics or constraints that must be met for a student architecture to be considered 'optimal'. Providing a clearer definition would help readers better understand the methodology and results of the paper.",
                "The paper provides a comparison of AutoKD with other KD methods, which is appreciated. However, the comparison with other automated approaches lacks detail. Providing a more detailed comparison with other automated approaches would offer a clearer understanding of AutoKD's strengths and weaknesses relative to these methods. This would also help validate the claim that AutoKD is more sample efficient than other NAS-based KD solutions, and provide more context for the performance of AutoKD. Therefore, I recommend expanding on the comparison of AutoKD with other automated approaches in the field of automated knowledge distillation.",
                "The paper should provide more specific details about how the surrogate model in the Bayesian Optimization and Neural Architecture Search methods used in AutoKD is updated based on the student performances. Specifically, it would be helpful to know what specific metrics or data are used to evaluate student performances and how this data is used to update the surrogate model. This information is crucial for readers to fully understand the methodology and reproduce the work.",
                "The paper could improve by explicitly labeling the definition of 'sample efficiency' and the explanation of how it is measured, perhaps in a dedicated subsection for clarity.",
                "Additionally, the paper claims that AutoKD is up to 20x more sample efficient than alternative NAS-based KD solutions, but it does not provide explicit details on how this comparison was made. It would be helpful to include a direct comparison or calculation to support this claim, which would provide more context and clarity to the readers.",
                "The paper lacks specific details about the hardware and software used in the experimental setup, such as the model and specifications of the hardware, and the version and configuration of the software. These details are crucial for other researchers to replicate the experiments under the same conditions and validate the results. Providing this information would significantly enhance the reproducibility of the study.",
                "The paper would benefit from a more detailed explanation of how the multi-fidelity correlation is measured and how the correlation coefficient is calculated. Specifically, it would be helpful to understand how the correlation between 30 and 120 epochs improves from 0.49 to 0.82 by using KD. Providing these details would enhance the clarity of the evaluation metrics and enable readers to better understand the results.",
                "The paper discusses the performance of the models in terms of their accuracy on various datasets, the number of parameters they use, the amount of memory they require, and their ability to emulate the performance of larger models. However, it lacks specific details on how these measurements are calculated or what they specifically entail. Providing these details would allow readers to better understand the benchmarks used in the paper and the significance of the results. For example, it would be helpful to know how the efficiency of the AutoKD method is measured and calculated, as this is a key claim of the paper.",
                "The paper provides a detailed description of the structure of the NAGO search space and how it is generated. However, it lacks explicit information on how the size of the networks in this search space is determined. It is also unclear if there is a way to adjust the size of these networks for different tasks or datasets. Providing this information would greatly enhance the understanding of the flexibility and adaptability of the proposed method. For instance, it would be helpful to know if the size of the networks is determined by the random graph generators and if these generators can be adjusted to produce networks of different sizes. It would also be beneficial to know if the method can handle tasks or datasets that require networks of a size different from the default 4M parameters.",
                "The paper would benefit from a more explicit discussion of the potential limitations or drawbacks of AutoKD. For instance, the paper suggests that future work should explore how to fully exploit the distribution of the optimal student family, implying that this is an area that could be improved. However, this is not clearly framed as a limitation of the current method. Additionally, the paper could discuss potential limitations related to the comparison with other methods, the efficiency of the search for the optimal student architecture, and the emulation of the performance of large models with smaller students. This would provide a more balanced view of the proposed method and help readers understand its potential challenges and areas for future improvement."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [
                {
                    "reference_id": 4,
                    "predicted_id": 11,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper does not provide comparisons with other architecture search algorithms for knowledge distillation or surrogate functions, therefore the introduction of a new framework may not be warranted under the claims of the authors.",
                    "predicted_comment": "The paper provides a comparison of AutoKD with other KD methods, which is appreciated. However, the comparison with other automated approaches lacks detail. Providing a more detailed comparison with other automated approaches would offer a clearer understanding of AutoKD's strengths and weaknesses relative to these methods. This would also help validate the claim that AutoKD is more sample efficient than other NAS-based KD solutions, and provide more context for the performance of AutoKD. Therefore, I recommend expanding on the comparison of AutoKD with other automated approaches in the field of automated knowledge distillation."
                }
            ],
            "clarity_reproducibility": [
                {
                    "reference_id": 0,
                    "predicted_id": 19,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper lacks conceptual innovation. While the results are solid, there is little insight about the behavior of the method. No ablations are performed beyond the comparison with NAGO, making it hard to assess the importance of the individual components.",
                    "predicted_comment": "The paper would benefit from a more explicit discussion of the potential limitations or drawbacks of AutoKD. For instance, the paper suggests that future work should explore how to fully exploit the distribution of the optimal student family, implying that this is an area that could be improved. However, this is not clearly framed as a limitation of the current method. Additionally, the paper could discuss potential limitations related to the comparison with other methods, the efficiency of the search for the optimal student architecture, and the emulation of the performance of large models with smaller students. This would provide a more balanced view of the proposed method and help readers understand its potential challenges and areas for future improvement."
                }
            ],
            "all": [
                {
                    "reference_id": 0,
                    "predicted_id": 19,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper lacks conceptual innovation. While the results are solid, there is little insight about the behavior of the method. No ablations are performed beyond the comparison with NAGO, making it hard to assess the importance of the individual components.",
                    "predicted_comment": "The paper would benefit from a more explicit discussion of the potential limitations or drawbacks of AutoKD. For instance, the paper suggests that future work should explore how to fully exploit the distribution of the optimal student family, implying that this is an area that could be improved. However, this is not clearly framed as a limitation of the current method. Additionally, the paper could discuss potential limitations related to the comparison with other methods, the efficiency of the search for the optimal student architecture, and the emulation of the performance of large models with smaller students. This would provide a more balanced view of the proposed method and help readers understand its potential challenges and areas for future improvement."
                },
                {
                    "reference_id": 4,
                    "predicted_id": 11,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper does not provide comparisons with other architecture search algorithms for knowledge distillation or surrogate functions, therefore the introduction of a new framework may not be warranted under the claims of the authors.",
                    "predicted_comment": "The paper provides a comparison of AutoKD with other KD methods, which is appreciated. However, the comparison with other automated approaches lacks detail. Providing a more detailed comparison with other automated approaches would offer a clearer understanding of AutoKD's strengths and weaknesses relative to these methods. This would also help validate the claim that AutoKD is more sample efficient than other NAS-based KD solutions, and provide more context for the performance of AutoKD. Therefore, I recommend expanding on the comparison of AutoKD with other automated approaches in the field of automated knowledge distillation."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 19,
                "n_generated": 3,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 19,
                "n_generated": 9,
                "n_total_aligns": 1,
                "recall": 0.06666666666666667,
                "precision": 0.037037037037037035,
                "pseudo_jaccard": 0.025641025641025644,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 19,
                "n_generated": 8,
                "n_total_aligns": 1,
                "recall": 0.06666666666666667,
                "precision": 0.041666666666666664,
                "pseudo_jaccard": 0.027777777777777776,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "all": {
                "n_reference": 19,
                "n_generated": 20,
                "n_total_aligns": 2,
                "recall": 0.13333333333333333,
                "precision": 0.03333333333333333,
                "pseudo_jaccard": 0.028985507246376812,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 9
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.2,
                    "precision": 0.1111111111111111,
                    "pseudo_jaccard": 0.07692307692307693,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 9
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.2,
                    "precision": 0.125,
                    "pseudo_jaccard": 0.08333333333333333,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 9
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                }
            ],
            "all": [
                {
                    "recall": 0.4,
                    "precision": 0.1,
                    "pseudo_jaccard": 0.08695652173913043,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 9
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                }
            ]
        }
    },
    {
        "doc_id": "KPEFXR1HdIo",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The authors should report the wall clock time to demonstrate the time performance of the fine-grained model.",
                "The authors should clarify the biggest time step the simulation can deal with, as the current time step is relatively small (0.001s).",
                "The authors should demonstrate how this method can scale to more complex topologies. All the experiments are currently on one square sheet, but it would be beneficial to see if the method can simulate something like a T-shirt or dress, which is more relevant to design and fabrication."
            ],
            [
                "The paper should include a discussion comparing this work and the latest work on differentiable cloth simulation in a mesh-based view, as mentioned in reference [1].",
                "The statement 'Previous research (Liang et al., 2019; Du et al., 2021) suffers from this problem, which unfortunately leads to learning implausible parameter values' is technically inaccurate. Leveraging prior knowledge for physical parameters is now a necessary step for system identification tasks.",
                "The motivation for using a yarn-level simulator is unclear. The experiments are too coarse to show the advantages of more accurate collision handling and detailed presentation, which are the reasons for proposing a yarn-level simulation. A more valid explanation for motivation or a set of better results that are more complex than 25*25 is expected.",
                "The manuscript should clarify why the authors chose the yarn-based method, how they plan to use this simulator, and where they see the simulator fits in. The motivation for using yarn-level, which is originally designed for fine details but applied on over-simplified models, is not convincing."
            ],
            [
                "The authors identify a major shortcoming of previous simulation models, namely the use of discontinuous force terms, and engineer alternative models that approximate the terms from domain literature but have the added benefit of being smooth everywhere.",
                "The paper raises a question about the non-linear stick-slip force term leading to oscillatory behavior, which could potentially lead to free energy being trapped in entropy and therefore thermal loss.",
                "The paper raises a question about why differences between inter/intra parameters (Table 2) and ground truth increase as the grid size increases.",
                "In the Comparison video, it is difficult to spot any significant difference between the different weaves, which is a weakness of the paper.",
                "The paper lacks real-world experiments, and the range of simulated experiments is not very wide, making it difficult to judge the expressivity / gamut of the model.",
                "The paper acknowledges that neither the specific problem nor the general idea of differentiable simulation is new, and it is unclear whether the specific solutions introduced in the paper generalize to other problems."
            ],
            [
                "The paper does not comment on the computational cost and comparisons to baseline methods. It would be beneficial to provide such information in the main paper, especially when compared with non-differentiable simulator [Cirio et al., 2014] and prior work on differentiable cloth simulation [Liang et al., In NeurIPS 2019].",
                "The quantitative results need to be presented in a clearer manner. For example, it would be good to show the material prediction error in percentage in Table 2-4 (rather than the absolute value). It would be beneficial to include the algorithm (refer to Section 3.1 in [Liang et al. 2019]) in the main paper.",
                "It is unclear whether the proposed differentiable simulation is deterministic or not? Whether the material estimation is sensitive to initialization? In this case, please consider reporting the standard deviation for multiple runs of the same setting.",
                "It is unclear what happens if the parameter prior was not provided to the proposed method in the control learning experiment. It would be beneficial to provide such ablation studies and discuss the results in the rebuttal."
            ],
            [
                "The design process of the model is heuristic and lacks analysis or ablation study to justify the design choices made.",
                "The paper does not discuss how the method performs as a function of the magnitude of the forces. Additional experiments should be conducted to illustrate this.",
                "There is a discrepancy between the claim in paragraph-2 of section 4.2.1 and the data presented in Fig-6 and Table-4. The paper claims that the error reduces by around 100 times when the training data increase from 5 frames to 25 frames, but the figures show only a 10-fold reduction.",
                "The experiments focus only on simulated woven cloth, and the models are heavily designed. This raises questions about the generalizability of the proposed method to other materials and weaving structures."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper presents a model that claims to be more explicable, have higher data efficiency, generate more accurate predictions, and be faster in control learning compared to other models. While the paper does provide some comparison with other models and addresses these aspects, the substantiation of these claims is inconsistent. For instance, the aspect of explicability is substantiated by comparing the proposed model with a general sheet model and showing that the proposed model can effectively estimate yarn parameters with underlying physics models of different sizes. However, it would be beneficial to provide more specific details or evidence to substantiate this claim, such as the metrics used, the datasets, and the results. In terms of data efficiency, the paper substantiates the claim by gradually increasing the training data from the first 5 frames to the first 25 frames and showing that the proposed model has high data efficiency. However, it would be helpful to provide more specific details about these evaluations, such as the metrics used, the datasets, and the results. In terms of prediction accuracy, the paper compares the proposed model with a general sheet model and a traditional parameter estimation method based on Bayesian Optimization combined with a yarn-level simulator. The paper shows that the proposed model can effectively estimate yarn parameters with underlying physics models of different sizes, which is not affected by the cloth size. However, it would be beneficial to provide more specific details or evidence to substantiate this claim, such as the metrics used, the datasets, and the results. In terms of speed in control learning, the paper compares the proposed model with a reinforcement learning baseline model: PPO. The paper shows that the full differentiability of the proposed model enables a quicker search for effective control forces. However, it would be beneficial to provide more specific details or evidence to substantiate this claim, such as the metrics used, the datasets, and the results. By providing more specific details and evidence to substantiate these claims, the paper would provide a clearer picture of the advantages of the proposed model over existing ones.",
                "The paper claims that the proposed model can closely approximate the Stribeck effect while maintaining differentiability. However, the paper lacks a detailed analysis to substantiate this claim. To strengthen the paper, it would be beneficial to include a comparative analysis between the proposed model and other existing models in terms of their ability to approximate the Stribeck effect. This could involve running simulations with both the proposed model and other models, and comparing the results. Such an analysis would provide empirical evidence to support the claim and would give readers a clearer understanding of the model's performance relative to other models.",
                "While the paper does discuss some limitations of the proposed model and potential areas for future work, these discussions could be more detailed and comprehensive. For instance, the limitations related to the assumptions of homogeneously distributed mass and straight yarn segments could be further elaborated, including the potential impact of these assumptions on the model's performance and applicability. The paper could also discuss how the model-based nature of the method and the reliance on prior knowledge to limit parameter learning might limit its use in scenarios where such domain knowledge or prior information is not available or accurate. In terms of future work, the paper could provide more specific examples of how the research could be extended to general composite materials and how the model could be embedded as a layer into a neural network. It would also be helpful to discuss in more detail the potential for additional experiments and comparisons, and how the model could be applied in real-world contexts where information such as woven patterns and yarn materials are easily available. Providing more detailed and comprehensive discussions on these points would provide a more balanced view of the work and help guide future research in this area."
            ],
            "novelty_impact": [
                "The paper presents a significant contribution with its novel differentiable physics model for fabrics. However, it would strengthen the paper if the authors could provide explicit validation for the forces in the new differentiable shear model. While the paper does mention that the model was evaluated for its learning capability, data efficiency, and fidelity, and it compares the model with the most similar work and traditional Bayesian optimization on inverse problems, explicit validation of the forces would provide direct evidence for the model's accuracy and effectiveness. This is particularly important given the novel nature of the model and the paper's claims about its capabilities.",
                "The paper lacks a clear explanation of how the penalty energy for yarn-to-yarn collisions was determined in the model. This information is crucial for readers to fully understand and assess the validity of the model. The authors should provide a detailed explanation of the method used to determine the penalty energy, including any assumptions made and how these were validated.",
                "The methods used to determine the ground truth cloth states are scattered across different sections of the paper and could be more clearly presented. Consolidating this information into a dedicated section or subsection would improve the readability of the paper and make it easier for readers to understand the accuracy of the model's predictions. This is crucial as the ground truth data forms the basis for evaluating the model's performance.",
                "The authors have provided some information about how they compared their model to a sheet-level simulator and a Bayesian optimization model, including the performance of their model on different numbers of frames. However, to fully understand the relative performance of the model, it would be helpful if the authors could provide more specific details about these comparisons. For example, what were the exact methods used for comparison? What data sets were involved? What were the quantitative results of the comparisons? Additionally, it would be beneficial if the authors could provide more details about the implementation and use of the Bayesian optimization model in the comparison, as well as the specific metrics used for comparison and the statistical significance of the results. Furthermore, a discussion on the limitations of the compared models and how the authors' model addresses these limitations would provide valuable context for the comparisons.",
                "The authors provide a detailed explanation of the different forces and their calculations, including stretching, bending, slide friction, shearing, gravity, wind force, and collisions. However, the paper lacks a clear explanation of how these forces interact with each other in the model. Understanding these interactions is crucial for assessing the complexity and realism of the model. The authors should provide more information about how these forces are integrated into a single model and how they interact with each other.",
                "The authors mention the potential extension of their model to general composite materials with mesh structures, ranging from metal/plastic nets/fabrics/meshes to buildings and large infrastructure. However, the paper lacks a detailed discussion or specific examples of how this extension would work in practice. Providing this information would greatly enhance the reader's understanding of the potential applicability and scalability of the model. It would also strengthen the paper's claims about the broad applicability of the model beyond the specific case of cloths.",
                "The authors should provide more information about potential sources of error or uncertainty in the mathematical derivations for the model, particularly with respect to the new differentiable forces proposed to facilitate gradient-based learning. This information is crucial for understanding the robustness of the model's mathematical foundation and the potential impact of these forces on the model's performance. Additionally, the authors should discuss how the limitations of the model-based approach and the need for domain knowledge could contribute to uncertainties in the model.",
                "The paper presents a detailed model focusing on the physical properties and forces acting on fabrics. However, it lacks consideration of potential variations in these properties and forces due to factors like temperature, humidity, and wear and tear. Incorporating these factors into the model would enhance its applicability to real-world scenarios where fabrics are often subjected to varying environmental conditions and wear and tear. This addition would provide a more comprehensive understanding of how these factors influence the material's behavior, thereby strengthening the model's predictive capabilities and practical utility.",
                "The authors have developed a comprehensive model that represents physical properties and forces acting on fabrics. However, the model may not fully capture the complexities of more complex or irregular fabric structures, or accurately represent the interactions between different types of yarns in a composite fabric. The authors should consider these complexities and discuss how they might affect the model's ability to accurately represent physical phenomena.",
                "Additionally, the authors should discuss the model's performance in real-world applications, where fabrics may be subject to a wide range of external forces and conditions. Addressing these points would provide a more complete understanding of the model's capabilities and limitations."
            ],
            "clarity_reproducibility": [
                "The concept of differentiable forces and their role in facilitating gradient-based learning could be explained more clearly and in a more consolidated manner. While the paper does mention that differentiable forces are crucial for the computation of gradients and the back-propagation process in gradient-based learning, these explanations are scattered throughout the paper and may not be easily understood by all readers. Providing a clear, consolidated explanation of differentiable forces and their role in gradient-based learning would make the paper more accessible and improve its overall quality.",
                "The paper mentions important parameters such as the damping coefficient, spring force, breakaway force, and shear stiffness, but does not provide clear definitions or explanations of these parameters. For example, the damping coefficient is mentioned in the context of the force model, but it is not explicitly defined. Similarly, the spring force is related to the parameter k_f in the force model, but its definition is not clear. The breakaway force is mentioned in the context of the Stribeck effect, but it is not explained how it is measured. The shear stiffness is defined as a function of the shear angle, shear modulus, and other parameters, but the shear modulus itself is not clearly defined. Providing clear definitions and explanations of these parameters would greatly improve the clarity and comprehensibility of the paper. Clear definitions and explanations of the parameters used in the model are crucial for readers to fully understand the model and its implications. Without these details, it is difficult for readers to grasp the nuances of the model and to replicate or build upon the authors' work. Therefore, providing these details would greatly enhance the paper's value to its readers and to the scientific community at large.",
                "The paper provides some details about the complex physical structures and materials incorporated in the model, such as the differentiable fabrics model for composite materials, the use of different force models, and the simulation of fine-grained effects. However, the level of detail provided is not consistent across all sections of the paper. It would be helpful if the authors could provide a more detailed and consistent explanation of these elements across all sections. This would make it easier for readers to fully understand the model and its implications.",
                "The paper discusses the evaluation of the model's ability to capture subtle dynamics in several ways, including the use of Mean Squared Error (MSE) for comparison with other models and methods, the model's data efficiency, learning capacity, and its ability to estimate yarn parameters with underlying physics models of different sizes. The authors also mention the use of forward simulations for 2000 steps with parameters learned by the model. However, the paper does not explicitly state the specific metrics or criteria used for evaluating the model's ability to capture subtle dynamics. Providing these details would make the evaluation process more transparent and allow readers to better understand the strengths and limitations of the model. It would also make it easier for other researchers to replicate the study and compare their results with those of this paper.",
                "The paper provides some information about the experimental settings, such as the use of a fully differentiable simulator and the running of 70 epochs for training. However, it lacks specific details about the physical equipment used in the experiments and the specifications of the materials. For example, it would be helpful to know the type of computer or software used for the simulations, the specifications of the yarn used in the cloth, and the physical conditions under which the experiments were conducted. These details are important because they can affect the reproducibility of the experiments and the applicability of the results to other settings. Without these details, it is difficult for other researchers to replicate the experiments or apply the findings to their own work.",
                "The paper mentions a hyperparameter 'p' in the differentiable yarn-to-yarn friction model, but it does not provide specific details on how this hyperparameter was chosen or tuned. Providing this information would help readers replicate the study and understand the robustness of the results. It would be beneficial to include a section detailing the process of hyperparameter selection and tuning, including any methods used (e.g., grid search, random search), the range of values considered, and the criteria for selecting the final values.",
                "The paper presents a detailed theoretical discussion of the differentiable physics model for fabrics, including the force models, derivatives of the simulator, learning physical parameters, prediction and data efficiency, and control learning. However, it lacks information on how to reproduce the experiments and the model. To enhance the reproducibility of your work, it would be beneficial to provide the code used for the experiments, the data used to train and test the model, and detailed instructions on how to use this code and data to reproduce the experiments and the model. This would allow other researchers to verify your results and build upon your work.",
                "The paper provides a detailed explanation of the methods used, but it assumes a high level of prior knowledge in physics and mathematical modeling. The authors use complex mathematical equations and technical terms without providing clear definitions or explanations. This could make it difficult for readers without a strong background in these areas to fully understand the methods used in the paper. I suggest that the authors provide clear definitions or explanations for the technical terms and mathematical symbols used in the paper, and consider including a section that provides a brief overview of the necessary background concepts in physics and mathematical modeling. This would make the paper more accessible to a wider audience and enhance its overall clarity and comprehensibility.",
                "The paper presents a novel differentiable physics model for simulating cloth behavior, which is commendable. However, the explanations of the methods, particularly the equations used in the 'System Equation for Simulation' and 'Force Models' sections, could be more detailed. For instance, the equation representing the force in the system (F = Mq = \u2202T \u2202q \u2212 \u2202V \u2202q \u2212 \u1e40 q) and the equation derived using implicit Euler (M \u2212 \u2202F t \u2202q h 2 \u2212 \u2202F t \u2202 q h qt+1 = h F t \u2212 \u2202F t \u2202 q qt + M qt) are central to the methods presented in the paper, but their explanations are somewhat brief. Expanding upon these explanations would enhance the clarity of the paper and make it more accessible to readers who are not experts in the field."
            ],
            "all": [
                "The paper presents a model that claims to be more explicable, have higher data efficiency, generate more accurate predictions, and be faster in control learning compared to other models. While the paper does provide some comparison with other models and addresses these aspects, the substantiation of these claims is inconsistent. For instance, the aspect of explicability is substantiated by comparing the proposed model with a general sheet model and showing that the proposed model can effectively estimate yarn parameters with underlying physics models of different sizes. However, it would be beneficial to provide more specific details or evidence to substantiate this claim, such as the metrics used, the datasets, and the results. In terms of data efficiency, the paper substantiates the claim by gradually increasing the training data from the first 5 frames to the first 25 frames and showing that the proposed model has high data efficiency. However, it would be helpful to provide more specific details about these evaluations, such as the metrics used, the datasets, and the results. In terms of prediction accuracy, the paper compares the proposed model with a general sheet model and a traditional parameter estimation method based on Bayesian Optimization combined with a yarn-level simulator. The paper shows that the proposed model can effectively estimate yarn parameters with underlying physics models of different sizes, which is not affected by the cloth size. However, it would be beneficial to provide more specific details or evidence to substantiate this claim, such as the metrics used, the datasets, and the results. In terms of speed in control learning, the paper compares the proposed model with a reinforcement learning baseline model: PPO. The paper shows that the full differentiability of the proposed model enables a quicker search for effective control forces. However, it would be beneficial to provide more specific details or evidence to substantiate this claim, such as the metrics used, the datasets, and the results. By providing more specific details and evidence to substantiate these claims, the paper would provide a clearer picture of the advantages of the proposed model over existing ones.",
                "The paper claims that the proposed model can closely approximate the Stribeck effect while maintaining differentiability. However, the paper lacks a detailed analysis to substantiate this claim. To strengthen the paper, it would be beneficial to include a comparative analysis between the proposed model and other existing models in terms of their ability to approximate the Stribeck effect. This could involve running simulations with both the proposed model and other models, and comparing the results. Such an analysis would provide empirical evidence to support the claim and would give readers a clearer understanding of the model's performance relative to other models.",
                "While the paper does discuss some limitations of the proposed model and potential areas for future work, these discussions could be more detailed and comprehensive. For instance, the limitations related to the assumptions of homogeneously distributed mass and straight yarn segments could be further elaborated, including the potential impact of these assumptions on the model's performance and applicability. The paper could also discuss how the model-based nature of the method and the reliance on prior knowledge to limit parameter learning might limit its use in scenarios where such domain knowledge or prior information is not available or accurate. In terms of future work, the paper could provide more specific examples of how the research could be extended to general composite materials and how the model could be embedded as a layer into a neural network. It would also be helpful to discuss in more detail the potential for additional experiments and comparisons, and how the model could be applied in real-world contexts where information such as woven patterns and yarn materials are easily available. Providing more detailed and comprehensive discussions on these points would provide a more balanced view of the work and help guide future research in this area.",
                "The paper presents a significant contribution with its novel differentiable physics model for fabrics. However, it would strengthen the paper if the authors could provide explicit validation for the forces in the new differentiable shear model. While the paper does mention that the model was evaluated for its learning capability, data efficiency, and fidelity, and it compares the model with the most similar work and traditional Bayesian optimization on inverse problems, explicit validation of the forces would provide direct evidence for the model's accuracy and effectiveness. This is particularly important given the novel nature of the model and the paper's claims about its capabilities.",
                "The paper lacks a clear explanation of how the penalty energy for yarn-to-yarn collisions was determined in the model. This information is crucial for readers to fully understand and assess the validity of the model. The authors should provide a detailed explanation of the method used to determine the penalty energy, including any assumptions made and how these were validated.",
                "The methods used to determine the ground truth cloth states are scattered across different sections of the paper and could be more clearly presented. Consolidating this information into a dedicated section or subsection would improve the readability of the paper and make it easier for readers to understand the accuracy of the model's predictions. This is crucial as the ground truth data forms the basis for evaluating the model's performance.",
                "The authors have provided some information about how they compared their model to a sheet-level simulator and a Bayesian optimization model, including the performance of their model on different numbers of frames. However, to fully understand the relative performance of the model, it would be helpful if the authors could provide more specific details about these comparisons. For example, what were the exact methods used for comparison? What data sets were involved? What were the quantitative results of the comparisons? Additionally, it would be beneficial if the authors could provide more details about the implementation and use of the Bayesian optimization model in the comparison, as well as the specific metrics used for comparison and the statistical significance of the results. Furthermore, a discussion on the limitations of the compared models and how the authors' model addresses these limitations would provide valuable context for the comparisons.",
                "The authors provide a detailed explanation of the different forces and their calculations, including stretching, bending, slide friction, shearing, gravity, wind force, and collisions. However, the paper lacks a clear explanation of how these forces interact with each other in the model. Understanding these interactions is crucial for assessing the complexity and realism of the model. The authors should provide more information about how these forces are integrated into a single model and how they interact with each other.",
                "The authors mention the potential extension of their model to general composite materials with mesh structures, ranging from metal/plastic nets/fabrics/meshes to buildings and large infrastructure. However, the paper lacks a detailed discussion or specific examples of how this extension would work in practice. Providing this information would greatly enhance the reader's understanding of the potential applicability and scalability of the model. It would also strengthen the paper's claims about the broad applicability of the model beyond the specific case of cloths.",
                "The authors should provide more information about potential sources of error or uncertainty in the mathematical derivations for the model, particularly with respect to the new differentiable forces proposed to facilitate gradient-based learning. This information is crucial for understanding the robustness of the model's mathematical foundation and the potential impact of these forces on the model's performance. Additionally, the authors should discuss how the limitations of the model-based approach and the need for domain knowledge could contribute to uncertainties in the model.",
                "The paper presents a detailed model focusing on the physical properties and forces acting on fabrics. However, it lacks consideration of potential variations in these properties and forces due to factors like temperature, humidity, and wear and tear. Incorporating these factors into the model would enhance its applicability to real-world scenarios where fabrics are often subjected to varying environmental conditions and wear and tear. This addition would provide a more comprehensive understanding of how these factors influence the material's behavior, thereby strengthening the model's predictive capabilities and practical utility.",
                "The authors have developed a comprehensive model that represents physical properties and forces acting on fabrics. However, the model may not fully capture the complexities of more complex or irregular fabric structures, or accurately represent the interactions between different types of yarns in a composite fabric. The authors should consider these complexities and discuss how they might affect the model's ability to accurately represent physical phenomena.",
                "Additionally, the authors should discuss the model's performance in real-world applications, where fabrics may be subject to a wide range of external forces and conditions. Addressing these points would provide a more complete understanding of the model's capabilities and limitations.",
                "The concept of differentiable forces and their role in facilitating gradient-based learning could be explained more clearly and in a more consolidated manner. While the paper does mention that differentiable forces are crucial for the computation of gradients and the back-propagation process in gradient-based learning, these explanations are scattered throughout the paper and may not be easily understood by all readers. Providing a clear, consolidated explanation of differentiable forces and their role in gradient-based learning would make the paper more accessible and improve its overall quality.",
                "The paper mentions important parameters such as the damping coefficient, spring force, breakaway force, and shear stiffness, but does not provide clear definitions or explanations of these parameters. For example, the damping coefficient is mentioned in the context of the force model, but it is not explicitly defined. Similarly, the spring force is related to the parameter k_f in the force model, but its definition is not clear. The breakaway force is mentioned in the context of the Stribeck effect, but it is not explained how it is measured. The shear stiffness is defined as a function of the shear angle, shear modulus, and other parameters, but the shear modulus itself is not clearly defined. Providing clear definitions and explanations of these parameters would greatly improve the clarity and comprehensibility of the paper. Clear definitions and explanations of the parameters used in the model are crucial for readers to fully understand the model and its implications. Without these details, it is difficult for readers to grasp the nuances of the model and to replicate or build upon the authors' work. Therefore, providing these details would greatly enhance the paper's value to its readers and to the scientific community at large.",
                "The paper provides some details about the complex physical structures and materials incorporated in the model, such as the differentiable fabrics model for composite materials, the use of different force models, and the simulation of fine-grained effects. However, the level of detail provided is not consistent across all sections of the paper. It would be helpful if the authors could provide a more detailed and consistent explanation of these elements across all sections. This would make it easier for readers to fully understand the model and its implications.",
                "The paper discusses the evaluation of the model's ability to capture subtle dynamics in several ways, including the use of Mean Squared Error (MSE) for comparison with other models and methods, the model's data efficiency, learning capacity, and its ability to estimate yarn parameters with underlying physics models of different sizes. The authors also mention the use of forward simulations for 2000 steps with parameters learned by the model. However, the paper does not explicitly state the specific metrics or criteria used for evaluating the model's ability to capture subtle dynamics. Providing these details would make the evaluation process more transparent and allow readers to better understand the strengths and limitations of the model. It would also make it easier for other researchers to replicate the study and compare their results with those of this paper.",
                "The paper provides some information about the experimental settings, such as the use of a fully differentiable simulator and the running of 70 epochs for training. However, it lacks specific details about the physical equipment used in the experiments and the specifications of the materials. For example, it would be helpful to know the type of computer or software used for the simulations, the specifications of the yarn used in the cloth, and the physical conditions under which the experiments were conducted. These details are important because they can affect the reproducibility of the experiments and the applicability of the results to other settings. Without these details, it is difficult for other researchers to replicate the experiments or apply the findings to their own work.",
                "The paper mentions a hyperparameter 'p' in the differentiable yarn-to-yarn friction model, but it does not provide specific details on how this hyperparameter was chosen or tuned. Providing this information would help readers replicate the study and understand the robustness of the results. It would be beneficial to include a section detailing the process of hyperparameter selection and tuning, including any methods used (e.g., grid search, random search), the range of values considered, and the criteria for selecting the final values.",
                "The paper presents a detailed theoretical discussion of the differentiable physics model for fabrics, including the force models, derivatives of the simulator, learning physical parameters, prediction and data efficiency, and control learning. However, it lacks information on how to reproduce the experiments and the model. To enhance the reproducibility of your work, it would be beneficial to provide the code used for the experiments, the data used to train and test the model, and detailed instructions on how to use this code and data to reproduce the experiments and the model. This would allow other researchers to verify your results and build upon your work.",
                "The paper provides a detailed explanation of the methods used, but it assumes a high level of prior knowledge in physics and mathematical modeling. The authors use complex mathematical equations and technical terms without providing clear definitions or explanations. This could make it difficult for readers without a strong background in these areas to fully understand the methods used in the paper. I suggest that the authors provide clear definitions or explanations for the technical terms and mathematical symbols used in the paper, and consider including a section that provides a brief overview of the necessary background concepts in physics and mathematical modeling. This would make the paper more accessible to a wider audience and enhance its overall clarity and comprehensibility.",
                "The paper presents a novel differentiable physics model for simulating cloth behavior, which is commendable. However, the explanations of the methods, particularly the equations used in the 'System Equation for Simulation' and 'Force Models' sections, could be more detailed. For instance, the equation representing the force in the system (F = Mq = \u2202T \u2202q \u2212 \u2202V \u2202q \u2212 \u1e40 q) and the equation derived using implicit Euler (M \u2212 \u2202F t \u2202q h 2 \u2212 \u2202F t \u2202 q h qt+1 = h F t \u2212 \u2202F t \u2202 q qt + M qt) are central to the methods presented in the paper, but their explanations are somewhat brief. Expanding upon these explanations would enhance the clarity of the paper and make it more accessible to readers who are not experts in the field."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The design process of the model is heuristic and lacks analysis or ablation study to justify the design choices made.",
                    "predicted_comment": "The paper presents a model that claims to be more explicable, have higher data efficiency, generate more accurate predictions, and be faster in control learning compared to other models. While the paper does provide some comparison with other models and addresses these aspects, the substantiation of these claims is inconsistent. For instance, the aspect of explicability is substantiated by comparing the proposed model with a general sheet model and showing that the proposed model can effectively estimate yarn parameters with underlying physics models of different sizes. However, it would be beneficial to provide more specific details or evidence to substantiate this claim, such as the metrics used, the datasets, and the results. In terms of data efficiency, the paper substantiates the claim by gradually increasing the training data from the first 5 frames to the first 25 frames and showing that the proposed model has high data efficiency. However, it would be helpful to provide more specific details about these evaluations, such as the metrics used, the datasets, and the results. In terms of prediction accuracy, the paper compares the proposed model with a general sheet model and a traditional parameter estimation method based on Bayesian Optimization combined with a yarn-level simulator. The paper shows that the proposed model can effectively estimate yarn parameters with underlying physics models of different sizes, which is not affected by the cloth size. However, it would be beneficial to provide more specific details or evidence to substantiate this claim, such as the metrics used, the datasets, and the results. In terms of speed in control learning, the paper compares the proposed model with a reinforcement learning baseline model: PPO. The paper shows that the full differentiability of the proposed model enables a quicker search for effective control forces. However, it would be beneficial to provide more specific details or evidence to substantiate this claim, such as the metrics used, the datasets, and the results. By providing more specific details and evidence to substantiate these claims, the paper would provide a clearer picture of the advantages of the proposed model over existing ones."
                }
            ],
            "novelty_impact": [
                {
                    "reference_id": 0,
                    "predicted_id": 6,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper does not comment on the computational cost and comparisons to baseline methods. It would be beneficial to provide such information in the main paper, especially when compared with non-differentiable simulator [Cirio et al., 2014] and prior work on differentiable cloth simulation [Liang et al., In NeurIPS 2019].",
                    "predicted_comment": "The authors have provided some information about how they compared their model to a sheet-level simulator and a Bayesian optimization model, including the performance of their model on different numbers of frames. However, to fully understand the relative performance of the model, it would be helpful if the authors could provide more specific details about these comparisons. For example, what were the exact methods used for comparison? What data sets were involved? What were the quantitative results of the comparisons? Additionally, it would be beneficial if the authors could provide more details about the implementation and use of the Bayesian optimization model in the comparison, as well as the specific metrics used for comparison and the statistical significance of the results. Furthermore, a discussion on the limitations of the compared models and how the authors' model addresses these limitations would provide valuable context for the comparisons."
                },
                {
                    "reference_id": 3,
                    "predicted_id": 11,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The experiments focus only on simulated woven cloth, and the models are heavily designed. This raises questions about the generalizability of the proposed method to other materials and weaving structures.",
                    "predicted_comment": "The authors have developed a comprehensive model that represents physical properties and forces acting on fabrics. However, the model may not fully capture the complexities of more complex or irregular fabric structures, or accurately represent the interactions between different types of yarns in a composite fabric. The authors should consider these complexities and discuss how they might affect the model's ability to accurately represent physical phenomena."
                }
            ],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 0,
                    "predicted_id": 6,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper does not comment on the computational cost and comparisons to baseline methods. It would be beneficial to provide such information in the main paper, especially when compared with non-differentiable simulator [Cirio et al., 2014] and prior work on differentiable cloth simulation [Liang et al., In NeurIPS 2019].",
                    "predicted_comment": "The authors have provided some information about how they compared their model to a sheet-level simulator and a Bayesian optimization model, including the performance of their model on different numbers of frames. However, to fully understand the relative performance of the model, it would be helpful if the authors could provide more specific details about these comparisons. For example, what were the exact methods used for comparison? What data sets were involved? What were the quantitative results of the comparisons? Additionally, it would be beneficial if the authors could provide more details about the implementation and use of the Bayesian optimization model in the comparison, as well as the specific metrics used for comparison and the statistical significance of the results. Furthermore, a discussion on the limitations of the compared models and how the authors' model addresses these limitations would provide valuable context for the comparisons."
                },
                {
                    "reference_id": 3,
                    "predicted_id": 11,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The experiments focus only on simulated woven cloth, and the models are heavily designed. This raises questions about the generalizability of the proposed method to other materials and weaving structures.",
                    "predicted_comment": "The authors have developed a comprehensive model that represents physical properties and forces acting on fabrics. However, the model may not fully capture the complexities of more complex or irregular fabric structures, or accurately represent the interactions between different types of yarns in a composite fabric. The authors should consider these complexities and discuss how they might affect the model's ability to accurately represent physical phenomena."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The design process of the model is heuristic and lacks analysis or ablation study to justify the design choices made.",
                    "predicted_comment": "The paper presents a model that claims to be more explicable, have higher data efficiency, generate more accurate predictions, and be faster in control learning compared to other models. While the paper does provide some comparison with other models and addresses these aspects, the substantiation of these claims is inconsistent. For instance, the aspect of explicability is substantiated by comparing the proposed model with a general sheet model and showing that the proposed model can effectively estimate yarn parameters with underlying physics models of different sizes. However, it would be beneficial to provide more specific details or evidence to substantiate this claim, such as the metrics used, the datasets, and the results. In terms of data efficiency, the paper substantiates the claim by gradually increasing the training data from the first 5 frames to the first 25 frames and showing that the proposed model has high data efficiency. However, it would be helpful to provide more specific details about these evaluations, such as the metrics used, the datasets, and the results. In terms of prediction accuracy, the paper compares the proposed model with a general sheet model and a traditional parameter estimation method based on Bayesian Optimization combined with a yarn-level simulator. The paper shows that the proposed model can effectively estimate yarn parameters with underlying physics models of different sizes, which is not affected by the cloth size. However, it would be beneficial to provide more specific details or evidence to substantiate this claim, such as the metrics used, the datasets, and the results. In terms of speed in control learning, the paper compares the proposed model with a reinforcement learning baseline model: PPO. The paper shows that the full differentiability of the proposed model enables a quicker search for effective control forces. However, it would be beneficial to provide more specific details or evidence to substantiate this claim, such as the metrics used, the datasets, and the results. By providing more specific details and evidence to substantiate these claims, the paper would provide a clearer picture of the advantages of the proposed model over existing ones."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 21,
                "n_generated": 3,
                "n_total_aligns": 1,
                "recall": 0.05,
                "precision": 0.06666666666666667,
                "pseudo_jaccard": 0.03333333333333333,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 21,
                "n_generated": 10,
                "n_total_aligns": 2,
                "recall": 0.1,
                "precision": 0.04,
                "pseudo_jaccard": 0.03076923076923077,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 21,
                "n_generated": 9,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 21,
                "n_generated": 22,
                "n_total_aligns": 3,
                "recall": 0.15,
                "precision": 0.02727272727272727,
                "pseudo_jaccard": 0.024666666666666663,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.25,
                    "precision": 0.3333333333333333,
                    "pseudo_jaccard": 0.16666666666666666,
                    "n_real": 4
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.25,
                    "precision": 0.1,
                    "pseudo_jaccard": 0.07692307692307693,
                    "n_real": 4
                },
                {
                    "recall": 0.25,
                    "precision": 0.1,
                    "pseudo_jaccard": 0.07692307692307693,
                    "n_real": 4
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.25,
                    "precision": 0.045454545454545456,
                    "pseudo_jaccard": 0.04,
                    "n_real": 4
                },
                {
                    "recall": 0.5,
                    "precision": 0.09090909090909091,
                    "pseudo_jaccard": 0.08333333333333333,
                    "n_real": 4
                }
            ]
        }
    },
    {
        "doc_id": "HyxLRTVKPH",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper does not clearly demonstrate the advantages of using a linear scaling schedule over other schedules. For example, step-based schedules outperform the proposed method in 2 of the 4 tasks in Table 2, and Poly and Cosine schedules perform better in some budgets in Figure 2.",
                "The comparisons in Figure 2 and Table 2 are not convincing without considering the variance of different trials. It is unclear whether the advantage is due to the learning rate schedule or randomness. The paper should report the mean and variance for multiple trials and ideally perform a significance test.",
                "The paper should include other learning rate schedules in Table 2. As shown in Figure 2, step-based schedule is not among the top three schedules for CIFAR-10.",
                "As shown in Table 3, the proposed method requires waiting until the end of training to get the best performing model, while step-based schedule can find the best model around 90% training. This means early stopping is not possible for linear scaling based schedule, which could be useful when the training budget is large enough and evaluation is cheap.",
                "The paper lacks a deeper understanding of why a linear learning rate schedule might work better. The authors attempt to provide an explanation from the relationship between learning rate and gradient magnitudes, but no clear conclusion is given. The paper should investigate when no weight decay is used and whether there is any correlation between the learning rate and gradient norms.",
                "The paper does not specify the learning rate decay unit for the linear schedule. Is it decaying per epoch or per mini-batch? If epoch-based learning rate decay is used, it is essentially step-based learning rate decay with many steps.",
                "The paper should investigate when the linear schedule starts to outperform step-based decay in terms of epochs or iterations. The difference between two schedules for small batch size at the early stage of training might be less in comparison with large batch training, especially when the number of iterations is enough.",
                "The paper only compares different schedules with a single initial learning rate. Different initial learning rates may result in different behaviors. The paper should consider scenarios where the initial learning rate is small, as there might not be much difference for different schedules in such cases.",
                "The authors' rebuttal did not fully address the concern about the lack of early stopping in the proposed method, which could limit its usage, especially for Hyperparameter Optimization (HPO).",
                "The authors' rebuttal did not fully address the concern about the lack of understanding of why the linear scaling schedule is better."
            ],
            [
                "The paper lacks a sound theory explaining why linear decay or other smooth decay methods work well.",
                "The novelty of the methodology proposed by the authors might be limited, as Mishkin et al. have already experimented with linear decay."
            ],
            [
                "The paper makes unclear claims about the phenomena of gradient magnitude vanishing and its effectiveness. For example, in section 5, the authors state 'We call this \u201cvanishing gradient\u201d phenomenon budgeted convergence. This correlation suggests that decaying schedules to near-zero rates (and using BAC) may be more effective than early stopping.' This claim appears to be an overreach as the paper merely shows gradient magnitude decreasing with learning rate.",
                "The paper does not perform extensive comparisons for commonly used algorithms like Adam, which are key motivating use cases for budget-aware training, such as providing approximations for problems like neural architecture search and hyperparameter tuning."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper presents a novel and interesting approach to budgeted training in machine learning, with the linear decay schedule playing a key role. However, the paper lacks an ablation study to understand the contribution of each component of the proposed method, particularly the linear decay schedule. An ablation study would allow for a more detailed understanding of the contribution of each component and would help to isolate the effects of individual components, such as the linear decay schedule, on the overall performance. This is particularly important given the paper's emphasis on the linear decay schedule's role in the method's performance. Therefore, I recommend conducting an ablation study and including the results in the experimental section of the paper. This would strengthen the paper by providing deeper insights into the workings of the proposed method, particularly the linear decay schedule, and potentially leading to further improvements."
            ],
            "novelty_impact": [
                "While the paper presents a robust and high-performing approach to budgeted training by focusing on the number of optimization iterations, it would be beneficial to discuss how this approach could be extended to other types of resources. Specifically, considering constraints such as memory or computational power could make the work more applicable to a wider range of scenarios. The authors' mention of abstracting out the specific constraint without loss of generality suggests that such an extension may be feasible. Providing a discussion on this could enhance the paper's contribution and broaden its impact.",
                "The authors have discussed the concept of 'budgeted convergence' in the context of budgeted training, where the gradient vanishes at the end of each allowed budget. This is an important aspect of the proposed method, as it suggests that the optimization has reached a critical point. However, the paper could benefit from a more detailed discussion on the implications if the gradient does not vanish at the end of each allowed budget. Specifically, it would be helpful to understand the potential risks associated with this scenario and how they could be mitigated. This could include discussing the impact on the performance of the method and any strategies for adjusting the learning rate schedule or other parameters to ensure convergence within the allowed budget.",
                "The paper provides a robust and high-performing linear schedule for budgeted training. However, it would be beneficial for the readers if the authors could provide more specific conditions under which the linear schedule might not be the optimal choice. For instance, the paper mentions that the linear schedule fails to generalize across budgets when the exponential schedule is well-tuned for a specific task. It would be helpful if the authors could elaborate on this point and provide more examples or conditions where other schedules like step decay, exponential decay, poly, cosine, or htd might be more suitable. This would guide readers on when to use the linear schedule and when to consider other schedules.",
                "The authors have conducted experiments on a variety of models and datasets, including image classification on ImageNet, video classification on Kinetics, object detection and instance segmentation on MS COCO, and semantic segmentation on Cityscapes. While the paper suggests that the linear schedule for learning rate decay is applicable across different settings, it would be beneficial for the authors to explicitly discuss the applicability of their approach to other types of models and datasets not included in their experiments. This would help readers understand the generalizability of the approach and its potential applicability in other contexts, such as neural architecture search, which is briefly mentioned but not explored in detail.",
                "The authors have presented a robust and high-performing concept of budgeted training for neural architecture search and demonstrated its effectiveness through extensive experiments. However, the paper could benefit from a more explicit discussion on how this concept could be applied to other areas of machine learning. While the authors mention the general applicability of their approach to any machine learning task that involves training a model under resource constraints, specific examples or case studies would provide a clearer picture of how this concept could be extended beyond neural architecture search. This would not only broaden the impact of the work but also provide a roadmap for future research in this direction.",
                "The authors present compelling evidence that limiting the number of iterations in a budgeted training scenario can outperform offline data subsampling strategies. However, the paper would benefit from a more balanced discussion. Specifically, the authors should discuss potential drawbacks of limiting the number of iterations, such as the risk of overfitting. This would provide readers with a more comprehensive understanding of the trade-offs involved in this approach.",
                "While the authors have demonstrated the effectiveness of their budget-aware learning rate schedule across a variety of tasks, including image classification, video classification, object detection, instance segmentation, and semantic segmentation, they have not provided specific guidance on how to adapt this approach to other tasks. Given that the authors' approach is not tied to any specific task or architecture and is parameter-free, it has the potential to be widely applicable. However, without explicit guidance on how to adapt the approach to other tasks, its utility to a wider audience may be limited. The authors could enhance the impact of their work by providing examples or guidelines on how to adapt their budget-aware learning rate schedule to other machine learning tasks, particularly those that may have different resource constraints or performance metrics.",
                "The authors present compelling empirical evidence that smooth-decaying schedules, such as linear or cosine, can achieve superior performance under budgeted training. This conclusion is a significant contribution to the paper's overall argument. However, the paper would benefit from a theoretical analysis that explains why these schedules perform better under budgeted training. Such an analysis would not only strengthen the authors' conclusion but also provide readers with a deeper understanding of the underlying mechanisms at play. Therefore, I recommend that the authors consider including a theoretical analysis in future work or as an addition to this paper.",
                "The authors have provided a comprehensive discussion of various factors that could potentially impact the performance of budgeted training, including the size and complexity of the training datasets, the complexity of the machine learning models, the learning rate schedule, the number of optimization iterations, and the use of budget-aware learning schedules, among others. However, there are other factors that could also impact the performance of budgeted training but are not discussed in the paper. These include the hardware and software infrastructure used for training, the skill and experience of the practitioners conducting the training, the quality of the data used for training, the complexity of the model being trained, the computational resources available for training, the architecture of the neural network, and the type of optimization algorithm used. Discussing these factors could provide a more comprehensive view of the challenges and solutions in budgeted training."
            ],
            "clarity_reproducibility": [
                "The paper introduces an interesting concept of budgeted training and proposes a linear decay as a robust and high-performing budget-aware learning schedule. However, the paper lacks a detailed explanation on several aspects of the learning rate adjustment, including how the learning rate is initially set, how it is adjusted over time, and how these adjustments relate to the given budget. These details are crucial for understanding and implementing the proposed method, and their absence makes it difficult for readers to reproduce the results. Therefore, I suggest the authors provide a more detailed explanation or a step-by-step guide on how the learning rate schedule is adjusted according to the given budget. This would not only make it easier for readers to understand and implement the proposed method, but also allow them to fully appreciate the advantages of the proposed linear schedule, especially its parameter-free nature in budgeted training.",
                "The paper introduces the concept of budgeted convergence, which is central to the paper's claims about budget-aware learning rate schedules and neural architecture search. However, this concept appears to be based primarily on empirical observations. The paper would be significantly strengthened by providing theoretical justifications or mathematical proofs for this concept. For example, a mathematical proof showing how budgeted convergence relates to the behavior of learning rate schedules under budgeted training could provide a solid foundation for the empirical observations. This would not only enhance the credibility of the concept but also potentially provide insights into why it works, thereby contributing to a deeper understanding of the phenomenon.",
                "The authors have clearly mentioned the metrics used to evaluate the performance of the models, such as validation accuracy, top-1 accuracy, COCO AP, and mIoU. However, the paper lacks detailed information on how these metrics are calculated. Providing this information would enhance the clarity of the experimental methodology and facilitate replication of the results by other researchers.",
                "The paper discusses the ranking of different architectures in the context of neural architecture search and uses Kendall's rank correlation coefficient as a metric. However, it would be beneficial if the authors could provide more details about how this ranking is done, especially in the context of different learning rate schedules. More specifically, how is the relative rank between all pairs of random architectures determined based on the validation accuracy? How is Kendall's rank correlation coefficient applied in this context? These details are important for understanding the methodology and replicating the results.",
                "The paper provides a valuable discussion on various learning rate schedules, including budget-aware, poly, cosine, htd, linear, and step decay schedules. However, the implementation details and specific algorithms for these schedules are not adequately detailed. Providing these details would enhance the reproducibility of the study and allow readers to better understand the nuances of these schedules. For instance, the paper could include pseudocode or a more explicit description of the calculation process for each schedule. This would be particularly beneficial for the proposed Budget-Aware Conversion (BAC) method and the implicitly defined budget-aware schedules. Readers interested in replicating or building upon this work may struggle without these details. Therefore, it is recommended that the authors provide more detailed information about the implementation of these learning rate schedules or references where these details can be found.",
                "The paper mentions data subsampling as a strategy for budgeted training but does not provide a detailed explanation or discussion about it. The authors mention it as a potential strategy, but they argue for a different strategy instead. They also mention that there are more complicated subset construction methods, but these are not suitable for extremely large datasets. It would be beneficial if the authors could provide more information about how data subsampling is implemented as a strategy for budgeted training, its advantages and disadvantages compared to the proposed method, and its impact on the results. This would help readers understand the full range of strategies for budgeted training and make informed decisions about which strategy to use in their own work.",
                "The experimental setup is well-described, but the paper could benefit from providing more explicit details about the choice of specific models and datasets used. While it can be inferred that these models and datasets are commonly used benchmarks in the field of machine learning and provide a diverse range of tasks for evaluating the performance of the learning rate schedules, explicitly stating these reasons would strengthen the paper. This would help readers understand the rationale behind the experimental design and make the results more easily comparable to other studies in the field.",
                "The paper discusses learning rates in convex optimization in the context of budget-aware training and introduces a new linear schedule. However, the discussion is not very detailed and is spread across different sections of the paper. It would be beneficial if the authors could consolidate this information and provide a more detailed discussion about learning rates in convex optimization, including the theoretical aspects and their practical implications. This would help readers better understand the importance of learning rates in convex optimization and how they relate to the proposed method.",
                "The paper mentions additional implementation details for various tasks, such as the adjustment of the learning rate schedule according to the given budget and the practical application of the proposed method. However, these details are not provided in the main text. Given their importance to understanding the practical implications of the theoretical concepts and methods discussed in the paper, it would be helpful if the authors could incorporate these details into the main text. This could be done by integrating them into the sections where the corresponding concepts or methods are introduced or discussed, presenting them in a concise manner, perhaps in bullet points or a table, or including them as footnotes or endnotes. This would allow readers to understand the practical implications without disrupting the flow of the paper."
            ],
            "all": [
                "The paper presents a novel and interesting approach to budgeted training in machine learning, with the linear decay schedule playing a key role. However, the paper lacks an ablation study to understand the contribution of each component of the proposed method, particularly the linear decay schedule. An ablation study would allow for a more detailed understanding of the contribution of each component and would help to isolate the effects of individual components, such as the linear decay schedule, on the overall performance. This is particularly important given the paper's emphasis on the linear decay schedule's role in the method's performance. Therefore, I recommend conducting an ablation study and including the results in the experimental section of the paper. This would strengthen the paper by providing deeper insights into the workings of the proposed method, particularly the linear decay schedule, and potentially leading to further improvements.",
                "While the paper presents a robust and high-performing approach to budgeted training by focusing on the number of optimization iterations, it would be beneficial to discuss how this approach could be extended to other types of resources. Specifically, considering constraints such as memory or computational power could make the work more applicable to a wider range of scenarios. The authors' mention of abstracting out the specific constraint without loss of generality suggests that such an extension may be feasible. Providing a discussion on this could enhance the paper's contribution and broaden its impact.",
                "The authors have discussed the concept of 'budgeted convergence' in the context of budgeted training, where the gradient vanishes at the end of each allowed budget. This is an important aspect of the proposed method, as it suggests that the optimization has reached a critical point. However, the paper could benefit from a more detailed discussion on the implications if the gradient does not vanish at the end of each allowed budget. Specifically, it would be helpful to understand the potential risks associated with this scenario and how they could be mitigated. This could include discussing the impact on the performance of the method and any strategies for adjusting the learning rate schedule or other parameters to ensure convergence within the allowed budget.",
                "The paper provides a robust and high-performing linear schedule for budgeted training. However, it would be beneficial for the readers if the authors could provide more specific conditions under which the linear schedule might not be the optimal choice. For instance, the paper mentions that the linear schedule fails to generalize across budgets when the exponential schedule is well-tuned for a specific task. It would be helpful if the authors could elaborate on this point and provide more examples or conditions where other schedules like step decay, exponential decay, poly, cosine, or htd might be more suitable. This would guide readers on when to use the linear schedule and when to consider other schedules.",
                "The authors have conducted experiments on a variety of models and datasets, including image classification on ImageNet, video classification on Kinetics, object detection and instance segmentation on MS COCO, and semantic segmentation on Cityscapes. While the paper suggests that the linear schedule for learning rate decay is applicable across different settings, it would be beneficial for the authors to explicitly discuss the applicability of their approach to other types of models and datasets not included in their experiments. This would help readers understand the generalizability of the approach and its potential applicability in other contexts, such as neural architecture search, which is briefly mentioned but not explored in detail.",
                "The authors have presented a robust and high-performing concept of budgeted training for neural architecture search and demonstrated its effectiveness through extensive experiments. However, the paper could benefit from a more explicit discussion on how this concept could be applied to other areas of machine learning. While the authors mention the general applicability of their approach to any machine learning task that involves training a model under resource constraints, specific examples or case studies would provide a clearer picture of how this concept could be extended beyond neural architecture search. This would not only broaden the impact of the work but also provide a roadmap for future research in this direction.",
                "The authors present compelling evidence that limiting the number of iterations in a budgeted training scenario can outperform offline data subsampling strategies. However, the paper would benefit from a more balanced discussion. Specifically, the authors should discuss potential drawbacks of limiting the number of iterations, such as the risk of overfitting. This would provide readers with a more comprehensive understanding of the trade-offs involved in this approach.",
                "While the authors have demonstrated the effectiveness of their budget-aware learning rate schedule across a variety of tasks, including image classification, video classification, object detection, instance segmentation, and semantic segmentation, they have not provided specific guidance on how to adapt this approach to other tasks. Given that the authors' approach is not tied to any specific task or architecture and is parameter-free, it has the potential to be widely applicable. However, without explicit guidance on how to adapt the approach to other tasks, its utility to a wider audience may be limited. The authors could enhance the impact of their work by providing examples or guidelines on how to adapt their budget-aware learning rate schedule to other machine learning tasks, particularly those that may have different resource constraints or performance metrics.",
                "The authors present compelling empirical evidence that smooth-decaying schedules, such as linear or cosine, can achieve superior performance under budgeted training. This conclusion is a significant contribution to the paper's overall argument. However, the paper would benefit from a theoretical analysis that explains why these schedules perform better under budgeted training. Such an analysis would not only strengthen the authors' conclusion but also provide readers with a deeper understanding of the underlying mechanisms at play. Therefore, I recommend that the authors consider including a theoretical analysis in future work or as an addition to this paper.",
                "The authors have provided a comprehensive discussion of various factors that could potentially impact the performance of budgeted training, including the size and complexity of the training datasets, the complexity of the machine learning models, the learning rate schedule, the number of optimization iterations, and the use of budget-aware learning schedules, among others. However, there are other factors that could also impact the performance of budgeted training but are not discussed in the paper. These include the hardware and software infrastructure used for training, the skill and experience of the practitioners conducting the training, the quality of the data used for training, the complexity of the model being trained, the computational resources available for training, the architecture of the neural network, and the type of optimization algorithm used. Discussing these factors could provide a more comprehensive view of the challenges and solutions in budgeted training.",
                "The paper introduces an interesting concept of budgeted training and proposes a linear decay as a robust and high-performing budget-aware learning schedule. However, the paper lacks a detailed explanation on several aspects of the learning rate adjustment, including how the learning rate is initially set, how it is adjusted over time, and how these adjustments relate to the given budget. These details are crucial for understanding and implementing the proposed method, and their absence makes it difficult for readers to reproduce the results. Therefore, I suggest the authors provide a more detailed explanation or a step-by-step guide on how the learning rate schedule is adjusted according to the given budget. This would not only make it easier for readers to understand and implement the proposed method, but also allow them to fully appreciate the advantages of the proposed linear schedule, especially its parameter-free nature in budgeted training.",
                "The paper introduces the concept of budgeted convergence, which is central to the paper's claims about budget-aware learning rate schedules and neural architecture search. However, this concept appears to be based primarily on empirical observations. The paper would be significantly strengthened by providing theoretical justifications or mathematical proofs for this concept. For example, a mathematical proof showing how budgeted convergence relates to the behavior of learning rate schedules under budgeted training could provide a solid foundation for the empirical observations. This would not only enhance the credibility of the concept but also potentially provide insights into why it works, thereby contributing to a deeper understanding of the phenomenon.",
                "The authors have clearly mentioned the metrics used to evaluate the performance of the models, such as validation accuracy, top-1 accuracy, COCO AP, and mIoU. However, the paper lacks detailed information on how these metrics are calculated. Providing this information would enhance the clarity of the experimental methodology and facilitate replication of the results by other researchers.",
                "The paper discusses the ranking of different architectures in the context of neural architecture search and uses Kendall's rank correlation coefficient as a metric. However, it would be beneficial if the authors could provide more details about how this ranking is done, especially in the context of different learning rate schedules. More specifically, how is the relative rank between all pairs of random architectures determined based on the validation accuracy? How is Kendall's rank correlation coefficient applied in this context? These details are important for understanding the methodology and replicating the results.",
                "The paper provides a valuable discussion on various learning rate schedules, including budget-aware, poly, cosine, htd, linear, and step decay schedules. However, the implementation details and specific algorithms for these schedules are not adequately detailed. Providing these details would enhance the reproducibility of the study and allow readers to better understand the nuances of these schedules. For instance, the paper could include pseudocode or a more explicit description of the calculation process for each schedule. This would be particularly beneficial for the proposed Budget-Aware Conversion (BAC) method and the implicitly defined budget-aware schedules. Readers interested in replicating or building upon this work may struggle without these details. Therefore, it is recommended that the authors provide more detailed information about the implementation of these learning rate schedules or references where these details can be found.",
                "The paper mentions data subsampling as a strategy for budgeted training but does not provide a detailed explanation or discussion about it. The authors mention it as a potential strategy, but they argue for a different strategy instead. They also mention that there are more complicated subset construction methods, but these are not suitable for extremely large datasets. It would be beneficial if the authors could provide more information about how data subsampling is implemented as a strategy for budgeted training, its advantages and disadvantages compared to the proposed method, and its impact on the results. This would help readers understand the full range of strategies for budgeted training and make informed decisions about which strategy to use in their own work.",
                "The experimental setup is well-described, but the paper could benefit from providing more explicit details about the choice of specific models and datasets used. While it can be inferred that these models and datasets are commonly used benchmarks in the field of machine learning and provide a diverse range of tasks for evaluating the performance of the learning rate schedules, explicitly stating these reasons would strengthen the paper. This would help readers understand the rationale behind the experimental design and make the results more easily comparable to other studies in the field.",
                "The paper discusses learning rates in convex optimization in the context of budget-aware training and introduces a new linear schedule. However, the discussion is not very detailed and is spread across different sections of the paper. It would be beneficial if the authors could consolidate this information and provide a more detailed discussion about learning rates in convex optimization, including the theoretical aspects and their practical implications. This would help readers better understand the importance of learning rates in convex optimization and how they relate to the proposed method.",
                "The paper mentions additional implementation details for various tasks, such as the adjustment of the learning rate schedule according to the given budget and the practical application of the proposed method. However, these details are not provided in the main text. Given their importance to understanding the practical implications of the theoretical concepts and methods discussed in the paper, it would be helpful if the authors could incorporate these details into the main text. This could be done by integrating them into the sections where the corresponding concepts or methods are introduced or discussed, presenting them in a concise manner, perhaps in bullet points or a table, or including them as footnotes or endnotes. This would allow readers to understand the practical implications without disrupting the flow of the paper."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [
                {
                    "reference_id": 6,
                    "predicted_id": 3,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper should investigate when the linear schedule starts to outperform step-based decay in terms of epochs or iterations. The difference between two schedules for small batch size at the early stage of training might be less in comparison with large batch training, especially when the number of iterations is enough.",
                    "predicted_comment": "The paper provides a robust and high-performing linear schedule for budgeted training. However, it would be beneficial for the readers if the authors could provide more specific conditions under which the linear schedule might not be the optimal choice. For instance, the paper mentions that the linear schedule fails to generalize across budgets when the exponential schedule is well-tuned for a specific task. It would be helpful if the authors could elaborate on this point and provide more examples or conditions where other schedules like step decay, exponential decay, poly, cosine, or htd might be more suitable. This would guide readers on when to use the linear schedule and when to consider other schedules."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 3,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper does not clearly demonstrate the advantages of using a linear scaling schedule over other schedules. For example, step-based schedules outperform the proposed method in 2 of the 4 tasks in Table 2, and Poly and Cosine schedules perform better in some budgets in Figure 2.",
                    "predicted_comment": "The paper provides a robust and high-performing linear schedule for budgeted training. However, it would be beneficial for the readers if the authors could provide more specific conditions under which the linear schedule might not be the optimal choice. For instance, the paper mentions that the linear schedule fails to generalize across budgets when the exponential schedule is well-tuned for a specific task. It would be helpful if the authors could elaborate on this point and provide more examples or conditions where other schedules like step decay, exponential decay, poly, cosine, or htd might be more suitable. This would guide readers on when to use the linear schedule and when to consider other schedules."
                },
                {
                    "reference_id": 9,
                    "predicted_id": 8,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The authors' rebuttal did not fully address the concern about the lack of understanding of why the linear scaling schedule is better.",
                    "predicted_comment": "The authors present compelling empirical evidence that smooth-decaying schedules, such as linear or cosine, can achieve superior performance under budgeted training. This conclusion is a significant contribution to the paper's overall argument. However, the paper would benefit from a theoretical analysis that explains why these schedules perform better under budgeted training. Such an analysis would not only strengthen the authors' conclusion but also provide readers with a deeper understanding of the underlying mechanisms at play. Therefore, I recommend that the authors consider including a theoretical analysis in future work or as an addition to this paper."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 8,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper lacks a sound theory explaining why linear decay or other smooth decay methods work well.",
                    "predicted_comment": "The authors present compelling empirical evidence that smooth-decaying schedules, such as linear or cosine, can achieve superior performance under budgeted training. This conclusion is a significant contribution to the paper's overall argument. However, the paper would benefit from a theoretical analysis that explains why these schedules perform better under budgeted training. Such an analysis would not only strengthen the authors' conclusion but also provide readers with a deeper understanding of the underlying mechanisms at play. Therefore, I recommend that the authors consider including a theoretical analysis in future work or as an addition to this paper."
                }
            ],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 6,
                    "predicted_id": 3,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper should investigate when the linear schedule starts to outperform step-based decay in terms of epochs or iterations. The difference between two schedules for small batch size at the early stage of training might be less in comparison with large batch training, especially when the number of iterations is enough.",
                    "predicted_comment": "The paper provides a robust and high-performing linear schedule for budgeted training. However, it would be beneficial for the readers if the authors could provide more specific conditions under which the linear schedule might not be the optimal choice. For instance, the paper mentions that the linear schedule fails to generalize across budgets when the exponential schedule is well-tuned for a specific task. It would be helpful if the authors could elaborate on this point and provide more examples or conditions where other schedules like step decay, exponential decay, poly, cosine, or htd might be more suitable. This would guide readers on when to use the linear schedule and when to consider other schedules."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 3,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper does not clearly demonstrate the advantages of using a linear scaling schedule over other schedules. For example, step-based schedules outperform the proposed method in 2 of the 4 tasks in Table 2, and Poly and Cosine schedules perform better in some budgets in Figure 2.",
                    "predicted_comment": "The paper provides a robust and high-performing linear schedule for budgeted training. However, it would be beneficial for the readers if the authors could provide more specific conditions under which the linear schedule might not be the optimal choice. For instance, the paper mentions that the linear schedule fails to generalize across budgets when the exponential schedule is well-tuned for a specific task. It would be helpful if the authors could elaborate on this point and provide more examples or conditions where other schedules like step decay, exponential decay, poly, cosine, or htd might be more suitable. This would guide readers on when to use the linear schedule and when to consider other schedules."
                },
                {
                    "reference_id": 9,
                    "predicted_id": 8,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The authors' rebuttal did not fully address the concern about the lack of understanding of why the linear scaling schedule is better.",
                    "predicted_comment": "The authors present compelling empirical evidence that smooth-decaying schedules, such as linear or cosine, can achieve superior performance under budgeted training. This conclusion is a significant contribution to the paper's overall argument. However, the paper would benefit from a theoretical analysis that explains why these schedules perform better under budgeted training. Such an analysis would not only strengthen the authors' conclusion but also provide readers with a deeper understanding of the underlying mechanisms at play. Therefore, I recommend that the authors consider including a theoretical analysis in future work or as an addition to this paper."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 8,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper lacks a sound theory explaining why linear decay or other smooth decay methods work well.",
                    "predicted_comment": "The authors present compelling empirical evidence that smooth-decaying schedules, such as linear or cosine, can achieve superior performance under budgeted training. This conclusion is a significant contribution to the paper's overall argument. However, the paper would benefit from a theoretical analysis that explains why these schedules perform better under budgeted training. Such an analysis would not only strengthen the authors' conclusion but also provide readers with a deeper understanding of the underlying mechanisms at play. Therefore, I recommend that the authors consider including a theoretical analysis in future work or as an addition to this paper."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 14,
                "n_generated": 1,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 14,
                "n_generated": 9,
                "n_total_aligns": 4,
                "recall": 0.26666666666666666,
                "precision": 0.1111111111111111,
                "pseudo_jaccard": 0.08383838383838384,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 14,
                "n_generated": 9,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 14,
                "n_generated": 19,
                "n_total_aligns": 4,
                "recall": 0.26666666666666666,
                "precision": 0.05263157894736842,
                "pseudo_jaccard": 0.04811320754716982,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 10
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.3,
                    "precision": 0.2222222222222222,
                    "pseudo_jaccard": 0.15151515151515152,
                    "n_real": 10
                },
                {
                    "recall": 0.5,
                    "precision": 0.1111111111111111,
                    "pseudo_jaccard": 0.1,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 10
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                }
            ],
            "all": [
                {
                    "recall": 0.3,
                    "precision": 0.10526315789473684,
                    "pseudo_jaccard": 0.09433962264150944,
                    "n_real": 10
                },
                {
                    "recall": 0.5,
                    "precision": 0.05263157894736842,
                    "pseudo_jaccard": 0.05,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                }
            ]
        }
    },
    {
        "doc_id": "giit4HdDNa",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "No theoretical or empirical evidence is presented that confirms that the training speed of N-CODE is significantly lower compared to NODE.",
                "While performance seems generally higher, the limitations of the proposed N-CODE method are not clear.",
                "The claim of better performance in less time is not fully supported by the results."
            ],
            [
                "The supervised classification part of the paper is highly related to the 'Data Controlled Neural ODEs' section in Massaroli et al (2020b), a paper that has been cited in the related work section but not addressed in the main content. The problems tackled and the results are highly related to those of Massaroli et al (2020b). This issue needs to be addressed sufficiently.",
                "The unsupervised section needs more work. The experiments and tables can be described more effectively. For example, it would be preferable to clearly explain which row section 5.3 refers to in Figure 7, and which row corresponds to section 5.4.",
                "The paper claims that the replacement of a linear layer with a Neural CODE improves image reconstruction quality. This should mean that all layers in the encoder can be replaced to give better encoding, taking care of dimensionality (such as in Normalizing Flows). However, this paper uses a typical neural network for image reconstruction and generation. This suggests that the majority of heavy-lifting is done by the decoder, hence the Neural CODE is more amenable to warping the latent space suitable for the decoder. For image generation as well, the latent space has been designed so that the decoder can produce nice images, this is not necessarily a win for the Neural CODE.",
                "To make the case for Neural CODE, especially for images, higher resolution images need to be tackled, since it is in higher dimensions that the success of the advancements in the methods listed in Figure 7 lies."
            ],
            [
                "The authors should investigate how the model performance changes as f grows. Learning the parameters of a neural network via another neural network(s) is a very challenging problem, and it would be beneficial to see this verified or not.",
                "The connection with control theory can be made clearer. There is very little reference to Pontryagin's maximum principle and the link is not visible.",
                "The authors should test vanilla NODE on experiments 5.3 and 5.4. It would be interesting to see if the improvement of N-CODE is significant when NODE is latent."
            ],
            [
                "The paper is closely related to other works that have also focused on making the weights of neural ODEs time dependent. A more thorough discussion of the differences between these models is needed to better understand where the novelty/contribution comes in.",
                "The closed loop model, which is the main innovation, is thoroughly discussed in the paper. However, most experiments are performed with the open loop model, which is very similar to other models already proposed in the literature. This puts into question whether the full closed loop model is actually useful in practice.",
                "There is no evaluation of the number of functions evaluations in the paper. According to the abstract, the model trains faster so it seems important to include this information in the paper.",
                "The authors claim that the model achieves state of the art reconstruction on CIFAR10. However, reconstruction doesn\u2019t really make sense as a task (since the identity function would be optimal in this case). This claim is questionable and should be removed."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "Comparative Analysis: The paper provides a comparative analysis with other models and demonstrates the effectiveness of the proposed model. However, the discussion of the comparison results, especially in the context of recent generative techniques, could be more detailed. A more thorough discussion would help to gauge the effectiveness of the proposed model in a broader context and could provide valuable insights for future research.",
                "The authors should discuss potential limitations or shortcomings of their N-CODE model. For instance, they could address potential drawbacks of using a sparse prediction strategy, the implications of the output dimension of the encoder growing quadratically with the desired latent code dimension of the bottleneck, and the impact of simplifying the convolution architecture. Discussing these limitations would provide a more balanced view of the work and help readers understand the trade-offs involved in the model's design.",
                "The paper provides a comprehensive description of the model architecture and training procedure. However, there seems to be an inconsistency in the level of detail provided about the hyperparameters used in the training process across different parts of the paper. To ensure replicability, it would be beneficial to consistently provide specific details about the hyperparameters used in all the experiments conducted. This includes specifying the hyperparameters for all the experiments, as these details are crucial for other researchers to reproduce the results accurately.",
                "While the paper provides evidence of the model's performance on a variety of tasks and datasets, a more comprehensive analysis of the model's robustness across these datasets and tasks would strengthen the paper. Specifically, it would be beneficial to include a comparison of the model's performance on different types of datasets (e.g., image, text, etc.) and tasks (e.g., supervised learning, unsupervised learning, etc.). This would provide a clearer picture of the model's generalizability and its potential limitations. Additionally, it would be helpful to include a discussion of why the chosen datasets and tasks are representative of the broader range of potential applications for the model.",
                "The paper presents a comparison of the performance of N-CODE, NODEs, and a vanilla autoencoder, indicating improvements in the Frechet Inception Distance (FID) over a vanilla autoencoder. However, the paper lacks explicit statistical analysis to support these claims. It is crucial to provide statistical tests and corresponding p-values to ensure that the observed differences are not due to random chance but represent a true difference in performance. Without this analysis, the validity of the results remains uncertain.",
                "The paper presents a variety of experiments and comparisons, which provide valuable insights into the performance of N-CODE. However, it lacks ablation studies that systematically evaluate the contribution of each component of N-CODE to its performance. Conducting such studies would provide a clearer understanding of the importance of each component and could potentially identify areas for further optimization. For example, it would be beneficial to know how much the linear homogeneous differential system contributes to the expressiveness of the model compared to a linear layer, or how the sparse prediction strategy affects the model's performance compared to a VAE with the same architecture. This additional analysis would strengthen the paper by providing a more comprehensive evaluation of the proposed method.",
                "Comparison to a Broader Range of Models: The authors have effectively compared N-CODE to Neural Ordinary Differential Equations (NODEs) and Augmented NODEs, demonstrating its superior performance in various tasks. However, to fully establish the effectiveness of N-CODE, it would be beneficial to compare it to a broader range of current state-of-the-art models. This would provide a more comprehensive understanding of where N-CODE stands in relation to the field as a whole. If N-CODE does not outperform or at least match the performance of these models, this could be a potential shortcoming that needs to be addressed.",
                "Interpretability: The paper presents the N-CODE model and demonstrates its increased expressivity compared to Neural Ordinary Differential Equations (NODEs) and other generative techniques. However, the paper does not discuss the interpretability of the N-CODE model. Given the importance of interpretability in the field of machine learning, especially in the context of continuous-time models for neural networks and unsupervised image representation learning, it would be beneficial for the authors to discuss whether N-CODE maintains or improves interpretability compared to other models. This could involve a comparison with specific models such as NODEs and recent generative techniques. Understanding the model's decisions is crucial for trust in the model's outputs, especially in critical applications.",
                "The paper mentions that N-CODE trains quicker and converges earlier than NODEs variants in an image classification task. However, it lacks specific details or metrics about the computational efficiency of N-CODE. Providing such details would help readers understand the computational cost of implementing N-CODE and how it compares to other models. This is important as it could impact the practical applicability of N-CODE. Therefore, I suggest the authors include a more detailed discussion on the computational efficiency of N-CODE, including specific metrics and a comparison with other models."
            ],
            "novelty_impact": [
                "The paper makes a significant claim in the introduction that Neural Ordinary Differential Equations (NODEs) have representational limitations due to their static weight parametrization, which restricts the type of functions they can learn compared to discrete architectures with layer-dependent weights. However, the paper does not provide empirical evidence or theoretical justification to support this claim. It would be helpful to include such evidence or justification, as it would strengthen the claim and provide a clearer understanding of the limitations of NODEs. This is particularly important given that these limitations could impact the performance of NODEs in various applications.",
                "The paper provides a detailed explanation of the trainable map in Neurally-Controlled ODEs (N-CODE) that governs the dynamic variables from initial or current activation state. However, there are a few areas where the paper could potentially provide more detail or clarity. Specifically, the paper could delve into the specific mathematical properties of the equations governing the model dynamics and the map. It could also explicitly state any assumptions made about the map, such as whether the map is assumed to be differentiable or continuous. Finally, the paper could discuss any potential limitations of the map, such as whether there are any types of data or tasks for which the map might not work well, or any potential issues with overfitting or underfitting. Providing these details would strengthen the paper's claims and make the proposed solution more robust.",
                "The paper lacks crucial details about the supervised learning tasks where N-CODE outperforms NODEs. Specifically, it does not adequately describe the nature of these tasks, such as the type of data used, the complexity of the tasks, and the specific problem domains. Furthermore, while the paper mentions training speed and testing accuracy as performance measures, it does not provide details about how these were calculated or any specific metrics used. The absence of these details hinders the evaluation of the robustness and generalizability of the N-CODE model. It is recommended that the authors provide these details to allow for a more comprehensive understanding and evaluation of the model's performance.",
                "The paper claims that N-CODE leads to state-of-the-art image reconstruction on the CIFAR-10 dataset when applied to an image autoencoder. However, it lacks a direct comparison with other state-of-the-art methods. To substantiate this claim, it would be beneficial to include a comparison with specific state-of-the-art methods such as [insert specific methods here]. This comparison is important to validate the claim and to provide a clear benchmark for the performance of N-CODE.",
                "The paper would benefit from a more detailed explanation of how the model that combines autoencoders and generative normalizing flows works in practice. Specifically, it would be helpful to provide a step-by-step breakdown of how the encoder infers a control parameterizing a linear system, perhaps using a simple example. A visual diagram illustrating the process of how the data-dependent system is solved and used as a low-dimensional representation for the decoder reconstruction would also enhance understanding. Additionally, more information on the sparse prediction strategy and how it contributes to the data-dependency of the latent generative flow would be valuable. These additions would help readers better understand the process and how the latent generative flow is data-dependent and parameterized by the encoder output, enhancing the paper's credibility.",
                "The authors claim that a simple linear homogeneous differential system is more expressive than a linear layer for shaping the latent representation. However, the paper does not provide specific metrics or criteria for evaluating this claim. It would be helpful if the authors could clarify how they measured expressivity in this context. For example, they could provide a definition of expressivity and explain how it relates to the performance metrics used in the paper (e.g., training speed, testing accuracy, image reconstruction quality). They could also compare the expressivity of the linear homogeneous differential system and the linear layer using these metrics. This would make the claim more concrete and easier to evaluate.",
                "The paper mentions a decrease in the Frechet Inception Distance (FID) of the sampled images with an increasing number of components in the mixture, suggesting a structural change in the latent manifold organization. However, the specific method used to measure this decrease is not detailed. Providing this information would help readers understand how the results were obtained. Additionally, while the paper indicates a trade-off between sample quality and generalization of images with different numbers of components, the specific implications of the decrease in FID on the quality of the generated images are not clearly explained. Expanding on this would give readers a better understanding of the significance of the results.",
                "The authors should provide a more detailed comparison of their approach, neurally-controlled ODEs (N-CODE), with other recent generative techniques. Specifically, they should highlight how learning a family of vector fields parameterized by data is a significant departure from previous continuous-time methods. They should also elaborate on the application of this approach in the context of unsupervised image representation learning.",
                "The authors should provide a more explicit outline of their plans for future work. Specifically, they should detail how they plan to investigate the robustness and generalization properties of such controlled models and their similarities with fast-synaptic modulation systems observed in neuroscience. They should also explain how they plan to test this on natural applications such as recurrent neural networks and robotics, and how they intend to explore the connection between their system and the theory of bifurcations in dynamical systems and neuroscience.",
                "The authors have provided specific details on the applications of their model in machine learning, both in supervised and unsupervised learning scenarios. However, it would be beneficial if the authors could further elaborate on the potential future applications of the model, such as its use in recurrent neural networks and robotics, and its connection with the theory of bifurcations in dynamical systems and neuroscience. This would provide a clearer picture of the broader impact and potential of the model.",
                "The exact mechanism of how the N-CODE module circumvents the limitations of NODEs could be explained in more detail. A more detailed explanation or a diagram could help in understanding this better.",
                "More examples or case studies could be provided to demonstrate how the N-CODE module improves the expressivity of NODEs in practical applications.",
                "The paper could elaborate more on how the N-CODE module relates to Hypernetworks. A more detailed comparison could be beneficial.",
                "The authors could elaborate on how their approach improves upon or differs from existing methods in these fields.",
                "The paper could provide more clarity on the practical implications of their work, such as how it could be applied in real-world scenarios or how it advances the state of the art in these fields."
            ],
            "clarity_reproducibility": [
                "The paper provides a high-level understanding of the trainable map in the N-CODE module, particularly in the context of open-loop and closed-loop control. However, for others to fully understand and implement the N-CODE module, more specific details about the implementation and training of this map are needed. This includes the architecture of the neural networks used for the mappings \u03b3 and g, the loss function used, how the gradients are computed, and any hyperparameters or other settings used in the training process. Providing these details would significantly enhance the reproducibility of your work and allow others in the field to build upon it.",
                "The paper lacks explicit mention of the specific metrics used to measure training speed and testing accuracy in the supervised learning context. This omission makes it difficult for others to reproduce the results and compare their methods with yours. To enhance the reproducibility and comparability of your work, please provide a clear description of the metrics used, including any relevant formulas or definitions, and explain why these particular metrics were chosen.",
                "The authors should provide more specific details about the performance and implementation of the AutoN-CODE model in the unsupervised learning section. For instance, it would be helpful to include quantitative results showing how well the model performs in comparison to other models. Additionally, details about the specific implementation of the model, such as the number of layers in the neural network and the specific type of ordinary differential equations used, would provide valuable insight into the practicality of the proposed model. Without these details, it is difficult to assess the effectiveness and feasibility of the AutoN-CODE model.",
                "The paper should provide more specific details about how the sparse prediction strategy is implemented, particularly how exactly the model makes as few as two elements of each row of (\u03b8 i,j ) 1\u2264i,j\u2264n non-zero. These details are crucial for understanding the full workings of the model and for replicating the results.",
                "The paper mentions that the learning rate is reduced by half every time the loss plateaus during training. However, it lacks specific details about how this process is implemented and how the plateau is determined. Providing these details would make it easier for others to reproduce the training process and ensure consistent results. For instance, it would be helpful to know what criteria are used to determine when a plateau has been reached and how the learning rate is adjusted in response.",
                "The paper mentions the use of the Frechet Inception Distance (FID) for evaluation but lacks a detailed explanation of how this metric is calculated. Providing a clear and detailed description of the calculation process, including any specific steps or parameters used, is crucial for readers to fully understand the evaluation metrics used in the paper. This level of detail is also necessary for others to accurately reproduce the results. Without it, the transparency and reproducibility of the research are compromised.",
                "The paper lacks sufficient details about the optimal encoding control formulation in the 'Unsupervised Learning: Image Autoencoding with Controlled Flow' section. Specifically, the paper does not provide specifics about the equations used, the encoding process, or how the control is implemented. This lack of detail makes it difficult for readers to fully understand the method and reproduce the results. Providing these details would not only enhance the clarity of the method but also facilitate reproducibility, which is a key aspect of scientific research.",
                "The paper mentions the similarities between the N-CODE module and fast-synaptic modulation systems observed in neuroscience as potential areas for future research. However, it does not provide specific details about these similarities or explain their relevance to the N-CODE module. Providing these details would help readers understand the significance of these similarities and their potential impact on future research. Therefore, I suggest that the authors include specific details about these similarities and explain their relevance to the N-CODE module in the paper."
            ],
            "all": [
                "Comparative Analysis: The paper provides a comparative analysis with other models and demonstrates the effectiveness of the proposed model. However, the discussion of the comparison results, especially in the context of recent generative techniques, could be more detailed. A more thorough discussion would help to gauge the effectiveness of the proposed model in a broader context and could provide valuable insights for future research.",
                "The authors should discuss potential limitations or shortcomings of their N-CODE model. For instance, they could address potential drawbacks of using a sparse prediction strategy, the implications of the output dimension of the encoder growing quadratically with the desired latent code dimension of the bottleneck, and the impact of simplifying the convolution architecture. Discussing these limitations would provide a more balanced view of the work and help readers understand the trade-offs involved in the model's design.",
                "The paper provides a comprehensive description of the model architecture and training procedure. However, there seems to be an inconsistency in the level of detail provided about the hyperparameters used in the training process across different parts of the paper. To ensure replicability, it would be beneficial to consistently provide specific details about the hyperparameters used in all the experiments conducted. This includes specifying the hyperparameters for all the experiments, as these details are crucial for other researchers to reproduce the results accurately.",
                "While the paper provides evidence of the model's performance on a variety of tasks and datasets, a more comprehensive analysis of the model's robustness across these datasets and tasks would strengthen the paper. Specifically, it would be beneficial to include a comparison of the model's performance on different types of datasets (e.g., image, text, etc.) and tasks (e.g., supervised learning, unsupervised learning, etc.). This would provide a clearer picture of the model's generalizability and its potential limitations. Additionally, it would be helpful to include a discussion of why the chosen datasets and tasks are representative of the broader range of potential applications for the model.",
                "The paper presents a comparison of the performance of N-CODE, NODEs, and a vanilla autoencoder, indicating improvements in the Frechet Inception Distance (FID) over a vanilla autoencoder. However, the paper lacks explicit statistical analysis to support these claims. It is crucial to provide statistical tests and corresponding p-values to ensure that the observed differences are not due to random chance but represent a true difference in performance. Without this analysis, the validity of the results remains uncertain.",
                "The paper presents a variety of experiments and comparisons, which provide valuable insights into the performance of N-CODE. However, it lacks ablation studies that systematically evaluate the contribution of each component of N-CODE to its performance. Conducting such studies would provide a clearer understanding of the importance of each component and could potentially identify areas for further optimization. For example, it would be beneficial to know how much the linear homogeneous differential system contributes to the expressiveness of the model compared to a linear layer, or how the sparse prediction strategy affects the model's performance compared to a VAE with the same architecture. This additional analysis would strengthen the paper by providing a more comprehensive evaluation of the proposed method.",
                "Comparison to a Broader Range of Models: The authors have effectively compared N-CODE to Neural Ordinary Differential Equations (NODEs) and Augmented NODEs, demonstrating its superior performance in various tasks. However, to fully establish the effectiveness of N-CODE, it would be beneficial to compare it to a broader range of current state-of-the-art models. This would provide a more comprehensive understanding of where N-CODE stands in relation to the field as a whole. If N-CODE does not outperform or at least match the performance of these models, this could be a potential shortcoming that needs to be addressed.",
                "Interpretability: The paper presents the N-CODE model and demonstrates its increased expressivity compared to Neural Ordinary Differential Equations (NODEs) and other generative techniques. However, the paper does not discuss the interpretability of the N-CODE model. Given the importance of interpretability in the field of machine learning, especially in the context of continuous-time models for neural networks and unsupervised image representation learning, it would be beneficial for the authors to discuss whether N-CODE maintains or improves interpretability compared to other models. This could involve a comparison with specific models such as NODEs and recent generative techniques. Understanding the model's decisions is crucial for trust in the model's outputs, especially in critical applications.",
                "The paper mentions that N-CODE trains quicker and converges earlier than NODEs variants in an image classification task. However, it lacks specific details or metrics about the computational efficiency of N-CODE. Providing such details would help readers understand the computational cost of implementing N-CODE and how it compares to other models. This is important as it could impact the practical applicability of N-CODE. Therefore, I suggest the authors include a more detailed discussion on the computational efficiency of N-CODE, including specific metrics and a comparison with other models.",
                "The paper makes a significant claim in the introduction that Neural Ordinary Differential Equations (NODEs) have representational limitations due to their static weight parametrization, which restricts the type of functions they can learn compared to discrete architectures with layer-dependent weights. However, the paper does not provide empirical evidence or theoretical justification to support this claim. It would be helpful to include such evidence or justification, as it would strengthen the claim and provide a clearer understanding of the limitations of NODEs. This is particularly important given that these limitations could impact the performance of NODEs in various applications.",
                "The paper provides a detailed explanation of the trainable map in Neurally-Controlled ODEs (N-CODE) that governs the dynamic variables from initial or current activation state. However, there are a few areas where the paper could potentially provide more detail or clarity. Specifically, the paper could delve into the specific mathematical properties of the equations governing the model dynamics and the map. It could also explicitly state any assumptions made about the map, such as whether the map is assumed to be differentiable or continuous. Finally, the paper could discuss any potential limitations of the map, such as whether there are any types of data or tasks for which the map might not work well, or any potential issues with overfitting or underfitting. Providing these details would strengthen the paper's claims and make the proposed solution more robust.",
                "The paper lacks crucial details about the supervised learning tasks where N-CODE outperforms NODEs. Specifically, it does not adequately describe the nature of these tasks, such as the type of data used, the complexity of the tasks, and the specific problem domains. Furthermore, while the paper mentions training speed and testing accuracy as performance measures, it does not provide details about how these were calculated or any specific metrics used. The absence of these details hinders the evaluation of the robustness and generalizability of the N-CODE model. It is recommended that the authors provide these details to allow for a more comprehensive understanding and evaluation of the model's performance.",
                "The paper claims that N-CODE leads to state-of-the-art image reconstruction on the CIFAR-10 dataset when applied to an image autoencoder. However, it lacks a direct comparison with other state-of-the-art methods. To substantiate this claim, it would be beneficial to include a comparison with specific state-of-the-art methods such as [insert specific methods here]. This comparison is important to validate the claim and to provide a clear benchmark for the performance of N-CODE.",
                "The paper would benefit from a more detailed explanation of how the model that combines autoencoders and generative normalizing flows works in practice. Specifically, it would be helpful to provide a step-by-step breakdown of how the encoder infers a control parameterizing a linear system, perhaps using a simple example. A visual diagram illustrating the process of how the data-dependent system is solved and used as a low-dimensional representation for the decoder reconstruction would also enhance understanding. Additionally, more information on the sparse prediction strategy and how it contributes to the data-dependency of the latent generative flow would be valuable. These additions would help readers better understand the process and how the latent generative flow is data-dependent and parameterized by the encoder output, enhancing the paper's credibility.",
                "The authors claim that a simple linear homogeneous differential system is more expressive than a linear layer for shaping the latent representation. However, the paper does not provide specific metrics or criteria for evaluating this claim. It would be helpful if the authors could clarify how they measured expressivity in this context. For example, they could provide a definition of expressivity and explain how it relates to the performance metrics used in the paper (e.g., training speed, testing accuracy, image reconstruction quality). They could also compare the expressivity of the linear homogeneous differential system and the linear layer using these metrics. This would make the claim more concrete and easier to evaluate.",
                "The paper mentions a decrease in the Frechet Inception Distance (FID) of the sampled images with an increasing number of components in the mixture, suggesting a structural change in the latent manifold organization. However, the specific method used to measure this decrease is not detailed. Providing this information would help readers understand how the results were obtained. Additionally, while the paper indicates a trade-off between sample quality and generalization of images with different numbers of components, the specific implications of the decrease in FID on the quality of the generated images are not clearly explained. Expanding on this would give readers a better understanding of the significance of the results.",
                "The authors should provide a more detailed comparison of their approach, neurally-controlled ODEs (N-CODE), with other recent generative techniques. Specifically, they should highlight how learning a family of vector fields parameterized by data is a significant departure from previous continuous-time methods. They should also elaborate on the application of this approach in the context of unsupervised image representation learning.",
                "The authors should provide a more explicit outline of their plans for future work. Specifically, they should detail how they plan to investigate the robustness and generalization properties of such controlled models and their similarities with fast-synaptic modulation systems observed in neuroscience. They should also explain how they plan to test this on natural applications such as recurrent neural networks and robotics, and how they intend to explore the connection between their system and the theory of bifurcations in dynamical systems and neuroscience.",
                "The authors have provided specific details on the applications of their model in machine learning, both in supervised and unsupervised learning scenarios. However, it would be beneficial if the authors could further elaborate on the potential future applications of the model, such as its use in recurrent neural networks and robotics, and its connection with the theory of bifurcations in dynamical systems and neuroscience. This would provide a clearer picture of the broader impact and potential of the model.",
                "The exact mechanism of how the N-CODE module circumvents the limitations of NODEs could be explained in more detail. A more detailed explanation or a diagram could help in understanding this better.",
                "More examples or case studies could be provided to demonstrate how the N-CODE module improves the expressivity of NODEs in practical applications.",
                "The paper could elaborate more on how the N-CODE module relates to Hypernetworks. A more detailed comparison could be beneficial.",
                "The authors could elaborate on how their approach improves upon or differs from existing methods in these fields.",
                "The paper could provide more clarity on the practical implications of their work, such as how it could be applied in real-world scenarios or how it advances the state of the art in these fields.",
                "The paper provides a high-level understanding of the trainable map in the N-CODE module, particularly in the context of open-loop and closed-loop control. However, for others to fully understand and implement the N-CODE module, more specific details about the implementation and training of this map are needed. This includes the architecture of the neural networks used for the mappings \u03b3 and g, the loss function used, how the gradients are computed, and any hyperparameters or other settings used in the training process. Providing these details would significantly enhance the reproducibility of your work and allow others in the field to build upon it.",
                "The paper lacks explicit mention of the specific metrics used to measure training speed and testing accuracy in the supervised learning context. This omission makes it difficult for others to reproduce the results and compare their methods with yours. To enhance the reproducibility and comparability of your work, please provide a clear description of the metrics used, including any relevant formulas or definitions, and explain why these particular metrics were chosen.",
                "The authors should provide more specific details about the performance and implementation of the AutoN-CODE model in the unsupervised learning section. For instance, it would be helpful to include quantitative results showing how well the model performs in comparison to other models. Additionally, details about the specific implementation of the model, such as the number of layers in the neural network and the specific type of ordinary differential equations used, would provide valuable insight into the practicality of the proposed model. Without these details, it is difficult to assess the effectiveness and feasibility of the AutoN-CODE model.",
                "The paper should provide more specific details about how the sparse prediction strategy is implemented, particularly how exactly the model makes as few as two elements of each row of (\u03b8 i,j ) 1\u2264i,j\u2264n non-zero. These details are crucial for understanding the full workings of the model and for replicating the results.",
                "The paper mentions that the learning rate is reduced by half every time the loss plateaus during training. However, it lacks specific details about how this process is implemented and how the plateau is determined. Providing these details would make it easier for others to reproduce the training process and ensure consistent results. For instance, it would be helpful to know what criteria are used to determine when a plateau has been reached and how the learning rate is adjusted in response.",
                "The paper mentions the use of the Frechet Inception Distance (FID) for evaluation but lacks a detailed explanation of how this metric is calculated. Providing a clear and detailed description of the calculation process, including any specific steps or parameters used, is crucial for readers to fully understand the evaluation metrics used in the paper. This level of detail is also necessary for others to accurately reproduce the results. Without it, the transparency and reproducibility of the research are compromised.",
                "The paper lacks sufficient details about the optimal encoding control formulation in the 'Unsupervised Learning: Image Autoencoding with Controlled Flow' section. Specifically, the paper does not provide specifics about the equations used, the encoding process, or how the control is implemented. This lack of detail makes it difficult for readers to fully understand the method and reproduce the results. Providing these details would not only enhance the clarity of the method but also facilitate reproducibility, which is a key aspect of scientific research.",
                "The paper mentions the similarities between the N-CODE module and fast-synaptic modulation systems observed in neuroscience as potential areas for future research. However, it does not provide specific details about these similarities or explain their relevance to the N-CODE module. Providing these details would help readers understand the significance of these similarities and their potential impact on future research. Therefore, I suggest that the authors include specific details about these similarities and explain their relevance to the N-CODE module in the paper."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 1,
                    "predicted_id": 1,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "While performance seems generally higher, the limitations of the proposed N-CODE method are not clear.",
                    "predicted_comment": "The authors should discuss potential limitations or shortcomings of their N-CODE model. For instance, they could address potential drawbacks of using a sparse prediction strategy, the implications of the output dimension of the encoder growing quadratically with the desired latent code dimension of the bottleneck, and the impact of simplifying the convolution architecture. Discussing these limitations would provide a more balanced view of the work and help readers understand the trade-offs involved in the model's design."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 8,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The claim of better performance in less time is not fully supported by the results.",
                    "predicted_comment": "The paper mentions that N-CODE trains quicker and converges earlier than NODEs variants in an image classification task. However, it lacks specific details or metrics about the computational efficiency of N-CODE. Providing such details would help readers understand the computational cost of implementing N-CODE and how it compares to other models. This is important as it could impact the practical applicability of N-CODE. Therefore, I suggest the authors include a more detailed discussion on the computational efficiency of N-CODE, including specific metrics and a comparison with other models."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 8,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "No theoretical or empirical evidence is presented that confirms that the training speed of N-CODE is significantly lower compared to NODE.",
                    "predicted_comment": "The paper mentions that N-CODE trains quicker and converges earlier than NODEs variants in an image classification task. However, it lacks specific details or metrics about the computational efficiency of N-CODE. Providing such details would help readers understand the computational cost of implementing N-CODE and how it compares to other models. This is important as it could impact the practical applicability of N-CODE. Therefore, I suggest the authors include a more detailed discussion on the computational efficiency of N-CODE, including specific metrics and a comparison with other models."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 8,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "There is no evaluation of the number of functions evaluations in the paper. According to the abstract, the model trains faster so it seems important to include this information in the paper.",
                    "predicted_comment": "The paper mentions that N-CODE trains quicker and converges earlier than NODEs variants in an image classification task. However, it lacks specific details or metrics about the computational efficiency of N-CODE. Providing such details would help readers understand the computational cost of implementing N-CODE and how it compares to other models. This is important as it could impact the practical applicability of N-CODE. Therefore, I suggest the authors include a more detailed discussion on the computational efficiency of N-CODE, including specific metrics and a comparison with other models."
                }
            ],
            "novelty_impact": [
                {
                    "reference_id": 0,
                    "predicted_id": 16,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper is closely related to other works that have also focused on making the weights of neural ODEs time dependent. A more thorough discussion of the differences between these models is needed to better understand where the novelty/contribution comes in.",
                    "predicted_comment": "The authors should provide a more detailed comparison of their approach, neurally-controlled ODEs (N-CODE), with other recent generative techniques. Specifically, they should highlight how learning a family of vector fields parameterized by data is a significant departure from previous continuous-time methods. They should also elaborate on the application of this approach in the context of unsupervised image representation learning."
                }
            ],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 1,
                    "predicted_id": 1,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "While performance seems generally higher, the limitations of the proposed N-CODE method are not clear.",
                    "predicted_comment": "The authors should discuss potential limitations or shortcomings of their N-CODE model. For instance, they could address potential drawbacks of using a sparse prediction strategy, the implications of the output dimension of the encoder growing quadratically with the desired latent code dimension of the bottleneck, and the impact of simplifying the convolution architecture. Discussing these limitations would provide a more balanced view of the work and help readers understand the trade-offs involved in the model's design."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 8,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The claim of better performance in less time is not fully supported by the results.",
                    "predicted_comment": "The paper mentions that N-CODE trains quicker and converges earlier than NODEs variants in an image classification task. However, it lacks specific details or metrics about the computational efficiency of N-CODE. Providing such details would help readers understand the computational cost of implementing N-CODE and how it compares to other models. This is important as it could impact the practical applicability of N-CODE. Therefore, I suggest the authors include a more detailed discussion on the computational efficiency of N-CODE, including specific metrics and a comparison with other models."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 8,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "No theoretical or empirical evidence is presented that confirms that the training speed of N-CODE is significantly lower compared to NODE.",
                    "predicted_comment": "The paper mentions that N-CODE trains quicker and converges earlier than NODEs variants in an image classification task. However, it lacks specific details or metrics about the computational efficiency of N-CODE. Providing such details would help readers understand the computational cost of implementing N-CODE and how it compares to other models. This is important as it could impact the practical applicability of N-CODE. Therefore, I suggest the authors include a more detailed discussion on the computational efficiency of N-CODE, including specific metrics and a comparison with other models."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 8,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "There is no evaluation of the number of functions evaluations in the paper. According to the abstract, the model trains faster so it seems important to include this information in the paper.",
                    "predicted_comment": "The paper mentions that N-CODE trains quicker and converges earlier than NODEs variants in an image classification task. However, it lacks specific details or metrics about the computational efficiency of N-CODE. Providing such details would help readers understand the computational cost of implementing N-CODE and how it compares to other models. This is important as it could impact the practical applicability of N-CODE. Therefore, I suggest the authors include a more detailed discussion on the computational efficiency of N-CODE, including specific metrics and a comparison with other models."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 16,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper is closely related to other works that have also focused on making the weights of neural ODEs time dependent. A more thorough discussion of the differences between these models is needed to better understand where the novelty/contribution comes in.",
                    "predicted_comment": "The authors should provide a more detailed comparison of their approach, neurally-controlled ODEs (N-CODE), with other recent generative techniques. Specifically, they should highlight how learning a family of vector fields parameterized by data is a significant departure from previous continuous-time methods. They should also elaborate on the application of this approach in the context of unsupervised image representation learning."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 14,
                "n_generated": 9,
                "n_total_aligns": 4,
                "recall": 0.3125,
                "precision": 0.08333333333333333,
                "pseudo_jaccard": 0.08662280701754385,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 14,
                "n_generated": 15,
                "n_total_aligns": 1,
                "recall": 0.0625,
                "precision": 0.016666666666666666,
                "pseudo_jaccard": 0.013888888888888888,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 14,
                "n_generated": 8,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 14,
                "n_generated": 32,
                "n_total_aligns": 5,
                "recall": 0.375,
                "precision": 0.03125,
                "pseudo_jaccard": 0.03393665158371041,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 1.0,
                    "precision": 0.2222222222222222,
                    "pseudo_jaccard": 0.2631578947368421,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.25,
                    "precision": 0.1111111111111111,
                    "pseudo_jaccard": 0.08333333333333333,
                    "n_real": 4
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.25,
                    "precision": 0.06666666666666667,
                    "pseudo_jaccard": 0.05555555555555555,
                    "n_real": 4
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                }
            ],
            "all": [
                {
                    "recall": 1.0,
                    "precision": 0.0625,
                    "pseudo_jaccard": 0.07692307692307693,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.5,
                    "precision": 0.0625,
                    "pseudo_jaccard": 0.058823529411764705,
                    "n_real": 4
                }
            ]
        }
    },
    {
        "doc_id": "b7ZRqEFXdQ",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper lacks novelty as it combines existing procedures such as relativistic discriminator, Gumbel-Softmax approximation for categorical distribution, and features matching to improve upon vanilla sequence generation networks.",
                "The generated sentences by the GAN, especially the lengthy ones, are not semantically meaningful as illustrated in Table 8.",
                "The Mean Distance Alignment (MDA) and the Mean Square alignment (MSA) used for features distribution alignment encode the same matching up to a power 2, and it's unclear why they lead to different empirical results.",
                "The paper should consider aligning the overall distributions of the latent representations by considering metrics such as MMD or Wasserstein distance.",
                "The paper should clarify earlier that the used features are extracted from the discrimination network and from which layer of the discrimination network the features are extracted.",
                "The findings of human evaluation are not unequivocal as MSA and MDA achieve higher scores than the real sentences, and the best model with MSA is not preferred due to a lack of diversity and quality."
            ],
            [
                "The authors claim that the use of Gumbel-softmax in GAN training is under-explored, but this is not clear. There are other methods using Gumbel-softmax, such as Gu et al. 2019, and similar softmax with temperature annealing. The authors need to clearly distinguish between using Gumbel-softmax and other smoothed softmax methods.",
                "The authors missed some related work, such as DialogWAE (Gu et al. 2019) and ARAML (Ke et al. 2019). In particular, DialogWAE uses GAN and Gumbel-softmax for text generation, even though it focuses on dialog generation.",
                "To verify mode collapse issues, the authors should consider using Self-BLUE in addition to BLUE scores as a metric to evaluate the diversity.",
                "The novelty of the paper might be incremental. It seems that the novelty comes from using feature statistics alignment. To emphasize the contribution of feature statistics, the authors should consider comparing between the latent feature visualization with and without FSA, in addition to the ablation study."
            ],
            [
                "Although the proposed model is thoroughly evaluated and empirically effective, it is not very different from existing methods, except for the Feature Statistics Alignment (FSA). The application of FSA in this context might be novel; however, the proposed approach seems to be a simple combination of two existing approaches. Therefore, the novelty of the model is limited."
            ],
            [
                "In Table 2, the LSTM model gets NLL lower than the real data, which is a clear evidence of overfitting.",
                "In Table 2, the NLL_{gen} metric, used in SAL (Zhou et. al, 2020) to evaluate the diversity of the generator, is missing without explanation.",
                "In Table 3, the BLEU metric used is the BLEU(F) metric from SAL (Zhou et. al, 2020), which evaluates the generated sentences using the test set as a reference and thus cannot show the diversity of examples.",
                "In Table 3, the BLEU(B) metric from SAL (Zhou et. al, 2020) is missing. This metric evaluates the test set using the generated sentences as a reference, so it can detect mode collapse of a generative model.",
                "The evaluation metrics used in the paper prefer overfitted models, i.e., if the model can remember one example from the training set and repeat that sentence, it can get a very high score.",
                "The authors need to provide more evidence on the diversity of the generated sentences, for example by showing the NLL_{gen} metric on the synthetic dataset, and BLEU(B) metric on real datasets."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper lacks a detailed ablation study to demonstrate the effectiveness of each component of the proposed method. While the 'B NEGATIVE RESULTS' section lists several unsuccessful approaches, it does not provide sufficient information on how these changes affected the overall performance of the system. This is a significant shortcoming as it prevents readers from understanding the contribution of each component to the overall performance. To address this issue, the authors should conduct a detailed ablation study where they systematically remove or alter each component of the proposed method and measure the impact on performance. This should include, but not be limited to, the types of generators, loss functions, models for the discriminator, and training methods used.",
                "The paper provides a comparison of the proposed method primarily with other GAN-based methods. While this is valuable, it would be beneficial to expand the comparison to include non-GAN-based methods, such as MLE and RL techniques, which are mentioned in the paper. This would provide a more comprehensive evaluation of the proposed method's performance across a broader range of methods, not just GAN-based ones. By doing so, the authors could potentially highlight the strengths of their method over a wider range of existing methods, thereby strengthening their argument and making their contribution more significant."
            ],
            "novelty_impact": [
                "While the paper provides a comparison between the proposed model and several existing GANs, it would be beneficial to include a more detailed discussion on the specific advantages and potential drawbacks of the proposed model. For instance, the paper could delve deeper into how the proposed model's Feature Statistics Alignment and Gumbel-Softmax relaxation for discrete sequence generation compare to the methods used in the referenced GANs. Additionally, the paper could discuss potential drawbacks of the proposed model, such as any limitations observed during the experiments or potential issues that could arise in different contexts or datasets.",
                "The paper presents results on synthetic data, the MS COCO Image Caption dataset, and the EMNLP WMT 2017 News dataset. However, it would be beneficial to discuss how the model might perform on other types of datasets. For instance, considering datasets with different characteristics such as those with more complex structures or those from different domains could provide insights into the model's generalizability. This would help readers understand the potential limitations and applicability of the model.",
                "The paper uses the negative log-likelihood (NLL) and BLEU score metrics to interpret the results, and also uses human evaluation as an additional metric. However, the paper does not discuss the limitations of the NLL and BLEU score metrics. It would be beneficial to include a discussion of these limitations, as it would provide a more nuanced understanding of the results. For example, the NLL metric measures the diversity of the generated sequences, but it may not capture other important aspects of the sequences. Similarly, the BLEU score evaluates the n-gram statistics overlapping on the whole dataset, but it may not fully capture the quality of the generated texts. Discussing these limitations would help readers better interpret the results and understand the strengths and weaknesses of the proposed framework. This is a major comment as these metrics are central to the evaluation of the proposed model's performance, and understanding their limitations is crucial for a comprehensive assessment of the model.",
                "The paper presents a novel GAN for sequence generation that uses a Feature Statistics Alignment (FSA) paradigm and a relativistic discriminator, and claims that this model outperforms several other models including MLE baseline, SeqGAN, RankGAN, LeakGAN, RelGAN, and Self-Adversarial Learning (SAL). However, the paper does not provide statistical significance values to support this claim. Providing these values would allow readers to better understand the performance of the proposed model and make the paper's claims more credible. Therefore, the authors should include statistical significance values for the comparisons with each of these models, both for the synthetic and real datasets used in the evaluation.",
                "While the paper does discuss several limitations of GANs for sequence generation and the proposed model, it does not explicitly discuss any assumptions made in the study. For instance, it is assumed that the FSA paradigm and the Gumbel-Softmax trick are effective methods for addressing the limitations of GANs, but this is not explicitly stated. Additionally, the paper assumes that LSTMs may forget the long-term dependencies as the sequence length increases, but this is not thoroughly discussed. It would be beneficial for the authors to explicitly discuss these assumptions and any others that were made in the study, as this would provide a more comprehensive understanding of the study's context and potential limitations.",
                "While the paper mentions that the discriminator is easy to be overtrained, which might be the reason for not pretraining the discriminator but only pretraining the generator using MLE for a few epochs, it would be beneficial to provide a more detailed explanation on this. Specifically, it would be helpful to elaborate on how this approach helps in addressing the issues of mode collapse and training instability in GANs. This would strengthen the paper by providing a clearer understanding of the methodology and its effectiveness.",
                "The paper lacks specific information about the computational requirements of the proposed model. This information is crucial for researchers who wish to reproduce the results or apply the model in a practical setting. While the paper mentions that testing the batch size to 256 required too much GPU resource, it does not provide further details such as the exact hardware specifications used, the memory requirements, or the computational time. Providing these details would greatly enhance the paper.",
                "The paper provides some information on the performance of the LSTM and RMC generators on sequences of different lengths. However, it would be beneficial to include more detailed performance analysis, such as graphs or tables, that show how these generators perform on sequences of different lengths. This would provide a clearer understanding of the strengths and weaknesses of each generator and help readers understand the trade-offs involved in choosing one over the other.",
                "The paper provides a detailed discussion of the model's applications in sequence generation tasks and how it addresses various challenges. However, when suggesting future directions like extending the model to conditional text generation, such as text style transfer, the paper does not discuss potential challenges or how the model could address them. Discussing these aspects in the context of the proposed future directions could provide valuable insights for readers and make the paper more impactful.",
                "The paper currently does not provide information on whether the code for the model is available. This is a significant omission as the availability of the code would allow other researchers to reproduce the results, understand the model better, and possibly extend it. If the code is not available, it would be helpful for the authors to discuss the reasons for this, as understanding these reasons can provide insights into the challenges faced during the development of the model."
            ],
            "clarity_reproducibility": [
                "The paper mentions the Feature Statistics Alignment (FSA) paradigm and its role in forcing the mean statistics of the fake data distribution to approach that of real data. However, it lacks specific details on how this process is achieved and its impact on the data distribution. Providing a more detailed explanation or a separate section discussing the FSA paradigm, its workings, and its role in the model would enhance the reader's understanding and the paper's clarity. This is particularly important as the FSA paradigm seems to play a significant role in the model's performance.",
                "The paper provides some information about the relativistic discriminator and how it compares the fake and real distributions. However, the specific mechanism of how the Relativistic Discriminator compares the fake and real distributions is not detailed. It would be helpful to elaborate on this mechanism, explaining how the discriminator estimates the relative confidence that the given real data is more realistic than the randomly sampled fake data, and how this contributes to the adversarial training process. This would provide a clearer understanding of the method and its effectiveness.",
                "The paper provides a general explanation of how the softmax temperature \u03c4 in the Gumbel-Softmax distribution encourages the generator to explore different options when it is high and tends to exploit during training when it is low. However, it would be beneficial to provide more specific details on this process. For instance, the paper could elaborate on how the approximation becomes nearly equiprobable when \u03c4 is high and how it resembles a one-hot operator when \u03c4 approaches 0. This would help readers better understand the role of \u03c4 in the Gumbel-Softmax distribution and its impact on the balance between exploration and exploitation during training.",
                "The paper provides a general overview of how the BLEU score, NLL of the generator, and human evaluation via crowdsourcing were used to assess the quality and diversity of the generated sequences. However, more specific details about these evaluation methods would strengthen the paper. For example, the paper could explain how the BLEU score was calculated and what specific aspects of the generated sequences it was used to assess. Similarly, the paper could provide more details about how the NLL of the generator was calculated and what it indicates about the quality and diversity of the generated sequences. The paper could also provide more information about the human evaluation via crowdsourcing, such as how the evaluators were selected, what instructions they were given, and how their evaluations were used to assess the quality and diversity of the generated sequences. Providing these details would make it easier for readers to understand the evaluation methods and interpret the results.",
                "The paper provides some insight into the performance of LSTM and RMC generators, indicating that the choice between these generators could be influenced by the length of the sequence that needs to be generated. However, the paper does not clearly explain the specific task requirements that should be considered when choosing between these generators. It would be helpful if the authors could provide more details on this, such as what types of tasks might benefit from the use of LSTM generators versus RMC generators, and why. This would make it easier for readers to understand how to choose the most appropriate generator for their specific tasks.",
                "While the paper briefly mentions the potential for utilizing higher-order statistics and extending the proposed model to conditional text generation, these points could be discussed in more detail. Specifically, the authors could elaborate on how higher-order statistics could be incorporated into the framework and what challenges might be encountered. Similarly, the authors could provide more detail on how the model could be extended to conditional text generation, such as text style transfer, and what benefits this could bring.",
                "The paper provides specific details about the parameters of the Adam optimizer, the learning rates for the generator and discriminator, and the batch sizes for both synthetic and real datasets. However, the paper could provide more details on why the specific parameters of the Adam optimizer and the learning rates for the generator and discriminator were chosen. The experiments or considerations that led to these choices could be explained more clearly. This would help readers understand the rationale behind these choices and assess the robustness of the results. The choice of batch size is well justified based on resource constraints and lack of improvement with a larger size.",
                "The 'Generative Adversarial Networks (GAN)' section could benefit from a simple analogy or a diagram to explain how the generator and discriminator work together in a GAN. A brief overview of the history and development of GANs would provide readers with more context.",
                "In the 'Feature Statistics Alignment (FSA)' section, a step-by-step explanation of how FSA works, perhaps using a simple example, would be beneficial. It would also be helpful to explain why FSA is beneficial in the context of GANs.",
                "The 'Adversarial Sequence Generation' section could provide more examples of applications that benefit from adversarial sequence generation. The exposure bias issue could be explained in more detail, perhaps using a diagram or a simple example.",
                "In the 'Methodology' section, a flowchart or diagram to illustrate how the FSA techniques and relativistic discriminator work together to improve GANs for sequence generation would be useful. A step-by-step walkthrough of the process would also be beneficial.",
                "The 'Gumbel-Softmax Distribution' section could provide a simple, intuitive explanation of what the Gumbel-Softmax distribution is and why it is useful in the context of GANs. A diagram or graph to illustrate the distribution would also be helpful.",
                "In the 'ARCHITECTURE AND ADVERSARIAL TRAINING' section, a brief introduction to adversarial training and Nash Equilibrium, perhaps with a simple example or analogy, would help readers understand these concepts.",
                "The 'SYNTHETIC DATA' section could provide a more detailed explanation of MSA and MDA, and perhaps include a step-by-step walkthrough of how the performance of different models on NLL oracle was evaluated.",
                "In the 'MS COCO DATASET' section, it would be helpful to explain what BLEU scores and NLL gen are, and how they are used to evaluate the model. A more detailed explanation of the evaluation process on the MS COCO image caption datasets would also be beneficial.",
                "The 'EMNLP2017 WMT NEWS DATASET' section could provide a more detailed explanation of the Gumbel-Softmax trick, and a step-by-step walkthrough of how the model was evaluated on the EMNLP2017 WMT News dataset.",
                "In the 'A.1 TRAINING DETAILS' section, a more detailed explanation of the training process for the generator and discriminator, perhaps with a flowchart or diagram, would help visualize the process.",
                "The 'CNN Discriminator' section could provide a more detailed explanation of the architecture and optimization of the CNN discriminator, perhaps with a diagram to help visualize the architecture.",
                "In the 'B NEGATIVE RESULTS' section, a more detailed explanation of the unsuccessful approaches that were tried during the research, perhaps with a brief discussion of why each approach was unsuccessful, would be beneficial.",
                "The 'C.2 HUMAN EVALUATION ANALYSIS' section could provide a more detailed explanation of the human evaluation of the generated sentences, perhaps with examples of the sentences and the feedback they received.",
                "In the 'D DETAILED RESULTS' section, a more detailed explanation of the model's performance, perhaps with charts or graphs, would help visualize the results.",
                "The 'E.2 GENERATED SAMPLES ON EMNLP2017 WMT NEWS DATASET' section could provide a more detailed explanation of the generated samples from the model on the EMNLP2017 WMT News Dataset, perhaps with examples of the samples and a discussion of their quality.",
                "The paper could benefit from a discussion on the potential limitations or weaknesses of the proposed GAN framework for sequence generation. For instance, the complexity of the model, the need for large batch sizes to reduce variance, and the reliance on the Gumbel-Softmax trick, which is still under-explored, could be addressed. Additionally, the paper could discuss the performance of the RMC generator for shorter sequences and the issue of mode dropping when generating long sentences. This would provide a more balanced view of the work and help readers understand the potential challenges and areas for improvement."
            ],
            "all": [
                "The paper lacks a detailed ablation study to demonstrate the effectiveness of each component of the proposed method. While the 'B NEGATIVE RESULTS' section lists several unsuccessful approaches, it does not provide sufficient information on how these changes affected the overall performance of the system. This is a significant shortcoming as it prevents readers from understanding the contribution of each component to the overall performance. To address this issue, the authors should conduct a detailed ablation study where they systematically remove or alter each component of the proposed method and measure the impact on performance. This should include, but not be limited to, the types of generators, loss functions, models for the discriminator, and training methods used.",
                "The paper provides a comparison of the proposed method primarily with other GAN-based methods. While this is valuable, it would be beneficial to expand the comparison to include non-GAN-based methods, such as MLE and RL techniques, which are mentioned in the paper. This would provide a more comprehensive evaluation of the proposed method's performance across a broader range of methods, not just GAN-based ones. By doing so, the authors could potentially highlight the strengths of their method over a wider range of existing methods, thereby strengthening their argument and making their contribution more significant.",
                "While the paper provides a comparison between the proposed model and several existing GANs, it would be beneficial to include a more detailed discussion on the specific advantages and potential drawbacks of the proposed model. For instance, the paper could delve deeper into how the proposed model's Feature Statistics Alignment and Gumbel-Softmax relaxation for discrete sequence generation compare to the methods used in the referenced GANs. Additionally, the paper could discuss potential drawbacks of the proposed model, such as any limitations observed during the experiments or potential issues that could arise in different contexts or datasets.",
                "The paper presents results on synthetic data, the MS COCO Image Caption dataset, and the EMNLP WMT 2017 News dataset. However, it would be beneficial to discuss how the model might perform on other types of datasets. For instance, considering datasets with different characteristics such as those with more complex structures or those from different domains could provide insights into the model's generalizability. This would help readers understand the potential limitations and applicability of the model.",
                "The paper uses the negative log-likelihood (NLL) and BLEU score metrics to interpret the results, and also uses human evaluation as an additional metric. However, the paper does not discuss the limitations of the NLL and BLEU score metrics. It would be beneficial to include a discussion of these limitations, as it would provide a more nuanced understanding of the results. For example, the NLL metric measures the diversity of the generated sequences, but it may not capture other important aspects of the sequences. Similarly, the BLEU score evaluates the n-gram statistics overlapping on the whole dataset, but it may not fully capture the quality of the generated texts. Discussing these limitations would help readers better interpret the results and understand the strengths and weaknesses of the proposed framework. This is a major comment as these metrics are central to the evaluation of the proposed model's performance, and understanding their limitations is crucial for a comprehensive assessment of the model.",
                "The paper presents a novel GAN for sequence generation that uses a Feature Statistics Alignment (FSA) paradigm and a relativistic discriminator, and claims that this model outperforms several other models including MLE baseline, SeqGAN, RankGAN, LeakGAN, RelGAN, and Self-Adversarial Learning (SAL). However, the paper does not provide statistical significance values to support this claim. Providing these values would allow readers to better understand the performance of the proposed model and make the paper's claims more credible. Therefore, the authors should include statistical significance values for the comparisons with each of these models, both for the synthetic and real datasets used in the evaluation.",
                "While the paper does discuss several limitations of GANs for sequence generation and the proposed model, it does not explicitly discuss any assumptions made in the study. For instance, it is assumed that the FSA paradigm and the Gumbel-Softmax trick are effective methods for addressing the limitations of GANs, but this is not explicitly stated. Additionally, the paper assumes that LSTMs may forget the long-term dependencies as the sequence length increases, but this is not thoroughly discussed. It would be beneficial for the authors to explicitly discuss these assumptions and any others that were made in the study, as this would provide a more comprehensive understanding of the study's context and potential limitations.",
                "While the paper mentions that the discriminator is easy to be overtrained, which might be the reason for not pretraining the discriminator but only pretraining the generator using MLE for a few epochs, it would be beneficial to provide a more detailed explanation on this. Specifically, it would be helpful to elaborate on how this approach helps in addressing the issues of mode collapse and training instability in GANs. This would strengthen the paper by providing a clearer understanding of the methodology and its effectiveness.",
                "The paper lacks specific information about the computational requirements of the proposed model. This information is crucial for researchers who wish to reproduce the results or apply the model in a practical setting. While the paper mentions that testing the batch size to 256 required too much GPU resource, it does not provide further details such as the exact hardware specifications used, the memory requirements, or the computational time. Providing these details would greatly enhance the paper.",
                "The paper provides some information on the performance of the LSTM and RMC generators on sequences of different lengths. However, it would be beneficial to include more detailed performance analysis, such as graphs or tables, that show how these generators perform on sequences of different lengths. This would provide a clearer understanding of the strengths and weaknesses of each generator and help readers understand the trade-offs involved in choosing one over the other.",
                "The paper provides a detailed discussion of the model's applications in sequence generation tasks and how it addresses various challenges. However, when suggesting future directions like extending the model to conditional text generation, such as text style transfer, the paper does not discuss potential challenges or how the model could address them. Discussing these aspects in the context of the proposed future directions could provide valuable insights for readers and make the paper more impactful.",
                "The paper currently does not provide information on whether the code for the model is available. This is a significant omission as the availability of the code would allow other researchers to reproduce the results, understand the model better, and possibly extend it. If the code is not available, it would be helpful for the authors to discuss the reasons for this, as understanding these reasons can provide insights into the challenges faced during the development of the model.",
                "The paper mentions the Feature Statistics Alignment (FSA) paradigm and its role in forcing the mean statistics of the fake data distribution to approach that of real data. However, it lacks specific details on how this process is achieved and its impact on the data distribution. Providing a more detailed explanation or a separate section discussing the FSA paradigm, its workings, and its role in the model would enhance the reader's understanding and the paper's clarity. This is particularly important as the FSA paradigm seems to play a significant role in the model's performance.",
                "The paper provides some information about the relativistic discriminator and how it compares the fake and real distributions. However, the specific mechanism of how the Relativistic Discriminator compares the fake and real distributions is not detailed. It would be helpful to elaborate on this mechanism, explaining how the discriminator estimates the relative confidence that the given real data is more realistic than the randomly sampled fake data, and how this contributes to the adversarial training process. This would provide a clearer understanding of the method and its effectiveness.",
                "The paper provides a general explanation of how the softmax temperature \u03c4 in the Gumbel-Softmax distribution encourages the generator to explore different options when it is high and tends to exploit during training when it is low. However, it would be beneficial to provide more specific details on this process. For instance, the paper could elaborate on how the approximation becomes nearly equiprobable when \u03c4 is high and how it resembles a one-hot operator when \u03c4 approaches 0. This would help readers better understand the role of \u03c4 in the Gumbel-Softmax distribution and its impact on the balance between exploration and exploitation during training.",
                "The paper provides a general overview of how the BLEU score, NLL of the generator, and human evaluation via crowdsourcing were used to assess the quality and diversity of the generated sequences. However, more specific details about these evaluation methods would strengthen the paper. For example, the paper could explain how the BLEU score was calculated and what specific aspects of the generated sequences it was used to assess. Similarly, the paper could provide more details about how the NLL of the generator was calculated and what it indicates about the quality and diversity of the generated sequences. The paper could also provide more information about the human evaluation via crowdsourcing, such as how the evaluators were selected, what instructions they were given, and how their evaluations were used to assess the quality and diversity of the generated sequences. Providing these details would make it easier for readers to understand the evaluation methods and interpret the results.",
                "The paper provides some insight into the performance of LSTM and RMC generators, indicating that the choice between these generators could be influenced by the length of the sequence that needs to be generated. However, the paper does not clearly explain the specific task requirements that should be considered when choosing between these generators. It would be helpful if the authors could provide more details on this, such as what types of tasks might benefit from the use of LSTM generators versus RMC generators, and why. This would make it easier for readers to understand how to choose the most appropriate generator for their specific tasks.",
                "While the paper briefly mentions the potential for utilizing higher-order statistics and extending the proposed model to conditional text generation, these points could be discussed in more detail. Specifically, the authors could elaborate on how higher-order statistics could be incorporated into the framework and what challenges might be encountered. Similarly, the authors could provide more detail on how the model could be extended to conditional text generation, such as text style transfer, and what benefits this could bring.",
                "The paper provides specific details about the parameters of the Adam optimizer, the learning rates for the generator and discriminator, and the batch sizes for both synthetic and real datasets. However, the paper could provide more details on why the specific parameters of the Adam optimizer and the learning rates for the generator and discriminator were chosen. The experiments or considerations that led to these choices could be explained more clearly. This would help readers understand the rationale behind these choices and assess the robustness of the results. The choice of batch size is well justified based on resource constraints and lack of improvement with a larger size.",
                "The 'Generative Adversarial Networks (GAN)' section could benefit from a simple analogy or a diagram to explain how the generator and discriminator work together in a GAN. A brief overview of the history and development of GANs would provide readers with more context.",
                "In the 'Feature Statistics Alignment (FSA)' section, a step-by-step explanation of how FSA works, perhaps using a simple example, would be beneficial. It would also be helpful to explain why FSA is beneficial in the context of GANs.",
                "The 'Adversarial Sequence Generation' section could provide more examples of applications that benefit from adversarial sequence generation. The exposure bias issue could be explained in more detail, perhaps using a diagram or a simple example.",
                "In the 'Methodology' section, a flowchart or diagram to illustrate how the FSA techniques and relativistic discriminator work together to improve GANs for sequence generation would be useful. A step-by-step walkthrough of the process would also be beneficial.",
                "The 'Gumbel-Softmax Distribution' section could provide a simple, intuitive explanation of what the Gumbel-Softmax distribution is and why it is useful in the context of GANs. A diagram or graph to illustrate the distribution would also be helpful.",
                "In the 'ARCHITECTURE AND ADVERSARIAL TRAINING' section, a brief introduction to adversarial training and Nash Equilibrium, perhaps with a simple example or analogy, would help readers understand these concepts.",
                "The 'SYNTHETIC DATA' section could provide a more detailed explanation of MSA and MDA, and perhaps include a step-by-step walkthrough of how the performance of different models on NLL oracle was evaluated.",
                "In the 'MS COCO DATASET' section, it would be helpful to explain what BLEU scores and NLL gen are, and how they are used to evaluate the model. A more detailed explanation of the evaluation process on the MS COCO image caption datasets would also be beneficial.",
                "The 'EMNLP2017 WMT NEWS DATASET' section could provide a more detailed explanation of the Gumbel-Softmax trick, and a step-by-step walkthrough of how the model was evaluated on the EMNLP2017 WMT News dataset.",
                "In the 'A.1 TRAINING DETAILS' section, a more detailed explanation of the training process for the generator and discriminator, perhaps with a flowchart or diagram, would help visualize the process.",
                "The 'CNN Discriminator' section could provide a more detailed explanation of the architecture and optimization of the CNN discriminator, perhaps with a diagram to help visualize the architecture.",
                "In the 'B NEGATIVE RESULTS' section, a more detailed explanation of the unsuccessful approaches that were tried during the research, perhaps with a brief discussion of why each approach was unsuccessful, would be beneficial.",
                "The 'C.2 HUMAN EVALUATION ANALYSIS' section could provide a more detailed explanation of the human evaluation of the generated sentences, perhaps with examples of the sentences and the feedback they received.",
                "In the 'D DETAILED RESULTS' section, a more detailed explanation of the model's performance, perhaps with charts or graphs, would help visualize the results.",
                "The 'E.2 GENERATED SAMPLES ON EMNLP2017 WMT NEWS DATASET' section could provide a more detailed explanation of the generated samples from the model on the EMNLP2017 WMT News Dataset, perhaps with examples of the samples and a discussion of their quality.",
                "The paper could benefit from a discussion on the potential limitations or weaknesses of the proposed GAN framework for sequence generation. For instance, the complexity of the model, the need for large batch sizes to reduce variance, and the reliance on the Gumbel-Softmax trick, which is still under-explored, could be addressed. Additionally, the paper could discuss the performance of the RMC generator for shorter sequences and the issue of mode dropping when generating long sentences. This would provide a more balanced view of the work and help readers understand the potential challenges and areas for improvement."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [],
            "clarity_reproducibility": [],
            "all": []
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 17,
                "n_generated": 2,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 17,
                "n_generated": 10,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "clarity_reproducibility": {
                "n_reference": 17,
                "n_generated": 23,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 17,
                "n_generated": 35,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 1
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                }
            ]
        }
    },
    {
        "doc_id": "zCu1BZYCueE",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The experiments focus only on selecting a single initial learning rate, which is less interesting because normally a learning rate scheduler that changes over time is used. The experiment does not consider different types of learning rate schedulers for comparisons.",
                "The learning rate is just a scalar, so a simple grid search could be used to coarsely find a good learning rate and then refine it. This makes the focus on learning rate less impactful.",
                "The work would be more impactful if it demonstrated improvement in other hyper-parameter optimizations such as weight decay and convolution filter size and channels."
            ],
            [
                "The paper lacks comparisons to baseline hyperparameter tuning setups. For example, in Figure 3, the best final validation and/or test accuracies achieved by autoHyper and by random search for the same number of trials are not shown.",
                "The paper does not compare the proposed algorithm to existing state-of-the-art methods, which would be useful for a solid paper.",
                "The suggested initial learning rates (LRs) seem problematic to compare to. It would be useful to highlight how the suggested initial LRs were tuned.",
                "The suggested initial LRs were tuned using less trials than the proposed algorithm, and using a grid search algorithm that may be worse than random search.",
                "The suggested initial LRs were for a different model than the models considered in the experiments here, which means they could serve as a poor baseline.",
                "The ranges of values tuned over in Wilson 2017 seem to be quite close to several of the values proposed by autoHyper, meaning that perhaps repeating the same grid search could be competitive.",
                "In the cases where the autoHyper values are not close, they are sometimes outside the range considered by Wilson 2017, which could bias the results towards the proposed algorithm.",
                "Several recent works have shown that when tuning optimizers one needs to be careful to report the ranges used, as changing the hyperparameter ranges can drastically affect experimental results.",
                "Figure 1a would be much more informative if you showed the entire training trajectory, including past the first 5 epochs, to see if the selected learning rates actually generalize noticeably better.",
                "In your conclusion you discuss that you could extend your tuning algorithm to multiple hyperparameters, and I believe in order to truly demonstrate its capabilities this would be required."
            ],
            [
                "The algorithm has an implicit assumption that the response function is monotonously decreasing. The paper should clarify what happens if the response function does not decrease, and how the range of the search area is chosen in such cases.",
                "The algorithm introduces new hyperparameters, such as alpha and the 'rate of change plateaus'. The paper should explain how these hyperparameters are chosen and how they influence the result.",
                "The paper should provide a reasonable explanation for why the initial learning rate influences the testing accuracy, not just the convergence rate for training loss.",
                "The paper only compares the algorithm with the baseline. The authors should discuss if there are any other works about tuning the initial learning rate, or if this paper is the first of its kind.",
                "The reviewer suggests that it may be more convincing if the whole parameters are chosen together, not just the initial learning rate."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The authors have developed an innovative dynamic tracking algorithm, autoHyper, which iteratively adjusts the learning rate based on the response surface model. However, the paper does not explicitly mention conducting multiple trials of learning rates or accounting for random initialization variations. While the autoHyper algorithm may indirectly account for these variations, it would be beneficial for the authors to explicitly conduct multiple trials of learning rates and account for random initialization variations. This would not only ensure the robustness of the results but also provide a more accurate representation of the performance of the autoHyper algorithm. Furthermore, it would address the potential susceptibility of the autoHyper method to random initialization variations, thereby enhancing the reliability of the results.",
                "While the paper provides a detailed description of the response surface model used by autoHyper and explains why it works, it would be beneficial to include specific suggestions for how this model could be improved. This could potentially lead to further improvements in the algorithm and would provide readers with a clearer understanding of the potential for future work in this area.",
                "While the paper presents a comprehensive set of experiments demonstrating the performance of the autoHyper algorithm across various datasets, network architectures, and optimizers, it lacks details on the conditions under which these experiments were conducted. Specifically, it would be beneficial to include information on the different initializations, noise levels in the function evaluations, and computational budgets used. This would provide a more robust evaluation of the algorithm's performance and its ability to generalize across different conditions. Additionally, the paper mentions a potential limitation of the method being susceptible to random initialization variations. It would be helpful to see how the method performs under different initializations to address this concern.",
                "The paper presents a comparison of the time efficiency of autoHyper with other methods, which is appreciated. However, to strengthen the claim of time efficiency, it would be beneficial to include a more comprehensive comparison. Specifically, the authors should consider comparing autoHyper's time efficiency with other existing HPO methods across a variety of network architectures. This would provide a more objective and quantifiable measure of autoHyper's efficiency, serving as a stronger validation of the authors' claims.",
                "While the authors have used a wide variety of models, optimizers, and datasets in their experiments, which should ensure the generalizability of the results, the specific details about these were not initially apparent. The authors should consider improving the clarity and organization of their paper to ensure that all important details are easily accessible to the reader.",
                "The authors have proposed a method that emphasizes computational efficiency, which is crucial for scalability. However, the paper lacks an ablation study that tests the scalability of the method by gradually increasing the number of hyperparameters being optimized. Conducting such a study would provide a clearer picture of the method's scalability and its ability to handle larger datasets and more complex models without a prohibitive increase in computational resources or time. This is particularly important given the authors' emphasis on computational efficiency."
            ],
            "novelty_impact": [
                "The paper discusses the use of low-rank factorization of the convolution weights of intermediate layers for optimizing the initial learning rate, but the explanation and justification for this method are not detailed enough. The authors should provide a more comprehensive theoretical basis for this choice, including how it compares to other methods in the related literature, and why it is particularly suited for the problem at hand. This will help readers better understand the rationale behind the method and its relevance to the field.",
                "The paper does not explicitly mention any hyperparameters that need to be set specifically for the low-rank factorization method. While it is possible that the method does not require any additional hyperparameters beyond those already mentioned in the paper, such as the initial learning rate, this point is not clear. Almost all optimization methods require some form of hyperparameter tuning, and the lack of explicit mention of this for the low-rank factorization method could potentially mislead readers or make it difficult for them to replicate the method. The authors should clarify whether any specific hyperparameters need to be set for this method, and if so, what they are and how they were chosen.",
                "While the authors have conducted a series of experiments using various network architectures, optimizers, and datasets, it may be beneficial to further validate the claim of generalizability by testing the method on additional types of neural networks and data. For instance, testing on recurrent neural networks or transformer-based models, and on non-image datasets, could provide a more comprehensive validation of the method's generalizability. This would strengthen the claim and provide more confidence in the method's applicability across a wider range of scenarios.",
                "While the paper provides some details about the computational resources used and the time taken for the method to converge on specific tasks, it lacks crucial information on how the computational cost of the method scales with the complexity of the model and the size of the dataset. For instance, it is unclear how the computational cost would change if a more complex model was used or if the size of the dataset was increased. This information is essential for understanding the scalability of the method and its applicability to different tasks. Without these details, it is challenging to evaluate the practicality of the method for use in different contexts. Therefore, the authors should provide more detailed information on this point to enhance the paper's value.",
                "The authors' claim that their method, autoHyper, revealed 'blind spots' in their HPO process is intriguing but needs further clarification. The term 'blind spots' seems to refer to the limitations of traditional HPO methods, such as the need for manual tuning, heavy computational overhead, and poor generalization across different models, datasets, and experimental configurations. It would be beneficial if the authors could provide more explicit details on these 'blind spots'. Furthermore, it would be helpful if the authors could elaborate on how autoHyper, by generating learning rates that differed from the author-suggested learning rates, revealed these 'blind spots', indicating areas of the hyperparameter space that were not originally explored.",
                "While the paper discusses the limitations of other state-of-the-art methods for HPO in terms of computational overhead, manual tuning, and generalization across different models, datasets, and experimental configurations, it does not mention or compare the proposed method with any specific state-of-the-art HPO methods. It would be beneficial for the authors to conduct additional experiments to compare their method, autoHyper, with specific state-of-the-art methods. This would provide a more direct comparison and help readers understand the relative performance and advantages of autoHyper.",
                "The paper provides some details on how the authors ensured the consistency of their results, such as the process for determining learning rates and the number of trials and epochs for each experiment. However, it does not specify the number of runs for each experiment or the use of statistical tests to compare the results. These details are crucial for understanding the robustness and reliability of the findings. The authors should provide more information on the number of runs for each experiment, including how they decided on this number, and explain if and how they used statistical tests to compare the results. This would help readers assess the validity of the results and the reproducibility of the experiments."
            ],
            "clarity_reproducibility": [],
            "all": [
                "The authors have developed an innovative dynamic tracking algorithm, autoHyper, which iteratively adjusts the learning rate based on the response surface model. However, the paper does not explicitly mention conducting multiple trials of learning rates or accounting for random initialization variations. While the autoHyper algorithm may indirectly account for these variations, it would be beneficial for the authors to explicitly conduct multiple trials of learning rates and account for random initialization variations. This would not only ensure the robustness of the results but also provide a more accurate representation of the performance of the autoHyper algorithm. Furthermore, it would address the potential susceptibility of the autoHyper method to random initialization variations, thereby enhancing the reliability of the results.",
                "While the paper provides a detailed description of the response surface model used by autoHyper and explains why it works, it would be beneficial to include specific suggestions for how this model could be improved. This could potentially lead to further improvements in the algorithm and would provide readers with a clearer understanding of the potential for future work in this area.",
                "While the paper presents a comprehensive set of experiments demonstrating the performance of the autoHyper algorithm across various datasets, network architectures, and optimizers, it lacks details on the conditions under which these experiments were conducted. Specifically, it would be beneficial to include information on the different initializations, noise levels in the function evaluations, and computational budgets used. This would provide a more robust evaluation of the algorithm's performance and its ability to generalize across different conditions. Additionally, the paper mentions a potential limitation of the method being susceptible to random initialization variations. It would be helpful to see how the method performs under different initializations to address this concern.",
                "The paper presents a comparison of the time efficiency of autoHyper with other methods, which is appreciated. However, to strengthen the claim of time efficiency, it would be beneficial to include a more comprehensive comparison. Specifically, the authors should consider comparing autoHyper's time efficiency with other existing HPO methods across a variety of network architectures. This would provide a more objective and quantifiable measure of autoHyper's efficiency, serving as a stronger validation of the authors' claims.",
                "While the authors have used a wide variety of models, optimizers, and datasets in their experiments, which should ensure the generalizability of the results, the specific details about these were not initially apparent. The authors should consider improving the clarity and organization of their paper to ensure that all important details are easily accessible to the reader.",
                "The authors have proposed a method that emphasizes computational efficiency, which is crucial for scalability. However, the paper lacks an ablation study that tests the scalability of the method by gradually increasing the number of hyperparameters being optimized. Conducting such a study would provide a clearer picture of the method's scalability and its ability to handle larger datasets and more complex models without a prohibitive increase in computational resources or time. This is particularly important given the authors' emphasis on computational efficiency.",
                "The paper discusses the use of low-rank factorization of the convolution weights of intermediate layers for optimizing the initial learning rate, but the explanation and justification for this method are not detailed enough. The authors should provide a more comprehensive theoretical basis for this choice, including how it compares to other methods in the related literature, and why it is particularly suited for the problem at hand. This will help readers better understand the rationale behind the method and its relevance to the field.",
                "The paper does not explicitly mention any hyperparameters that need to be set specifically for the low-rank factorization method. While it is possible that the method does not require any additional hyperparameters beyond those already mentioned in the paper, such as the initial learning rate, this point is not clear. Almost all optimization methods require some form of hyperparameter tuning, and the lack of explicit mention of this for the low-rank factorization method could potentially mislead readers or make it difficult for them to replicate the method. The authors should clarify whether any specific hyperparameters need to be set for this method, and if so, what they are and how they were chosen.",
                "While the authors have conducted a series of experiments using various network architectures, optimizers, and datasets, it may be beneficial to further validate the claim of generalizability by testing the method on additional types of neural networks and data. For instance, testing on recurrent neural networks or transformer-based models, and on non-image datasets, could provide a more comprehensive validation of the method's generalizability. This would strengthen the claim and provide more confidence in the method's applicability across a wider range of scenarios.",
                "While the paper provides some details about the computational resources used and the time taken for the method to converge on specific tasks, it lacks crucial information on how the computational cost of the method scales with the complexity of the model and the size of the dataset. For instance, it is unclear how the computational cost would change if a more complex model was used or if the size of the dataset was increased. This information is essential for understanding the scalability of the method and its applicability to different tasks. Without these details, it is challenging to evaluate the practicality of the method for use in different contexts. Therefore, the authors should provide more detailed information on this point to enhance the paper's value.",
                "The authors' claim that their method, autoHyper, revealed 'blind spots' in their HPO process is intriguing but needs further clarification. The term 'blind spots' seems to refer to the limitations of traditional HPO methods, such as the need for manual tuning, heavy computational overhead, and poor generalization across different models, datasets, and experimental configurations. It would be beneficial if the authors could provide more explicit details on these 'blind spots'. Furthermore, it would be helpful if the authors could elaborate on how autoHyper, by generating learning rates that differed from the author-suggested learning rates, revealed these 'blind spots', indicating areas of the hyperparameter space that were not originally explored.",
                "While the paper discusses the limitations of other state-of-the-art methods for HPO in terms of computational overhead, manual tuning, and generalization across different models, datasets, and experimental configurations, it does not mention or compare the proposed method with any specific state-of-the-art HPO methods. It would be beneficial for the authors to conduct additional experiments to compare their method, autoHyper, with specific state-of-the-art methods. This would provide a more direct comparison and help readers understand the relative performance and advantages of autoHyper.",
                "The paper provides some details on how the authors ensured the consistency of their results, such as the process for determining learning rates and the number of trials and epochs for each experiment. However, it does not specify the number of runs for each experiment or the use of statistical tests to compare the results. These details are crucial for understanding the robustness and reliability of the findings. The authors should provide more information on the number of runs for each experiment, including how they decided on this number, and explain if and how they used statistical tests to compare the results. This would help readers assess the validity of the results and the reproducibility of the experiments."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 9,
                    "predicted_id": 5,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "In your conclusion you discuss that you could extend your tuning algorithm to multiple hyperparameters, and I believe in order to truly demonstrate its capabilities this would be required.",
                    "predicted_comment": "The authors have proposed a method that emphasizes computational efficiency, which is crucial for scalability. However, the paper lacks an ablation study that tests the scalability of the method by gradually increasing the number of hyperparameters being optimized. Conducting such a study would provide a clearer picture of the method's scalability and its ability to handle larger datasets and more complex models without a prohibitive increase in computational resources or time. This is particularly important given the authors' emphasis on computational efficiency."
                }
            ],
            "novelty_impact": [
                {
                    "reference_id": 1,
                    "predicted_id": 11,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "The paper does not compare the proposed algorithm to existing state-of-the-art methods, which would be useful for a solid paper.",
                    "predicted_comment": "While the paper discusses the limitations of other state-of-the-art methods for HPO in terms of computational overhead, manual tuning, and generalization across different models, datasets, and experimental configurations, it does not mention or compare the proposed method with any specific state-of-the-art HPO methods. It would be beneficial for the authors to conduct additional experiments to compare their method, autoHyper, with specific state-of-the-art methods. This would provide a more direct comparison and help readers understand the relative performance and advantages of autoHyper."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 7,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The algorithm introduces new hyperparameters, such as alpha and the 'rate of change plateaus'. The paper should explain how these hyperparameters are chosen and how they influence the result.",
                    "predicted_comment": "The paper does not explicitly mention any hyperparameters that need to be set specifically for the low-rank factorization method. While it is possible that the method does not require any additional hyperparameters beyond those already mentioned in the paper, such as the initial learning rate, this point is not clear. Almost all optimization methods require some form of hyperparameter tuning, and the lack of explicit mention of this for the low-rank factorization method could potentially mislead readers or make it difficult for them to replicate the method. The authors should clarify whether any specific hyperparameters need to be set for this method, and if so, what they are and how they were chosen."
                }
            ],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 9,
                    "predicted_id": 5,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "In your conclusion you discuss that you could extend your tuning algorithm to multiple hyperparameters, and I believe in order to truly demonstrate its capabilities this would be required.",
                    "predicted_comment": "The authors have proposed a method that emphasizes computational efficiency, which is crucial for scalability. However, the paper lacks an ablation study that tests the scalability of the method by gradually increasing the number of hyperparameters being optimized. Conducting such a study would provide a clearer picture of the method's scalability and its ability to handle larger datasets and more complex models without a prohibitive increase in computational resources or time. This is particularly important given the authors' emphasis on computational efficiency."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 11,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 5,
                    "score": 1.0,
                    "reference_comment": "The paper does not compare the proposed algorithm to existing state-of-the-art methods, which would be useful for a solid paper.",
                    "predicted_comment": "While the paper discusses the limitations of other state-of-the-art methods for HPO in terms of computational overhead, manual tuning, and generalization across different models, datasets, and experimental configurations, it does not mention or compare the proposed method with any specific state-of-the-art HPO methods. It would be beneficial for the authors to conduct additional experiments to compare their method, autoHyper, with specific state-of-the-art methods. This would provide a more direct comparison and help readers understand the relative performance and advantages of autoHyper."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 7,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The algorithm introduces new hyperparameters, such as alpha and the 'rate of change plateaus'. The paper should explain how these hyperparameters are chosen and how they influence the result.",
                    "predicted_comment": "The paper does not explicitly mention any hyperparameters that need to be set specifically for the low-rank factorization method. While it is possible that the method does not require any additional hyperparameters beyond those already mentioned in the paper, such as the initial learning rate, this point is not clear. Almost all optimization methods require some form of hyperparameter tuning, and the lack of explicit mention of this for the low-rank factorization method could potentially mislead readers or make it difficult for them to replicate the method. The authors should clarify whether any specific hyperparameters need to be set for this method, and if so, what they are and how they were chosen."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 18,
                "n_generated": 6,
                "n_total_aligns": 1,
                "recall": 0.03333333333333333,
                "precision": 0.05555555555555555,
                "pseudo_jaccard": 0.022222222222222223,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 18,
                "n_generated": 7,
                "n_total_aligns": 2,
                "recall": 0.10000000000000002,
                "precision": 0.09523809523809523,
                "pseudo_jaccard": 0.05113636363636364,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 18,
                "n_generated": 0,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 18,
                "n_generated": 13,
                "n_total_aligns": 3,
                "recall": 0.13333333333333333,
                "precision": 0.07692307692307693,
                "pseudo_jaccard": 0.05135387488328665,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.1,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.06666666666666667,
                    "n_real": 10
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.1,
                    "precision": 0.14285714285714285,
                    "pseudo_jaccard": 0.0625,
                    "n_real": 10
                },
                {
                    "recall": 0.2,
                    "precision": 0.14285714285714285,
                    "pseudo_jaccard": 0.09090909090909091,
                    "n_real": 5
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 10
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.2,
                    "precision": 0.15384615384615385,
                    "pseudo_jaccard": 0.09523809523809523,
                    "n_real": 10
                },
                {
                    "recall": 0.2,
                    "precision": 0.07692307692307693,
                    "pseudo_jaccard": 0.058823529411764705,
                    "n_real": 5
                }
            ]
        }
    },
    {
        "doc_id": "rrWeE9ZDw_",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper claims that the learned object-centric abstraction can be transferred to a new task with the same object type, but it does not define how 'same' is with respect to the same object type in the new task. For example, an apple on a table and an apple on a tree are semantically the same but have different preconditions for the 'pick' action. This suggests that the object-centric abstractions are still more task-specific, even for semantically identical objects.",
                "The paper suggests using Problem-Specific Instantiation to solve the problem of same-type objects having different preconditions in different scenarios. However, this process is similar to manually adjusting the planning domain to adapt to a different scenario, which does not solve the fundamental limitation of the human-defined planning domain.",
                "The learned symbolic predicates of specific object types are not general enough to be applied across different tasks. The proposed object-centric abstraction only groups similar conditions it observed, but does not abstract the preconditions of why an action could be performed. For example, if an apple is placed at different heights, the proposed method will lead to different apple object types as they have different preconditions. If the apple is placed even higher--a condition out of the previously seen cases, the proposed method cannot solve this problem as it does not abstract the 'reachable' concept for the picking action."
            ],
            [
                "The paper claims that the proposed method results in being able to obtain a successful agent for long-term planning with considerably fewer environment interactions, but does not provide a quantitative evaluation against a baseline to demonstrate this reduction in environment interactions.",
                "The paper's comparison with prior works like Kaiser et al. (2020) and Hafner et al. (2021) may not be fair, as the presented method assumes that environment observations are pre-factored and pre-processed into object components, which is a non-trivial task that might explain the sample inefficiency of earlier methods.",
                "The paper should either significantly reduce the prominence of the sample efficiency claims or provide a fair experimental comparison against a baseline that has access to the same factored, pre-processed observations and abstract options as the presented technique."
            ],
            [
                "The paper should clarify why a grouping based on effect distributions alone is preferred over one that also considers the pre-condition.",
                "The paper should provide more details on how it is determined what objects/operators require grounding.",
                "The paper should explain how the clustering proceeds for X, including how X is partitioned and how the result from clustering X is connected to clustering S.",
                "The paper should clarify how much domain knowledge is injected in the method for integrating task-specific information in the learned representations.",
                "The paper should provide more details on the steps needed to achieve the result in the Craft environment experiment, including how the need for having identities is identified, how it is determined which objects require ids, and how the id is connected to the object.",
                "The paper should discuss the limitations of the approach, including the assumptions made such as the frame assumption, that options only affect the object they act upon, and that the world comes pre-segmented according to objects.",
                "The paper should clarify how the grounding is achieved and what knowledge is assumed to arrive at the presented results."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper claims that the learned representations can be transferred between tasks that share the same types of objects, resulting in agents that require fewer samples to learn a model of a new task. However, the paper does not provide a detailed analysis or specific experiments to support this claim. To strengthen this claim, the authors should consider adding an experiment specifically designed to evaluate the transferability of learned operators. This could involve training the model on one task, then transferring the learned operators to a new task and measuring the performance. Additionally, it would be beneficial to compare the performance of agents using transferred operators versus agents learning from scratch in the new task. This would provide empirical evidence to support the claim and would also give insights into the limitations of the proposed method.",
                "The paper provides a detailed description of the proposed method and its performance on various tasks. However, it lacks a comprehensive comparison with other existing methods that aim to learn object-centric representations or claim transferability. Such comparisons are crucial to demonstrate the relative strengths and weaknesses of the proposed method. The authors should consider comparing their method with methods such as [insert methods identified by agents], focusing on aspects such as performance on similar tasks, the quality of the learned representations, and the number of samples required to learn a model of a new task. This would provide a clearer picture of the novelty and effectiveness of the proposed method."
            ],
            "novelty_impact": [
                "The paper provides a detailed explanation of the concept of 'object-centric' and includes several examples and case studies. However, the reviewer found some of these explanations and examples to be unclear. It would be helpful if the authors could provide a more straightforward explanation of the concept and its benefits. Additionally, the authors could consider including more step-by-step walkthroughs of their examples or case studies to better illustrate how the 'object-centric' approach is applied in practice.",
                "The authors have clearly articulated the motivation behind their work - to improve sample efficiency in reinforcement learning by building a compact, high-level model. However, the method used to augment the state space with problem-specific information could introduce potential bias or limitations. Specifically, the method assumes that the agent can individuate objects in its environment. This could introduce bias if the agent's ability to individuate objects is flawed or limited. Furthermore, the method might not be as effective if the environment contains objects that the agent has not previously encountered or if the dynamics of the objects change in different tasks. It would be beneficial for the authors to explore these potential biases and limitations further, and discuss how they might be mitigated.",
                "While the paper provides a comprehensive discussion on the object-centric approach and its benefits, it lacks an explicit discussion on its limitations, particularly in tasks where the dynamics cannot be fully described by the state of the objects. For instance, in tasks where the object-centric state space is not Markov, the state of the objects alone is insufficient to describe the dynamics. The paper does suggest augmenting the object-centric state space with problem-specific, allocentric information to preserve the Markov property, but a more detailed discussion on this aspect would be beneficial. This would provide a more balanced view of the proposed method and help readers understand its potential limitations and areas for further research.",
                "The key findings of the paper are interesting and potentially impactful. However, the paper should provide specific quantitative details on how the preprocessing steps, such as learning an abstract representation of the environment, generating a propositional forward model, merging objects into types, lifting abstractions based on object type, and the application of Principal Component Analysis (PCA) to a batch of images, affect the performance of the model. These details are crucial for understanding the impact of each preprocessing step on the model's performance and for replicating the study. Providing such details would strengthen the paper by giving readers a clearer understanding of the preprocessing steps and their significance.",
                "The paper provides a method that learns the type system, predicates, and high-level operators from pixel data, fitting well into the related literature. The method uses the DBSCAN clustering algorithm to partition options based on terminating states and an SVM with Platt scaling to estimate the preconditions for each partitioned option. However, the paper does not provide a detailed explanation or comparison of these specific methods with other potential methods. It would be beneficial for the reader to understand the rationale behind these choices. For instance, why was DBSCAN chosen for clustering over other algorithms? What advantages does it offer in this context? Similarly, why was an SVM with Platt scaling chosen for precondition estimation? How does it compare to other potential methods? Providing this information would strengthen the paper by giving the reader a better understanding of the method's design choices.",
                "The authors have demonstrated the application of their object-centric representation learning approach to Minecraft tasks and a Crafting domain. The evidence provided, such as the successful application of the method to these diverse tasks/domains, the reduction in the number of samples required to learn a model for a new task over time, and the decrease in the number of new operators that must be learned as the number of tasks increases, supports the generalizability of the method. However, to further strengthen the claim of generalizability, the authors could consider applying their method to tasks or domains that do not share the same types of objects. This would provide a more robust test of the method's ability to generalize across different contexts, which is crucial for reducing the amount of new learning required when applying the learned model to new tasks.",
                "The paper presents a novel approach to learning PPDDL operators from raw data, which are used to represent high-level actions in a given environment. However, the evaluation of these operators seems to be based on their ability to accurately represent the actions and their effects in the environment, and their transferability across different tasks, without a specific metric or method for measuring their quality. For a more objective evaluation of the PPDDL operators, it would be beneficial to propose a specific metric that quantifies the accuracy of the operators in predicting the outcomes of actions, and measures the reduction in learning time when these operators are transferred to new tasks. Another possible metric could be the efficiency of the planning process, such as the number of steps or the amount of time it takes for the agent to plan and execute a task using the PPDDL operators. These metrics would provide a more objective measure of the quality of the PPDDL operators and would allow for a more rigorous evaluation of the proposed approach. This would not only strengthen the current work but also provide a benchmark for future research in this area.",
                "The paper could benefit from a more rigorous and detailed evaluation of the transferability of the learned abstractions. While the paper mentions that the proposed method was demonstrated on a 2D crafting domain and a series of Minecraft tasks, it does not provide explicit details about how the effectiveness of the method was evaluated. It would be helpful if the authors could conduct experiments that specifically test the transferability of the abstractions across different tasks and domains, and provide a detailed description of the evaluation method used. This would make the results more convincing and allow other researchers to replicate the experiments.",
                "The authors have chosen tasks that appear to be representative of the range the method is intended to handle, and there is a discussion on how the results might generalize to other tasks. However, the justification for the choice of tasks and the discussion on generalizability are largely speculative and lack empirical evidence. The authors should provide more concrete evidence or examples to support their claims. For instance, they could include additional experiments or analyses that demonstrate the method's performance on a wider variety of tasks, or they could provide a more detailed explanation of why the chosen tasks are representative and how the results might generalize.",
                "The authors should provide a more precise definition or measure for 'fewer' in the context of the proposed method's sample efficiency and the number of environment interactions required to form complex, long-term plans. This could involve quantifying the reduction in the number of samples or environment interactions required by their method, or comparing it with other methods in a more objective manner. This would allow for a more objective evaluation of their claim and enhance the clarity and credibility of the paper."
            ],
            "clarity_reproducibility": [
                "The paper lacks explicit metrics for measuring the transferability of the method between tasks that share similar objects, such as blocks in the Blocks World domain and Minecraft tasks, or items like wood, grass, iron, rock, water, gold, and workshops in the 2D crafting environment. Providing such metrics would allow readers to better understand the effectiveness of the method in different contexts. For example, it would be helpful to know how the number of new operators that must be learned changes as the number of tasks increases, as this could indicate successful transfer of knowledge between tasks.",
                "While the paper does discuss model-based methods in reinforcement learning and how the proposed method extends these, the connection between the two is not explicitly made in all sections. It would be beneficial to clearly link the proposed method to model-based methods throughout the paper, particularly in the sections discussing the development of the method and its application to tasks. This would provide more background information and make the paper more accessible to readers unfamiliar with these methods.",
                "The paper outlines a five-step process for learning abstract representations of tasks in different environments. However, it lacks specific details that are crucial for understanding and applying this process. For instance, the paper does not elaborate on how objects are merged into types (Step 3) and how abstractions are lifted based on object type (Step 4). A clear, general description of the process before diving into specific examples is also missing. Furthermore, the paper does not provide a clear, step-by-step breakdown of the process, specifics of the preprocessing steps to reduce the dimensionality of the state space, and a detailed explanation of how the DBSCAN clustering algorithm and SVM are applied in the context of their work. Providing these details would greatly enhance the reader's understanding of the process and its applicability to different tasks and environments.",
                "The paper discusses the concept of transferability of the learned abstractions to additional procedurally-generated Minecraft tasks, but it lacks specific metrics or benchmarks to measure this transferability. This makes it difficult for readers to evaluate the effectiveness of the learned abstractions. It would be beneficial to include quantitative measures such as the number of samples required to learn a new task, the speed at which the agent is able to construct plans composed of low-level actions in new tasks, the number of new operators that need to be learned when transferring to a new task, and the success rate of the agent in completing new, unseen tasks using the learned abstractions. These metrics would provide a clearer picture of the transferability of the learned abstractions and would allow readers to better evaluate the effectiveness of your approach.",
                "The paper presents a method for learning an object-centric representation of a continuous and high-dimensional environment from pixel data, which is applied to a 2D crafting domain and a series of Minecraft tasks. The method involves learning the type system, predicates, and high-level operators. However, the paper lacks explicit details about how this learning from pixel data is done. Providing more details about this process would help readers understand the method better and appreciate its potential applications. For instance, it would be helpful to know how the agent partitions the options and fits a classifier to each partition's initiation states, and a density estimator to its terminating states. It would also be beneficial to understand how the agent generates a propositional PDDL using these learned preconditions and effects, and how it lifts its representation by replacing the learned propositions with predicates parameterised by the determined object types. Lastly, it would be useful to know how the agent performs problem-specific instantiation.",
                "The paper provides some details about the visualization of operators in the context of Minecraft and the potential errors that can occur when constructing these abstract representations. However, these aspects could be elaborated on further. For instance, the paper could provide more examples of visualizing operators for various skills and discuss in more detail the potential errors that can occur, such as insufficient data, suboptimal hyperparameters, PPDDL construction errors, and type inference errors. This would help readers better understand the challenges and limitations of the method.",
                "The paper should provide more details about the quality of the PPDDL operators. Specifically, it would be beneficial to discuss how the precondition classifier and the effect density estimator are learned from the option execution data, and what measures are taken to ensure their accuracy. This is important because inaccuracies in these components could lead to incomplete or incorrect representations of the task, affecting the agent's ability to plan and execute tasks efficiently. Furthermore, the paper should elaborate on how errors in learning can result in imperfect operators. For instance, if the agent incorrectly estimates the effect distribution for an option, it could lead to an inaccurate representation of the possible states the agent could end up in after executing that option. This could result in the agent making suboptimal or even incorrect decisions during planning. Similarly, errors in learning the precondition could lead to the agent incorrectly believing that it can execute an option in a state where it actually cannot, or vice versa. By providing more details on these aspects, readers will gain a better understanding of the potential issues and how they can be addressed.",
                "The paper provides a detailed description of the tasks used in the experiments, but it lacks specific information about the experimental settings that are crucial for replication and further research. Specifically, the paper should provide the following: 1. Hyperparameters: While some hyperparameters are specified for the Crafting domain, the paper should provide all hyperparameters used in all tasks. This includes the hyperparameters used in the Blocks World task, the 2D crafting environment, the Minecraft task, and the inter-task transfer in Minecraft. 2. Equipment and material specifications: The paper does not provide any information about the computational resources used to run the experiments, such as the type of computer or the amount of memory used. This information is important as it can affect the performance and results of the experiments. 3. Other implementation details: The paper should provide any other details that are important for replicating the experimental settings. This could include, for example, the version of the software used, the specific settings of the software, or the preprocessing steps applied to the data. By providing these details, the paper would enable other researchers to replicate the study more accurately and build upon the work. This would also increase the transparency and reproducibility of the research, which are key principles in scientific research."
            ],
            "all": [
                "The paper claims that the learned representations can be transferred between tasks that share the same types of objects, resulting in agents that require fewer samples to learn a model of a new task. However, the paper does not provide a detailed analysis or specific experiments to support this claim. To strengthen this claim, the authors should consider adding an experiment specifically designed to evaluate the transferability of learned operators. This could involve training the model on one task, then transferring the learned operators to a new task and measuring the performance. Additionally, it would be beneficial to compare the performance of agents using transferred operators versus agents learning from scratch in the new task. This would provide empirical evidence to support the claim and would also give insights into the limitations of the proposed method.",
                "The paper provides a detailed description of the proposed method and its performance on various tasks. However, it lacks a comprehensive comparison with other existing methods that aim to learn object-centric representations or claim transferability. Such comparisons are crucial to demonstrate the relative strengths and weaknesses of the proposed method. The authors should consider comparing their method with methods such as [insert methods identified by agents], focusing on aspects such as performance on similar tasks, the quality of the learned representations, and the number of samples required to learn a model of a new task. This would provide a clearer picture of the novelty and effectiveness of the proposed method.",
                "The paper provides a detailed explanation of the concept of 'object-centric' and includes several examples and case studies. However, the reviewer found some of these explanations and examples to be unclear. It would be helpful if the authors could provide a more straightforward explanation of the concept and its benefits. Additionally, the authors could consider including more step-by-step walkthroughs of their examples or case studies to better illustrate how the 'object-centric' approach is applied in practice.",
                "The authors have clearly articulated the motivation behind their work - to improve sample efficiency in reinforcement learning by building a compact, high-level model. However, the method used to augment the state space with problem-specific information could introduce potential bias or limitations. Specifically, the method assumes that the agent can individuate objects in its environment. This could introduce bias if the agent's ability to individuate objects is flawed or limited. Furthermore, the method might not be as effective if the environment contains objects that the agent has not previously encountered or if the dynamics of the objects change in different tasks. It would be beneficial for the authors to explore these potential biases and limitations further, and discuss how they might be mitigated.",
                "While the paper provides a comprehensive discussion on the object-centric approach and its benefits, it lacks an explicit discussion on its limitations, particularly in tasks where the dynamics cannot be fully described by the state of the objects. For instance, in tasks where the object-centric state space is not Markov, the state of the objects alone is insufficient to describe the dynamics. The paper does suggest augmenting the object-centric state space with problem-specific, allocentric information to preserve the Markov property, but a more detailed discussion on this aspect would be beneficial. This would provide a more balanced view of the proposed method and help readers understand its potential limitations and areas for further research.",
                "The key findings of the paper are interesting and potentially impactful. However, the paper should provide specific quantitative details on how the preprocessing steps, such as learning an abstract representation of the environment, generating a propositional forward model, merging objects into types, lifting abstractions based on object type, and the application of Principal Component Analysis (PCA) to a batch of images, affect the performance of the model. These details are crucial for understanding the impact of each preprocessing step on the model's performance and for replicating the study. Providing such details would strengthen the paper by giving readers a clearer understanding of the preprocessing steps and their significance.",
                "The paper provides a method that learns the type system, predicates, and high-level operators from pixel data, fitting well into the related literature. The method uses the DBSCAN clustering algorithm to partition options based on terminating states and an SVM with Platt scaling to estimate the preconditions for each partitioned option. However, the paper does not provide a detailed explanation or comparison of these specific methods with other potential methods. It would be beneficial for the reader to understand the rationale behind these choices. For instance, why was DBSCAN chosen for clustering over other algorithms? What advantages does it offer in this context? Similarly, why was an SVM with Platt scaling chosen for precondition estimation? How does it compare to other potential methods? Providing this information would strengthen the paper by giving the reader a better understanding of the method's design choices.",
                "The authors have demonstrated the application of their object-centric representation learning approach to Minecraft tasks and a Crafting domain. The evidence provided, such as the successful application of the method to these diverse tasks/domains, the reduction in the number of samples required to learn a model for a new task over time, and the decrease in the number of new operators that must be learned as the number of tasks increases, supports the generalizability of the method. However, to further strengthen the claim of generalizability, the authors could consider applying their method to tasks or domains that do not share the same types of objects. This would provide a more robust test of the method's ability to generalize across different contexts, which is crucial for reducing the amount of new learning required when applying the learned model to new tasks.",
                "The paper presents a novel approach to learning PPDDL operators from raw data, which are used to represent high-level actions in a given environment. However, the evaluation of these operators seems to be based on their ability to accurately represent the actions and their effects in the environment, and their transferability across different tasks, without a specific metric or method for measuring their quality. For a more objective evaluation of the PPDDL operators, it would be beneficial to propose a specific metric that quantifies the accuracy of the operators in predicting the outcomes of actions, and measures the reduction in learning time when these operators are transferred to new tasks. Another possible metric could be the efficiency of the planning process, such as the number of steps or the amount of time it takes for the agent to plan and execute a task using the PPDDL operators. These metrics would provide a more objective measure of the quality of the PPDDL operators and would allow for a more rigorous evaluation of the proposed approach. This would not only strengthen the current work but also provide a benchmark for future research in this area.",
                "The paper could benefit from a more rigorous and detailed evaluation of the transferability of the learned abstractions. While the paper mentions that the proposed method was demonstrated on a 2D crafting domain and a series of Minecraft tasks, it does not provide explicit details about how the effectiveness of the method was evaluated. It would be helpful if the authors could conduct experiments that specifically test the transferability of the abstractions across different tasks and domains, and provide a detailed description of the evaluation method used. This would make the results more convincing and allow other researchers to replicate the experiments.",
                "The authors have chosen tasks that appear to be representative of the range the method is intended to handle, and there is a discussion on how the results might generalize to other tasks. However, the justification for the choice of tasks and the discussion on generalizability are largely speculative and lack empirical evidence. The authors should provide more concrete evidence or examples to support their claims. For instance, they could include additional experiments or analyses that demonstrate the method's performance on a wider variety of tasks, or they could provide a more detailed explanation of why the chosen tasks are representative and how the results might generalize.",
                "The authors should provide a more precise definition or measure for 'fewer' in the context of the proposed method's sample efficiency and the number of environment interactions required to form complex, long-term plans. This could involve quantifying the reduction in the number of samples or environment interactions required by their method, or comparing it with other methods in a more objective manner. This would allow for a more objective evaluation of their claim and enhance the clarity and credibility of the paper.",
                "The paper lacks explicit metrics for measuring the transferability of the method between tasks that share similar objects, such as blocks in the Blocks World domain and Minecraft tasks, or items like wood, grass, iron, rock, water, gold, and workshops in the 2D crafting environment. Providing such metrics would allow readers to better understand the effectiveness of the method in different contexts. For example, it would be helpful to know how the number of new operators that must be learned changes as the number of tasks increases, as this could indicate successful transfer of knowledge between tasks.",
                "While the paper does discuss model-based methods in reinforcement learning and how the proposed method extends these, the connection between the two is not explicitly made in all sections. It would be beneficial to clearly link the proposed method to model-based methods throughout the paper, particularly in the sections discussing the development of the method and its application to tasks. This would provide more background information and make the paper more accessible to readers unfamiliar with these methods.",
                "The paper outlines a five-step process for learning abstract representations of tasks in different environments. However, it lacks specific details that are crucial for understanding and applying this process. For instance, the paper does not elaborate on how objects are merged into types (Step 3) and how abstractions are lifted based on object type (Step 4). A clear, general description of the process before diving into specific examples is also missing. Furthermore, the paper does not provide a clear, step-by-step breakdown of the process, specifics of the preprocessing steps to reduce the dimensionality of the state space, and a detailed explanation of how the DBSCAN clustering algorithm and SVM are applied in the context of their work. Providing these details would greatly enhance the reader's understanding of the process and its applicability to different tasks and environments.",
                "The paper discusses the concept of transferability of the learned abstractions to additional procedurally-generated Minecraft tasks, but it lacks specific metrics or benchmarks to measure this transferability. This makes it difficult for readers to evaluate the effectiveness of the learned abstractions. It would be beneficial to include quantitative measures such as the number of samples required to learn a new task, the speed at which the agent is able to construct plans composed of low-level actions in new tasks, the number of new operators that need to be learned when transferring to a new task, and the success rate of the agent in completing new, unseen tasks using the learned abstractions. These metrics would provide a clearer picture of the transferability of the learned abstractions and would allow readers to better evaluate the effectiveness of your approach.",
                "The paper presents a method for learning an object-centric representation of a continuous and high-dimensional environment from pixel data, which is applied to a 2D crafting domain and a series of Minecraft tasks. The method involves learning the type system, predicates, and high-level operators. However, the paper lacks explicit details about how this learning from pixel data is done. Providing more details about this process would help readers understand the method better and appreciate its potential applications. For instance, it would be helpful to know how the agent partitions the options and fits a classifier to each partition's initiation states, and a density estimator to its terminating states. It would also be beneficial to understand how the agent generates a propositional PDDL using these learned preconditions and effects, and how it lifts its representation by replacing the learned propositions with predicates parameterised by the determined object types. Lastly, it would be useful to know how the agent performs problem-specific instantiation.",
                "The paper provides some details about the visualization of operators in the context of Minecraft and the potential errors that can occur when constructing these abstract representations. However, these aspects could be elaborated on further. For instance, the paper could provide more examples of visualizing operators for various skills and discuss in more detail the potential errors that can occur, such as insufficient data, suboptimal hyperparameters, PPDDL construction errors, and type inference errors. This would help readers better understand the challenges and limitations of the method.",
                "The paper should provide more details about the quality of the PPDDL operators. Specifically, it would be beneficial to discuss how the precondition classifier and the effect density estimator are learned from the option execution data, and what measures are taken to ensure their accuracy. This is important because inaccuracies in these components could lead to incomplete or incorrect representations of the task, affecting the agent's ability to plan and execute tasks efficiently. Furthermore, the paper should elaborate on how errors in learning can result in imperfect operators. For instance, if the agent incorrectly estimates the effect distribution for an option, it could lead to an inaccurate representation of the possible states the agent could end up in after executing that option. This could result in the agent making suboptimal or even incorrect decisions during planning. Similarly, errors in learning the precondition could lead to the agent incorrectly believing that it can execute an option in a state where it actually cannot, or vice versa. By providing more details on these aspects, readers will gain a better understanding of the potential issues and how they can be addressed.",
                "The paper provides a detailed description of the tasks used in the experiments, but it lacks specific information about the experimental settings that are crucial for replication and further research. Specifically, the paper should provide the following: 1. Hyperparameters: While some hyperparameters are specified for the Crafting domain, the paper should provide all hyperparameters used in all tasks. This includes the hyperparameters used in the Blocks World task, the 2D crafting environment, the Minecraft task, and the inter-task transfer in Minecraft. 2. Equipment and material specifications: The paper does not provide any information about the computational resources used to run the experiments, such as the type of computer or the amount of memory used. This information is important as it can affect the performance and results of the experiments. 3. Other implementation details: The paper should provide any other details that are important for replicating the experimental settings. This could include, for example, the version of the software used, the specific settings of the software, or the preprocessing steps applied to the data. By providing these details, the paper would enable other researchers to replicate the study more accurately and build upon the work. This would also increase the transparency and reproducibility of the research, which are key principles in scientific research."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [
                {
                    "reference_id": 2,
                    "predicted_id": 11,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper should either significantly reduce the prominence of the sample efficiency claims or provide a fair experimental comparison against a baseline that has access to the same factored, pre-processed observations and abstract options as the presented technique.",
                    "predicted_comment": "The authors should provide a more precise definition or measure for 'fewer' in the context of the proposed method's sample efficiency and the number of environment interactions required to form complex, long-term plans. This could involve quantifying the reduction in the number of samples or environment interactions required by their method, or comparing it with other methods in a more objective manner. This would allow for a more objective evaluation of their claim and enhance the clarity and credibility of the paper."
                },
                {
                    "reference_id": 5,
                    "predicted_id": 4,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper should discuss the limitations of the approach, including the assumptions made such as the frame assumption, that options only affect the object they act upon, and that the world comes pre-segmented according to objects.",
                    "predicted_comment": "While the paper provides a comprehensive discussion on the object-centric approach and its benefits, it lacks an explicit discussion on its limitations, particularly in tasks where the dynamics cannot be fully described by the state of the objects. For instance, in tasks where the object-centric state space is not Markov, the state of the objects alone is insufficient to describe the dynamics. The paper does suggest augmenting the object-centric state space with problem-specific, allocentric information to preserve the Markov property, but a more detailed discussion on this aspect would be beneficial. This would provide a more balanced view of the proposed method and help readers understand its potential limitations and areas for further research."
                }
            ],
            "clarity_reproducibility": [
                {
                    "reference_id": 6,
                    "predicted_id": 16,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper should clarify how the grounding is achieved and what knowledge is assumed to arrive at the presented results.",
                    "predicted_comment": "The paper presents a method for learning an object-centric representation of a continuous and high-dimensional environment from pixel data, which is applied to a 2D crafting domain and a series of Minecraft tasks. The method involves learning the type system, predicates, and high-level operators. However, the paper lacks explicit details about how this learning from pixel data is done. Providing more details about this process would help readers understand the method better and appreciate its potential applications. For instance, it would be helpful to know how the agent partitions the options and fits a classifier to each partition's initiation states, and a density estimator to its terminating states. It would also be beneficial to understand how the agent generates a propositional PDDL using these learned preconditions and effects, and how it lifts its representation by replacing the learned propositions with predicates parameterised by the determined object types. Lastly, it would be useful to know how the agent performs problem-specific instantiation."
                }
            ],
            "all": [
                {
                    "reference_id": 2,
                    "predicted_id": 11,
                    "relative_specificity": "same",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "same",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper should either significantly reduce the prominence of the sample efficiency claims or provide a fair experimental comparison against a baseline that has access to the same factored, pre-processed observations and abstract options as the presented technique.",
                    "predicted_comment": "The authors should provide a more precise definition or measure for 'fewer' in the context of the proposed method's sample efficiency and the number of environment interactions required to form complex, long-term plans. This could involve quantifying the reduction in the number of samples or environment interactions required by their method, or comparing it with other methods in a more objective manner. This would allow for a more objective evaluation of their claim and enhance the clarity and credibility of the paper."
                },
                {
                    "reference_id": 5,
                    "predicted_id": 4,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper should discuss the limitations of the approach, including the assumptions made such as the frame assumption, that options only affect the object they act upon, and that the world comes pre-segmented according to objects.",
                    "predicted_comment": "While the paper provides a comprehensive discussion on the object-centric approach and its benefits, it lacks an explicit discussion on its limitations, particularly in tasks where the dynamics cannot be fully described by the state of the objects. For instance, in tasks where the object-centric state space is not Markov, the state of the objects alone is insufficient to describe the dynamics. The paper does suggest augmenting the object-centric state space with problem-specific, allocentric information to preserve the Markov property, but a more detailed discussion on this aspect would be beneficial. This would provide a more balanced view of the proposed method and help readers understand its potential limitations and areas for further research."
                },
                {
                    "reference_id": 6,
                    "predicted_id": 16,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper should clarify how the grounding is achieved and what knowledge is assumed to arrive at the presented results.",
                    "predicted_comment": "The paper presents a method for learning an object-centric representation of a continuous and high-dimensional environment from pixel data, which is applied to a 2D crafting domain and a series of Minecraft tasks. The method involves learning the type system, predicates, and high-level operators. However, the paper lacks explicit details about how this learning from pixel data is done. Providing more details about this process would help readers understand the method better and appreciate its potential applications. For instance, it would be helpful to know how the agent partitions the options and fits a classifier to each partition's initiation states, and a density estimator to its terminating states. It would also be beneficial to understand how the agent generates a propositional PDDL using these learned preconditions and effects, and how it lifts its representation by replacing the learned propositions with predicates parameterised by the determined object types. Lastly, it would be useful to know how the agent performs problem-specific instantiation."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 13,
                "n_generated": 2,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 13,
                "n_generated": 10,
                "n_total_aligns": 2,
                "recall": 0.15873015873015872,
                "precision": 0.06666666666666667,
                "pseudo_jaccard": 0.048611111111111105,
                "specificity_more": 0.5,
                "specificity_less": 0.0,
                "specificity_same": 0.5
            },
            "clarity_reproducibility": {
                "n_reference": 13,
                "n_generated": 8,
                "n_total_aligns": 1,
                "recall": 0.047619047619047616,
                "precision": 0.041666666666666664,
                "pseudo_jaccard": 0.023809523809523808,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "all": {
                "n_reference": 13,
                "n_generated": 20,
                "n_total_aligns": 3,
                "recall": 0.20634920634920637,
                "precision": 0.05000000000000001,
                "pseudo_jaccard": 0.04181818181818182,
                "specificity_more": 0.6666666666666666,
                "specificity_less": 0.0,
                "specificity_same": 0.3333333333333333
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.1,
                    "pseudo_jaccard": 0.08333333333333333,
                    "n_real": 3
                },
                {
                    "recall": 0.14285714285714285,
                    "precision": 0.1,
                    "pseudo_jaccard": 0.0625,
                    "n_real": 7
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.14285714285714285,
                    "precision": 0.125,
                    "pseudo_jaccard": 0.07142857142857142,
                    "n_real": 7
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.05,
                    "pseudo_jaccard": 0.045454545454545456,
                    "n_real": 3
                },
                {
                    "recall": 0.2857142857142857,
                    "precision": 0.1,
                    "pseudo_jaccard": 0.08,
                    "n_real": 7
                }
            ]
        }
    },
    {
        "doc_id": "EG5Pgd7-MY",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The contributions of the paper are not clearly stated in the introduction section, making it difficult to identify the main contributions.",
                "The proposed framework, which connects membership inference attacks with binary hypothesis testing, lacks novelty as it has been previously proposed in other works.",
                "The paper heavily relies on the assumption introduced in equation (1) to characterize the outcome of the algorithm obtained via SGD training, but no valid proof or formal justification for this assumption is provided.",
                "The paper lacks numerical results and comparisons with competing methods for building attacks in the literature.",
                "The tradeoffs between Type I and Type II error are not always reported.",
                "There is a lack of diverse datasets and relevant architectures to validate the results."
            ],
            [
                "The paper does not clearly explain how having multiple specific forms of attacks can provide a comprehensive picture of whether an algorithm is private or not.",
                "The paper does not provide any justification as to why their set of attacks are 'complete'.",
                "The paper's plans are cryptic and each page has many ambiguous sentences with unclear goals.",
                "The paper does not clearly justify why they pick certain criteria and focus on them.",
                "The paper's definition of privacy is an 'average case, computational, indistinguishability-based' variation of DP, which is not standard DP.",
                "The first attack writes probabilities P(theta|D) as if we are aware of the learning algorithm, which is an assumption that needs to be clarified.",
                "The paper's assertion that 'The population data used for constructing the attack algorithm, and evaluating the inference game, need to be similar, in distribution, to the training data' needs proof.",
                "The paper's claim that 'By violating this principle, we might overestimate the privacy loss' is incorrect as the whole point of DP is that the adversary might have arbitrary auxiliary information.",
                "The paper's statement 'The adversary knows the underlying data distribution' is unclear.",
                "The paper's use of pi(z) in Equation (4) is questionable as z should be independent of theta and D in that case.",
                "The paper does not specify the learner in the experiments of the main body.",
                "The paper needs a much more detailed and justified approach, arguing for 'complete' attacks that at least 'capture known attack techniques so far'.",
                "The paper needs to be much clearer in its criteria that it proposes (in addition to the success rate of the attacks) to be part of the 'report' on privacy."
            ],
            [
                "Many of the attacks proposed could still be inefficient for auditing purposes as one might need to train 1000 reference models to perform the attacks. The numbers are also only high for models that are extremely overfitting. The paper should discuss how to audit models that are well-generalized.",
                "The comparison and difference between this work and prior work is vaguely discussed. There is no detailed comparison with [1], which also formalized MIA with an indistinguishability based framework. The hypothesis testing framework in this work is very similar to the threshold adversary in [1]. The authors should discuss the differences in detail. There is also no comparison to white-box models (e.g. [2]). The authors should discuss how this compares to the proposed framework.",
                "No connection is made between this work and differential privacy, although the indistinguishability setup is highly related. The authors should discuss how the framework translates to privacy loss in DP."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "Limited Dataset Variety: The experiments are conducted on three datasets: Purchase100, CIFAR100, and MNIST. These datasets, while commonly used, are all image-based and may not fully represent the diverse scenarios where membership inference attacks can be applied. Given the importance of dataset variety in ensuring the robustness and generalizability of the proposed method, it is recommended to include more diverse datasets. Specifically, datasets from different domains (e.g., text, audio, medical, etc.) could provide a more comprehensive evaluation of the proposed method. This would help to ensure that the results are not biased towards a particular type of data and that the method performs well under different privacy concerns.",
                "The paper presents several new attack strategies and provides comparisons among them. However, it lacks direct comparisons with existing methods or baselines. This makes it difficult to evaluate the effectiveness of the proposed methods. To improve the paper, please include comparisons with existing methods that are widely used in the field. The choice of methods for comparison should be justified based on their relevance and widespread use in the field. Present these comparisons in a clear and structured way, such as in a table or a graph, and discuss the results in the text. Explain why your methods perform better or worse than the existing methods, and what this means for the field. This will help readers understand the value of your proposed methods and how they advance the field.",
                "Lack of Ablation Studies: The paper does not conduct ablation studies to understand the contribution of different components of the proposed methods. Specifically, the hypothesis testing framework, the different attack algorithms derived from it, and the factors affecting the privacy risk estimated using different attack strategies are not tested individually. These components are crucial to understanding the effectiveness and robustness of the proposed methods. Without these ablation studies, it leaves certain aspects of the proposed methods unexplored and unexplained. This could potentially lead to an overestimation or underestimation of the effectiveness of the proposed methods. Furthermore, without these ablation studies, it is difficult to identify areas for improvement or optimization in the proposed methods. Therefore, the inclusion of these ablation studies could significantly improve the overall quality of the paper by providing a more thorough and accurate evaluation of the proposed methods. Please include ablation studies in your experiments to provide a more comprehensive understanding of the proposed methods and their components.",
                "Lack of Analysis on Different Model Architectures: The paper provides details on the types of models used in the experiments, such as a 4 layer MLP for the Purchase100 configurations and a 2 layer CNN for CIFAR100 and MNIST configurations. However, it lacks a comprehensive analysis on how different model architectures impact the performance of the proposed methods against membership inference attacks. It would be beneficial to include such an analysis, as different model architectures can have different vulnerabilities to these attacks. For instance, the paper could include experiments on architectures like RNNs or Transformer models, which are commonly used in various domains. This would provide a more complete understanding of the performance of the proposed methods across a wider range of model architectures.",
                "The paper presents an interesting study on the use of membership inference attacks to measure the privacy loss on models trained with differentially private algorithms. However, the paper could be significantly improved by providing a more detailed analysis on the types of differentially private algorithms used in the experiments. Understanding how different privacy techniques impact the performance of membership inference attacks is crucial for a comprehensive understanding of the topic. Without this analysis, the paper may not provide a complete picture of the performance of the proposed methods under different conditions. Therefore, I strongly recommend conducting experiments on a variety of privacy techniques and discussing their impact on the performance of the proposed methods in detail. This would not only enhance the quality of the paper by providing a more thorough analysis, but also improve its completeness by covering a wider range of scenarios and techniques."
            ],
            "novelty_impact": [
                "The paper uses the AUC score as the primary performance metric for the new attack algorithms derived from the proposed framework. While the AUC score is a common metric for classification problems, it would be beneficial for the paper to provide a more detailed explanation on why the AUC score is chosen as the primary performance metric for these algorithms. Specifically, the paper should discuss how the AUC score is suitable for the scenarios considered in the paper, such as different configurations of training data points and regularization penalties, and how it accurately measures the strength of an attack. This would help readers understand the rationale behind the choice of the AUC score and its relevance to the problem at hand.",
                "While the paper provides some theoretical justification for the claim of capturing a precise approximation of privacy loss in models, a more complete theoretical proof is needed. This proof should ideally show how the proposed methods accurately estimate privacy loss. Providing such a proof would strengthen the paper by providing a solid theoretical foundation for the empirical results, increasing their credibility and reliability.",
                "The paper mentions the inherent randomness in differentially private algorithms and its potential impact on membership inference attacks. However, it lacks a detailed discussion on how this randomness is handled and how it could potentially bias the results of the attacks. It would be beneficial to provide a more in-depth analysis of this issue, as it is crucial for understanding the robustness of the proposed method against membership inference attacks. This could include discussing potential strategies for managing the randomness and mitigating its impact on the results.",
                "The paper discusses the type I and type II errors in the context of the hypothesis testing framework used for membership inference attacks. However, it does not explicitly address the issue of multiple testing problem in the hypothesis testing framework. This omission could potentially inflate the type I error rate, leading to an overestimation of the privacy risk from the model. I recommend that the authors include a discussion on the multiple testing problem, its potential impact on the type I error rate, and how this could affect the interpretation of the results. This would strengthen the validity of the results and provide a more comprehensive understanding of the privacy risks associated with the model.",
                "While the authors have used three diverse datasets (Purchase100, CIFAR100, and MNIST) for their empirical evaluation, it would be beneficial to discuss the generalizability of the results to other types of data. Specifically, the authors could discuss how their proposed framework and derived attack strategies would perform on datasets with different characteristics, such as those with high dimensionality, those with different types of data (e.g., text, time-series), or those from different domains (e.g., healthcare, finance). This would provide a more comprehensive understanding of the applicability and robustness of the proposed method in real-world applications.",
                "While the paper discusses the challenge of ensuring that the population data used for constructing the attack algorithm is similar in distribution to the training data, it does not fully explore the practical implications of this requirement. In real-world scenarios, achieving this similarity might not always be feasible or straightforward. Furthermore, the paper does not discuss the potential for false positives or false negatives in the attacks, which could be a significant issue in real-world applications. The paper also does not discuss how the attacks might perform on models trained on other types of data or under different configurations, limiting the generalizability of the results. Finally, the paper does not discuss any potential countermeasures against the attacks, leaving open the question of how to protect machine learning models against these types of attacks. Discussing these limitations and potential challenges could provide a more comprehensive understanding of the method and its implications."
            ],
            "clarity_reproducibility": [
                "The paper provides a clear and detailed explanation of the implicit assumptions and simplifications made in prior work that the proposed hypothesis testing framework aims to explain. However, the paper is quite technical and uses a lot of jargon specific to the field of machine learning and privacy. While the paper does a good job of explaining these concepts, it might be challenging for readers who are not familiar with these terms to fully understand the explanations. The paper could benefit from providing more examples or analogies to help less experienced readers understand the concepts better. This would make the paper more accessible to a wider audience and could potentially increase its impact.",
                "The paper could provide more specific details about the potential errors or limitations of each type of membership inference attack (Attack P, Attack R, Attack D, and Attack L). While the paper discusses general factors that can influence the success of these attacks and compares their performance, it does not provide specific errors or limitations for individual attacks. Providing this information would help readers better understand the strengths and weaknesses of each attack and the conditions under which they might be most effective.",
                "While the paper provides a detailed explanation of how the new attack algorithms are derived from the proposed framework, it lacks specific numerical or empirical results demonstrating how these algorithms achieve a high AUC score. Providing such results, possibly in the form of experimental results or case studies, would greatly enhance the understanding of the effectiveness of these algorithms. This is crucial for readers to fully appreciate the value of the proposed framework and its derived attack algorithms.",
                "While the paper provides some details about the experimental settings, it does not specify all the hyperparameters used in the experiments or any preprocessing steps for the datasets. This information is crucial for replicating the experiments and fully understanding the results. Although the lack of this information does not necessarily undermine the paper's claims, providing it would strengthen the paper by allowing others to reproduce the work and verify the results.",
                "The paper provides a detailed explanation of how the performance of different attacks is measured and compared, and the metrics used are standard in the field and well justified. However, the paper could improve by providing clear definitions or explanations for some terms such as 'AUC score', 'type-I and type-II errors', and 'hypothesis testing'. This would make the paper more accessible to readers who are not familiar with these terms.",
                "The paper provides valuable insights into the use of membership inference attacks to measure privacy loss in models trained with differentially private algorithms. However, the explanation could be made clearer by providing more context about how these attacks work and why they are a suitable tool for measuring privacy loss. Additionally, while the paper mentions that the empirical performance of these attacks is used to provide lower bounds on the privacy guarantees of these algorithms, it would be helpful to elaborate on this point. Specifically, the paper could explain how the performance of different attacks (P, R, D, L, S) contributes to these lower bounds and what these bounds imply about the privacy guarantees of the algorithms.",
                "The paper assumes the reader is familiar with several key concepts and terms such as 'membership inference attacks', 'differential privacy', 'shadow models', 'likelihood ratio test (LRT)', 'false positive rate (FPR)', and 'AUC (Area Under the ROC Curve) Score' among others. Providing brief explanations or definitions of these terms within the paper would enhance its readability and accessibility, particularly for readers who may not be familiar with these specific terms. This would also ensure that the reader fully understands the methods and results presented in the paper."
            ],
            "all": [
                "Limited Dataset Variety: The experiments are conducted on three datasets: Purchase100, CIFAR100, and MNIST. These datasets, while commonly used, are all image-based and may not fully represent the diverse scenarios where membership inference attacks can be applied. Given the importance of dataset variety in ensuring the robustness and generalizability of the proposed method, it is recommended to include more diverse datasets. Specifically, datasets from different domains (e.g., text, audio, medical, etc.) could provide a more comprehensive evaluation of the proposed method. This would help to ensure that the results are not biased towards a particular type of data and that the method performs well under different privacy concerns.",
                "The paper presents several new attack strategies and provides comparisons among them. However, it lacks direct comparisons with existing methods or baselines. This makes it difficult to evaluate the effectiveness of the proposed methods. To improve the paper, please include comparisons with existing methods that are widely used in the field. The choice of methods for comparison should be justified based on their relevance and widespread use in the field. Present these comparisons in a clear and structured way, such as in a table or a graph, and discuss the results in the text. Explain why your methods perform better or worse than the existing methods, and what this means for the field. This will help readers understand the value of your proposed methods and how they advance the field.",
                "Lack of Ablation Studies: The paper does not conduct ablation studies to understand the contribution of different components of the proposed methods. Specifically, the hypothesis testing framework, the different attack algorithms derived from it, and the factors affecting the privacy risk estimated using different attack strategies are not tested individually. These components are crucial to understanding the effectiveness and robustness of the proposed methods. Without these ablation studies, it leaves certain aspects of the proposed methods unexplored and unexplained. This could potentially lead to an overestimation or underestimation of the effectiveness of the proposed methods. Furthermore, without these ablation studies, it is difficult to identify areas for improvement or optimization in the proposed methods. Therefore, the inclusion of these ablation studies could significantly improve the overall quality of the paper by providing a more thorough and accurate evaluation of the proposed methods. Please include ablation studies in your experiments to provide a more comprehensive understanding of the proposed methods and their components.",
                "Lack of Analysis on Different Model Architectures: The paper provides details on the types of models used in the experiments, such as a 4 layer MLP for the Purchase100 configurations and a 2 layer CNN for CIFAR100 and MNIST configurations. However, it lacks a comprehensive analysis on how different model architectures impact the performance of the proposed methods against membership inference attacks. It would be beneficial to include such an analysis, as different model architectures can have different vulnerabilities to these attacks. For instance, the paper could include experiments on architectures like RNNs or Transformer models, which are commonly used in various domains. This would provide a more complete understanding of the performance of the proposed methods across a wider range of model architectures.",
                "The paper presents an interesting study on the use of membership inference attacks to measure the privacy loss on models trained with differentially private algorithms. However, the paper could be significantly improved by providing a more detailed analysis on the types of differentially private algorithms used in the experiments. Understanding how different privacy techniques impact the performance of membership inference attacks is crucial for a comprehensive understanding of the topic. Without this analysis, the paper may not provide a complete picture of the performance of the proposed methods under different conditions. Therefore, I strongly recommend conducting experiments on a variety of privacy techniques and discussing their impact on the performance of the proposed methods in detail. This would not only enhance the quality of the paper by providing a more thorough analysis, but also improve its completeness by covering a wider range of scenarios and techniques.",
                "The paper uses the AUC score as the primary performance metric for the new attack algorithms derived from the proposed framework. While the AUC score is a common metric for classification problems, it would be beneficial for the paper to provide a more detailed explanation on why the AUC score is chosen as the primary performance metric for these algorithms. Specifically, the paper should discuss how the AUC score is suitable for the scenarios considered in the paper, such as different configurations of training data points and regularization penalties, and how it accurately measures the strength of an attack. This would help readers understand the rationale behind the choice of the AUC score and its relevance to the problem at hand.",
                "While the paper provides some theoretical justification for the claim of capturing a precise approximation of privacy loss in models, a more complete theoretical proof is needed. This proof should ideally show how the proposed methods accurately estimate privacy loss. Providing such a proof would strengthen the paper by providing a solid theoretical foundation for the empirical results, increasing their credibility and reliability.",
                "The paper mentions the inherent randomness in differentially private algorithms and its potential impact on membership inference attacks. However, it lacks a detailed discussion on how this randomness is handled and how it could potentially bias the results of the attacks. It would be beneficial to provide a more in-depth analysis of this issue, as it is crucial for understanding the robustness of the proposed method against membership inference attacks. This could include discussing potential strategies for managing the randomness and mitigating its impact on the results.",
                "The paper discusses the type I and type II errors in the context of the hypothesis testing framework used for membership inference attacks. However, it does not explicitly address the issue of multiple testing problem in the hypothesis testing framework. This omission could potentially inflate the type I error rate, leading to an overestimation of the privacy risk from the model. I recommend that the authors include a discussion on the multiple testing problem, its potential impact on the type I error rate, and how this could affect the interpretation of the results. This would strengthen the validity of the results and provide a more comprehensive understanding of the privacy risks associated with the model.",
                "While the authors have used three diverse datasets (Purchase100, CIFAR100, and MNIST) for their empirical evaluation, it would be beneficial to discuss the generalizability of the results to other types of data. Specifically, the authors could discuss how their proposed framework and derived attack strategies would perform on datasets with different characteristics, such as those with high dimensionality, those with different types of data (e.g., text, time-series), or those from different domains (e.g., healthcare, finance). This would provide a more comprehensive understanding of the applicability and robustness of the proposed method in real-world applications.",
                "While the paper discusses the challenge of ensuring that the population data used for constructing the attack algorithm is similar in distribution to the training data, it does not fully explore the practical implications of this requirement. In real-world scenarios, achieving this similarity might not always be feasible or straightforward. Furthermore, the paper does not discuss the potential for false positives or false negatives in the attacks, which could be a significant issue in real-world applications. The paper also does not discuss how the attacks might perform on models trained on other types of data or under different configurations, limiting the generalizability of the results. Finally, the paper does not discuss any potential countermeasures against the attacks, leaving open the question of how to protect machine learning models against these types of attacks. Discussing these limitations and potential challenges could provide a more comprehensive understanding of the method and its implications.",
                "The paper provides a clear and detailed explanation of the implicit assumptions and simplifications made in prior work that the proposed hypothesis testing framework aims to explain. However, the paper is quite technical and uses a lot of jargon specific to the field of machine learning and privacy. While the paper does a good job of explaining these concepts, it might be challenging for readers who are not familiar with these terms to fully understand the explanations. The paper could benefit from providing more examples or analogies to help less experienced readers understand the concepts better. This would make the paper more accessible to a wider audience and could potentially increase its impact.",
                "The paper could provide more specific details about the potential errors or limitations of each type of membership inference attack (Attack P, Attack R, Attack D, and Attack L). While the paper discusses general factors that can influence the success of these attacks and compares their performance, it does not provide specific errors or limitations for individual attacks. Providing this information would help readers better understand the strengths and weaknesses of each attack and the conditions under which they might be most effective.",
                "While the paper provides a detailed explanation of how the new attack algorithms are derived from the proposed framework, it lacks specific numerical or empirical results demonstrating how these algorithms achieve a high AUC score. Providing such results, possibly in the form of experimental results or case studies, would greatly enhance the understanding of the effectiveness of these algorithms. This is crucial for readers to fully appreciate the value of the proposed framework and its derived attack algorithms.",
                "While the paper provides some details about the experimental settings, it does not specify all the hyperparameters used in the experiments or any preprocessing steps for the datasets. This information is crucial for replicating the experiments and fully understanding the results. Although the lack of this information does not necessarily undermine the paper's claims, providing it would strengthen the paper by allowing others to reproduce the work and verify the results.",
                "The paper provides a detailed explanation of how the performance of different attacks is measured and compared, and the metrics used are standard in the field and well justified. However, the paper could improve by providing clear definitions or explanations for some terms such as 'AUC score', 'type-I and type-II errors', and 'hypothesis testing'. This would make the paper more accessible to readers who are not familiar with these terms.",
                "The paper provides valuable insights into the use of membership inference attacks to measure privacy loss in models trained with differentially private algorithms. However, the explanation could be made clearer by providing more context about how these attacks work and why they are a suitable tool for measuring privacy loss. Additionally, while the paper mentions that the empirical performance of these attacks is used to provide lower bounds on the privacy guarantees of these algorithms, it would be helpful to elaborate on this point. Specifically, the paper could explain how the performance of different attacks (P, R, D, L, S) contributes to these lower bounds and what these bounds imply about the privacy guarantees of the algorithms.",
                "The paper assumes the reader is familiar with several key concepts and terms such as 'membership inference attacks', 'differential privacy', 'shadow models', 'likelihood ratio test (LRT)', 'false positive rate (FPR)', and 'AUC (Area Under the ROC Curve) Score' among others. Providing brief explanations or definitions of these terms within the paper would enhance its readability and accessibility, particularly for readers who may not be familiar with these specific terms. This would also ensure that the reader fully understands the methods and results presented in the paper."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 3,
                    "predicted_id": 1,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper lacks numerical results and comparisons with competing methods for building attacks in the literature.",
                    "predicted_comment": "The paper presents several new attack strategies and provides comparisons among them. However, it lacks direct comparisons with existing methods or baselines. This makes it difficult to evaluate the effectiveness of the proposed methods. To improve the paper, please include comparisons with existing methods that are widely used in the field. The choice of methods for comparison should be justified based on their relevance and widespread use in the field. Present these comparisons in a clear and structured way, such as in a table or a graph, and discuss the results in the text. Explain why your methods perform better or worse than the existing methods, and what this means for the field. This will help readers understand the value of your proposed methods and how they advance the field."
                },
                {
                    "reference_id": 5,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "There is a lack of diverse datasets and relevant architectures to validate the results.",
                    "predicted_comment": "Limited Dataset Variety: The experiments are conducted on three datasets: Purchase100, CIFAR100, and MNIST. These datasets, while commonly used, are all image-based and may not fully represent the diverse scenarios where membership inference attacks can be applied. Given the importance of dataset variety in ensuring the robustness and generalizability of the proposed method, it is recommended to include more diverse datasets. Specifically, datasets from different domains (e.g., text, audio, medical, etc.) could provide a more comprehensive evaluation of the proposed method. This would help to ensure that the results are not biased towards a particular type of data and that the method performs well under different privacy concerns."
                }
            ],
            "novelty_impact": [],
            "clarity_reproducibility": [
                {
                    "reference_id": 10,
                    "predicted_id": 14,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper does not specify the learner in the experiments of the main body.",
                    "predicted_comment": "While the paper provides some details about the experimental settings, it does not specify all the hyperparameters used in the experiments or any preprocessing steps for the datasets. This information is crucial for replicating the experiments and fully understanding the results. Although the lack of this information does not necessarily undermine the paper's claims, providing it would strengthen the paper by allowing others to reproduce the work and verify the results."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 11,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper's plans are cryptic and each page has many ambiguous sentences with unclear goals.",
                    "predicted_comment": "The paper provides a clear and detailed explanation of the implicit assumptions and simplifications made in prior work that the proposed hypothesis testing framework aims to explain. However, the paper is quite technical and uses a lot of jargon specific to the field of machine learning and privacy. While the paper does a good job of explaining these concepts, it might be challenging for readers who are not familiar with these terms to fully understand the explanations. The paper could benefit from providing more examples or analogies to help less experienced readers understand the concepts better. This would make the paper more accessible to a wider audience and could potentially increase its impact."
                }
            ],
            "all": [
                {
                    "reference_id": 3,
                    "predicted_id": 1,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper lacks numerical results and comparisons with competing methods for building attacks in the literature.",
                    "predicted_comment": "The paper presents several new attack strategies and provides comparisons among them. However, it lacks direct comparisons with existing methods or baselines. This makes it difficult to evaluate the effectiveness of the proposed methods. To improve the paper, please include comparisons with existing methods that are widely used in the field. The choice of methods for comparison should be justified based on their relevance and widespread use in the field. Present these comparisons in a clear and structured way, such as in a table or a graph, and discuss the results in the text. Explain why your methods perform better or worse than the existing methods, and what this means for the field. This will help readers understand the value of your proposed methods and how they advance the field."
                },
                {
                    "reference_id": 5,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "There is a lack of diverse datasets and relevant architectures to validate the results.",
                    "predicted_comment": "Limited Dataset Variety: The experiments are conducted on three datasets: Purchase100, CIFAR100, and MNIST. These datasets, while commonly used, are all image-based and may not fully represent the diverse scenarios where membership inference attacks can be applied. Given the importance of dataset variety in ensuring the robustness and generalizability of the proposed method, it is recommended to include more diverse datasets. Specifically, datasets from different domains (e.g., text, audio, medical, etc.) could provide a more comprehensive evaluation of the proposed method. This would help to ensure that the results are not biased towards a particular type of data and that the method performs well under different privacy concerns."
                },
                {
                    "reference_id": 10,
                    "predicted_id": 14,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper does not specify the learner in the experiments of the main body.",
                    "predicted_comment": "While the paper provides some details about the experimental settings, it does not specify all the hyperparameters used in the experiments or any preprocessing steps for the datasets. This information is crucial for replicating the experiments and fully understanding the results. Although the lack of this information does not necessarily undermine the paper's claims, providing it would strengthen the paper by allowing others to reproduce the work and verify the results."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 11,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The paper's plans are cryptic and each page has many ambiguous sentences with unclear goals.",
                    "predicted_comment": "The paper provides a clear and detailed explanation of the implicit assumptions and simplifications made in prior work that the proposed hypothesis testing framework aims to explain. However, the paper is quite technical and uses a lot of jargon specific to the field of machine learning and privacy. While the paper does a good job of explaining these concepts, it might be challenging for readers who are not familiar with these terms to fully understand the explanations. The paper could benefit from providing more examples or analogies to help less experienced readers understand the concepts better. This would make the paper more accessible to a wider audience and could potentially increase its impact."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 22,
                "n_generated": 5,
                "n_total_aligns": 2,
                "recall": 0.1111111111111111,
                "precision": 0.13333333333333333,
                "pseudo_jaccard": 0.07407407407407407,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 22,
                "n_generated": 6,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "clarity_reproducibility": {
                "n_reference": 22,
                "n_generated": 7,
                "n_total_aligns": 2,
                "recall": 0.05128205128205129,
                "precision": 0.09523809523809523,
                "pseudo_jaccard": 0.037037037037037035,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "all": {
                "n_reference": 22,
                "n_generated": 18,
                "n_total_aligns": 4,
                "recall": 0.1623931623931624,
                "precision": 0.07407407407407407,
                "pseudo_jaccard": 0.05329153605015674,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.4,
                    "pseudo_jaccard": 0.2222222222222222,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 13
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 13
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.15384615384615385,
                    "precision": 0.2857142857142857,
                    "pseudo_jaccard": 0.1111111111111111,
                    "n_real": 13
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "all": [
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.1111111111111111,
                    "pseudo_jaccard": 0.09090909090909091,
                    "n_real": 6
                },
                {
                    "recall": 0.15384615384615385,
                    "precision": 0.1111111111111111,
                    "pseudo_jaccard": 0.06896551724137931,
                    "n_real": 13
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "DILxQP08O3B",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper doesn't fully address why DETR performs better than FasterRCNN features. It's not clear why DETR is doing better than Faster-RCNN especially when the detection accuracy of DETR is also better than Faster RCNN.",
                "The method for obtaining the appearance features from Faster RCNN based method is not clear. It's not explained how it's different from the approach taken by Bottom-Up, Top-Down paper in which 2048-dim appearance features are extracted for each visual region.",
                "The experimental setup isn't fully reflective of the object goal navigation task. The experiments are conducted in AI2 thor scenes which only contain one room. It's not clear how this method will perform when evaluated on significantly more complicated environments like Matterport / Gibson.",
                "The navigation task is simplified by discretizing into a grid. Single room environments and discrete grids simplify a lot of navigation-related challenges and the authors don't discuss how the proposed architecture will generalize to more complex object navigation tasks.",
                "The use of spatial embeddings as well as appearance embedding isn't all that surprising. Existing work including Du et al. uses bounding box coordinates to help learn spatial associations between objects.",
                "It's not clear how the agent learns long-term associations between objects across multiple frames. The proposed architecture puts all the burden of learning these long-term object relationships across multiple frames on the LSTM policy since the VT only learns association within a single frame."
            ],
            [
                "The paper adopts the Transformer and adapts it for the navigation problem, but does not propose a new architecture or model.",
                "A similar usage of the Transformer has already been demonstrated in the vision-and-language navigation task. The referenced paper also shows that pre-training of navigation tasks using Transformers can help to boost performance."
            ],
            [
                "The writing in the approach section isn't very clear. It would be better to define clear notations for all the features/descriptors and use such notations in the figure. The current writing uses various terms like 'instance feature', 'global feature', 'positional-global', 'spatial feature', 'spatial-enhanced', which are confusing.",
                "The details in Fig.2 are not sufficient and it is not as informative as the detailed version (Fig.4) in Appendix. These two figures are not consistent, for example, the 'add' symbol for positional enhancement is missing in Fig.4.",
                "In Sec.4.2, it is not clear how the reduction of the channel dimension of a high-level activation map from D to a smaller dimension d is done.",
                "The terms $h$, 'w', and $L$ are used inconsistently in the paper, causing confusion.",
                "The statement in Sec.4.1 about Faster RCNN being blamed for scale-sensitive is not fully convincing. It is unclear what scale-sensitive means here and why DETR doesn't suffer from it.",
                "The statement about 'early stopping' in Sec.4.4 is not fully convincing. It is unclear why the transformer-based representation learner suffers from 'early stopping'.",
                "The SPL formulation in Sec.5.1 seems to be incorrect. The success indicator seems to be missing.",
                "It is unclear why global features are not also added into the transformer encoder."
            ],
            [
                "The introduction of the paper is difficult to understand as it discusses details such as the difference between DETR and Faster RCNN or difficulty of training the transformers without providing a high-level overview of the proposed method.",
                "The paper does not provide a satisfactory explanation for why nothing is learned without the imitation pre-training.",
                "The paper should run the method on other frameworks which include slightly larger scenes, such as RoboTHOR, to test if the method generalizes to those as well."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "Robustness Test: The paper presents a comprehensive evaluation of VTNet's performance in comparison with other methods and through an ablation study. However, it lacks robustness tests under various conditions such as different lighting conditions, object densities, or navigation complexities. These tests are crucial to understand how well VTNet can adapt to different environments and challenges, which is a key aspect of its practical applicability. We recommend conducting additional experiments to test the robustness of VTNet under these conditions and including the results in the paper. This would provide a more comprehensive understanding of VTNet's performance and its potential limitations.",
                "The paper claims that VTNet significantly outperforms other methods, but it does not provide a statistical significance test to support this claim. To ensure that the observed differences in performance are not due to random chance, we recommend conducting a specific statistical significance test such as a t-test or ANOVA, depending on the nature of your data. Including the results of this test in the paper would strengthen your claims and provide more confidence in the results."
            ],
            "novelty_impact": [
                "While the paper provides some details about the environment in which VTNet operates, such as the type of scenes, the number of rooms, and the lack of prior knowledge and additional sensors for the agent, these details are not explicitly stated as assumptions about the environment. Explicitly stating these assumptions would help readers understand the conditions under which VTNet can operate effectively and its applicability in real-world scenarios. For example, the paper could clarify whether VTNet can operate in environments other than the AI2-Thor environment, or in environments with different types of scenes, number of rooms, or availability of prior knowledge and additional sensors.",
                "The paper would benefit from a more detailed discussion on how the pre-training scheme of VTNet handles variations in visual representations, such as different lighting conditions, object occlusions, or changes in object appearances. Specifically, it would be helpful to understand how these variations are accounted for during pre-training and how they affect the performance of VTNet in unseen testing environments. This information is crucial for assessing the robustness of VTNet and its ability to generalize to different visual conditions.",
                "The paper compares VTNet with several methods but does not provide specific details about the training conditions of these other methods. To ensure a fair and comprehensive comparison, it would be beneficial to include information about the training conditions of the compared methods. This would allow readers to understand the context in which VTNet outperforms these methods and assess the robustness of VTNet in different training conditions.",
                "The evaluation of VTNet could benefit from the inclusion of additional metrics that specifically measure the system's robustness to unexpected obstacles. While the current metrics, success rate and SPL, provide valuable insights into the effectiveness and efficiency of navigation, they do not fully capture the system's ability to adapt to unseen environments and handle unexpected obstacles. This is a crucial aspect of VTNet's performance given its operating context. Therefore, it would be beneficial to consider metrics that can quantify this aspect of performance. For example, a metric that measures the system's ability to recover from encountering an unexpected obstacle or a metric that quantifies the system's adaptability to rapidly changing environments could provide a more comprehensive evaluation of VTNet's performance.",
                "The ablation study provides valuable insights into the individual contributions of the components of VTNet. As a potential area for further study, the authors could consider extending the ablation study to include scenarios where multiple components are removed or modified at the same time. This could potentially provide more insight into the interdependencies between the components. However, the authors should also consider the potential complexity this could add to the interpretation of the results.",
                "The paper claims that VTNet can expedite navigation policy learning, but it does not provide explicit evidence or data to support this claim. A direct comparison of the learning speed between VTNet and other methods would strengthen this claim. This comparison could include specific metrics related to learning speed, such as the number of training iterations required to reach a certain performance level, or the time taken to train the model. Providing this comparison would make the claim more convincing and would allow readers to better understand the advantages of VTNet.",
                "While the paper demonstrates that VTNet outperforms several state-of-the-art methods in the AI2-Thor environment, it would be beneficial to test VTNet in additional environments or more diverse scenarios. This could include outdoor environments like parks or city streets, complex indoor environments like shopping malls or office buildings, and environments with dynamic elements like moving vehicles or people. Testing in these environments would provide a more robust evaluation of VTNet's performance and further strengthen the claim that VTNet outperforms other methods in unseen testing environments."
            ],
            "clarity_reproducibility": [
                "The paper provides details about how the DETR object detector transforms encoded features to detection results and how the spatial-enhanced local descriptor is calculated. However, these details could be elaborated further for better understanding. Specifically, the process of transforming encoded features to detection results using DETR could be explained with more examples or diagrams. Similarly, the calculation of the spatial-enhanced local descriptor could be described in a more step-by-step manner, possibly with a flowchart or algorithm pseudocode. This would greatly aid in understanding the paper.",
                "The paper lacks specific details about how the global observation is divided into multiple regions based on spatial layouts for the positional global descriptor. These details are crucial for understanding and replicating the methodology of the Visual Transformer Network (VTNet). Without this information, it is difficult to fully comprehend how VTNet processes and interprets visual observations, which is central to its function and performance. The authors should provide a clear explanation of this process, including how the division of regions is determined and how this influences the calculation of the positional global descriptor.",
                "The paper mentions a pre-training scheme where the VT learns to imitate optimal navigation action selection under the supervision of optimal action instructions. However, it lacks specific details about the human instructions used in the warming-up process. These details are crucial for understanding how the VT learns to encode directional signals. Could you provide more information on the nature of these instructions? For example, how are they generated? What do they entail? How do they guide the VT in the warming-up process? Providing this information would greatly enhance the clarity and replicability of your methodology.",
                "The paper lacks specific details about the size of the LSTM network used in the training and evaluation protocols. This information, including the number of layers and the number of units in each layer, is crucial for reproducing the work and comparing it fairly with other methods. Please provide these details to enhance the reproducibility of your study.",
                "The paper provides a general overview of the AI2-Thor environment, including the types of scenes, the number of rooms, and the distribution of rooms for training, validation, and testing. However, it lacks specific details about the content of each room, such as the exact placement of furniture and items. These details are crucial for understanding the experimental settings and for reproducing the work, as they could potentially affect the performance of the AI system. Therefore, I suggest that the authors include a more detailed description of each room in the AI2-Thor environment, or provide a reference to where these details can be found.",
                "The paper provides some details about the experimental setup in the AI2-Thor environment, such as the types of scenes, the number of rooms, and the target classes. However, it lacks specific details about which specific rooms or objects were used in the experiments. This information is crucial for understanding the experimental settings and for reproducing the work. Please provide these details to enhance the clarity and reproducibility of your study.",
                "The paper lacks specific details about how the length of each episode and its optimal path are determined for calculating the success rate and SPL. For instance, it is unclear how the allowed steps for each episode are set and how the optimal path is defined and calculated. This information is crucial for evaluating the model's performance and for comparing it with other methods, as it directly impacts the success rate and SPL. Providing these details would strengthen the paper by making the evaluation process more transparent and reproducible.",
                "While the paper does provide details about the variant and ablation study, it may be beneficial to organize this information more clearly. For example, the paper could include a dedicated section or table summarizing the results of the study and the impact of each component in VTNet. This would make it easier for readers to understand the importance of each component and the role they play in the overall performance of VTNet.",
                "The paper lacks explicit information on the specific software or hardware requirements for implementing VTNet. While the use of AI2-Thor, DETR for object detection, Adam optimizer, and a two-stage training strategy with 16 asynchronous agents is mentioned, these do not provide clear software requirements. It would be beneficial to include specific details about the software (e.g., versions of AI2-Thor, DETR, and Adam optimizer used) and hardware (e.g., CPU/GPU specifications, memory requirements) used. This information is crucial for researchers attempting to reproduce the work."
            ],
            "all": [
                "Robustness Test: The paper presents a comprehensive evaluation of VTNet's performance in comparison with other methods and through an ablation study. However, it lacks robustness tests under various conditions such as different lighting conditions, object densities, or navigation complexities. These tests are crucial to understand how well VTNet can adapt to different environments and challenges, which is a key aspect of its practical applicability. We recommend conducting additional experiments to test the robustness of VTNet under these conditions and including the results in the paper. This would provide a more comprehensive understanding of VTNet's performance and its potential limitations.",
                "The paper claims that VTNet significantly outperforms other methods, but it does not provide a statistical significance test to support this claim. To ensure that the observed differences in performance are not due to random chance, we recommend conducting a specific statistical significance test such as a t-test or ANOVA, depending on the nature of your data. Including the results of this test in the paper would strengthen your claims and provide more confidence in the results.",
                "While the paper provides some details about the environment in which VTNet operates, such as the type of scenes, the number of rooms, and the lack of prior knowledge and additional sensors for the agent, these details are not explicitly stated as assumptions about the environment. Explicitly stating these assumptions would help readers understand the conditions under which VTNet can operate effectively and its applicability in real-world scenarios. For example, the paper could clarify whether VTNet can operate in environments other than the AI2-Thor environment, or in environments with different types of scenes, number of rooms, or availability of prior knowledge and additional sensors.",
                "The paper would benefit from a more detailed discussion on how the pre-training scheme of VTNet handles variations in visual representations, such as different lighting conditions, object occlusions, or changes in object appearances. Specifically, it would be helpful to understand how these variations are accounted for during pre-training and how they affect the performance of VTNet in unseen testing environments. This information is crucial for assessing the robustness of VTNet and its ability to generalize to different visual conditions.",
                "The paper compares VTNet with several methods but does not provide specific details about the training conditions of these other methods. To ensure a fair and comprehensive comparison, it would be beneficial to include information about the training conditions of the compared methods. This would allow readers to understand the context in which VTNet outperforms these methods and assess the robustness of VTNet in different training conditions.",
                "The evaluation of VTNet could benefit from the inclusion of additional metrics that specifically measure the system's robustness to unexpected obstacles. While the current metrics, success rate and SPL, provide valuable insights into the effectiveness and efficiency of navigation, they do not fully capture the system's ability to adapt to unseen environments and handle unexpected obstacles. This is a crucial aspect of VTNet's performance given its operating context. Therefore, it would be beneficial to consider metrics that can quantify this aspect of performance. For example, a metric that measures the system's ability to recover from encountering an unexpected obstacle or a metric that quantifies the system's adaptability to rapidly changing environments could provide a more comprehensive evaluation of VTNet's performance.",
                "The ablation study provides valuable insights into the individual contributions of the components of VTNet. As a potential area for further study, the authors could consider extending the ablation study to include scenarios where multiple components are removed or modified at the same time. This could potentially provide more insight into the interdependencies between the components. However, the authors should also consider the potential complexity this could add to the interpretation of the results.",
                "The paper claims that VTNet can expedite navigation policy learning, but it does not provide explicit evidence or data to support this claim. A direct comparison of the learning speed between VTNet and other methods would strengthen this claim. This comparison could include specific metrics related to learning speed, such as the number of training iterations required to reach a certain performance level, or the time taken to train the model. Providing this comparison would make the claim more convincing and would allow readers to better understand the advantages of VTNet.",
                "While the paper demonstrates that VTNet outperforms several state-of-the-art methods in the AI2-Thor environment, it would be beneficial to test VTNet in additional environments or more diverse scenarios. This could include outdoor environments like parks or city streets, complex indoor environments like shopping malls or office buildings, and environments with dynamic elements like moving vehicles or people. Testing in these environments would provide a more robust evaluation of VTNet's performance and further strengthen the claim that VTNet outperforms other methods in unseen testing environments.",
                "The paper provides details about how the DETR object detector transforms encoded features to detection results and how the spatial-enhanced local descriptor is calculated. However, these details could be elaborated further for better understanding. Specifically, the process of transforming encoded features to detection results using DETR could be explained with more examples or diagrams. Similarly, the calculation of the spatial-enhanced local descriptor could be described in a more step-by-step manner, possibly with a flowchart or algorithm pseudocode. This would greatly aid in understanding the paper.",
                "The paper lacks specific details about how the global observation is divided into multiple regions based on spatial layouts for the positional global descriptor. These details are crucial for understanding and replicating the methodology of the Visual Transformer Network (VTNet). Without this information, it is difficult to fully comprehend how VTNet processes and interprets visual observations, which is central to its function and performance. The authors should provide a clear explanation of this process, including how the division of regions is determined and how this influences the calculation of the positional global descriptor.",
                "The paper mentions a pre-training scheme where the VT learns to imitate optimal navigation action selection under the supervision of optimal action instructions. However, it lacks specific details about the human instructions used in the warming-up process. These details are crucial for understanding how the VT learns to encode directional signals. Could you provide more information on the nature of these instructions? For example, how are they generated? What do they entail? How do they guide the VT in the warming-up process? Providing this information would greatly enhance the clarity and replicability of your methodology.",
                "The paper lacks specific details about the size of the LSTM network used in the training and evaluation protocols. This information, including the number of layers and the number of units in each layer, is crucial for reproducing the work and comparing it fairly with other methods. Please provide these details to enhance the reproducibility of your study.",
                "The paper provides a general overview of the AI2-Thor environment, including the types of scenes, the number of rooms, and the distribution of rooms for training, validation, and testing. However, it lacks specific details about the content of each room, such as the exact placement of furniture and items. These details are crucial for understanding the experimental settings and for reproducing the work, as they could potentially affect the performance of the AI system. Therefore, I suggest that the authors include a more detailed description of each room in the AI2-Thor environment, or provide a reference to where these details can be found.",
                "The paper provides some details about the experimental setup in the AI2-Thor environment, such as the types of scenes, the number of rooms, and the target classes. However, it lacks specific details about which specific rooms or objects were used in the experiments. This information is crucial for understanding the experimental settings and for reproducing the work. Please provide these details to enhance the clarity and reproducibility of your study.",
                "The paper lacks specific details about how the length of each episode and its optimal path are determined for calculating the success rate and SPL. For instance, it is unclear how the allowed steps for each episode are set and how the optimal path is defined and calculated. This information is crucial for evaluating the model's performance and for comparing it with other methods, as it directly impacts the success rate and SPL. Providing these details would strengthen the paper by making the evaluation process more transparent and reproducible.",
                "While the paper does provide details about the variant and ablation study, it may be beneficial to organize this information more clearly. For example, the paper could include a dedicated section or table summarizing the results of the study and the impact of each component in VTNet. This would make it easier for readers to understand the importance of each component and the role they play in the overall performance of VTNet.",
                "The paper lacks explicit information on the specific software or hardware requirements for implementing VTNet. While the use of AI2-Thor, DETR for object detection, Adam optimizer, and a two-stage training strategy with 16 asynchronous agents is mentioned, these do not provide clear software requirements. It would be beneficial to include specific details about the software (e.g., versions of AI2-Thor, DETR, and Adam optimizer used) and hardware (e.g., CPU/GPU specifications, memory requirements) used. This information is crucial for researchers attempting to reproduce the work."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [
                {
                    "reference_id": 3,
                    "predicted_id": 2,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The navigation task is simplified by discretizing into a grid. Single room environments and discrete grids simplify a lot of navigation-related challenges and the authors don't discuss how the proposed architecture will generalize to more complex object navigation tasks.",
                    "predicted_comment": "While the paper provides some details about the environment in which VTNet operates, such as the type of scenes, the number of rooms, and the lack of prior knowledge and additional sensors for the agent, these details are not explicitly stated as assumptions about the environment. Explicitly stating these assumptions would help readers understand the conditions under which VTNet can operate effectively and its applicability in real-world scenarios. For example, the paper could clarify whether VTNet can operate in environments other than the AI2-Thor environment, or in environments with different types of scenes, number of rooms, or availability of prior knowledge and additional sensors."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 8,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The experimental setup isn't fully reflective of the object goal navigation task. The experiments are conducted in AI2 thor scenes which only contain one room. It's not clear how this method will perform when evaluated on significantly more complicated environments like Matterport / Gibson.",
                    "predicted_comment": "While the paper demonstrates that VTNet outperforms several state-of-the-art methods in the AI2-Thor environment, it would be beneficial to test VTNet in additional environments or more diverse scenarios. This could include outdoor environments like parks or city streets, complex indoor environments like shopping malls or office buildings, and environments with dynamic elements like moving vehicles or people. Testing in these environments would provide a more robust evaluation of VTNet's performance and further strengthen the claim that VTNet outperforms other methods in unseen testing environments."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 8,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper should run the method on other frameworks which include slightly larger scenes, such as RoboTHOR, to test if the method generalizes to those as well.",
                    "predicted_comment": "While the paper demonstrates that VTNet outperforms several state-of-the-art methods in the AI2-Thor environment, it would be beneficial to test VTNet in additional environments or more diverse scenarios. This could include outdoor environments like parks or city streets, complex indoor environments like shopping malls or office buildings, and environments with dynamic elements like moving vehicles or people. Testing in these environments would provide a more robust evaluation of VTNet's performance and further strengthen the claim that VTNet outperforms other methods in unseen testing environments."
                }
            ],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 3,
                    "predicted_id": 2,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The navigation task is simplified by discretizing into a grid. Single room environments and discrete grids simplify a lot of navigation-related challenges and the authors don't discuss how the proposed architecture will generalize to more complex object navigation tasks.",
                    "predicted_comment": "While the paper provides some details about the environment in which VTNet operates, such as the type of scenes, the number of rooms, and the lack of prior knowledge and additional sensors for the agent, these details are not explicitly stated as assumptions about the environment. Explicitly stating these assumptions would help readers understand the conditions under which VTNet can operate effectively and its applicability in real-world scenarios. For example, the paper could clarify whether VTNet can operate in environments other than the AI2-Thor environment, or in environments with different types of scenes, number of rooms, or availability of prior knowledge and additional sensors."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 8,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The experimental setup isn't fully reflective of the object goal navigation task. The experiments are conducted in AI2 thor scenes which only contain one room. It's not clear how this method will perform when evaluated on significantly more complicated environments like Matterport / Gibson.",
                    "predicted_comment": "While the paper demonstrates that VTNet outperforms several state-of-the-art methods in the AI2-Thor environment, it would be beneficial to test VTNet in additional environments or more diverse scenarios. This could include outdoor environments like parks or city streets, complex indoor environments like shopping malls or office buildings, and environments with dynamic elements like moving vehicles or people. Testing in these environments would provide a more robust evaluation of VTNet's performance and further strengthen the claim that VTNet outperforms other methods in unseen testing environments."
                },
                {
                    "reference_id": 2,
                    "predicted_id": 8,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper should run the method on other frameworks which include slightly larger scenes, such as RoboTHOR, to test if the method generalizes to those as well.",
                    "predicted_comment": "While the paper demonstrates that VTNet outperforms several state-of-the-art methods in the AI2-Thor environment, it would be beneficial to test VTNet in additional environments or more diverse scenarios. This could include outdoor environments like parks or city streets, complex indoor environments like shopping malls or office buildings, and environments with dynamic elements like moving vehicles or people. Testing in these environments would provide a more robust evaluation of VTNet's performance and further strengthen the claim that VTNet outperforms other methods in unseen testing environments."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 19,
                "n_generated": 2,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 19,
                "n_generated": 7,
                "n_total_aligns": 3,
                "recall": 0.16666666666666666,
                "precision": 0.10714285714285714,
                "pseudo_jaccard": 0.07323232323232323,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 19,
                "n_generated": 9,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 19,
                "n_generated": 18,
                "n_total_aligns": 3,
                "recall": 0.16666666666666666,
                "precision": 0.041666666666666664,
                "pseudo_jaccard": 0.03522727272727273,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.2857142857142857,
                    "pseudo_jaccard": 0.18181818181818182,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.14285714285714285,
                    "pseudo_jaccard": 0.1111111111111111,
                    "n_real": 3
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "all": [
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.1111111111111111,
                    "pseudo_jaccard": 0.09090909090909091,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 2
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 8
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.05555555555555555,
                    "pseudo_jaccard": 0.05,
                    "n_real": 3
                }
            ]
        }
    },
    {
        "doc_id": "fmOOI2a3tQP",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The paper does not clearly demonstrate if any insight is gained from the theoretical analysis, as the derivation of value and sample complexity bounds for approximate bisimulation was already performed in Gelada et al. 2019.",
                "The empirical gains are modest and it's unclear if they are due to the image representation learning component of the loss or other aspects of the method.",
                "The paper uses a very small number of training tasks - only 4. It's unclear if using more training tasks would improve the generalization of single-task methods like DeepMDP.",
                "There seems to be a missing baseline - a generic multi-task algorithm that conditions on the environment ID but does use the bisimulation loss to help process the image observations.",
                "It's unclear why there's a gap between your method in the multi-task experiments but not in the meta-RL ones.",
                "It's not explained why HiP-BMDP handles the sticky observation setting better than the baselines.",
                "The paper does not convincingly demonstrate that the structural assumptions on the MDP used by the proposed algorithm yield performance improvement."
            ],
            []
        ],
        "generated_comments": {
            "experiments_proofs": [
                "Depth of Analysis: The authors have conducted a thorough and detailed analysis of their proposed methods, providing theoretical analysis, proofs, and theorems. They have also explained the results and their implications well, discussing the results of their experiments in the context of the theoretical analysis and explaining how the results support their claims. They have validated their claims through extensive experimentation, testing their method in multiple environments and comparing it to other baselines. However, it would be beneficial if the authors could delve deeper into the implications of their results, particularly in terms of how the optimal performance between any two environments is controlled by the distance between the hidden parameters for corresponding environments. This would provide a more comprehensive understanding of the results and their implications.",
                "The authors have compared their method with several baselines, including DeepMDP, HiP-BMDP-nobisim, Distral, PCGrad, GradNorm, and PEARL. However, it would be beneficial to include comparisons with other state-of-the-art methods in multi-task and Meta-RL setups to further validate the effectiveness of the proposed method. This would help ensure that the results are not specific to the current set of comparisons and can generalize across different settings. Additionally, providing a detailed discussion on why the proposed method outperforms each baseline could offer more insights into the strengths and weaknesses of the proposed method.",
                "The authors hypothesize that Distral-Ensemble and other multi-task learning baselines perform poorly compared to HiP-BMDP because they may not be leveraging a shared global dynamics model effectively. While the authors provide indirect evidence to support this hypothesis, such as the superior performance of HiP-BMDP and the fact that its effectiveness cannot be attributed to task embeddings alone, they do not provide direct evidence or conduct specific experiments to test this hypothesis. This lack of direct evidence could make the authors' claims about the superiority of their proposed method less convincing. Therefore, it would be beneficial for the authors to provide more direct evidence or conduct additional experiments to support their hypothesis. This could include, for example, experiments that specifically test the ability of Distral-Ensemble and other baselines to leverage a shared global dynamics model, or a more detailed analysis of the factors contributing to the superior performance of HiP-BMDP.",
                "The authors have acknowledged that Meta-RL techniques are too time-intensive to train on pixel observations directly. However, this limitation is not discussed in detail, which leaves the reader with questions about the potential impact on the applicability and scalability of the proposed method. It would be beneficial for the authors to provide a more in-depth discussion on this limitation, including its implications for the use of Meta-RL techniques in different environments and setups. Furthermore, suggesting potential strategies or future research directions to overcome this limitation would greatly enhance the paper."
            ],
            "novelty_impact": [
                "The paper presents a novel framework that combines the concepts of Hidden-Parameter Markov Decision Processes (HiP-MDPs) and Block MDPs to improve sample efficiency in multi-task reinforcement learning (MTRL) and meta-reinforcement learning (Meta-RL) settings. This is a significant theoretical contribution to the field. However, the paper would greatly benefit from a direct comparison of the sample efficiency of the proposed method with other existing methods in a controlled setting. This comparison is crucial to empirically support your claim of improved sample efficiency. It would provide concrete evidence of the proposed method's performance relative to existing methods, allowing for a more objective evaluation. This addition would strengthen your claims and enhance the paper's impact.",
                "The authors assume that the task-specific hidden parameter \u03b8 captures the task-specific dynamics, which is a crucial assumption that underpins their method. While the paper provides theoretical analysis and experimental results that suggest this assumption is valid, it would be helpful for the authors to provide more explicit validation. This could include a discussion of why they believe \u03b8 captures task-specific dynamics and how the experimental results support this assumption. Providing this validation would strengthen the paper by making the assumption about \u03b8 more clear and convincing.",
                "The authors have defined task similarity in terms of the distance between the task parameters \u03b8 in the HiP-BMDP setting, using a distance metric defined in terms of the Wasserstein-1 metric between transition probability distributions of different tasks. State similarity is defined in terms of bisimulation metrics. While these definitions are clear and well-justified within the context of the authors' method, it is not clear how these measures compare with those used in other methods. To enhance the clarity and impact of the paper, it would be beneficial for the authors to explicitly compare their measures of task and state similarity with those used in other methods. This would provide readers with a better understanding of the unique contributions of the authors' method and its potential advantages and disadvantages compared to other methods.",
                "The authors provide a theoretical proof for the 'Transfer bound' formula, which is a significant contribution. This formula is crucial for measuring the transferability of a policy learned on one task to another, taking into account the error from the learned representation. However, to ensure its robustness and applicability in real-world scenarios, it would be beneficial if the authors could empirically test this formula in reinforcement learning environments. For instance, the agent could be trained on one game and then tested on a different game with similar mechanics. This would provide empirical evidence supporting the theoretical proof and demonstrate the practical utility of the formula.",
                "The authors use the Wasserstein distance in two key contexts: to define a distance metric between two environment settings and to learn a HiP-BMDP approximation of a family of MDPs. However, the paper lacks a detailed explanation or justification for this choice. It is crucial for readers to understand why the Wasserstein distance was chosen over other distance measures, and how it contributes to the methodology and results of the paper. The authors should provide this explanation or justification to allow readers to evaluate the appropriateness of this measure.",
                "While the authors have commendably tested their method in a variety of environments from the Deepmind Control Suite (DMC), including Cartpole-Swingup-V0, Cheetah-Run-V0, Walker-Run-V0, Walker-Run-V1, and Finger-Spin-V0, it would be beneficial to test the method in environments with more complex dynamics or more variables. This would provide a more rigorous test of the method's ability to handle complex tasks and adapt to different situations, thereby providing a more comprehensive understanding of the method's capabilities and limitations. Ensuring that the method can generalize well across a wide range of tasks and environments is crucial for its practical applicability in real-world scenarios.",
                "The authors provide a detailed discussion of the robustness of their method (HiP-BMDP) to changes in environment dynamics, including theoretical analysis, performance in the context of multi-task reinforcement learning, mathematical proofs and equations, and performance in different environments. However, the paper could benefit from a more explicit discussion of the significance of changes in environment dynamics. Specifically, it would be helpful if the authors could clarify how significant changes in environment dynamics affect the performance of the method, and why this is important for the method's robustness. This would provide a clearer understanding of the method's robustness and its implications for different environments.",
                "The authors have compared their recommended approach for PCGrad with the traditional approach and found it to outperform the latter. However, the specifics of this comparison are not clear in the paper. It would be beneficial for the authors to provide a more detailed comparison of the two approaches in a controlled setting. This could include specifics such as how the gradients are projected in both approaches and the impact of these differences on the performance. This would provide a more direct comparison of the two approaches and help readers understand why the recommended approach is superior.",
                "The authors claim that their model adapts quickly in unseen environments, and they provide some evidence to support this claim, such as the model's performance in two settings (MTRL and meta-RL) and in an unseen environment where only the task parameter \u03b8 is adapted. However, they do not specify the exact number of unseen environments they tested their model in. Providing this information would strengthen their claim by giving readers a clearer idea of the range of environments the model can adapt to. It would also be helpful if the authors could explain how the number of unseen environments relates to the model's adaptability, as this is not immediately clear from the paper.",
                "While the authors discuss some limitations of their proposed method, such as the reliance on an environment id and the use of L \u221e norms for measuring error and the value and transfer bounds, the discussion could be more comprehensive. Specifically, the authors could discuss potential limitations of their method when applied to more complex or real-world environments, as the current experiments seem to be conducted in relatively controlled settings. This would provide a more balanced view of their method and help readers understand its applicability and potential limitations in different contexts."
            ],
            "clarity_reproducibility": [
                "The paper mentions an encoder that maps observations from state space to a learned, latent representation, but it does not provide specific details about the type of encoder used or the process of how it learns the latent representation. These details are crucial for understanding how the model works and how it achieves its performance. Therefore, I recommend that the authors include this information in the paper.",
                "The paper introduces the concepts of Transfer bound and Sample Complexity in Theorems 3 and 4 respectively, and provides their calculations. However, these concepts are not explained in detail in all sections of the paper, making it difficult for readers to fully understand these key performance metrics. It would be beneficial to provide a more comprehensive explanation of these concepts, including their significance and how they are calculated, in all relevant sections. This would help readers better understand the performance of the proposed method and its advantages over prior work.",
                "While the paper provides some information about the characteristics of the Deepmind Control Suite (DMC) environments used in the experiments, it would be beneficial to include more explicit details about how these specific environments were selected. For example, were they chosen because they present different challenges for reinforcement learning algorithms? If so, what are these challenges? Providing this information would help readers understand the rationale behind the choice of environments and could potentially make the results more generalizable.",
                "While the paper provides some information about the hyperparameters used in the MetaRL algorithm for different environments, it does not explain how these hyperparameters were determined. This information is crucial for understanding the algorithm's performance and its application to different environments. Without it, it would be challenging to replicate the results or apply the algorithm to new environments. Therefore, I suggest that the authors include a section detailing how the hyperparameters were determined, including any optimization or tuning processes used.",
                "The paper uses \u03b1 values in various contexts, such as scaling the \u0398 learning error, as a learning rate in an update rule for a parameter \u03c8, and in the MetaRL and MTRL algorithms. Different \u03b1 values are used for different environments and algorithms. However, the paper lacks specific details on how these \u03b1 values were determined and what they represent in different environments. Providing this information would enhance the reproducibility of the study and allow readers to better understand the role and significance of these \u03b1 values in the proposed methods.",
                "The paper mentions the Distral-Ensemble baseline and its inability to leverage a shared global dynamics model. However, it would be beneficial for the reader to understand why this is the case. Could the authors provide more specific details about the limitations of the Distral-Ensemble baseline in leveraging a shared global dynamics model? This would help to clarify the comparison between the proposed method and the Distral-Ensemble baseline.",
                "The paper provides a general overview of the use of the task parameter \u03b8 in the context of Hidden-Parameter Block MDPs (HiP-BMDPs) and its role in conditioning a universal dynamics model. However, it lacks specific details or results regarding the performance of the transition model in an unseen environment when only the task parameter \u03b8 is adapted. Providing these details would strengthen the paper by giving readers a clearer understanding of how the proposed method performs in unseen environments.",
                "Additionally, the paper discusses the adaptation of the task parameter \u03b8 in the context of value bounds and expected error bounds, and in the context of evaluating the universal transition model. However, it does not provide a detailed, step-by-step process of how the task parameter \u03b8 is adapted. Including this information would help readers understand the mechanics of the proposed method and its effects on the model's performance.",
                "The paper provides a high-level overview of the mathematical proofs and theorems related to value bounds, expected error bounds, and additional results and proofs for HiP-BMDP results. However, the detailed proofs and additional results are not included in the main body of the paper. Including these details in the main body of the paper would make it easier for readers to understand the mathematical foundations of your work and how they support your claims. If these details are included in the appendices, consider referencing them more explicitly in the main body of the paper.",
                "The paper lacks specific implementation details of the Meta-RL algorithm and the HiP-MDP setting. This includes the specific steps taken to implement these algorithms, the parameters used, and any challenges that were encountered. These details are crucial for readers who wish to understand the methodology in depth or replicate the study. Providing these details would significantly enhance the paper's clarity and reproducibility."
            ],
            "all": [
                "Depth of Analysis: The authors have conducted a thorough and detailed analysis of their proposed methods, providing theoretical analysis, proofs, and theorems. They have also explained the results and their implications well, discussing the results of their experiments in the context of the theoretical analysis and explaining how the results support their claims. They have validated their claims through extensive experimentation, testing their method in multiple environments and comparing it to other baselines. However, it would be beneficial if the authors could delve deeper into the implications of their results, particularly in terms of how the optimal performance between any two environments is controlled by the distance between the hidden parameters for corresponding environments. This would provide a more comprehensive understanding of the results and their implications.",
                "The authors have compared their method with several baselines, including DeepMDP, HiP-BMDP-nobisim, Distral, PCGrad, GradNorm, and PEARL. However, it would be beneficial to include comparisons with other state-of-the-art methods in multi-task and Meta-RL setups to further validate the effectiveness of the proposed method. This would help ensure that the results are not specific to the current set of comparisons and can generalize across different settings. Additionally, providing a detailed discussion on why the proposed method outperforms each baseline could offer more insights into the strengths and weaknesses of the proposed method.",
                "The authors hypothesize that Distral-Ensemble and other multi-task learning baselines perform poorly compared to HiP-BMDP because they may not be leveraging a shared global dynamics model effectively. While the authors provide indirect evidence to support this hypothesis, such as the superior performance of HiP-BMDP and the fact that its effectiveness cannot be attributed to task embeddings alone, they do not provide direct evidence or conduct specific experiments to test this hypothesis. This lack of direct evidence could make the authors' claims about the superiority of their proposed method less convincing. Therefore, it would be beneficial for the authors to provide more direct evidence or conduct additional experiments to support their hypothesis. This could include, for example, experiments that specifically test the ability of Distral-Ensemble and other baselines to leverage a shared global dynamics model, or a more detailed analysis of the factors contributing to the superior performance of HiP-BMDP.",
                "The authors have acknowledged that Meta-RL techniques are too time-intensive to train on pixel observations directly. However, this limitation is not discussed in detail, which leaves the reader with questions about the potential impact on the applicability and scalability of the proposed method. It would be beneficial for the authors to provide a more in-depth discussion on this limitation, including its implications for the use of Meta-RL techniques in different environments and setups. Furthermore, suggesting potential strategies or future research directions to overcome this limitation would greatly enhance the paper.",
                "The paper presents a novel framework that combines the concepts of Hidden-Parameter Markov Decision Processes (HiP-MDPs) and Block MDPs to improve sample efficiency in multi-task reinforcement learning (MTRL) and meta-reinforcement learning (Meta-RL) settings. This is a significant theoretical contribution to the field. However, the paper would greatly benefit from a direct comparison of the sample efficiency of the proposed method with other existing methods in a controlled setting. This comparison is crucial to empirically support your claim of improved sample efficiency. It would provide concrete evidence of the proposed method's performance relative to existing methods, allowing for a more objective evaluation. This addition would strengthen your claims and enhance the paper's impact.",
                "The authors assume that the task-specific hidden parameter \u03b8 captures the task-specific dynamics, which is a crucial assumption that underpins their method. While the paper provides theoretical analysis and experimental results that suggest this assumption is valid, it would be helpful for the authors to provide more explicit validation. This could include a discussion of why they believe \u03b8 captures task-specific dynamics and how the experimental results support this assumption. Providing this validation would strengthen the paper by making the assumption about \u03b8 more clear and convincing.",
                "The authors have defined task similarity in terms of the distance between the task parameters \u03b8 in the HiP-BMDP setting, using a distance metric defined in terms of the Wasserstein-1 metric between transition probability distributions of different tasks. State similarity is defined in terms of bisimulation metrics. While these definitions are clear and well-justified within the context of the authors' method, it is not clear how these measures compare with those used in other methods. To enhance the clarity and impact of the paper, it would be beneficial for the authors to explicitly compare their measures of task and state similarity with those used in other methods. This would provide readers with a better understanding of the unique contributions of the authors' method and its potential advantages and disadvantages compared to other methods.",
                "The authors provide a theoretical proof for the 'Transfer bound' formula, which is a significant contribution. This formula is crucial for measuring the transferability of a policy learned on one task to another, taking into account the error from the learned representation. However, to ensure its robustness and applicability in real-world scenarios, it would be beneficial if the authors could empirically test this formula in reinforcement learning environments. For instance, the agent could be trained on one game and then tested on a different game with similar mechanics. This would provide empirical evidence supporting the theoretical proof and demonstrate the practical utility of the formula.",
                "The authors use the Wasserstein distance in two key contexts: to define a distance metric between two environment settings and to learn a HiP-BMDP approximation of a family of MDPs. However, the paper lacks a detailed explanation or justification for this choice. It is crucial for readers to understand why the Wasserstein distance was chosen over other distance measures, and how it contributes to the methodology and results of the paper. The authors should provide this explanation or justification to allow readers to evaluate the appropriateness of this measure.",
                "While the authors have commendably tested their method in a variety of environments from the Deepmind Control Suite (DMC), including Cartpole-Swingup-V0, Cheetah-Run-V0, Walker-Run-V0, Walker-Run-V1, and Finger-Spin-V0, it would be beneficial to test the method in environments with more complex dynamics or more variables. This would provide a more rigorous test of the method's ability to handle complex tasks and adapt to different situations, thereby providing a more comprehensive understanding of the method's capabilities and limitations. Ensuring that the method can generalize well across a wide range of tasks and environments is crucial for its practical applicability in real-world scenarios.",
                "The authors provide a detailed discussion of the robustness of their method (HiP-BMDP) to changes in environment dynamics, including theoretical analysis, performance in the context of multi-task reinforcement learning, mathematical proofs and equations, and performance in different environments. However, the paper could benefit from a more explicit discussion of the significance of changes in environment dynamics. Specifically, it would be helpful if the authors could clarify how significant changes in environment dynamics affect the performance of the method, and why this is important for the method's robustness. This would provide a clearer understanding of the method's robustness and its implications for different environments.",
                "The authors have compared their recommended approach for PCGrad with the traditional approach and found it to outperform the latter. However, the specifics of this comparison are not clear in the paper. It would be beneficial for the authors to provide a more detailed comparison of the two approaches in a controlled setting. This could include specifics such as how the gradients are projected in both approaches and the impact of these differences on the performance. This would provide a more direct comparison of the two approaches and help readers understand why the recommended approach is superior.",
                "The authors claim that their model adapts quickly in unseen environments, and they provide some evidence to support this claim, such as the model's performance in two settings (MTRL and meta-RL) and in an unseen environment where only the task parameter \u03b8 is adapted. However, they do not specify the exact number of unseen environments they tested their model in. Providing this information would strengthen their claim by giving readers a clearer idea of the range of environments the model can adapt to. It would also be helpful if the authors could explain how the number of unseen environments relates to the model's adaptability, as this is not immediately clear from the paper.",
                "While the authors discuss some limitations of their proposed method, such as the reliance on an environment id and the use of L \u221e norms for measuring error and the value and transfer bounds, the discussion could be more comprehensive. Specifically, the authors could discuss potential limitations of their method when applied to more complex or real-world environments, as the current experiments seem to be conducted in relatively controlled settings. This would provide a more balanced view of their method and help readers understand its applicability and potential limitations in different contexts.",
                "The paper mentions an encoder that maps observations from state space to a learned, latent representation, but it does not provide specific details about the type of encoder used or the process of how it learns the latent representation. These details are crucial for understanding how the model works and how it achieves its performance. Therefore, I recommend that the authors include this information in the paper.",
                "The paper introduces the concepts of Transfer bound and Sample Complexity in Theorems 3 and 4 respectively, and provides their calculations. However, these concepts are not explained in detail in all sections of the paper, making it difficult for readers to fully understand these key performance metrics. It would be beneficial to provide a more comprehensive explanation of these concepts, including their significance and how they are calculated, in all relevant sections. This would help readers better understand the performance of the proposed method and its advantages over prior work.",
                "While the paper provides some information about the characteristics of the Deepmind Control Suite (DMC) environments used in the experiments, it would be beneficial to include more explicit details about how these specific environments were selected. For example, were they chosen because they present different challenges for reinforcement learning algorithms? If so, what are these challenges? Providing this information would help readers understand the rationale behind the choice of environments and could potentially make the results more generalizable.",
                "While the paper provides some information about the hyperparameters used in the MetaRL algorithm for different environments, it does not explain how these hyperparameters were determined. This information is crucial for understanding the algorithm's performance and its application to different environments. Without it, it would be challenging to replicate the results or apply the algorithm to new environments. Therefore, I suggest that the authors include a section detailing how the hyperparameters were determined, including any optimization or tuning processes used.",
                "The paper uses \u03b1 values in various contexts, such as scaling the \u0398 learning error, as a learning rate in an update rule for a parameter \u03c8, and in the MetaRL and MTRL algorithms. Different \u03b1 values are used for different environments and algorithms. However, the paper lacks specific details on how these \u03b1 values were determined and what they represent in different environments. Providing this information would enhance the reproducibility of the study and allow readers to better understand the role and significance of these \u03b1 values in the proposed methods.",
                "The paper mentions the Distral-Ensemble baseline and its inability to leverage a shared global dynamics model. However, it would be beneficial for the reader to understand why this is the case. Could the authors provide more specific details about the limitations of the Distral-Ensemble baseline in leveraging a shared global dynamics model? This would help to clarify the comparison between the proposed method and the Distral-Ensemble baseline.",
                "The paper provides a general overview of the use of the task parameter \u03b8 in the context of Hidden-Parameter Block MDPs (HiP-BMDPs) and its role in conditioning a universal dynamics model. However, it lacks specific details or results regarding the performance of the transition model in an unseen environment when only the task parameter \u03b8 is adapted. Providing these details would strengthen the paper by giving readers a clearer understanding of how the proposed method performs in unseen environments.",
                "Additionally, the paper discusses the adaptation of the task parameter \u03b8 in the context of value bounds and expected error bounds, and in the context of evaluating the universal transition model. However, it does not provide a detailed, step-by-step process of how the task parameter \u03b8 is adapted. Including this information would help readers understand the mechanics of the proposed method and its effects on the model's performance.",
                "The paper provides a high-level overview of the mathematical proofs and theorems related to value bounds, expected error bounds, and additional results and proofs for HiP-BMDP results. However, the detailed proofs and additional results are not included in the main body of the paper. Including these details in the main body of the paper would make it easier for readers to understand the mathematical foundations of your work and how they support your claims. If these details are included in the appendices, consider referencing them more explicitly in the main body of the paper.",
                "The paper lacks specific implementation details of the Meta-RL algorithm and the HiP-MDP setting. This includes the specific steps taken to implement these algorithms, the parameters used, and any challenges that were encountered. These details are crucial for readers who wish to understand the methodology in depth or replicate the study. Providing these details would significantly enhance the paper's clarity and reproducibility."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [],
            "clarity_reproducibility": [],
            "all": []
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 7,
                "n_generated": 4,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 7,
                "n_generated": 10,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "clarity_reproducibility": {
                "n_reference": 7,
                "n_generated": 10,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 7,
                "n_generated": 24,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 0
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 0
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 0
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 0
                }
            ]
        }
    },
    {
        "doc_id": "hbzCPZEIUU",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The comparison in the experiment section is weak without any reference to existing prior arts such as Chen et al., Fine-grained representation learning and recognition by exploiting hierarchical semantic embedding, ACM-MM 2018.",
                "There is a significant performance gap between the proposed method and the latest methods when considering the 5 experiment datasets.",
                "The baseline performance in the experiments shows considerable gaps to standard baseline training without bells and whistles, raising questions about its solidity.",
                "The performance gain of the proposed method diminishes very quickly on bigger datasets such as Tiny ImageNet, and the results on ImageNet are not provided.",
                "The proposed method depends on a pre-defined semantic hierarchical graph rather than a learned one, which potentially limits the technical value of this work.",
                "There is concern about the selection of initial radius $R_0$ and its decay policy, as these parameters should be dataset dependent due to different numbers of categories and the densities of class distributions.",
                "Forcing a fixed radius does not seem as reasonable as allowing a learnable radius with soft regularization."
            ],
            [
                "The empirical evaluation is relatively weak and the evaluation metric does not well reflect the advantages of hierarchically modelling the label space. It would be more informative to incorporate the classification accuracy of the super-classes.",
                "More experiments that analyze the difference in feature distributions between normally trained neural networks and the hierarchically trained neural networks are needed. An intuitive visualization of the feature space would be of great interest.",
                "Important ablation studies to justify some heuristic designs are necessary. For example, the effect of the hyperparameter in the radius decay on performance is crucial. Evaluations should be conducted on what happens if no sphericity constraint is applied, or if no radius decay is used.",
                "The empirical evaluation does not convincingly demonstrate the usefulness of modeling the hierarchical label space, especially in experiments on CIFAR-100 and Tiny-ImageNet. More discussions and insights on this issue would be useful.",
                "The usefulness of the hierarchical label structure should be evaluated and verified. A simple way to evaluate it is to use some random assignment or simple K-means assignments for the super-classes. If using the ground truth hierarchical structure consistently outperforms the random or K-means super-class assignment, then it can be argued that incorporating the ground truth hierarchical label structure is indeed useful."
            ],
            [
                "Section 2 needs more specific definitions for terms such as 'classifier' and 'separators'. For example, in equation (2), Wp and Wpi are called classifiers. Are these hyperplanes?",
                "In Definition 1, it would be helpful to provide early examples of P and L to make it easier to interpret the matrix H.",
                "The notation for Delta in equation (8) could be confusing as it may remind an operator on H in equation (9). Consider using a different notation.",
                "In equation (9), it is unclear whether deltas in the same subtree are required or observed to be in roughly the same direction.",
                "In Section 3, the claim that no hyperparameters are added seems incorrect. Initial radius R0, radius decay parameter, and the organization of classes could all be considered additional hyperparameters.",
                "In practice, defining mutually exclusive class partitions to form the required class tree in Figure 1 can be non-trivial or even impossible. The authors should discuss how different class hierarchies affect the classification accuracy, for example, in Table 2."
            ],
            [
                "The third paragraph of Section 1 is unclear about why Euclidean distance is not sufficient for learning with hierarchical regularization.",
                "The paper does not clearly explain whether the whole model follows end-to-end training or stage-wise training.",
                "In Section 2.2, the definition of 'separator' is unclear and the sentence about classifiers identifying dog breeds being similar is confusing.",
                "The paper does not provide a clear rationale or related work for the use of radius decay in Section 3.1.",
                "The optimization method used for learning other layers is not specified.",
                "The paper does not provide a clear explanation or justification for the construction of the spherical fully-connected layer.",
                "The paper does not present the hierarchical structure in a visually appealing or clear way.",
                "The paper does not clearly explain whether images from datasets Dogs and Tiny-Imagenet are part of ImageNet or if the classes in the two datasets are included in the set of ImageNet classes.",
                "The definition of 'plain networks' in Section 4.1 is unclear.",
                "The paper does not justify its claim of 'high efficiency' in Section 4.2.1.",
                "The paper does not formally compare Riemannian gradient and 'projected gradient' in Figure 2 right.",
                "The paper does not discuss how the proposed method may work if classes do not follow a tree hierarchy.",
                "The explanation of why 'Euclidean distance is not sufficient for learning such a hierarchical regularization' in the rebuttal is not intuitive or concrete.",
                "The paper does not clearly explain whether other methods suffer severely from the choice of learning rate and scheduler.",
                "The blue line in Page-2 is confusing and the logic of why diversity reduces over-fitting is unclear.",
                "The tSNE visuals in Figure 5 are not adequately analyzed."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The paper provides a comparison with other methods that incorporate hierarchical information into the network, such as hyperbolic networks and Poincar\u00e9 networks, which is appreciated. However, it is not clear whether the same datasets and evaluation metrics are used for all methods. For a fair and comprehensive comparison, it would be beneficial if the authors explicitly state whether the same datasets and evaluation metrics are used for all methods. If different datasets or evaluation metrics are used, the authors should justify their choices and discuss how these choices may affect the comparison.",
                "The paper presents a complex method involving several components, including a technique that forces similar classes to be closer in the embedding by making their hyperplanes follow a given hierarchy, a spherical fully-connected layer, a hierarchically connected layer, Riemannian optimization, the use of ResNet and DenseNet for parameter optimization, the construction of hierarchy trees for various datasets, and the generalization performance along different radius decay values. However, the paper does not provide an ablation study or any discussion about the individual contribution of each of these components. An ablation study would provide insights into the contribution of each component to the overall performance. We recommend that the authors conduct an ablation study to understand the importance of each component and how they contribute to the overall performance improvement. This would help readers understand which components are critical to the method's performance and which ones could potentially be modified or omitted without significantly affecting the results.",
                "The paper does not discuss the limitations of the proposed method. Specifically, the method assumes that the hierarchical structure of the class labels is known and only works on the last layer of the deep neural network. This could limit its applicability in real-world applications where the hierarchical structure is not known or when dealing with more complex models. Additionally, the method's performance could be sensitive to the choice of hyperparameters and the construction of the hierarchy for datasets. Discussing these limitations would provide a more balanced view of the method and suggest directions for future work. For instance, future work could explore techniques for inferring the hierarchical structure from the data itself, extending the method to work on multiple layers of the network, automated methods for hyperparameter tuning, and automated or semi-automated methods for constructing the hierarchy for datasets. We suggest that the authors include a section discussing these limitations of their proposed method and potential ways to overcome these limitations in future work."
            ],
            "novelty_impact": [],
            "clarity_reproducibility": [
                "The paper provides a detailed description of the regularization method, including the concept of a Hierarchical layer and a Hierarchically connected layer, the use of weight decay, and the strategy of radius decay. However, it lacks the exact mathematical formulation of the regularization term that constrains the distance between children nodes and parent nodes. Providing this formulation would allow readers to fully understand and implement the proposed regularization method. It would be particularly helpful to include the mathematical details of how the Hierarchical layer and the Hierarchically connected layer are used to parametrize the classifiers, and how the radius decay strategy is applied.",
                "The paper mentions the use of the Wordnet hierarchy to build the hierarchy for the (Tiny) Imagenet dataset, but the specific process is not thoroughly explained. For the sake of reproducibility and to provide a clear understanding of the methodology, it would be beneficial to include a detailed explanation of how the Wordnet hierarchy was used in the initial stages of building the dataset hierarchy, prior to the post-processing steps. This could include, for example, how the Wordnet categories were mapped to the images in the dataset, and how the hierarchical structure was derived from the Wordnet graph.",
                "The paper provides some information about the radius decay, such as its definition as a function of the path and the use of a simple strategy where the radius decreases with respect to the path length. However, it lacks specific details on how the radius decay is calculated and implemented. For example, it would be helpful to know how the optimal radius decay is found using cross-validation and how the predefined diagonal matrix D is used to apply the radius decay. Without these details, it is difficult to fully understand and implement the method. Furthermore, the paper mentions that the radius decay may influence the accuracy of the network, but it does not explain how or why this is the case. Providing this information would make the paper more complete and easier to understand.",
                "The paper provides some information about the baseline methods used for comparison in the numerical experiments, which include the 'Plain', 'Multitask', 'Hierarchy', '+Manifold', and '+Riemann' methods. However, to fully evaluate the relative performance of the proposed method, it would be helpful to include more details about these baseline methods. Specifically, the paper could benefit from including information about the specific parameters used, the configuration of the methods, and how they were implemented. This additional information would provide readers with a clearer understanding of the baseline methods and allow for a more accurate comparison with the proposed method.",
                "The paper presents an interesting method and provides results from experiments, but it lacks specific details on how these results were analyzed and interpreted. It is unclear what statistical tests were used to determine the significance of the results, and how accuracy improvements were quantified and compared across different methods and datasets. Providing this information would greatly enhance the reader's understanding of the significance and impact of the results."
            ],
            "all": [
                "The paper provides a comparison with other methods that incorporate hierarchical information into the network, such as hyperbolic networks and Poincar\u00e9 networks, which is appreciated. However, it is not clear whether the same datasets and evaluation metrics are used for all methods. For a fair and comprehensive comparison, it would be beneficial if the authors explicitly state whether the same datasets and evaluation metrics are used for all methods. If different datasets or evaluation metrics are used, the authors should justify their choices and discuss how these choices may affect the comparison.",
                "The paper presents a complex method involving several components, including a technique that forces similar classes to be closer in the embedding by making their hyperplanes follow a given hierarchy, a spherical fully-connected layer, a hierarchically connected layer, Riemannian optimization, the use of ResNet and DenseNet for parameter optimization, the construction of hierarchy trees for various datasets, and the generalization performance along different radius decay values. However, the paper does not provide an ablation study or any discussion about the individual contribution of each of these components. An ablation study would provide insights into the contribution of each component to the overall performance. We recommend that the authors conduct an ablation study to understand the importance of each component and how they contribute to the overall performance improvement. This would help readers understand which components are critical to the method's performance and which ones could potentially be modified or omitted without significantly affecting the results.",
                "The paper does not discuss the limitations of the proposed method. Specifically, the method assumes that the hierarchical structure of the class labels is known and only works on the last layer of the deep neural network. This could limit its applicability in real-world applications where the hierarchical structure is not known or when dealing with more complex models. Additionally, the method's performance could be sensitive to the choice of hyperparameters and the construction of the hierarchy for datasets. Discussing these limitations would provide a more balanced view of the method and suggest directions for future work. For instance, future work could explore techniques for inferring the hierarchical structure from the data itself, extending the method to work on multiple layers of the network, automated methods for hyperparameter tuning, and automated or semi-automated methods for constructing the hierarchy for datasets. We suggest that the authors include a section discussing these limitations of their proposed method and potential ways to overcome these limitations in future work.",
                "The paper provides a detailed description of the regularization method, including the concept of a Hierarchical layer and a Hierarchically connected layer, the use of weight decay, and the strategy of radius decay. However, it lacks the exact mathematical formulation of the regularization term that constrains the distance between children nodes and parent nodes. Providing this formulation would allow readers to fully understand and implement the proposed regularization method. It would be particularly helpful to include the mathematical details of how the Hierarchical layer and the Hierarchically connected layer are used to parametrize the classifiers, and how the radius decay strategy is applied.",
                "The paper mentions the use of the Wordnet hierarchy to build the hierarchy for the (Tiny) Imagenet dataset, but the specific process is not thoroughly explained. For the sake of reproducibility and to provide a clear understanding of the methodology, it would be beneficial to include a detailed explanation of how the Wordnet hierarchy was used in the initial stages of building the dataset hierarchy, prior to the post-processing steps. This could include, for example, how the Wordnet categories were mapped to the images in the dataset, and how the hierarchical structure was derived from the Wordnet graph.",
                "The paper provides some information about the radius decay, such as its definition as a function of the path and the use of a simple strategy where the radius decreases with respect to the path length. However, it lacks specific details on how the radius decay is calculated and implemented. For example, it would be helpful to know how the optimal radius decay is found using cross-validation and how the predefined diagonal matrix D is used to apply the radius decay. Without these details, it is difficult to fully understand and implement the method. Furthermore, the paper mentions that the radius decay may influence the accuracy of the network, but it does not explain how or why this is the case. Providing this information would make the paper more complete and easier to understand.",
                "The paper provides some information about the baseline methods used for comparison in the numerical experiments, which include the 'Plain', 'Multitask', 'Hierarchy', '+Manifold', and '+Riemann' methods. However, to fully evaluate the relative performance of the proposed method, it would be helpful to include more details about these baseline methods. Specifically, the paper could benefit from including information about the specific parameters used, the configuration of the methods, and how they were implemented. This additional information would provide readers with a clearer understanding of the baseline methods and allow for a more accurate comparison with the proposed method.",
                "The paper presents an interesting method and provides results from experiments, but it lacks specific details on how these results were analyzed and interpreted. It is unclear what statistical tests were used to determine the significance of the results, and how accuracy improvements were quantified and compared across different methods and datasets. Providing this information would greatly enhance the reader's understanding of the significance and impact of the results."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 4,
                    "predicted_id": 2,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The proposed method depends on a pre-defined semantic hierarchical graph rather than a learned one, which potentially limits the technical value of this work.",
                    "predicted_comment": "The paper does not discuss the limitations of the proposed method. Specifically, the method assumes that the hierarchical structure of the class labels is known and only works on the last layer of the deep neural network. This could limit its applicability in real-world applications where the hierarchical structure is not known or when dealing with more complex models. Additionally, the method's performance could be sensitive to the choice of hyperparameters and the construction of the hierarchy for datasets. Discussing these limitations would provide a more balanced view of the method and suggest directions for future work. For instance, future work could explore techniques for inferring the hierarchical structure from the data itself, extending the method to work on multiple layers of the network, automated methods for hyperparameter tuning, and automated or semi-automated methods for constructing the hierarchy for datasets. We suggest that the authors include a section discussing these limitations of their proposed method and potential ways to overcome these limitations in future work."
                },
                {
                    "reference_id": 11,
                    "predicted_id": 2,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper does not discuss how the proposed method may work if classes do not follow a tree hierarchy.",
                    "predicted_comment": "The paper does not discuss the limitations of the proposed method. Specifically, the method assumes that the hierarchical structure of the class labels is known and only works on the last layer of the deep neural network. This could limit its applicability in real-world applications where the hierarchical structure is not known or when dealing with more complex models. Additionally, the method's performance could be sensitive to the choice of hyperparameters and the construction of the hierarchy for datasets. Discussing these limitations would provide a more balanced view of the method and suggest directions for future work. For instance, future work could explore techniques for inferring the hierarchical structure from the data itself, extending the method to work on multiple layers of the network, automated methods for hyperparameter tuning, and automated or semi-automated methods for constructing the hierarchy for datasets. We suggest that the authors include a section discussing these limitations of their proposed method and potential ways to overcome these limitations in future work."
                }
            ],
            "novelty_impact": [],
            "clarity_reproducibility": [
                {
                    "reference_id": 5,
                    "predicted_id": 5,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "There is concern about the selection of initial radius $R_0$ and its decay policy, as these parameters should be dataset dependent due to different numbers of categories and the densities of class distributions.",
                    "predicted_comment": "The paper provides some information about the radius decay, such as its definition as a function of the path and the use of a simple strategy where the radius decreases with respect to the path length. However, it lacks specific details on how the radius decay is calculated and implemented. For example, it would be helpful to know how the optimal radius decay is found using cross-validation and how the predefined diagonal matrix D is used to apply the radius decay. Without these details, it is difficult to fully understand and implement the method. Furthermore, the paper mentions that the radius decay may influence the accuracy of the network, but it does not explain how or why this is the case. Providing this information would make the paper more complete and easier to understand."
                },
                {
                    "reference_id": 3,
                    "predicted_id": 5,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper does not provide a clear rationale or related work for the use of radius decay in Section 3.1.",
                    "predicted_comment": "The paper provides some information about the radius decay, such as its definition as a function of the path and the use of a simple strategy where the radius decreases with respect to the path length. However, it lacks specific details on how the radius decay is calculated and implemented. For example, it would be helpful to know how the optimal radius decay is found using cross-validation and how the predefined diagonal matrix D is used to apply the radius decay. Without these details, it is difficult to fully understand and implement the method. Furthermore, the paper mentions that the radius decay may influence the accuracy of the network, but it does not explain how or why this is the case. Providing this information would make the paper more complete and easier to understand."
                }
            ],
            "all": [
                {
                    "reference_id": 5,
                    "predicted_id": 5,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "There is concern about the selection of initial radius $R_0$ and its decay policy, as these parameters should be dataset dependent due to different numbers of categories and the densities of class distributions.",
                    "predicted_comment": "The paper provides some information about the radius decay, such as its definition as a function of the path and the use of a simple strategy where the radius decreases with respect to the path length. However, it lacks specific details on how the radius decay is calculated and implemented. For example, it would be helpful to know how the optimal radius decay is found using cross-validation and how the predefined diagonal matrix D is used to apply the radius decay. Without these details, it is difficult to fully understand and implement the method. Furthermore, the paper mentions that the radius decay may influence the accuracy of the network, but it does not explain how or why this is the case. Providing this information would make the paper more complete and easier to understand."
                },
                {
                    "reference_id": 4,
                    "predicted_id": 2,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The proposed method depends on a pre-defined semantic hierarchical graph rather than a learned one, which potentially limits the technical value of this work.",
                    "predicted_comment": "The paper does not discuss the limitations of the proposed method. Specifically, the method assumes that the hierarchical structure of the class labels is known and only works on the last layer of the deep neural network. This could limit its applicability in real-world applications where the hierarchical structure is not known or when dealing with more complex models. Additionally, the method's performance could be sensitive to the choice of hyperparameters and the construction of the hierarchy for datasets. Discussing these limitations would provide a more balanced view of the method and suggest directions for future work. For instance, future work could explore techniques for inferring the hierarchical structure from the data itself, extending the method to work on multiple layers of the network, automated methods for hyperparameter tuning, and automated or semi-automated methods for constructing the hierarchy for datasets. We suggest that the authors include a section discussing these limitations of their proposed method and potential ways to overcome these limitations in future work."
                },
                {
                    "reference_id": 11,
                    "predicted_id": 2,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper does not discuss how the proposed method may work if classes do not follow a tree hierarchy.",
                    "predicted_comment": "The paper does not discuss the limitations of the proposed method. Specifically, the method assumes that the hierarchical structure of the class labels is known and only works on the last layer of the deep neural network. This could limit its applicability in real-world applications where the hierarchical structure is not known or when dealing with more complex models. Additionally, the method's performance could be sensitive to the choice of hyperparameters and the construction of the hierarchy for datasets. Discussing these limitations would provide a more balanced view of the method and suggest directions for future work. For instance, future work could explore techniques for inferring the hierarchical structure from the data itself, extending the method to work on multiple layers of the network, automated methods for hyperparameter tuning, and automated or semi-automated methods for constructing the hierarchy for datasets. We suggest that the authors include a section discussing these limitations of their proposed method and potential ways to overcome these limitations in future work."
                },
                {
                    "reference_id": 3,
                    "predicted_id": 5,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 3,
                    "score": 0.6,
                    "reference_comment": "The paper does not provide a clear rationale or related work for the use of radius decay in Section 3.1.",
                    "predicted_comment": "The paper provides some information about the radius decay, such as its definition as a function of the path and the use of a simple strategy where the radius decreases with respect to the path length. However, it lacks specific details on how the radius decay is calculated and implemented. For example, it would be helpful to know how the optimal radius decay is found using cross-validation and how the predefined diagonal matrix D is used to apply the radius decay. Without these details, it is difficult to fully understand and implement the method. Furthermore, the paper mentions that the radius decay may influence the accuracy of the network, but it does not explain how or why this is the case. Providing this information would make the paper more complete and easier to understand."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 34,
                "n_generated": 3,
                "n_total_aligns": 2,
                "recall": 0.05133928571428571,
                "precision": 0.16666666666666666,
                "pseudo_jaccard": 0.041666666666666664,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 34,
                "n_generated": 0,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "clarity_reproducibility": {
                "n_reference": 34,
                "n_generated": 5,
                "n_total_aligns": 2,
                "recall": 0.05133928571428571,
                "precision": 0.1,
                "pseudo_jaccard": 0.03522727272727273,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "all": {
                "n_reference": 34,
                "n_generated": 8,
                "n_total_aligns": 4,
                "recall": 0.10267857142857142,
                "precision": 0.125,
                "pseudo_jaccard": 0.06118881118881119,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.14285714285714285,
                    "precision": 0.3333333333333333,
                    "pseudo_jaccard": 0.1111111111111111,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0625,
                    "precision": 0.3333333333333333,
                    "pseudo_jaccard": 0.05555555555555555,
                    "n_real": 16
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 16
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.14285714285714285,
                    "precision": 0.2,
                    "pseudo_jaccard": 0.09090909090909091,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.0625,
                    "precision": 0.2,
                    "pseudo_jaccard": 0.05,
                    "n_real": 16
                }
            ],
            "all": [
                {
                    "recall": 0.2857142857142857,
                    "precision": 0.25,
                    "pseudo_jaccard": 0.15384615384615385,
                    "n_real": 7
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 5
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                },
                {
                    "recall": 0.125,
                    "precision": 0.25,
                    "pseudo_jaccard": 0.09090909090909091,
                    "n_real": 16
                }
            ]
        }
    },
    {
        "doc_id": "ab7lBP7Fb60",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The proposed method is not compatible with federated averaging, which is a significant limitation in federated learning due to the communication bottleneck characteristic of federated settings.",
                "The paper does not compare the proposed method to prior work on group fairness in the centralized setting, such as the work by Zafar et al., (2017). An in-depth experimental evaluation of this aspect is needed to motivate the proposed approach.",
                "The proposed method may be susceptible to a potential privacy violation, as the statistics shared at every round include the number of individuals per protected attribute for every client. This could be exploited to learn an unfair model with respect to certain sensitive attributes.",
                "The paper incorrectly characterizes prior fair federated learning approaches, such as the work by Li et al. (2019), as individual fairness methods. These methods enforce uniform performance across all devices, which is neither individual nor group fairness, but a novel notion tailored to the federated learning setting."
            ],
            [
                "The paper is missing important literature review, including methods that enforce group fairness in Federated Learning such as 'Fair and Consistent Federated Learning' by Cui et al. 2021. These could have been considered as a baseline for comparing their method.",
                "The paper does not include enough baselines for Fair Federated Learning to compare against. Even if some methods do not satisfy privacy considerations, they should still be included for the reader to understand how the proposed method compares against such methods, especially given that the results are not promising. Some baselines to consider include Cui et al or Tran et al.",
                "The authors need to justify some of the incompatibility results shown in previous work between fairness and privacy.",
                "Algorithm 1 needs improvement. It is unclear what the clients do, what 'UpdateMultiplier', 'UpdateParameters', and 'Calculate noiseScale' mean.",
                "The results are not promising for predictive parity among groups. There is also a reduction in accuracy. The authors should discuss why this is the case. Including the suggested baselines could provide a sense of what other methods achieve in terms of loss in accuracy, etc.",
                "The results for the central setting should be included in Table 2."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "The authors have demonstrated the effectiveness of the FPFL algorithm in enforcing group fairness in private federated learning. However, the paper lacks a detailed analysis of how different cohort sizes affect the performance of the FPFL algorithm. Given that the paper suggests that the FPFL algorithm is more sensitive to DP noise than other algorithms for PFL, which requires increasing the cohort size or ensuring that enough users take part in training, it would be beneficial to conduct additional experiments to explore the impact of different cohort sizes on the performance of the FPFL algorithm. This could provide valuable insights into the scalability of the algorithm and its applicability in real-world scenarios where the number of users may vary significantly. Specifically, the authors should provide a detailed analysis of how different cohort sizes affect the accuracy, fairness, and sensitivity to DP noise of the FPFL algorithm. This would strengthen the paper's claim that the proposed algorithm can be tailored to enforce the majority of the group fairness metrics, can consider any number of attributes determining the groups, and can consider both classification and regression tasks."
            ],
            "novelty_impact": [
                "The paper presents a significant contribution to the field with its novel approach to enforcing fairness in private federated learning. However, to enhance the reader's understanding of how the FPFL algorithm ensures fairness, it would be beneficial to provide specific details about the calculation and usage of False Negative Rate (FNR) parity and Accuracy parity within the algorithm. These metrics are key to understanding the fairness of the algorithm, especially in a multi-class classification context. Without these details, it may be difficult for readers to fully comprehend how the algorithm achieves its goal of fairness. Providing these details would likely enhance the reader's understanding of the algorithm and its fairness.",
                "The paper mentions the tolerance for function variation between groups and the overall population, denoted as \u03b1, and its role in the algorithm. However, it does not provide explicit details on how this tolerance is determined. This is a key parameter in the algorithm, and its determination could significantly impact the algorithm's performance and fairness. The authors should provide a detailed explanation or method for determining this value. For example, they could explain if there are any guidelines or criteria for setting its value, or if it is determined empirically based on the data or problem at hand. This would help readers understand how to set this parameter in their own applications of the algorithm.",
                "While the paper mentions that the FPFL algorithm's sensitivity to DP noise was considered in its design and in the experiments conducted, it does not provide specific details on how this sensitivity was measured. For example, it would be helpful to know how the variance of the Gaussian noise was calculated according to the refined moments accountant privacy analysis from (Wang et al., 2019), and how this calculation took into account the number of iterations T, the cohort size m, the total number of users (or population size) K, and the privacy parameters \u03b5 and \u03b4. Additionally, it would be beneficial to understand how the larger signal-to-DP noise ratio helped the models trained with FPFL to maintain desired levels of FNR gap and lower unfairness measured with any other metric. Providing these details would allow readers to better assess the algorithm's robustness and reliability.",
                "The paper presents a series of experiments using a modified version of the FEMNIST dataset, including a class of digits written with white chalk on a blackboard, to evaluate the performance and fairness of the FPFL algorithm. While the experiments serve as a practical application to evaluate the algorithm's effectiveness, it would be beneficial for the authors to provide more explicit details on how these specific experiments contribute to the evaluation of the FPFL algorithm's performance and fairness. For instance, the authors could elaborate on why this particular class was chosen and how the results from this class compare to others. This would help readers better understand the relevance of these experiments to the evaluation of the FPFL algorithm.",
                "The paper could benefit from a more explicit and detailed discussion on the generalizability of the FPFL algorithm. While the paper mentions that the FPFL algorithm can be applied to any model that can be learned using SGD or its variants, it would be helpful to discuss specific examples of other types of data or tasks where the algorithm could be applied. This could include different types of machine learning tasks (e.g., clustering, anomaly detection) or different types of data (e.g., image data, time-series data). Discussing how the FPFL algorithm might perform on datasets with different characteristics, such as those with different distributions of data among users, different levels of imbalance among groups, or different types of tasks (e.g., regression tasks, multi-class classification tasks) would provide a more comprehensive understanding of the algorithm's versatility.",
                "The paper could provide examples of real-world applications where the FPFL algorithm could be particularly useful. This could include scenarios where privacy is paramount, such as healthcare or finance, and where group fairness is a significant concern. Providing such examples would help readers to better assess the potential utility of the FPFL algorithm for their own work.",
                "The paper should discuss potential limitations of the FPFL algorithm. This could include situations where the algorithm might not perform as well, any assumptions made by the algorithm that might not hold in certain scenarios, or any computational or data requirements that might limit its applicability. For example, the paper mentions that the FPFL algorithm is more sensitive to DP noise than other algorithms for PFL. It would be helpful to discuss what this means in practical terms, such as what types of scenarios or datasets this might pose a problem for, and how significant this limitation might be. Discussing these limitations would provide readers with a more complete understanding of the FPFL algorithm's potential constraints, helping them to evaluate its suitability for their own needs.",
                "While the paper introduces the FPFL algorithm as a potential solution to mitigate the issue of DP disproportionately degrading the performance of models on under-represented groups, it would be beneficial to discuss other potential solutions or mitigation strategies as well. This would provide a more comprehensive view of the possible ways to address this issue.",
                "The paper presents the FPFL algorithm as an extension of the MMDM algorithm to enforce group fairness in private federated learning. However, a direct comparison between the two algorithms is missing. It would be beneficial to provide a detailed comparison of the FPFL and MMDM algorithms, particularly in terms of their performance, fairness, and privacy guarantees. This comparison should include a discussion of the specific advantages and disadvantages of using the FPFL algorithm over the MMDM algorithm. Such a comparison would help readers understand the unique contributions of the FPFL algorithm and its potential benefits and drawbacks in different contexts."
            ],
            "clarity_reproducibility": [
                "The paper introduces the FPFL algorithm and extends the MMDM to empirical risk minimization with fairness constraints. However, the specific steps involved in these algorithms and how they enforce fairness are not clearly explained. For the FPFL algorithm, it would be beneficial to provide a step-by-step breakdown of the algorithm and explain how each step contributes to enforcing group fairness in PFL. Similarly, for the MMDM, it would be helpful to provide more details about how it is extended to empirical risk minimization with fairness constraints and how this extension enforces fairness. Providing these details would make it easier for readers to understand the algorithms and their fairness enforcement mechanisms.",
                "While the paper discusses group fairness in the context of private federated learning and proposes an algorithm (FPFL) to enforce fairness, it does not provide a specific definition of group fairness. Providing a clear and detailed definition of group fairness would help readers better understand the concept and its importance in machine learning models. Additionally, the paper could elaborate on how the fairness tolerance parameter \u03b1 is chosen and its impact on the level of fairness. This would provide readers with a better understanding of how fairness is measured and controlled in the proposed algorithm.",
                "The paper provides some details about the Adult and FEMNIST datasets and the FederatedSGD and FPFL models used in the experiments. However, it would be helpful to provide more specific details about how these datasets were prepared and how these models were trained for the experiments. For the datasets, were any preprocessing steps taken, such as data cleaning or normalization? Were the datasets split into training and testing sets, and if so, how was this done? For the models, what were the specific hyperparameters used, such as learning rate, batch size, and number of iterations? Were any special techniques used, such as early stopping or regularization? Providing these details would allow others to reproduce the experiments and would strengthen the paper's reproducibility.",
                "While the paper provides some details about the image modifications, the convolutional network used in the experiments, the specific layer configurations and activation functions, and the specific values of the learning rate, Lagrange multipliers, damping parameter, cohort sizes, and clipping bound used in the experiments, it could benefit from a more detailed explanation of why these specific values were chosen and how the experiments were repeated with different cohort sizes. For instance, it would be helpful to understand the rationale behind the choice of the specific image modifications and how they contribute to the overall research objective. Similarly, a more detailed explanation of the specific layer configurations and activation functions used in the convolutional network could provide more insight into the design of the experiments. In terms of the specific values of the learning rate, Lagrange multipliers, damping parameter, cohort sizes, and clipping bound used in the experiments, it would be beneficial to understand why these specific values were chosen and how they contribute to the overall performance of the models. Furthermore, it would be helpful to understand how the experiments were repeated with different cohort sizes and how these different cohort sizes were chosen. This could provide more insight into the robustness of the models and their performance under different conditions. Finally, a more detailed explanation of how the results were compared and what metrics were used to evaluate the performance of the models could provide more clarity on the evaluation process and the overall results of the experiments. By providing more detailed explanations and justifications for these aspects of the paper, the authors could enhance the clarity and comprehensibility of their work, making it more accessible and useful to other researchers in the field."
            ],
            "all": [
                "The authors have demonstrated the effectiveness of the FPFL algorithm in enforcing group fairness in private federated learning. However, the paper lacks a detailed analysis of how different cohort sizes affect the performance of the FPFL algorithm. Given that the paper suggests that the FPFL algorithm is more sensitive to DP noise than other algorithms for PFL, which requires increasing the cohort size or ensuring that enough users take part in training, it would be beneficial to conduct additional experiments to explore the impact of different cohort sizes on the performance of the FPFL algorithm. This could provide valuable insights into the scalability of the algorithm and its applicability in real-world scenarios where the number of users may vary significantly. Specifically, the authors should provide a detailed analysis of how different cohort sizes affect the accuracy, fairness, and sensitivity to DP noise of the FPFL algorithm. This would strengthen the paper's claim that the proposed algorithm can be tailored to enforce the majority of the group fairness metrics, can consider any number of attributes determining the groups, and can consider both classification and regression tasks.",
                "The paper presents a significant contribution to the field with its novel approach to enforcing fairness in private federated learning. However, to enhance the reader's understanding of how the FPFL algorithm ensures fairness, it would be beneficial to provide specific details about the calculation and usage of False Negative Rate (FNR) parity and Accuracy parity within the algorithm. These metrics are key to understanding the fairness of the algorithm, especially in a multi-class classification context. Without these details, it may be difficult for readers to fully comprehend how the algorithm achieves its goal of fairness. Providing these details would likely enhance the reader's understanding of the algorithm and its fairness.",
                "The paper mentions the tolerance for function variation between groups and the overall population, denoted as \u03b1, and its role in the algorithm. However, it does not provide explicit details on how this tolerance is determined. This is a key parameter in the algorithm, and its determination could significantly impact the algorithm's performance and fairness. The authors should provide a detailed explanation or method for determining this value. For example, they could explain if there are any guidelines or criteria for setting its value, or if it is determined empirically based on the data or problem at hand. This would help readers understand how to set this parameter in their own applications of the algorithm.",
                "While the paper mentions that the FPFL algorithm's sensitivity to DP noise was considered in its design and in the experiments conducted, it does not provide specific details on how this sensitivity was measured. For example, it would be helpful to know how the variance of the Gaussian noise was calculated according to the refined moments accountant privacy analysis from (Wang et al., 2019), and how this calculation took into account the number of iterations T, the cohort size m, the total number of users (or population size) K, and the privacy parameters \u03b5 and \u03b4. Additionally, it would be beneficial to understand how the larger signal-to-DP noise ratio helped the models trained with FPFL to maintain desired levels of FNR gap and lower unfairness measured with any other metric. Providing these details would allow readers to better assess the algorithm's robustness and reliability.",
                "The paper presents a series of experiments using a modified version of the FEMNIST dataset, including a class of digits written with white chalk on a blackboard, to evaluate the performance and fairness of the FPFL algorithm. While the experiments serve as a practical application to evaluate the algorithm's effectiveness, it would be beneficial for the authors to provide more explicit details on how these specific experiments contribute to the evaluation of the FPFL algorithm's performance and fairness. For instance, the authors could elaborate on why this particular class was chosen and how the results from this class compare to others. This would help readers better understand the relevance of these experiments to the evaluation of the FPFL algorithm.",
                "The paper could benefit from a more explicit and detailed discussion on the generalizability of the FPFL algorithm. While the paper mentions that the FPFL algorithm can be applied to any model that can be learned using SGD or its variants, it would be helpful to discuss specific examples of other types of data or tasks where the algorithm could be applied. This could include different types of machine learning tasks (e.g., clustering, anomaly detection) or different types of data (e.g., image data, time-series data). Discussing how the FPFL algorithm might perform on datasets with different characteristics, such as those with different distributions of data among users, different levels of imbalance among groups, or different types of tasks (e.g., regression tasks, multi-class classification tasks) would provide a more comprehensive understanding of the algorithm's versatility.",
                "The paper could provide examples of real-world applications where the FPFL algorithm could be particularly useful. This could include scenarios where privacy is paramount, such as healthcare or finance, and where group fairness is a significant concern. Providing such examples would help readers to better assess the potential utility of the FPFL algorithm for their own work.",
                "The paper should discuss potential limitations of the FPFL algorithm. This could include situations where the algorithm might not perform as well, any assumptions made by the algorithm that might not hold in certain scenarios, or any computational or data requirements that might limit its applicability. For example, the paper mentions that the FPFL algorithm is more sensitive to DP noise than other algorithms for PFL. It would be helpful to discuss what this means in practical terms, such as what types of scenarios or datasets this might pose a problem for, and how significant this limitation might be. Discussing these limitations would provide readers with a more complete understanding of the FPFL algorithm's potential constraints, helping them to evaluate its suitability for their own needs.",
                "While the paper introduces the FPFL algorithm as a potential solution to mitigate the issue of DP disproportionately degrading the performance of models on under-represented groups, it would be beneficial to discuss other potential solutions or mitigation strategies as well. This would provide a more comprehensive view of the possible ways to address this issue.",
                "The paper presents the FPFL algorithm as an extension of the MMDM algorithm to enforce group fairness in private federated learning. However, a direct comparison between the two algorithms is missing. It would be beneficial to provide a detailed comparison of the FPFL and MMDM algorithms, particularly in terms of their performance, fairness, and privacy guarantees. This comparison should include a discussion of the specific advantages and disadvantages of using the FPFL algorithm over the MMDM algorithm. Such a comparison would help readers understand the unique contributions of the FPFL algorithm and its potential benefits and drawbacks in different contexts.",
                "The paper introduces the FPFL algorithm and extends the MMDM to empirical risk minimization with fairness constraints. However, the specific steps involved in these algorithms and how they enforce fairness are not clearly explained. For the FPFL algorithm, it would be beneficial to provide a step-by-step breakdown of the algorithm and explain how each step contributes to enforcing group fairness in PFL. Similarly, for the MMDM, it would be helpful to provide more details about how it is extended to empirical risk minimization with fairness constraints and how this extension enforces fairness. Providing these details would make it easier for readers to understand the algorithms and their fairness enforcement mechanisms.",
                "While the paper discusses group fairness in the context of private federated learning and proposes an algorithm (FPFL) to enforce fairness, it does not provide a specific definition of group fairness. Providing a clear and detailed definition of group fairness would help readers better understand the concept and its importance in machine learning models. Additionally, the paper could elaborate on how the fairness tolerance parameter \u03b1 is chosen and its impact on the level of fairness. This would provide readers with a better understanding of how fairness is measured and controlled in the proposed algorithm.",
                "The paper provides some details about the Adult and FEMNIST datasets and the FederatedSGD and FPFL models used in the experiments. However, it would be helpful to provide more specific details about how these datasets were prepared and how these models were trained for the experiments. For the datasets, were any preprocessing steps taken, such as data cleaning or normalization? Were the datasets split into training and testing sets, and if so, how was this done? For the models, what were the specific hyperparameters used, such as learning rate, batch size, and number of iterations? Were any special techniques used, such as early stopping or regularization? Providing these details would allow others to reproduce the experiments and would strengthen the paper's reproducibility.",
                "While the paper provides some details about the image modifications, the convolutional network used in the experiments, the specific layer configurations and activation functions, and the specific values of the learning rate, Lagrange multipliers, damping parameter, cohort sizes, and clipping bound used in the experiments, it could benefit from a more detailed explanation of why these specific values were chosen and how the experiments were repeated with different cohort sizes. For instance, it would be helpful to understand the rationale behind the choice of the specific image modifications and how they contribute to the overall research objective. Similarly, a more detailed explanation of the specific layer configurations and activation functions used in the convolutional network could provide more insight into the design of the experiments. In terms of the specific values of the learning rate, Lagrange multipliers, damping parameter, cohort sizes, and clipping bound used in the experiments, it would be beneficial to understand why these specific values were chosen and how they contribute to the overall performance of the models. Furthermore, it would be helpful to understand how the experiments were repeated with different cohort sizes and how these different cohort sizes were chosen. This could provide more insight into the robustness of the models and their performance under different conditions. Finally, a more detailed explanation of how the results were compared and what metrics were used to evaluate the performance of the models could provide more clarity on the evaluation process and the overall results of the experiments. By providing more detailed explanations and justifications for these aspects of the paper, the authors could enhance the clarity and comprehensibility of their work, making it more accessible and useful to other researchers in the field."
            ]
        },
        "alignments": {
            "experiments_proofs": [],
            "novelty_impact": [],
            "clarity_reproducibility": [],
            "all": []
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 10,
                "n_generated": 1,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "novelty_impact": {
                "n_reference": 10,
                "n_generated": 9,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "clarity_reproducibility": {
                "n_reference": 10,
                "n_generated": 4,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 10,
                "n_generated": 14,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                }
            ],
            "all": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 6
                }
            ]
        }
    },
    {
        "doc_id": "rsf1z-JSj87",
        "method": "gpt_specialized_multi_agent",
        "reference_comments": [
            [
                "The performance of the proposed model is promising, but still below the baselines.",
                "The claim of the model being end-to-end is misleading as the character-based model is not performing well, and the phoneme-based model is not truly end-to-end, as the grapheme-to-phoneme (g2p) part is not trained jointly.",
                "The reason behind using an adversarial loss is not adequately explained in the paper. A few lines before section 2.1 would help clarify that.",
                "It's not clear which dataset was used in the experiment. If it is a private dataset, please state it clearly."
            ],
            [
                "The paper does not provide a comparison over the computational cost or model size. This is particularly important because the proposed model is non-autoregressive and may be capable of real-time inference.",
                "The paper does not use widely accepted benchmark datasets. A more direct comparison would be beneficial.",
                "The proposed aligner module doesn't seem to be very useful compared with the attention-based aligner as seen in the ablation study (Table 1): there is only a very small improvement from 3.551 to 3.559 MOS. More explanation is needed."
            ],
            [
                "The elegance in the architecture is overshadowed by the complicated training algorithm, making the method look a bit heuristic and harder to reproduce.",
                "The authors should provide brief guidelines for readers trying to reimplement EATS, such as how to tune the hyperparameters.",
                "EATS performs slightly worse than GAN-TTS, which does not quite show the benefit of end-to-end training. It would be worthwhile to expand the discussion a bit by showing further experiments that demonstrate the potential benefit of end-to-end training.",
                "Without a clearly interpretable hidden representation, it is harder to have direct control over prosody. The authors should discuss how prosody control would be possible under the end-to-end framework."
            ],
            [
                "The paper lacks a comparison with Tacotron 2 in Table 1. This would be informative as all the baselines are trained on aligned data while Tacotron is a legitimate contender for EATS as it can be trained on the same data.",
                "The authors claim that their method is simpler because the training is in one stage. However, given the large number of losses and components in their model, with their respective hyper-parameters to tune, the simplicity argument is not entirely convincing.",
                "The performance of the transformer attention is surprisingly poor, particularly in the audio samples where the output for this model is garbage towards the end of the signal. The authors should provide an explanation for this."
            ]
        ],
        "generated_comments": {
            "experiments_proofs": [
                "Diversity of Datasets: The paper currently tests the model on a dataset consisting of North American English speakers. To demonstrate the model's generalizability, it would be beneficial to test it on a more diverse range of datasets. This could include datasets representing different languages, accents, and speaking styles. For example, testing the model on datasets in Spanish, Mandarin, or Arabic, and datasets representing a variety of accents and speaking styles within English, would provide a more robust demonstration of the model's applicability across diverse linguistic contexts. This would strengthen the paper by providing evidence of the model's potential for broad applicability.",
                "The authors have compared their model with several state-of-the-art TTS systems, which is commendable. However, to fully demonstrate the incremental improvement offered by their model, it would be beneficial to also compare with simpler baseline models. Specifically, traditional concatenative or parametric TTS systems could serve as useful points of comparison. Concatenative TTS systems use large databases of small speech fragments that are concatenated to form complete utterances, while parametric TTS systems use mathematical models to generate speech. Comparing the proposed model with these simpler models could highlight the specific areas where the authors' model excels, such as in generating more natural-sounding speech or in handling longer sequences. It could also provide a benchmark for evaluating the complexity and computational efficiency of the authors' model. This additional comparison would strengthen the paper by providing a more comprehensive evaluation of the proposed model.",
                "Analysis of Component Contributions: The paper provides a detailed description of the components of the method, including the aligner, the adversarial discriminators, the spectrogram prediction loss, and the use of dynamic time warping. However, it would be beneficial to include a more detailed analysis of how each of these components contributes to the overall performance of the model. For instance, how does the aligner's prediction of the duration of each input token affect the model's performance? How does the spectrogram prediction loss guide learning, and what is its impact on the final results? How does the use of dynamic time warping affect the alignment of the generated and target spectrograms, and how does this influence the model's performance? Providing this analysis would give readers a better understanding of the importance of each component and the interplay between them in achieving the reported results.",
                "In-depth Analysis of Failure Cases: The paper discusses some limitations of the model and areas for future improvement, but it would be beneficial to include a more detailed analysis of specific cases where the model fails or performs poorly. For example, the paper mentions that the model struggles with learning alignment from adversarial feedback alone and that the spectrogram prediction loss incorrectly assumes that token lengths are deterministic. An in-depth analysis of these issues, including examples of specific inputs that lead to these problems and a discussion of their impact on the model's performance, could provide valuable insights for future improvements. Additionally, the paper acknowledges a gap in fidelity between the speech produced by the model and state-of-the-art systems. An analysis of the types of inputs or situations that lead to this gap could help to identify areas for future research and improvement.",
                "Given the nature of the model as a text-to-speech system, it is crucial to evaluate its robustness to various perturbations that could occur in real-world scenarios. The paper currently lacks this evaluation. Specifically, it would be beneficial to test how the model performs when there is noise in the input data or changes in the speaking rate or pitch. These perturbations could significantly affect the quality and naturalness of the synthesized speech, and understanding the model's performance under these conditions would provide valuable insights into its robustness and applicability.",
                "The paper presents an interesting application of the adversarial approach in the EATS model for text-to-speech synthesis. However, the evaluation of this approach could be improved. Specifically, it would be beneficial to include an analysis of how the performance of the model changes as the strength of the supervisory signal is varied. This would provide more concrete evidence to support the claim that the adversarial approach can learn from a relatively weak supervisory signal. Additionally, it would be helpful to clarify the role of the soft dynamic time warping (DTW) procedure in the spectrogram prediction loss and whether it is related to the supervisory signal."
            ],
            "novelty_impact": [
                "The paper currently lacks specific details on how the model would handle non-North American accents or other languages. Given that the model was trained on North American English speakers and uses a phonemizer for input text, it would be beneficial to clarify how these factors influence the model's ability to handle different accents or languages. This could include, for example, discussing whether the model would require retraining for different accents or languages, or whether the phonemizer allows for some degree of accent or language versatility. Providing these details would help to demonstrate the model's versatility and broaden its applicability.",
                "The paper lacks a discussion on how the model would handle languages or dialects that the phonemizer may not support. Given that the phonemizer is used for partial normalization and phonemisation of the input text, it is crucial to understand how the model would handle unsupported languages or dialects. This is particularly important considering the model's application to diverse datasets. The paper would benefit from a detailed discussion on this, including potential strategies for handling unsupported languages or dialects and the impact of these strategies on the model's performance. This would provide a clearer understanding of the model's robustness and applicability to diverse datasets.",
                "The paper notes a discrepancy between MOS and FDSD scores and mentions that the sample sizes commonly used for MOS testing are usually smaller than those used for metrics based on Fr\u00e9chet distance. However, the paper does not explicitly discuss the limitations of these metrics. It would be beneficial to include a discussion on the potential limitations of the MOS and FDSD metrics in capturing all aspects of speech synthesis quality. This could help to prevent an overestimation of the model's performance and provide a more comprehensive understanding of the results.",
                "While the paper provides some comparisons of the aligner architecture with attention-based architectures and the transformer-based attention aligner, it would be beneficial to include a direct comparison with the GAN-TTS and BigGAN-deep discriminator architectures, which are also mentioned in the paper. This comparison should focus on specific aspects of efficiency, such as the use of monotonic interpolation, the prediction of token durations, and the inference speed. Including these comparisons would provide a more comprehensive assessment of the aligner's efficiency and would help readers understand how the aligner's design contributes to its efficiency. For example, it would be helpful to know how the aligner's use of monotonic interpolation compares to the recurrent nature of many monotonic attention mechanisms in terms of efficiency. Similarly, a comparison of the aligner's method of varying predicted token lengths using the latent vector z with the methods used by other architectures could provide valuable insights. Finally, a comparison of the aligner's inference speed with that of other architectures would provide a clear measure of its efficiency.",
                "While the paper discusses the use of dynamic time warping (DTW) in the model and how the model handles local variations in the speech signal, it lacks a detailed discussion on the sensitivity of DTW to such variations. Given the importance of this aspect in understanding the model's performance and robustness, it would be beneficial to include a more thorough discussion on this topic. This could include, for example, how the sensitivity of DTW to local variations affects the model's performance, and how the model mitigates any potential issues arising from this sensitivity.",
                "The paper discusses the challenge of adversarial feedback being insufficient to learn alignment, but it could benefit from a more comprehensive discussion on the potential challenges of using adversarial feedback and domain-specific loss functions in training the system. For instance, adversarial feedback could lead to stability issues during training, the need for large amounts of training data, difficulty in balancing the adversarial loss with other losses, the potential for mode collapse, and the need for careful hyperparameter tuning. Similarly, using domain-specific loss functions could present challenges such as the need for careful design and tuning of these loss functions, the difficulty in balancing different loss functions, the potential for overfitting to the training data, and the need for careful hyperparameter tuning to ensure stable training. Discussing these challenges could help others in replicating the results or extending the model.",
                "While the paper provides a detailed discussion of the computational requirements of the EATS model, including its architecture, training process, and inference speed, it could benefit from including more specific details about the memory usage and the time complexity of the model. These details are crucial for potential users to assess the applicability of the model in their specific, potentially resource-constrained, environments.",
                "The paper provides a valuable discussion of the challenges in automatic quantitative evaluation of text-to-speech models, particularly the limitations of Fr\u00e9chet DeepSpeech Distances (FDSD) and Mean Opinion Scores (MOS). However, it would significantly enhance the paper's impact if the authors could propose specific solutions or strategies to address these challenges. This would not only advance the field's understanding of these evaluation metrics but also increase the practical applicability of the proposed model and similar models in the future.",
                "The paper provides a detailed technical description of the EATS model and its performance, which is commendable. However, it lacks a discussion on how EATS could be integrated with existing TTS systems or its potential real-world applications. This is a significant omission as it leaves the reader wondering about the practical use and potential impact of the model. To enhance the impact of your work, consider adding a section on future work or potential applications. This could include a discussion on how the efficiency of EATS in both training and inference, and its independence from autoregressive sampling or teacher forcing, could facilitate its integration with existing systems. Also, discuss potential real-world applications of EATS and the potential challenges and solutions in implementing the model in real-world scenarios. This would provide a clearer understanding of the practicality and usability of the model, and could significantly increase the impact of your work.",
                "The paper presents a comparison with a transformer-based attention aligner baseline in section 'G TRANSFORMER-BASED ATTENTION ALIGNER BASELINE', but it does not discuss the potential limitations of this approach. Discussing the limitations, such as computational complexity, scalability, or performance under certain conditions, would provide a more comprehensive understanding of the model and its applicability. This would be beneficial for others attempting to replicate the results or extend the model."
            ],
            "clarity_reproducibility": [],
            "all": [
                "Diversity of Datasets: The paper currently tests the model on a dataset consisting of North American English speakers. To demonstrate the model's generalizability, it would be beneficial to test it on a more diverse range of datasets. This could include datasets representing different languages, accents, and speaking styles. For example, testing the model on datasets in Spanish, Mandarin, or Arabic, and datasets representing a variety of accents and speaking styles within English, would provide a more robust demonstration of the model's applicability across diverse linguistic contexts. This would strengthen the paper by providing evidence of the model's potential for broad applicability.",
                "The authors have compared their model with several state-of-the-art TTS systems, which is commendable. However, to fully demonstrate the incremental improvement offered by their model, it would be beneficial to also compare with simpler baseline models. Specifically, traditional concatenative or parametric TTS systems could serve as useful points of comparison. Concatenative TTS systems use large databases of small speech fragments that are concatenated to form complete utterances, while parametric TTS systems use mathematical models to generate speech. Comparing the proposed model with these simpler models could highlight the specific areas where the authors' model excels, such as in generating more natural-sounding speech or in handling longer sequences. It could also provide a benchmark for evaluating the complexity and computational efficiency of the authors' model. This additional comparison would strengthen the paper by providing a more comprehensive evaluation of the proposed model.",
                "Analysis of Component Contributions: The paper provides a detailed description of the components of the method, including the aligner, the adversarial discriminators, the spectrogram prediction loss, and the use of dynamic time warping. However, it would be beneficial to include a more detailed analysis of how each of these components contributes to the overall performance of the model. For instance, how does the aligner's prediction of the duration of each input token affect the model's performance? How does the spectrogram prediction loss guide learning, and what is its impact on the final results? How does the use of dynamic time warping affect the alignment of the generated and target spectrograms, and how does this influence the model's performance? Providing this analysis would give readers a better understanding of the importance of each component and the interplay between them in achieving the reported results.",
                "In-depth Analysis of Failure Cases: The paper discusses some limitations of the model and areas for future improvement, but it would be beneficial to include a more detailed analysis of specific cases where the model fails or performs poorly. For example, the paper mentions that the model struggles with learning alignment from adversarial feedback alone and that the spectrogram prediction loss incorrectly assumes that token lengths are deterministic. An in-depth analysis of these issues, including examples of specific inputs that lead to these problems and a discussion of their impact on the model's performance, could provide valuable insights for future improvements. Additionally, the paper acknowledges a gap in fidelity between the speech produced by the model and state-of-the-art systems. An analysis of the types of inputs or situations that lead to this gap could help to identify areas for future research and improvement.",
                "Given the nature of the model as a text-to-speech system, it is crucial to evaluate its robustness to various perturbations that could occur in real-world scenarios. The paper currently lacks this evaluation. Specifically, it would be beneficial to test how the model performs when there is noise in the input data or changes in the speaking rate or pitch. These perturbations could significantly affect the quality and naturalness of the synthesized speech, and understanding the model's performance under these conditions would provide valuable insights into its robustness and applicability.",
                "The paper presents an interesting application of the adversarial approach in the EATS model for text-to-speech synthesis. However, the evaluation of this approach could be improved. Specifically, it would be beneficial to include an analysis of how the performance of the model changes as the strength of the supervisory signal is varied. This would provide more concrete evidence to support the claim that the adversarial approach can learn from a relatively weak supervisory signal. Additionally, it would be helpful to clarify the role of the soft dynamic time warping (DTW) procedure in the spectrogram prediction loss and whether it is related to the supervisory signal.",
                "The paper currently lacks specific details on how the model would handle non-North American accents or other languages. Given that the model was trained on North American English speakers and uses a phonemizer for input text, it would be beneficial to clarify how these factors influence the model's ability to handle different accents or languages. This could include, for example, discussing whether the model would require retraining for different accents or languages, or whether the phonemizer allows for some degree of accent or language versatility. Providing these details would help to demonstrate the model's versatility and broaden its applicability.",
                "The paper lacks a discussion on how the model would handle languages or dialects that the phonemizer may not support. Given that the phonemizer is used for partial normalization and phonemisation of the input text, it is crucial to understand how the model would handle unsupported languages or dialects. This is particularly important considering the model's application to diverse datasets. The paper would benefit from a detailed discussion on this, including potential strategies for handling unsupported languages or dialects and the impact of these strategies on the model's performance. This would provide a clearer understanding of the model's robustness and applicability to diverse datasets.",
                "The paper notes a discrepancy between MOS and FDSD scores and mentions that the sample sizes commonly used for MOS testing are usually smaller than those used for metrics based on Fr\u00e9chet distance. However, the paper does not explicitly discuss the limitations of these metrics. It would be beneficial to include a discussion on the potential limitations of the MOS and FDSD metrics in capturing all aspects of speech synthesis quality. This could help to prevent an overestimation of the model's performance and provide a more comprehensive understanding of the results.",
                "While the paper provides some comparisons of the aligner architecture with attention-based architectures and the transformer-based attention aligner, it would be beneficial to include a direct comparison with the GAN-TTS and BigGAN-deep discriminator architectures, which are also mentioned in the paper. This comparison should focus on specific aspects of efficiency, such as the use of monotonic interpolation, the prediction of token durations, and the inference speed. Including these comparisons would provide a more comprehensive assessment of the aligner's efficiency and would help readers understand how the aligner's design contributes to its efficiency. For example, it would be helpful to know how the aligner's use of monotonic interpolation compares to the recurrent nature of many monotonic attention mechanisms in terms of efficiency. Similarly, a comparison of the aligner's method of varying predicted token lengths using the latent vector z with the methods used by other architectures could provide valuable insights. Finally, a comparison of the aligner's inference speed with that of other architectures would provide a clear measure of its efficiency.",
                "While the paper discusses the use of dynamic time warping (DTW) in the model and how the model handles local variations in the speech signal, it lacks a detailed discussion on the sensitivity of DTW to such variations. Given the importance of this aspect in understanding the model's performance and robustness, it would be beneficial to include a more thorough discussion on this topic. This could include, for example, how the sensitivity of DTW to local variations affects the model's performance, and how the model mitigates any potential issues arising from this sensitivity.",
                "The paper discusses the challenge of adversarial feedback being insufficient to learn alignment, but it could benefit from a more comprehensive discussion on the potential challenges of using adversarial feedback and domain-specific loss functions in training the system. For instance, adversarial feedback could lead to stability issues during training, the need for large amounts of training data, difficulty in balancing the adversarial loss with other losses, the potential for mode collapse, and the need for careful hyperparameter tuning. Similarly, using domain-specific loss functions could present challenges such as the need for careful design and tuning of these loss functions, the difficulty in balancing different loss functions, the potential for overfitting to the training data, and the need for careful hyperparameter tuning to ensure stable training. Discussing these challenges could help others in replicating the results or extending the model.",
                "While the paper provides a detailed discussion of the computational requirements of the EATS model, including its architecture, training process, and inference speed, it could benefit from including more specific details about the memory usage and the time complexity of the model. These details are crucial for potential users to assess the applicability of the model in their specific, potentially resource-constrained, environments.",
                "The paper provides a valuable discussion of the challenges in automatic quantitative evaluation of text-to-speech models, particularly the limitations of Fr\u00e9chet DeepSpeech Distances (FDSD) and Mean Opinion Scores (MOS). However, it would significantly enhance the paper's impact if the authors could propose specific solutions or strategies to address these challenges. This would not only advance the field's understanding of these evaluation metrics but also increase the practical applicability of the proposed model and similar models in the future.",
                "The paper provides a detailed technical description of the EATS model and its performance, which is commendable. However, it lacks a discussion on how EATS could be integrated with existing TTS systems or its potential real-world applications. This is a significant omission as it leaves the reader wondering about the practical use and potential impact of the model. To enhance the impact of your work, consider adding a section on future work or potential applications. This could include a discussion on how the efficiency of EATS in both training and inference, and its independence from autoregressive sampling or teacher forcing, could facilitate its integration with existing systems. Also, discuss potential real-world applications of EATS and the potential challenges and solutions in implementing the model in real-world scenarios. This would provide a clearer understanding of the practicality and usability of the model, and could significantly increase the impact of your work.",
                "The paper presents a comparison with a transformer-based attention aligner baseline in section 'G TRANSFORMER-BASED ATTENTION ALIGNER BASELINE', but it does not discuss the potential limitations of this approach. Discussing the limitations, such as computational complexity, scalability, or performance under certain conditions, would provide a more comprehensive understanding of the model and its applicability. This would be beneficial for others attempting to replicate the results or extend the model."
            ]
        },
        "alignments": {
            "experiments_proofs": [
                {
                    "reference_id": 0,
                    "predicted_id": 1,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The performance of the proposed model is promising, but still below the baselines.",
                    "predicted_comment": "The authors have compared their model with several state-of-the-art TTS systems, which is commendable. However, to fully demonstrate the incremental improvement offered by their model, it would be beneficial to also compare with simpler baseline models. Specifically, traditional concatenative or parametric TTS systems could serve as useful points of comparison. Concatenative TTS systems use large databases of small speech fragments that are concatenated to form complete utterances, while parametric TTS systems use mathematical models to generate speech. Comparing the proposed model with these simpler models could highlight the specific areas where the authors' model excels, such as in generating more natural-sounding speech or in handling longer sequences. It could also provide a benchmark for evaluating the complexity and computational efficiency of the authors' model. This additional comparison would strengthen the paper by providing a more comprehensive evaluation of the proposed model."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper does not use widely accepted benchmark datasets. A more direct comparison would be beneficial.",
                    "predicted_comment": "Diversity of Datasets: The paper currently tests the model on a dataset consisting of North American English speakers. To demonstrate the model's generalizability, it would be beneficial to test it on a more diverse range of datasets. This could include datasets representing different languages, accents, and speaking styles. For example, testing the model on datasets in Spanish, Mandarin, or Arabic, and datasets representing a variety of accents and speaking styles within English, would provide a more robust demonstration of the model's applicability across diverse linguistic contexts. This would strengthen the paper by providing evidence of the model's potential for broad applicability."
                }
            ],
            "novelty_impact": [
                {
                    "reference_id": 0,
                    "predicted_id": 12,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper does not provide a comparison over the computational cost or model size. This is particularly important because the proposed model is non-autoregressive and may be capable of real-time inference.",
                    "predicted_comment": "While the paper provides a detailed discussion of the computational requirements of the EATS model, including its architecture, training process, and inference speed, it could benefit from including more specific details about the memory usage and the time complexity of the model. These details are crucial for potential users to assess the applicability of the model in their specific, potentially resource-constrained, environments."
                }
            ],
            "clarity_reproducibility": [],
            "all": [
                {
                    "reference_id": 0,
                    "predicted_id": 1,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 2,
                    "score": 0.4,
                    "reference_comment": "The performance of the proposed model is promising, but still below the baselines.",
                    "predicted_comment": "The authors have compared their model with several state-of-the-art TTS systems, which is commendable. However, to fully demonstrate the incremental improvement offered by their model, it would be beneficial to also compare with simpler baseline models. Specifically, traditional concatenative or parametric TTS systems could serve as useful points of comparison. Concatenative TTS systems use large databases of small speech fragments that are concatenated to form complete utterances, while parametric TTS systems use mathematical models to generate speech. Comparing the proposed model with these simpler models could highlight the specific areas where the authors' model excels, such as in generating more natural-sounding speech or in handling longer sequences. It could also provide a benchmark for evaluating the complexity and computational efficiency of the authors' model. This additional comparison would strengthen the paper by providing a more comprehensive evaluation of the proposed model."
                },
                {
                    "reference_id": 0,
                    "predicted_id": 12,
                    "relative_specificity": "more",
                    "relatedness": "high",
                    "pairwise_result": {
                        "relatedness": "high",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper does not provide a comparison over the computational cost or model size. This is particularly important because the proposed model is non-autoregressive and may be capable of real-time inference.",
                    "predicted_comment": "While the paper provides a detailed discussion of the computational requirements of the EATS model, including its architecture, training process, and inference speed, it could benefit from including more specific details about the memory usage and the time complexity of the model. These details are crucial for potential users to assess the applicability of the model in their specific, potentially resource-constrained, environments."
                },
                {
                    "reference_id": 1,
                    "predicted_id": 0,
                    "relative_specificity": "more",
                    "relatedness": "medium",
                    "pairwise_result": {
                        "relatedness": "medium",
                        "relative_specificity": "more",
                        "same_meaning": true
                    },
                    "count": 4,
                    "score": 0.8,
                    "reference_comment": "The paper does not use widely accepted benchmark datasets. A more direct comparison would be beneficial.",
                    "predicted_comment": "Diversity of Datasets: The paper currently tests the model on a dataset consisting of North American English speakers. To demonstrate the model's generalizability, it would be beneficial to test it on a more diverse range of datasets. This could include datasets representing different languages, accents, and speaking styles. For example, testing the model on datasets in Spanish, Mandarin, or Arabic, and datasets representing a variety of accents and speaking styles within English, would provide a more robust demonstration of the model's applicability across diverse linguistic contexts. This would strengthen the paper by providing evidence of the model's potential for broad applicability."
                }
            ]
        },
        "metrics": {
            "experiments_proofs": {
                "n_reference": 14,
                "n_generated": 6,
                "n_total_aligns": 2,
                "recall": 0.14583333333333331,
                "precision": 0.08333333333333333,
                "pseudo_jaccard": 0.059027777777777776,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "novelty_impact": {
                "n_reference": 14,
                "n_generated": 10,
                "n_total_aligns": 1,
                "recall": 0.08333333333333333,
                "precision": 0.025,
                "pseudo_jaccard": 0.020833333333333332,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            },
            "clarity_reproducibility": {
                "n_reference": 14,
                "n_generated": 0,
                "n_total_aligns": 0,
                "recall": 0.0,
                "precision": 0.0,
                "pseudo_jaccard": 0.0,
                "specificity_more": NaN,
                "specificity_less": NaN,
                "specificity_same": NaN
            },
            "all": {
                "n_reference": 14,
                "n_generated": 16,
                "n_total_aligns": 3,
                "recall": 0.22916666666666666,
                "precision": 0.046875,
                "pseudo_jaccard": 0.04256965944272446,
                "specificity_more": 1.0,
                "specificity_less": 0.0,
                "specificity_same": 0.0
            }
        },
        "metrics_for_reviews": {
            "experiments_proofs": [
                {
                    "recall": 0.25,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.1111111111111111,
                    "n_real": 4
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.16666666666666666,
                    "pseudo_jaccard": 0.125,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "novelty_impact": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.3333333333333333,
                    "precision": 0.1,
                    "pseudo_jaccard": 0.08333333333333333,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "clarity_reproducibility": [
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ],
            "all": [
                {
                    "recall": 0.25,
                    "precision": 0.0625,
                    "pseudo_jaccard": 0.05263157894736842,
                    "n_real": 4
                },
                {
                    "recall": 0.6666666666666666,
                    "precision": 0.125,
                    "pseudo_jaccard": 0.11764705882352941,
                    "n_real": 3
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 4
                },
                {
                    "recall": 0.0,
                    "precision": 0.0,
                    "pseudo_jaccard": 0.0,
                    "n_real": 3
                }
            ]
        }
    }
]